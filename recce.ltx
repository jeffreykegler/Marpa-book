% Copyright 2013 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}

% This is now a "paper", but may be a chapter
% or something else someday
% This command will make any such change easier.
\newcommand{\doc}{paper}

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\var{#1}}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mapsto}
\newcommand{\nderives}[1]{
\mathrel{%
  \ifthenelse{\equal{#1}{}}{%
    {\not\mapsto}%
  }{%
    {\mbox{$\:\stackrel{\!{#1}}{\not\mapsto\!}\:$}}%
  }%
}}%
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\mapsto\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\mapsto\!}\:$}}}
\newcommand{\ndestar}{\nderives{\ast}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\mapsto\!}\:$}}}
\newcommand{\ndeplus}{\nderives{+}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\mapsto\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\mapsto\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\Tvar}[2]{\ensuremath{\var{#1}_{\var{#2}}}}
\newcommand{\Tvarset}[2]{\ensuremath{\var{#1}_{\set{\var{#2}}}}}
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }

\newcommand{\Vbool}[1]{\Tvar{#1}{BOOL}}
\newcommand{\Vdr}[1]{\Tvar{#1}{DR}}
\newcommand{\Vdrset}[1]{\Tvarset{#1}{DR}}
\newcommand{\Veim}[1]{\Tvar{#1}{EIM}}
\newcommand{\Veimset}[1]{\Tvarset{#1}{EIM}}
\newcommand{\Ves}[1]{\Tvar{#1}{ES}}
\newcommand{\Vext}[1]{\Tvar{#1}{EXT}}
\newcommand{\Vint}[1]{\Tvar{#1}{INT}}
\newcommand{\Vloc}[1]{\Tvar{#1}{LOC}}
\newcommand{\Vlim}[1]{\Tvar{#1}{LIM}}
\newcommand{\Vpim}[1]{\Tvar{#1}{PIM}}
\newcommand{\Vpimset}[1]{\Tvarset{#1}{PIM}}
\newcommand{\Vrule}[1]{\Tvar{#1}{RULE}}
\newcommand{\Vruleset}[1]{\Tvarset{#1}{RULE}}
\newcommand{\Vstr}[1]{\Tvar{#1}{STR}}
\newcommand{\Vsym}[1]{\Tvar{#1}{SYM}}
\newcommand{\Vsymset}[1]{\Tvarset{#1}{SYM}}
\newcommand{\Vorig}[1]{\Tvar{#1}{ORIG}}
\newcommand{\Vtoken}[1]{\Tvar{#1}{TOKEN}}

\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}

% types to be deleted with A&H LR(0) stuff
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}

% TODO to be deleted with A&H LR(0) stuff
\newcommand{\AH}{\ensuremath{\alg{AH}}}

% TODO to be deleted with A&H LR(0) stuff
\newcommand{\Cfa}{\var{fa}}

\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\Vsym{\Cw[\var{#1}]}}}
\newcommand{\NT}[1]{\ensuremath{\mymathop{NT}(#1)}}
\newcommand{\Term}[1]{\ensuremath{\mymathop{Term}(#1)}}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}
\newcommand{\GOTO}{\mymathop{GOTO}}
\newcommand{\Next}{\mymathop{Next}}
\newcommand{\Predict}[1]{\mymathop{Predict}(#1)}
\newcommand{\Postdot}[1]{\mymathop{Postdot}(#1)}
\newcommand{\Penult}[1]{\mymathop{Penult}(#1)}
\newcommand{\LHS}[1]{\mymathop{LHS}(#1)}
\newcommand{\RHS}[1]{\mymathop{RHS}(#1)}
\newcommand{\RightRecursive}[1]{\mymathop{Right-Recursive}(#1)}
\newcommand{\Rightmost}[1]{\mymathop{Rightmost}(#1)}
\newcommand{\LeoEligible}[1]{\mymathop{Leo-Eligible}(#1)}
\newcommand{\LeoUnique}[1]{\mymathop{Leo-Unique}(#1)}
\newcommand{\ID}[1]{\mymathop{ID}(#1)}
\newcommand{\PSL}[2]{\mymathop{PSL}[#1][#2]}
\newcommand{\myL}[1]{\ensuremath{\mymathop{L}(#1)}}
\newcommand\Etable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\bigEtable[1]{\ensuremath{\mymathop{table}\bigl[#1\bigr]}}
\newcommand\Rtable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\Rtablesize[1]{\ensuremath{\bigl| \mymathop{table}[#1] \bigr|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\mymathop{table}[#1,#2]}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}[theorem]{Observation}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

\begin{document}

\date{\today}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2019 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
The Marpa recognizer is described.
Marpa is
a fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa is an Earley parser
whichb includes the first
practical implementation
of the improvements
in Joop Leo's 1991 paper.
In addition,
Marpa tracks the full state of the parse,
at run-time,
in a form accessible by the application.
This greatly improves error detection;
enables event-driven parsing;
and allows techniques such as
``Ruby Slippers'' parsing,
in which
the input is altered in response
to the parser's expectations.
\end{abstract}

\maketitle

\section{Introduction}

The Marpa project was intended to create
a practical and highly available tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite an encouraging academic literature,
no such tool had existed for context-free parsing.

The first stable version of Marpa was uploaded to
a public archive on Solstice Day 2011.
This version describes the algorithm used
in the most recent version of Marpa,
Marpa::R2~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in the first version of this paper.

While the presentation in this paper is theoretical,
the approach is practical.
The Marpa::R2 implementation has been widely available
for some time,
and has seen considerable use,
including in production environments.
Many of the ideas in the parsing literature
satisfy theoretical criteria,
but in practice turn out to face significant obstacles.
An algorithm may be as fast as reported, but may turn
out not to allow
adequate error reporting.
Or a modification may speed up the recognizer,
but require additional processing at evaluation time,
leaving no advantage to compensate for
the additional complexity.

Section
\ref{s:features}
describes the important features of Marpa
and Section
\ref{s:using-features}
goes into detail about their application.
Section
\ref{s:preliminaries} describes the notation and conventions
of this \doc.
Section \ref{s:external-grammars} deals with Marpa's external grammars ---
the grammars which the users see.
Section \ref{s:improper} deals with the implementation of improper
context free grammars.
Section \ref{s:rewrite} explain how Marpa rewrites its external
grammar into internal grammars.
Section \ref{s:internal-grammars} describes Marpa's internal grammars.
Sections \ref{s:earley} and \ref{s:earley-ops}
introduce Earley's algorithm.
Section \ref{s:leo} describes Leo's modification
to Earley's algorithm.
Section \ref{s:pseudocode} presents the pseudocode
for Marpa's recognizer.
Section
\ref{s:proof-preliminaries}
describes notation and deals with other
preliminaries
to the theoretical results.
Section
\ref{s:correct}
contain a proof of Marpa's correctness,
while Section \ref{s:complexity} contains
its complexity results.
Finally,
section \ref{s:input}
generalizes Marpa's input model.

A full description of Marpa's relationship to
prior work in parsing can be found in \cite{Timeline}.
Readers may find it helpful to read
\cite{Timeline}
before this paper.

\section{Features}
\label{s:features}

\subsection{General context-free parsing}
The Marpa algorithm is capable of parsing all
context-free grammars.
As implemented,
Marpa parses
all cycle-free context-free
grammars.\footnote{
Earlier implementations of Marpa allowed cycles,
but support for them
was removed because
practical interest seems to be non-existent.
For more detail, see section
\ref{s:improper}.
}
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},
and therefore never worse than $\order{\var{n}^3}$.

\subsection{Linear time for practical grammars}
Currently, the grammars suitable for practical
use are thought to be a subset
of the deterministic context-free grammars.
Using a technique discovered by
Leo~\cite{Leo1991},
Marpa parses all of these in linear time.
Leo's modification of Earley's algorithm is
\On{} for LR-regular grammars.
Leo's modification
also parses many ambiguous grammars in linear
time.

\subsection{Left-eidetic}
The original Earley algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
Marpa's parse engine makes available
its
information about the state of the parse so far.
This information is
in useful form,
and can be accessed efficiently.

\subsection{Recoverable from read errors}
When
Marpa reads a token which it cannot accept,
the error is fully recoverable.
An application can try to read another
token.
The application can do this repeatedly
as long as none of the tokens are accepted.
Once the application provides
a token that is accepted by the parser,
parsing will continue
as if the unsuccessful read attempts had never been made.

\subsection{Ambiguous tokens}
Marpa allows ambiguous tokens.
These are often useful in natural language processing
where, for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
recovery from rejected tokens so that,
for example, an application could react to the
rejection of a token by reading two others.

\section{Using the features}
\label{s:using-features}

\subsection{Error reporting}
An obvious application of left-eideticism is error
reporting.
Marpa's abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar.
Marpa, as currently implemented,
can detect an ambiguity and report
specifically where it occurred
and what the alternatives were.

\subsection{Event driven parsing}
As implemented,
Marpa::R2~\cite{Marpa-R2}
allows the user to define ``events''.
Events can be defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\subsection{Ruby slippers parsing}
Left-eideticism, efficient error recovery,
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
In traditional parser practice,
error detection is an act of desperation.
In contrast,
Marpa's error detection is so painless
that it can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of Marpa::R2~\cite{Marpa-R2},
the author has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\subsection{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very different from the practice in the languages humans choose
when communicating with each other.
For example,
the language of this paper, English, is notoriously
ambiguous.
Human languages exploit ambiguity in order to allow expression
to be more flexible and powerful.

Ambiguity of course can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Syntax errors, for example, are undesired, but nobody tries
to design languages that make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be not just cumbersome, but dangerous.
A parser would never warn the user about typos,
because all typos in such a language would have unintended
meanings.

With Marpa, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
The language can be designed to be ambiguous,
but any actual ambiguity can be detected
and reported at parse time.
This exploits Marpa's ability
to report exactly where
and what the ambiguity is.
Marpa::R2 own parser description language, the SLIF,
uses ambiguity in this way.

\subsection{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LR-regular languages
opens the way to auto-generated languages.
In particular,
\cite{Culik1973} notes that a parser which
can parse any LR-regular language will be
able to parse a language generated using syntax macros.

\subsection{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
which are useful for second-order programming.
True second-order languages --- languages which
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With Marpa, this barrier is raised.
As an example,
Marpa::R2's own parser description language, the SLIF,
allows ``precedenced rules''.
Precedenced rules are specified in an extended BNF.
The BNF extensions allow precedence and associativity
to be specified for each RHS.

Marpa::R2's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to create a BNF grammar which is equivalent,
and which has the desired precedence.
Essentially,
the SLIF does a standard textbook transformation.
The transformation starts
with a set of rules,
each of which has a precedence and
an associativity specified.
The result of the transformation is a set of
rules in pure BNF.
The SLIF's advantage is that it is powered by Marpa,
and therefore the SLIF can be certain that the grammar
that it auto-generates will
parse in linear time.

Notationally, Marpa's precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison.
In the SLIF,
there are two important differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity greater than three.

Second, and more important, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies.
The user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\section{Preliminaries}
\label{s:preliminaries}

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.
Multi-character variable names will be common and
operations will never be implicit.
\begin{center}
\begin{tabular}{ll}
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \cat \var{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\end{tabular}
\end{center}

This paper will
use subscripts to indicate commonly occurring types.
\begin{center}
\begin{tabular}{ll}
$\var{X}_\var{T}$ & The variable \var{X} of type \type{T} \\
$\var{set-one}_\set{\var{T}}$ & The variable \var{set-one} of type set of \type{T} \\
\type{SYM} & The type for a symbol \\
\type{STR} & The type for a string \\
\type{EIM} & The type for an Earley item \\
\Vsym{a} & A variable \var{a} of type \type{SYM} \\
\Vstr{a} & A variable \var{a} of type \type{STR} \\
\Veim{a} & A variable \var{a} of type \type{EIM} \\
\Vsymset{set-two} & The variable \var{set-two}, a set of strings \\
\Veimset{set-two} & The variable \var{set-two}, a set of Earley items
\end{tabular}
\end{center}
The notation for
constants is the same as that for variables.
Type names are often used in the text
as a convenient way to refer to
their type.
Subscripts may be omitted when the type
is obvious from the context.

Where \Vsymset{vocab} is non-empty set of symbols,
let $\var{vocab}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Where \Vstr{s} is a string,
let \size{\Vstr{s}} be its length, counted in symbols.
Let $\var{vocab}^+$ be
\begin{equation*}
\bigl\{ \Vstr{x}
\bigm| \Vstr{x} \in \var{vocab}^\ast \land \Vsize{\Vstr{x}} > 0
\bigr\}.
\end{equation*}

\section{External grammars}
\label{s:external-grammars}

Let the \dfn{external grammar} \Vext{g} be the 4-tuple
\begin{equation*}
    (\Vsymset{nt}, \Vsymset{term}, \Vruleset{rules}, \Vsym{accept}).
\end{equation*}
For convenience in refering to the elements of \Vext{g},
\begin{equation*}
\begin{alignedat}{3}
& \NT{\var{g}} && \defined && \quad \Vsymset{nt}, \\
& \Term{\var{g}} && \defined && \quad \Vsymset{term}, \\
& \Accept{\var{g}} && \defined && \quad \Vsym{accept}, \\
& \Rules{\var{g}} && \defined && \quad \Vruleset{rules} \quad \text{and} \\
& \Vocab{\var{g}} && \defined && \quad \Vsymset{nt} \cup \Vsymset{term}. \\
\end{alignedat}
\end{equation*}
The type \type{RULE} will be defined shortly.
\NT{\var{g}} is a set of symbols called non-terminals,
and \Term{\var{g}} is a set of symbols called terminals.
\Accept{\var{g}} is the \dfn{accept symbol},
which in the parsing literature is often called
the ``start symbol''.
The accept symbol must be a non-terminal:
$\Accept{\var{g}} \in \var{nt}$.
\Vocab{\var{g}} is
the \dfn{vocabulary} of \Vext{g}.

A string of symbols from \Vocab{\var{g}}
is called a \dfn{sentential form}.
A string of symbols from \Term{\var{g}}
is called a \dfn{sentence}.

\Vruleset{rules} is a set of rules (type \type{RULE}),
where a rule is a duple
\begin{equation*}
[\Vsym{lhs} \de \Vstr{rhs}]
\end{equation*}
such that
\begin{equation*}
\Vsym{lhs} \in \NT{\var{g}} \; \land \;
\Vstr{rhs} \in \Vocab{\var{g}}^\ast.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\var{r}}$ and $\RHS{\var{r}}$, respectively.

The rules imply the traditional rewriting system,
in which $\Vstr{x} \derives \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in exactly one step;
$\Vstr{x} \deplus \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in one or more steps;
and $\Vstr{x} \destar \Vstr{y}$
states that \Vstr{x} derives \Vstr{y} in zero or more steps.

Every symbol derives itself in zero steps.
The zero step derivation is call a \dfn{trivial derivation}.
A derivation of more than zero steps
is called a \dfn{non-trivial derivation}.

We say that symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} \destar \epsilon$.
\Vsym{x} is \dfn{nulling} if and only if it always derives the null
string, that is
\begin{equation*}
\forall \, \Vstr{y} \quad \Vsym{x} \destar \Vstr{y} \implies \Vstr{y} \destar \epsilon.
\end{equation*}
\Vsym{x} is \dfn{non-nullable} if and only if it is not nullable.

By tradition, the input to the parse is written as \Cw{}.
\Cw{} must be a sentence of \Vext{g},
that is,
$\Cw \in \Term{\var{g}}^\ast$.
\Vsize{w} will be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.

The language of \Vext{g},
\myL{\var{g}},
is the set of sentences that
that \Accept{\Vext{g}} derives.
That is,
\begin{equation*}
\myL{\Vext{g}} =
\set{
\quad
\begin{aligned}
& \Vstr{sentence} \; \mid \; \\
& \quad \Accept{\var{g}} \destar \Vstr{sentence} \\
& \quad \land \; \Vstr{sentence} \in \Term{\var{g}}
\end{aligned}
\quad
}
\end{equation*}

Locations in the input will be of type \type{LOC}.
We represent
the \var{i}'th character of the input
by \CVw{i},
$0 \le \Vloc{i} < \Vsize{w}$.

\section{Improper context free grammars}
\label{s:improper}

A \dfn{proper CFG} is a CFG with no
\begin{itemize}
\item No unproductive symbols;
\item No inaccessible symbols; and
\item No cycles.
\end{itemize}
A \dfn{improper CFG} is a CFG which is
not proper.

\subsection{Unproductive symbols}

The current implementation of Marpa supports
unproductive symbols.
By default, Marpa reports unproductive symbols.
The user may silence the error report
by marking the unproductive symbol as a unicorn.

A \dfn{unproductive symbol}
is one which never derives a sentence.
That is, \Vsym{unprod} is unproductive if
and only if, for all \Vstr{sentence}
\begin{equation*}
\Vstr{sentence} \in \Term{\var{g}}
\;  \implies \;
  \Vsym{unprod} \ndestar \Vstr{sentence}
\end{equation*}
In the current version of Marpa,
unproductive symbols can be implemented by
defining \dfn{unicorn} terminals ---
terminals which can never be
found in the input.

In the Marpa implementation,
a terminal can be made into a unicorn by
requiring that
a terminal match the POSIX expression
\texttt{[\char`^ \char`\\ D\char`\\ d]}
indicates that the terminal
must match a single character which
is both a digit and a non-digit.
Since this is not possible,
the terminal will never match anything in the input.

Unproductive symbols in fact often prove useful in Marpa
applications.
Unproductive symbols can be
used to implement an external lexer for a terminal by setting
an event to trigger when that terminal is expected.
In such a case, it is important to make sure the terminal
in not recognized in the input, and this can be done by
marking the terminal as unproductive, that is,
as a unicorn.

\subsection{Inaccessible symbols}

The current implementation of Marpa supports inaccessible symbols.
By default, they are reported as errors.
This error can be silenced for the whole grammar, or symbol by symbol.

An \dfn{inaccessible symbol} is one which can never be derived from
the accept symbol.
That is, \Vsym{noacc} is inaccessible in \Vext{g} if and only if
\begin{equation*}
\neg \; \exists \; \Vstr{pre}, \Vstr{post} \; \mid \; \Accept{g} \destar \Vstr{pre} \cat \Vsym{noacc} \cat \Vstr{post}.
\end{equation*}

Inaccessible symbols have been found useful in Marpa applications
that use higher-level languages ---
languages which generate other languages.
Often the generated grammar consists of a ``custom''
rules which vary,
and ``boilerplate'' rules,
which are the same in every generated grammar.
It can be useful to have sections of the ``boilerplace''
be optional,
so that some of the ``boilerplate'' rules are only used
if the ``custom'' rules call for them.
If the custom rules do not call for a set of optional rules
within the boilerplate,
those boilerplate rules become inaccessible.

Optional boilerplace can be implemented
by silencing inaccessibility errors for the symbols in the optional boilerplate,
Usually it is possible to locate a topmost symbol for
each section of optional boilerplace,
in which case the warning need only be silences for those
topmost symbols.

\subsection{Cycles}

The current implementation of Marpa does not support cycles,
but earlier implementation have.
A \dfn{cycle} is a symbol which non-trivially
derives a string which consists only of that
symbol.
A cycle is the parsing equivalent of an infinite loop.
That is, \Vsym{cycle} is a cycle in \Vext{g} if and only if
\begin{equation*}
 \Vsym{cycle} \deplus \Vsym{cycle}.
\end{equation*}

Cycles are recursions but most recursions (and apparently all
recursion of practical interest) are not cycles.
In a recursion, a symbol \Vsym{recurse} derives itself as part of a string.
That is, \Vsym{recurse} is recursive if and only if
\begin{equation*}
 \Vsym{recurse} \deplus \Vstr{pre} \cat \Vsym{recurse} \cat \Vstr{post}.
\end{equation*}
\Vsym{recurse} is a cycle only if
both \Vstr{pre} and \Vstr{post} are the null string:
\begin{equation*}
 \Vstr{pre} = \Vstr{post} = \epsilon.
\end{equation*}

\section{Rewriting the grammar}
\label{s:rewrite}

A parser that claims to be practical must accept
grammars with nullable and nulling symbols
and empty rules.
But difficulties in handling these correctly
have bedeviled previous attempts
at Earley parsing.

Marpa follows Aycock and Horspool\cite{AH2002} in using
grammar rewrites to handle these issues.
Marpa's grammar rewriting allows the user to specify
their parse in terms of an external grammar.

An {\bf external grammar} may be any cycle-free grammar.\footnote{
As mentioned, earlier versions of Marpa lifted even the
restriction on cycles,
but there seems to be no use for cycles.
}
The external grammar is rewritten into an {\bf internal grammar}.
This internal grammar is required to obey severe restrictions.

\subsection{Semantics-safe rewriting}

Many grammar rewriting schemes are possible,
but for a parser to be useful,
it must implement a semantics that is specified
in terms of the external grammar.
If the semantics cannot be efficiently recovered
from a parse performed using an internal grammar,
then that rewriting scheme is not useful in practice.

The grammar rewrites in this paper are "semantics-safe".
They allow the external rules and symbols to be reconstructed
efficiently not just at evaluation time,
but at run-time as well.
Marpa's parsing and evaluation
are performed
in such a way that the internal grammar
is invisible to
the user.

This rewrite can be undone easily.
In fact,
in the current implementation of Marpa,
the reverse rewrite,
from internal to external,
is often done ``on the fly'',
as the parse proceeds.
This translation back and forth is
efficient,
and is done for error reporting, tracing,
and in the implementation of Marpa::R2's event
mechanism.

For example,
Marpa allows run-time events to be defined,
including ``nulling events'' ---
events which trigger when a symbol is nulled.
In the internal grammar, there are no nullable
symbols but, for event-triggering purposes,
Marpa keeps track of where
the nulling symbols would be.
The recognizer runs on an internal grammar with
no nullable symbols,
but the nulling events that Marpa user are triggered
and reported in terms of an external grammar which
does allow nullable symbols.
All of this works smoothly ---
so smoothly that a standard Marpa programming technique is to
add nulling symbols to a grammar which exist only for the
purpose of generating events.

The rest of this subsection will describe the restrictions
that rewrites must follow in order to be ``semantics-safe''.
Marpa's internal grammars have
``brick'' and ``mortar'' symbols.
Internal brick symbols correspond, many-to-one, to
external symbols.
Internal mortar symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.

Assume that we have a parse, using internal symbols.
We define a ``brick traversal'' from a ``root'' brick non-terminal instance.
The ``brick traversal'' is pre-order
and stops traversing any path when it hits
a brick symbol instance other than the ``brick root''.
In this way, it traces a subtree of the parse,
where the root of the subtree is the brick root symbol instance,
and the leaves of the subtree are a sequence of other brick
symbol instances.
The leaves of the subtree, as encountered in pre-order,
constitute its ``brick frontier''.
Mortar symbols will only occur in the interior of this ``brick'' subtree,
never as its root or its leaf.

An {\bf instance} of a symbol is a symbol matched with a substring
of the input \Cw{}.
An internal symbol instance \textbf{matches}
an external symbol instance if and only if
\begin{itemize}
\item they have the same symbol;
\item they have the same left location in \Cw{}; and
\item they have the same right location in \Cw{}.
\end{itemize}

The {\bf terminal sequence} of a parse is the sequence
of symbol instances for the terminals of a parse.

For a rewrite to be ``safe'':
\begin{itemize}
\item Every brick symbol must translate to exactly one
external symbol.
\item Every terminal symbol instance must be a brick symbol.
\item The internal and external input sequences must be the same length.
\item The internal and external input sequences must allow
a shared indexing
scheme, which indexes the input symbols consecutively from
left to right.
\item
For every index \var{i},
the \var{i}'th symbol instance in the internal input sequence
must match
the \var{i}'th symbol instance in the external input sequence.
\item Every brick traveral must translate to an external rule
instance,
and vice versa,
as follows:
\begin{itemize}
\item The brick root symbol instance must match
the LHS symbol instance of the external rule instance.
\item
The brick frontier must be the same length as
the RHS of the rule.
\item
The brick frontier and the external rule RHS must allow a shared indexing
scheme,
which indexes both of them
consecutively
from left to right.
\item
For every index \var{i},
the \var{i}'th
instance in the brick frontier must
match the \var{i}'th symbol instance
of the external rule RHS.
\end{itemize}
\end{itemize}

For Marpa to deliver on its
claims to be a practical parser,
it is important to emphasize
that all grammar rewrites in this \doc{}
are done in such a way that the semantics
of the original grammar can be reconstructed
simply and efficiently at evaluation time.
As one example,
when a rewrite involves the introduction of new rule,
semantics for the new rule can be defined to pass its operands
up to a parent rule as a list.
Where needed, the original semantics
of a pre-existing parent rule can
be ``wrappered'' to reassemble these lists
into operands that are properly formed
for that original semantics.

\subsection{Augmenting the grammar}

It is assumed
that there is a dedicated acceptance rule,
such that for some symbol \Vsym{top},
\begin{equation*}
\Vrule{accept} = [\Vsym{accept} \de \Vsym{top}].
\end{equation*}
It is also assumed that
the only occurence of \Vsym{accept} in the
grammar is as
the LHS of \Vrule{accept} or, more formally,
that
\begin{equation*}
\LHS{\Vrule{r}} = \Vsym{accept} \implies \Vrule{r} = \Vrule{accept}
\end{equation*}
and that for every rule, \var{rule}, in \Cg{}
\begin{equation*}
\Vsym{accept} \notin \RHS{\Vrule{rule}}.
\end{equation*}

\subsection{Aycock-Horspool rewrites}

The rewrite currently used by Marpa is an improvement over
that of~\cite{AH2002}.
Rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
(A ``proper nullable'' is a nullable symbol which is not nulling.)
This is similar to a rewrite into Chomsky form.

The proper nullable symbol is then cloned into two others:
one nulling and one telluric.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols,
and new rules are created as necessary to ensure that all possible combinations
of nulling and telluric symbols are accounted for.

The rewrite in~\cite{AH2002} was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the internal grammar could be
an exponential function of numbers in the external grammar.
In our version, the worst case growth in the number of rules in linear.

We have already noted
that no rules of \Cg{}
have a zero-length RHS,
and that all symbols must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
\cite{AH2002} shows how to do this
without loss of generality.

As implemented,
the Marpa parser allows users to associate
semantics with an original grammar
that has none of the restrictions imposed
on grammars in this \doc{}.
The user of a Marpa parser
may specify any context-free grammar,
including one with properly nullable symbols,
empty rules, etc.
The user specifies his semantics in terms
of this original, ``free-form'', grammar.
Marpa implements the rewrites,
and performs evaluation,
in such a way as to keep them invisible to
the user.
From the user's point of view,
the ``free-form'' of his grammar is the
one being used for the parse,
and the one to which
his semantics are applied.

Translation from the rewritten grammar is
efficient enough to be done ``on the fly''.
This allows run-time event-driven semantics
to be written in terms of the original grammar.

\subsection{The Aycock-Horspool rewrite}

Following Aycock and Horspool\cite{AH2002},
all nullable symbols in grammar \Cg{} are nulling -- every symbol
which can derive the null string always derives the null string.
It is shown in \cite{AH2002} how to do this without losing generality
or the ability to efficiently evaluate a semantics that is
defined in terms of an original grammar that includes symbols which
are both nullable and non-nulling,
empty rules, etc.

\subsection{Trivial grammars and null parses}

Null parses are parse of zero length inputs.
Marpa handles null parses by treating them
as a special case.

Trivial grammars are those which only accept
zero length inputs.
No Marpa internal grammar can be a trivial grammar.
Trivial grammars are also handled as a special case.

\section{Marpa internal grammars}
\label{s:internal-grammars}

The conventions of this section describe
Marpa internal grammars.
Readers familiar with the parsing literature should
note that notation and definitions in this section
and in the rest of this paper may be simplified
for dealing with Marpa internal grammars,
and may not be appropriate for general,
context-free grammars.

\begin{itemize}
\item Marpa internal grammars do not allow nulling or nullable symbols.
\item Marpa internal grammars do not allow an empty RHS.
That is, for every rule, \Vrule{r},
$\size{\RHS{r}} \ge 1$.
\item Where \Cw{} is the input of the parse,
$\size{\Cw{}} \ge 1$.
This is, a zero length parse input is not allowed.
\end{itemize}

We define the rightmost symbol of a string
\Vstr{x}
as \Vsym{rightmost} such that
\begin{equation*}
\exists \Vstr{pre} \; \mid \; \Vstr{x} = \Vstr{pre} \cat \Vsym{rightmost}
\end{equation*}
The rightmost symbol of a rule \Vrule{r} as
$\Rightmost{\Vrule{r}}$.
A rule \Vrule{x} is \dfn{directly right-recursive}
if and only if
\begin{equation*}
\LHS{\Vrule{x}} = \Rightmost{\Vrule{x}}.
\end{equation*}
\Vrule{x} is \dfn{indirectly right-recursive}
if and only if
\begin{equation*}
\exists \, \Vstr{y} \mid \Rightmost{\Vrule{x}} \deplus \Vstr{y} \land \Rightmost{\Vstr{y}} = \LHS{\Vrule{x}}.
\end{equation*}
\Vrule{x} is \dfn{right recursive},
$\RightRecursive{\Vrule{x}}$,
if and only if is either directly or indirectly right-recursive.

In this \doc{},
\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the parser described in
this \doc{}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Cg}$ will be the language accepted by $\alg{Recce}$
when parsing \Cg{}.

\section{Earley's algorithm}
\label{s:earley}

Let $\Vrule{r} \in \Rules{\var{g}}$
be a rule,
and let $\Vsize{r}$
be the length of its RHS.
A dotted rule (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \cat \Vsym{Y} \cat \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \var{X} \cat \var{Y} \mydot \var{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.

We consider the size of a grammar, \Vsize{\var{g}}, to be the
number of distinct dotted rules it allows.
\Vsize{g} is a constant that depends on \var{g}.

If we let \Vdr{x} be a dotted rule, such that
\begin{equation*}
\Vdr{x} =
\bigl[ [\Vsym{A} \de \Vstr{pre} \cat \Vsym{next} \cat \Vstr{post}],
    \var{pos} \bigr],
\end{equation*}
then
%
\begin{equation*}
\begin{alignedat}{3}
& \LHS{\Vdr{x}} && \defined && \quad
\Vsym{A} \\
& \Postdot{\Vdr{x}} && \defined && \quad
\begin{cases}
\Vsym{next}, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \mydot \var{next} \cat \var{post}]$} \\
\Lambda, \quad \text{if $\var{x} = [\var{A} \de \var{pre} \cat \var{next} \cat \var{post} \mydot]$}
\end{cases} \\
%
& \Next(\Vdr{x}) && \defined && \quad
\begin{cases}
[\var{A} \de \var{pre} \cat \var{next} \mydot \var{post}],  \\
\qquad \text{if $\Postdot{\Vdr{x}} = \var{next}$} \\
\text{$\Lambda$, otherwise}
\end{cases} \\
%
& \Penult{\Vdr{x}} && \defined && \quad
\begin{cases}
\Vsym{next}, \quad \text{if} \\
\qquad \Postdot{\Vdr{x}} = \var{next} \\
\qquad \land \quad \Vstr{post} \destar \epsilon \\
\qquad \land \quad \neg (\Vsym{next} \destar \epsilon) \\
\Lambda, \quad \text{otherwise}
\end{cases}
%
\end{alignedat}
\end{equation*}

A \dfn{penult} is a dotted rule \Vdr{d} such that $\Penult{\var{d}} \neq \Lambda$.
Note that $\Penult{\Vdr{x}}$
is never a nullable symbol.
The \dfn{initial dotted rule} is
\begin{equation*}
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation*}
A \dfn{predicted dotted rule} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{alpha} ].
\end{equation*}
A \dfn{confirmed dotted rule}
is the initial dotted rule,
or a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule} is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{alpha} \mydot ].
\end{equation*}
Predicted, confirmed and completed dotted rules
are also called, respectively,
\dfn{predictions}, \dfn{confirmations} and \dfn{completions}.

A Earley item (type \type{EIM}) is a duple
\[
    [\Vdr{dotted-rule}, \Vorig{x}]
\]
of dotted rule and origin.
(The origin is the location where recognition of the rule
started.
It is sometimes called the ``parent''.)
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.

An Earley parser builds a table of Earley sets,
\begin{equation*}
\EVtable{\Earley}{i},
\quad \text{where} \quad
0 \le \Vloc{i} \le \size{\Cw}.
\end{equation*}
Earley sets are of type \type{ES}.
Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
The type designator \type{ES} is often omitted to avoid clutter,
especially in cases where the Earley set is not
named by location.

At points,
we will need to compare the Earley sets
produced by the different recognizers.
\EVtable{\alg{Recce}}{i} will be
the Earley set at \Vloc{i}
in the table of Earley sets of
the \alg{Recce} recognizer.
For example,
\EVtable{\Marpa}{j} will be Earley set \Vloc{j}
in \Marpa's table of Earley sets.
In contexts where it is clear which recognizer is
intended,
\Vtable{k}, or \Ves{k}, will symbolize Earley set \Vloc{k}
in that recognizer's table of Earley sets.
If \var{working} is an Earley set,
$\Vsize{working}$ is the number of Earley items
in \var{working}.

\Rtablesize{\alg{Recce}} is the total number
of Earley items in all Earley sets for \alg{Recce},
\begin{equation*}
\Rtablesize{\alg{Recce}} =
     \sum\limits_{\Vloc{i}=0}^{\size{\Cw}}
	{\bigsize{\EVtable{\alg{Recce}}{i}}}.
\end{equation*}
For example,
\Rtablesize{\Marpa} is the total number
of Earley items in all the Earley sets of
a \Marpa{} parse.

Recall that
there was a unique acceptance symbol,
\Vsym{accept}, in \Cg{}.
The input \Cw{} is accepted if and only if,
for some \Vstr{rhs},
\begin{equation*}
\bigl[[\Vsym{accept} \de \Vstr{rhs} \mydot], 0\bigr] \in \bigEtable{\Vsize{\Cw}}
\end{equation*}

The following theorem will prove useful.

\begin{theorem}\label{t:es-size}
The maximum size of an Earley set at \Vloc{i}
is \order{\var{i}}.
\end{theorem}

\begin{proof}
The size of an Earley set is the number of EIM's
that it contains.
Each EIM is of the form
$[ \Vdr{x}, \Vloc{x} ]$.
Recall that
the number of dotted rules is a \Vsize{g},
a constant depending on the grammar.
The possible values of \Vloc{x} range from
0 to \Vloc{i}.
Therefore the number of possible Earley items
in \Ves{i} is
is $\Vloc{i} \times \Oc{}$.
It follows by the definition of the size of an
Earley set that the maximum size of the Earley
set at \Vloc{i} is
\begin{equation*}
\Vloc{i} \times \Oc{} = \order{\var{i}} \times \Oc{} = \order{\var{i}}.
\end{equation*}
\end{proof}

\section{Operations of the Earley algorithm}
\label{s:earley-ops}

In this section,
each Earley operation is shown in the form of
an inference rule,
the conclusion of which
is the set of the Earley items
that is that operation's \dfn{result}.
The Earley sets correspond to parse locations,
and for any Earley operation there is a
current parse location, \Vloc{current}, and
a current Earley set, \Ves{current}.

Each location starts with an empty Earley set.
For the purposes of this description of
\Earley{}, the order of the
Earley operations
when building an
Earley set is non-deterministic.
After each Earley operation is performed,
its result is unioned with the
current Earley set.
When no more Earley items
can be added, the Earley set is
complete.
The Earley sets are built in
order from 0 to \Vsize{w}.

\subsection{Initialization}
\label{d:initial}
\begin{equation*}
\inference{
   \Vloc{current} = 0
}{
    \Bigset{\bigl[ [ \Vsym{accept} \de \mydot \Vsym{start} ], 0 \bigr]}
}
\end{equation*}
Earley {\bf initialization} has no operands
and only takes
place in Earley set 0.

\subsection{Scanning}
\label{d:scan}
\begin{equation*}
\inference{
    \begin{array}{c}
	\Vloc{current} > 0 \\
	\Vloc{previous} = \Vloc{current} \subtract 1
	\\[3pt]
	\Vsym{token} = \Cw\bigl[\Vloc{previous}\bigr]
	\\[3pt]
	\Veim{predecessor} = [ \Vdr{before}, \Vorig{predecessor} ] \\
	\Veim{predecessor} \in \Ves{previous} \\
	\Postdot{\Vdr{before}} = \Vsym{token}
    \end{array}
}{
    \bigset{ [ \Next(\Vdr{before}), \Vorig{predecessor} ] }
}
\end{equation*}
\Veim{predecessor}
and \Vsym{token} are the operands of an Earley scan.
\Veim{predecessor} is called the predecessor of the scanning operation.
The token, \Vsym{token}, is the transition symbol
of the scanning operation.

\subsection{Reduction}
\label{d:reduction}
\begin{equation*}
\inference{
    \begin{array}{c}
    \Veim{component} = \bigl[
     [ \Vsym{lhs} \de \Vstr{rhs} \mydot ]
    , \Vloc{component-orig} \bigr] \\
    \Veim{component} \in \Ves{current} \\[3pt]
    \Veim{predecessor} = [ \Vdr{before}, \Vorig{predecessor} ] \\
    \Veim{predecessor} \in \Ves{component-orig} \\
    \Postdot{\Vdr{before}} = \Vsym{lhs} \\
    \end{array}
}{
    \bigset{ [ \Next(\Vdr{before}), \Vorig{predecessor} ] }
}
\end{equation*}

\Veim{component} is called
the component of the reduction operation.\footnote{
The term ``component'' comes from Irons \cite{Irons}.
}
\Veim{predecessor} is the predecessor
of the reduction operation.
\Vsym{lhs} is the transition symbol
of the reduction operation.

\Veim{predecessor} and
\Veim{component} are the operands of the reduction
operation.
In some contexts, it is convenient to treat the transition
symbol
as an operand,
so that the operands
are \Veim{predecessor} and \Vsym{lhs}.

\subsection{Prediction}
\label{d:prediction}
\begin{equation*}
\inference{
    \begin{array}{c}
	\Veim{predecessor} = [ \Vdr{predecessor}, \Vorig{predecessor} ] \\
	\Veim{predecessor} \in \Ves{current} \\
	\bigl[ \Vsym{L} \de \Vstr{rh} \bigr] \in \Rules{\var{g}}  \\
	\bigl( \exists \, \Vstr{z} \mid
	\Postdot{\Vdr{predecessor}} \destar \Vsym{L} \cat \Vstr{z} \bigr)
    \end{array}
}{
  \Bigl\{
  \bigl[ \left[ \Vsym{L} \de \mydot \Vstr{rh} \right],
  \Vloc{current}
  \bigr]
  \Bigr\}
}
\end{equation*}
A prediction operation can add several Earley items
to Earley set \Vloc{current}.
\Veim{predecessor} is called the predecessor of the prediction operation,
and is its only operand.

\section{The Leo algorithm}
\label{s:leo}

\subsection{The history of a conjecture}

Jay Earley~\cite[p. 60]{Earley1968}
conjectured that \Earley{} could be
modified to be \On{} for
all deterministic context-free grammars (DCFG's).
(The DCFG's are the union of the
\var{LR}(\var{k})
grammars for all \var{k}.)
He gave no details of the method he had in mind.
Earley left the field shortly thereafter ---
by 1973 he had earned a second Ph.D. and
was a practicing psychotherapist in California.

In the late 1980's, Joop Leo
also conjectured that
Earley's algorithm could be modified to be linear for all
DCFG's.
Leo was initially unaware of Earley's earlier conjecture ---
he discovered it in the course of writing up his
discovery.
Leo published his method in~\cite{Leo1991}.

The problem both investigators noticed was that,
while \Earley{} is \On{}
for left recursion,
and in fact very efficient,
it is $\order{\var{n}^2}$ for right recursion.
This is because all the EIM's necessary for a right
recursion are created at every parse location
where the right recursion might end.

For simplicity,
in the discussion of this section,
we will assume that the grammar is unambiguous,
or at least that it is not ambiguous in a way that affects
the right recursion we are discussing.
We will call each potential end location of a right recursion,
an \dfn{EORR},
and will will call
the EIM's needed to represent the right recursion
at an EORR, an \dfn{EORR set}.

In \Earley,
if the length of the right
recursion at an EORR is \var{n}, then the EORR set contains \var{n} EIM's.
The total number of EIM's in the EORR sets
needed for a right recursion of length \var{n} is
\[
  \sum_{\var{i} = 1}^\var{n} \var{i} = \order{\var{n}^2}.
\]
Of these EIM's, only \var{n} will actually be needed ---
the rest are useless.

Leo's idea was to memoize the right recursions.
With Leo memoization, each EORR set is represented by
a pair of EIM's and a memo.
There are \Oc{} Leo memos per Earley set,
so the time and space complexity
of an EORR set is \Oc{}.
The time and space complexity of all the EORR
sets in
a right recursion of length \var{n} will
be
\[
  \sum_{\var{i} = 1}^\var{n} 1 = \On.
\]

If, at evaluation time,
it is desirable to expand the Leo memoizations,
only the EORR set actually used in the parse
needs to be expanded.
All of the EIM's actually used in a right recursion
will be in a single EORR set.
The number of memoized
EIM's that need to be expanded
will be \On{},
where \var{n} is the length of the recursion.
As a result,
even if the time and space
required to expand Leo memoization
during evaluation
are taken into account,
the time and space complexity of
a right recursion become
$\On{} = \On{} + \On{}$.

In \cite{Leo1991}, Joop Leo
showed that,
with his modification, Earley's algorithm
is \On{} for all LR-regular grammars.
LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.
Earley did not claim \On{} for LR-regular
because LR-regular was not introduced
to the literature until 1973~\cite{Culik1973}.
Even in 1991, LR-regular was not well-known,
which is why Leo only claims
the weaker
\var{LR}(\var{k})
bound in
his title.

% TODO: Beginning of original material --
% check for duplication

\subsection{Leo's algorithm}

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this \doc{} Leo's ``transitive items''
will be called Leo items.
Leo items
will have type \type{LIM}.

In each Earley set, there is at most one Leo item per symbol.
A Leo item (LIM) is the triple
\begin{equation*}
[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]
\end{equation*}
where \Vsym{transition} is the transition symbol,
and
\begin{equation*}
\Veim{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley item to be added on reductions over
\Vsym{transition}.

Sometimes it is useful to deal with sets
which can contain both EIMs and LIMs.
For reasons that will be clearer later,
the union type of EIM and LIM is called a ``postdot items''.
Postdot items have type \type{PIM}.

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
so that the top of the sequence can represent
the entire sequence --
the only role the other EIM's in the sequence
play in the parse is to derive the top EIM.
We will call these memoized sequences, Leo sequences.

To quarantee that a Leo sequence is deterministic,
\Leo{} enforced \dfn{Leo uniqueness}.
Define containment of a dotted rule in a Earley set
of EIM's as
\begin{equation*}
\begin{split}
& \mymathop{Contains}(\Ves{i}, \Vdr{d}) \defined \\
& \qquad \exists \, \Veim{b}, \Vorig{j} \; \mid  \\
& \qquad\qquad \Veim{b} = [ \Vdr{d}, \Vorig{j} ]
\; \land \; \Veim{b} \in \Ves{i}.
\\
\end{split}
\end{equation*}
A dotted rule \Vdr{d} is \dfn{Leo unique} in the Earley set
at \Ves{i}
if and only if
\begin{equation*}
\begin{split}
&    \Penult{\Vdr{d}} \neq \Lambda \\
\end{split}
\end{equation*}
and, for all \Vdr{d2},
\begin{equation*}
\begin{split}
&    \mymathop{Contains}(\Ves{i}, \Vdr{d2}) \land
\Postdot{\Vdr{d}} = \Postdot{\Vdr{d2}}
\\
&    \qquad \qquad \implies \Vdr{d} = \Vdr{d2}.
\end{split}
\end{equation*}
It is important to emphasize that \Vdr{d2} ranges over all the dotted
rules of Earley set \Ves{i},
even those which are ineligible for Leo memoization.

If \Vdr{d} is Leo unique, then the symbol $\Postdot{\Vdr{d}}$ is
also said to be \dfn{Leo unique}.
Similarly,
in cases where a dotted rule is Leo unique in \Ves{i},
we can speak of the
\dfn{Leo unique}
rule for \Vsym{transition}.

Let \var{n} be the length of a Leo sequence.
In \Earley, each such sequence would be expanded in every
Earley set that is the origin of an EIM included in the
sequence, and the total number of EIM's would be
\order{\var{n}^2}.

With Leo memoization, a single EIM stands in for the sequence.
There are \Oc{} Leo items per Earley set,
so the cost of the sequence is \Oc{} per Earley set,
or \On{} for the entire sequence.
If, at evaluation time,
it is desirable to expand the Leo sequence,
only those items actually involved in the parse
need to be expanded.
All the EIM's of a potential right-recursion
will be in one Earley set and the number of EIM's
will be \On{},
so that even including expansion of the Leo sequence
for evaluation, the time and space complexity of
the sequence remains \On{}.

\begin{sloppypar}
Recall that we
call a dotted rule \Vdr{d} a \dfn{penult} if $\Penult{\var{d}} \neq \Lambda$.
In Leo's original algorithm, any penult
was treated as a potential right-recursion.
\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\begin{equation*}
\Vdr{candidate} = [\Vrule{candidate}, \var{i}]
\end{equation*}
for Leo memoization,
\Vdr{candidate} must be a penult and
\Vrule{candidate} must be right-recursive.
\end{sloppypar}

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be infinitely
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff is therefore very limited.

A future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
justify inclusion in the Leo memoizations.
Such cases would be unusual, but may occur.

Omission of a memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
Later in this \doc{} we will
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

Implementing the Leo logic requires
adding Leo reduction as a new basic operation,
adding a new premise to the Earley reduction
operation,
and extending the Earley sets to memoize Earley
items as LIM's.

\subsection{Leo reduction}

\begin{equation*}
\inference{
    \begin{array}{c}
    \Veim{component} = \bigl[
     [ \Vsym{lhs} \de \Vstr{rhs} \mydot ]
    , \Vloc{component-origin} \bigr] \\
    \Veim{component} \in \Ves{current} \\[3pt]
	\Vlim{predecessor} = [ \Vdr{top}, \Vsym{lhs}, \Vorig{top} ] \\
	\Vlim{predecessor} \in \Ves{component-orig} \\
    \end{array}
}{
    \bigset{ [ \Vdr{top}, \Vorig{top} ] }
}
\end{equation*}
The new Leo reduction operation resembles the Earley reduction
operation, except that it looks for an LIM,
instead of a predecessor EIM.
\Vlim{predecessor} and
\Veim{component} are the operands of the Leo reduction
operation.
\Vsym{lhs} is the transition symbol
of the Leo reduction.
As with Earley reduction,
it may be convenient to treat the transition
symbol as an operand,
so that the operands
are \Vlim{predecessor} and \Vsym{lhs}.

\subsection{Changes to Earley reduction}

Earley reduction still applies, with an additional premise:
\begin{equation*}
\begin{split}
& \neg \exists \, \Vlim{x} \; \mid \\
&  \qquad \Vlim{x} \in \Ves{component-orig} \\
&	\qquad \qquad  \land \Vlim{x} = [ \Vdr{x}, \Vsym{lhs}, \Vorig{x} ]
\end{split}
\end{equation*}
The additional premise
prevents Earley reduction from being applied
where there is an LIM with \Vsym{lhs} as its transition symbol.
This reflects the fact that
Leo reduction replaces Earley reduction if and only if
there is a Leo memoization.

\subsection{Leo memoization operations}

Finally, we define two new operations for
the creation of LIM's or,
in other words,
for Leo memoization.
We define uniqueness of a penult in an Earley set as
\begin{equation*}
\begin{split}
& \mymathop{Penult-Unique}(\Vsym{penult},\Ves{i}) \defined \\
& \qquad \forall \, \Vdr{y} \bigl( \mymathop{Contains}(\Ves{current}, \Vdr{y})
\land \Vsym{penult} = \Penult{\Vdr{y}} \bigr) \\
& \qquad \qquad \implies \Vdr{x} = \Vdr{y}.
\end{split}
\end{equation*}
We define the
Leo uniqueness of a dotted rule within an Earley set as
\begin{equation*}
\begin{split}
& \LeoUnique{\Vdr{x},\Vloc{current}} \defined \\
& \qquad \mymathop{Contains}(\Ves{current}, \Vdr{x})  \\
& \qquad \qquad \land \Penult{\Vdr{x}} \neq \Lambda \\
& \qquad \qquad \land \mymathop{Penult-Unique}({\Penult{\Vdr{x}}}, \Ves{current})
\end{split}
\end{equation*}
and defined the Leo eligibility
of a dotted rule within an Earley set as
\begin{equation*}
\begin{split}
& \LeoEligible{\Vdr{x},\Vloc{current}} \defined \\
& \qquad \exists \, \Vrule{x}, \Vorig{x} \; \mid \\
& \qquad \qquad \Vdr{x} = [ \Vrule{x}, \Vorig{i} ] \\
& \qquad \qquad \quad \land \RightRecursive{\Vrule{x}} \\
& \qquad \qquad \quad \land \LeoUnique{\Ves{current},\Vdr{x}}.
\end{split}
\end{equation*}
We define
the LIM predecessor of \Veim{eim} as follows:
\begin{multline*}
\mymathop{LIM-Predecessor}({\Veim{bottom}}) = \Vlim{pred} \quad \text{such that} \\
\text{there exists} \qquad \Ves{bottom-origin}, \Vdr{bottom}, \\
\qquad \Vdr{pred}, \Vloc{pred-origin}, \Vloc{bottom-origin} \\
\end{multline*}
where
\begin{gather}
\Veim{bottom} = [ \Vdr{bottom}, \Vloc{bottom-origin} ] \\
\text{and} \quad \Vlim{pred} = [ \Vdr{pred}, \LHS{\Vdr{bottom}}, \Vloc{pred-origin} ] \\
\text{and} \quad \Vlim{pred} \in \Ves{bottom-origin}
\end{gather}
There may not be a LIM predecessor for \Veim{bottom},
in which case
$\mymathop{LIM-Predecessor}({\Veim{bottom}}) = \Lambda$.

We are now ready to define an inference rule which holds if
a LIM predecessor can be found for an EIM \Veim{bottom}
\begin{equation*}
\inference{
    \begin{array}{c}
    \mymathop{LIM-Predecessor}({\Vlim{pred},\Veim{bottom}}) \\
    \Vlim{pred} = [ \Vdr{pred}, \LHS{\Vdr{bottom}}, \Vorig{pred} ] \\
    \Veim{bottom} = [ \Vdr{bottom}, \Vloc{bottom} ] \\
    \Veim{bottom} \in \Ves{current} \\
    \LeoEligible{\Vdr{bottom}}
    \end{array}
}{
    \left \lbrace [ \Vdr{pred}, \Penult{\Vdr{bottom}}, \Vorig{pred} ] \right \rbrace
}
\end{equation*}
and another, which holds if
\Veim{bottom} has no predecessor LIMT,
\begin{equation*}
\inference{
    \begin{array}{c}
    \neg \mymathop{LIMT-Predecessor}({\Vlim{pred},\Veim{bottom}}) \\
    \Veim{bottom} = [ \Vdr{bottom}, \Vorig{bottom} ] \\
    \Veim{bottom} \in \Ves{current} \\
    \LeoEligible{\Vdr{bottom}}
    \end{array}
}{
    \bigset{ [ \Next(\Vdr{bottom}), \Penult{\Vdr{bottom}}, \Vorig{bottom} ] }
}
\end{equation*}

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

\subsection{Pseudocode conventions}

Earley sets may be represented by \Ves{i},
where \var{i} is the Earley set's location \Vloc{i}.
The two notations should be regarded as interchangeable.
The actual implementation of either
should be the equivalent of a pointer to
a data structure containing,
at a minimum,
the Earley items,
a memoization of the Earley set's location as an integer,
and a per-set-list.
Per-set-lists will be described in Section \ref{s:per-set-lists}.

The main purpose of the pseudocode is to
support the proofs of this paper's claims.
Therefore, alongside the pseudocode of this section
are observations about its space and time complexity.

Some readers will only be interested in
the pseudocode itself, and will want to
skip the complexity discussions.
For their convenience,
every complexity
discussion occurs in a numbered section with ``Complexity''
in the title,
or in an unnumbered subsection that begins
with the ``Complexity summary''
or the the ``Complexity'' subheader.
The first such subsection follows
immediately.

{\bf Complexity}:
In what follows,
we will charge all time and space resources
to Earley items,
or to attempts to add Earley items.
We will show that,
to each Earley item actually added,
or to each attempt to add a duplicate Earley item,
we can charge amortized \Oc{} time and space.

At points, it will not be immediately
convenient to speak of
charging a resource
to an Earley item
or to an attempt to add a duplicate
Earley item.
In those circumstances,
we speak of charging time and space
\begin{itemize}
\item to the parse; or
\item to the Earley set; or
\item to the current procedure's caller.
\end{itemize}

We can charge time and space to the parse itself,
as long as the total time and space charged is \Oc.
Afterwards, this resource can be re-charged to
the initial Earley item, which is present in all parses.
Soft and hard failures of the recognizer use
worst-case \Oc{} resource,
and are charged to the parse.

We can charge resources to the Earley set,
as long as the time or space is \Oc.
Afterwards,
the resource charged to the Earley set can be
re-charged to an arbitrary member of the Earley set,
for example, the first.
If an Earley set is empty,
the parse must fail,
and the time can be charged to the parse.

In a procedure,
resource
can be ``caller-included''.
Caller-included resource is not accounted for in
the current procedure,
but passed upward to the procedure's caller,
to be accounted for there.
A procedure to which caller-included resource is passed will
sometimes pass the resource upward to its own caller,
although of course the top-level procedure must not do this.

For each procedure, we will state whether
the time and space we are charging is inclusive or exclusive.
The exclusive time or space of a procedure is that
which it uses directly,
and ignores resource charges passed up from called procedures.
Inclusive time or space includes
resource passed upward to the
current procedure from called procedures.

\begin{algorithm}[h]
\caption{Marpa Top-level}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initial}{}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\Ves{x}$ is complete, for $0 \le \var{x} < \var{i}$
\State \Call{Scan pass}{$\var{i}, \var{w}[\var{i} \subtract 1]$}
\If{$\size{\Ves{i}} = 0$}
\State reject \Cw{} and return
\EndIf
\State \Call{Reduction pass}{\var{i}}
\EndFor
\For{every $[\Vdr{x}, 0] \in \Etable{\Vsize{w}}$}
\If{$\Vdr{accept} = \Vdr{x}$}
\label{line:accept-loop}
\State accept \Cw{} and return
\EndIf
\EndFor
\State reject \Cw{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Top-level code}

Time and space for
\call{Scan pass}{}
and
\call{Reduction pass}{}
are accounted within those procedures,
so that exclusive and inclusive time and space
for
\call{Main}{}
are identical.
No exclusive space is used,
and time is \Oc{} except for the accept loop
starting at line \ref{line:accept-loop}.
The time for the accept loop is
$\Oc{} \times \var{accept-passes}$
where
\var{accept-passes} is the number of passes
made through the accept loop.
\var{accept-passes} is limited by the number of
EIM's in \Ves{\Vsize{w}}.
From Theorem \ref{t:es-size},
we know that this is $\order{\Vsize{w}} = \On$.
The inclusive time of \On{} is charged to
the parse.

\subsection{Ruby Slippers parsing}
This top-level code represents a significant change
from \AH{}.
\call{Scan pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when the scanning of tokens that start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.


\begin{algorithm}[h]
\caption{Initialization}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add Predictions}{$0_{ES}, \Vsym{accept}$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}
\label{p:initial-op}

From section
\ref{p:add-predictions-complexity} we know that the resources
charged by
\call{Add Predictions}{} are caller included and
that \call{Add Predictions}{}
passes up a time and space charge of \Oc{}.
Overhead from this call is \Oc{},
so that total inclusive time and space
for \call{Initial}{}
is $\Oc{} + \Oc{} = \Oc{}$.
The inclusive time and space resource used
by \call{Initial}{}
is charged to the parse.

\begin{algorithm}[h]
\caption{Marpa Scan pass}
\begin{algorithmic}[1]
\Procedure{Scan pass}{$\Vloc{i},\Vsym{a}$}
\For{each $\Veim{predecessor} \in \var{transitions}((\var{i} \subtract 1),\var{a})$}
\label{line:scan-pass-35}
\State $[\Vdr{from}, \Vloc{origin}] \gets \Veim{predecessor}$
\State $\Vdr{to} \gets \Next(\Vdr{from})$
\State $\Veim{to} \gets [\Vdr{to}, \Vloc{origin}]$
\State \Call{Attempt confirmed EIM}{$\Ves{i}, \Veim{to}$}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Transitions table complexity}
\label{p:transitions}

We first look at resource usage for
reading the transitions table,
\var{transitions}.
\var{transitions} is a set of tables, one per Earley set.
The tables in the set are indexed by symbol.
Symbol indexing is \Oc{}, since the number of symbols
is a constant, but
since the number of Earley sets grows with
the length of the parse,
it cannot be assumed that Earley sets can be indexed by location
in \Oc{} time.
For the operation $\var{transitions}(\ell_{\var{ES}}, \Vsym{s})$
to be in \Oc{} time,
$\ell_{\var{ES}}$ must represent a link directly to the Earley set.
In the case of scanning,
the lookup is always in the previous Earley set,
which can easily retrieved in \Oc{} time,
without using any additional space.

\subsection{Scan pass}
\label{p:scan-op}

Resource for the call of
\call{Attempt confirmed EIM}{} is caller-included.
In section \ref{p:add-confirmed-eim-complexity},
we show that space is \Oc{} when
\call{Attempt confirmed EIM}{} adds an Earley item,
and that no space is used when
\call{Attempt confirmed EIM}{} does not add an Earley item.
In section \ref{p:add-confirmed-eim-complexity},
we also show that time is \Oc{}
for each call of
\call{Attempt confirmed EIM}{}.

In section \ref{p:transitions}.
we showed that the resource usage of \var{transition}
is zero space and \Oc{} time.
The predecessor loop is the loop that begins
at line \ref{line:scan-pass-35}.
and of the call of
\call{Attempt confirmed EIM}{}.
The other logic in the loop obviously uses no space and
\Oc{} time.

Having examining the resource usage of \var{transitions},
\call{Attempt confirmed EIM}{},
and of the other logic in the predecessor loop,
we now can sum up the resource usage of one pass over
the predecessor loop.
When
\call{Attempt confirmed EIM}{}
adds an EIM,
the space usage is $0 + 0 + \Oc = \Oc$.
When \call{Attempt confirmed EIM}{}
does not add an EIM,
the space usage is zero.
Time usage is always $\Oc + \Oc + \Oc = \Oc$.
The time and space for each pass
through the predecessor loop
is charged to
\Veim{to}.

Overhead for the call to
\call{Scan pass}{} uses no space
and \Oc{} time.
The overhead
of \call{Scan pass}{}
is charged to the Earley set at \Vloc{i}.

\begin{algorithm}[h]
\caption{Reduction pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\State Note: \Vtable{i} is modified during traversal
\State \hspace{2.5em} by \Call{Reduce one LHS}{}
\For{each Earley item $\Veim{work} \in \Vtable{i}$}
\State $[\Vdr{work}, \Vloc{origin}] \gets \Veim{work}$
\State $\Vsymset{lh-sides} \gets$ a set containing the LHS
\State \hspace\algorithmicindent of every completed rule in \Veim{work}
\For{each $\Vsym{lhs} \in \Vsymset{lh-sides}$}
\State \Call{Reduce one LHS}{\Ves{i}, \Vloc{origin}, \Vsym{lhs}}
\EndFor
\EndFor
\State \Call{Memoize Leo transitions}{\Ves{i}}
\State \Call{Memoize Earley transitions}{\Ves{i}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduction pass}

The loop over \Vtable{i} must also include
any items added by \call{Reduce one LHS}{}.
This can be done by implementing \Vtable{i} as an ordered
set and adding new items at the end.

Exclusive time is clearly \Oc{} per
\Veim{work},
and is charged to the \Veim{work}.
Additionally,
some of the time required by
\call{Reduce one LHS}{} is caller-included,
and therefore charged to this procedure.
Inclusive time from \call{Reduce one LHS}{}
is \Oc{} per call,
as will be seen in section \ref{p:reduce-one-lhs}.
Overhead is \Oc{}
and is charged to the Earley set at \Vloc{i}.

\begin{algorithm}[h]
\caption{Memoize Leo transitions}
\begin{algorithmic}[1]
\Procedure{Memoize Leo transitions}{\Ves{i}}
\State \hspace\algorithmicindent EIMs symbols in \Ves{i}
\For{$\Veim{eligible} \in \LeoEligible{\Ves{i}}$}
\State $[\Vdr{eligible}, \Vloc{eligible}] \gets \Veim{eligible}$
\State $\Vsym{transition} \gets \LHS{\Vdr{eligible}}$
\State $\Vpimset{predecessors} \gets \var{transitions}(\Vorig{eligible},\Vsym{transition})$
\State Note: \Vpimset{predecessors} will contain at most one LIM.
\For{every \Vlim{pred} in \Vpimset{predecessors}}
\If{$\Vlim{pred} = \Lambda$}
\State Set $\var{transitions}(\Vloc{i},\Vsym{transition})$
\State \hspace\algorithmicindent to $\set{[\Vdr{eligible}, \Vsym{transition}, \Vorig{eligible}]}$
\Else
\State $[\Vdr{pred}, \Vsym{pred}, \Vorig{pred}] \gets \Vlim{pred}$
\State Set $\var{transitions}(\Vloc{i},\Vsym{transition})$
\State \hspace\algorithmicindent to $\set{[\Vdr{pred}, \Vsym{transition}, \Vorig{pred}]}$
\EndIf
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize Leo transitions}
\label{p:memoize-leo-transitions}

In the pseudocode for memoizing Leo transitions,
$\LeoEligible{\Ves{i}}$ is the set of EIM's in \Ves{i}
that are considered Leo eligible.
In the Marpa algorithm,
an EIM is Leo eligible if and only if
it is a penult;
its postdot symbol is unique in \Ves{i};
the rule of the EIM is right recursive.

\subsection{Complexity of Memoizing Leo transitions}
\label{p:memoize-leo-transitions-complexity}

We now look at the resource used in the Leo processing.
A transition symbol \Vsym{transition}
is Leo eligible if it is Leo unique
and its rule is right recursive.
(If \Vsym{transition} is Leo unique in \Ves{i}, it will be the
postdot symbol of only one rule in \Ves{i}.)
All but one of the determinations needed to decide
if \Vsym{transition} is Leo eligible can be precomputed
from the grammar,
and the resource to do this is charged to the parse.
As one example,
precomputation, for every rule,
can determine if it is right recursive.

One part of the test for
Leo eligibility cannot be done as a precomputation.
This is the determination whether there is only one dotted
rule in \Ves{i} whose postdot symbol is
\Vsym{transition}.
This can be done
in a single pass over the EIM's of \Ves{i}
that notes the postdot symbols as they are encountered
and whether any is enountered twice.
The time and space,
including that for the creation of a LIM if necessary,
will be \Oc{} time per EIM examined,
and can be charged to EIM being examined.

\begin{algorithm}[h]
\caption{Memoize Earley transitions}
\begin{algorithmic}[1]
\Procedure{Memoize Earley transitions}{\Ves{i}}
\For{every \Veim{eim} in \Ves{i}}
\label{line:memoize-earley-transitions-10}
\State $\Vsym{postdot} \gets \Postdot{\Veim{i}}$
\State If $\Vsym{postdot} = \Lambda$, try next \Veim{eim}
\State If $\var{transitions}(\Vloc{i},\Vsym{postdot})$ contains a LIM,
\label{line:memoize-earley-transitions-50}
\State \hspace\algorithmicindent try next \Veim{eim}
\State Add \Veim{eim} to the set of EIMs
\label{line:memoize-earley-transitions-70}
\State \hspace\algorithmicindent in $\var{transitions}(\Vloc{i},\Vsym{postdot})$
\EndFor
\label{line:memoize-earley-transitions-last}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize Earley transitions}
\label{p:memoize-earley-transitions}

\begin{sloppypar}
At line
\ref{line:memoize-earley-transitions-50},
note that if
$\var{transitions}(\Vloc{i},\Vsym{postdot})$
contains a LIM,
it is a singleton set containing only a LIM.
The \var{transitions} table for \Ves{i}
is built after all EIMs have been
added to \Ves{i}.
\end{sloppypar}

\subsection{Complexity of Memoizing Earley transitions}
\label{p:memoize-earley-transitions-complexity}

Inclusive and exclusive resource are the same.
\Oc{} space may be charged at line
\ref{line:memoize-earley-transitions-70}.
For the purposes of this complexity analysis,
we will call loop starting at line
\ref{line:memoize-earley-transitions-10}
is the ``EIM loop''.

Every line in the EIM loop uses \Oc{} time.
This is obvious except for
lines
\ref{line:memoize-earley-transitions-50}
and
\ref{line:memoize-earley-transitions-70}.
To show that line
\ref{line:memoize-earley-transitions-50}
is \Oc{},
we recall,
from section
\ref{p:transitions},
that reading the \var{transitions}
table is \Oc{}.
To show that line
\ref{line:memoize-earley-transitions-50}
is \Oc{},
we note that adding an element to set can
be \Oc{} if,
for example,
the set is implemented as a linked list.

Therefore, each pass through the EIM loop
uses
\begin{equation*}
\sum_{\var{line}=\ref{line:memoize-earley-transitions-10}}
^{\ref{line:memoize-earley-transitions-last}} \Oc{} = \Oc{}
\end{equation*}
time
and \Oc{} space.

{\bf Summary}:
The inclusive \Oc{} time and space
for
each pass through the EIM loop
is charged to \Veim{eim}.
There is no space overhead.
The inclusive time overhead of \Oc{}
is charged to \Ves{i}.

\begin{algorithm}[h]
\caption{Reduce one LHS symbol}
\begin{algorithmic}[1]
\Procedure{Reduce one LHS}{\Ves{i}, \Vloc{origin}, \Vsym{lhs}}
\State Note: Each pass through this loop is an EIM attempt
\For{each $\Vpim{trans} \in \var{transitions}(\Vloc{origin},\Vsym{lhs})$}
\label{line:reduce-one-lhs-20}
\If{\Vpim{trans} is a LIM}
\State Set $\Vlim{trans} \gets \Vpim{trans}$
\State Perform a \Call{Leo reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Vlim{trans}
\Else
\State Set $\Veim{trans} \gets \Vpim{trans}$
\State Perform an \Call{Earley reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Veim{trans}
\EndIf
\EndFor
\label{line:reduce-one-lhs-last}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{``Reduce one LHS'' Complexity}
\label{p:reduce-one-lhs}

None of the resource in the calls of
\call{Leo reduction operation}{}
and
\call{Earley reduction operation}{}
is caller-included,
so that,
for \call{Reduce one LHS symbol}{}
inclusive and exclusive resource
are the same.
Also, none of the lines in
\call{Reduce one LHS symbol}{}
use space.

The \var{pim} loop is from line
\ref{line:reduce-one-lhs-20}
to line
\ref{line:reduce-one-lhs-last}.
All lines in the \var{pim} loop use \Oc{} time, so that
time used by one pass through the \var{pim} loop is
$\sum_{\var{line}=\ref{line:reduce-one-lhs-20}}
^{\ref{line:reduce-one-lhs-last}} \Oc{} = \Oc{}$.
To show that the time for line
\ref{line:reduce-one-lhs-20} is \Oc{},
we recall that,
in section \ref{p:transitions}, we showed
that
$\var{transitions}(\Ves{origin},\Vsym{lhs})$
can be traversed in \Oc{} time.

{\bf Summary}:
No space is charged.
Inclusive time
for the \var{pim} loop
is charged to the postdot item \Veim{pim} or \Vlim{pim}.
The trivial amount of time outside
the \var{pim} loop is \Oc{}
and is caller-included.

\begin{algorithm}[h]
\caption{Earley reduction operation}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{i}, \Veim{from}}
\State $[\Vdr{from}, \Vloc{origin}] \gets \Veim{from}$
\State $\Vdr{to} \gets \Next(\Vdr{from})$
\State $\Veim{to} \gets [\Vdr{to}, \Vorig{i}]$
\State \Call{Attempt confirmed EIM}{\Ves{i}, \Veim{to}}
\label{line:earley-reduction-last}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Earley Reduction operation complexity}
\label{p:reduction-op}

Inclusive space at line
\ref{line:earley-reduction-last}
is \Oc{}
as shown in section \ref{p:add-confirmed-eim-complexity}.
Inclusive space at all other lines is zero,
so that total inclusive space is \Oc{}.

For line
\ref{line:earley-reduction-last}
inclusive time is \Oc,
as is shown in section \ref{p:add-confirmed-eim-complexity}.
For the other lines, it is directly clear
that inclusive time is \Oc.
Therefore inclusive time for the
\call{Earley reduction operation}{} is
$\sum_{\var{line}=1}^{\ref{line:earley-reduction-last}} \Oc{} = \Oc{}$.

{ \bf Summary }:
For
\call{Earley reduction operation}{} \Oc{} time
and \Oc{} space
is charged to \Veim{to}.

\begin{algorithm}[h]
\caption{Leo reduction operation}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{i}, \Vlim{from}}
\State $[\Vdr{from}, \Vsym{trans}, \Vloc{origin}] \gets \Vlim{from}$
\State $\Vdr{to} \gets \Next(\Vdr{from})$
\State $\Veim{to} \gets [\Vdr{to}, \Vorig{i}]$
\State \Call{Attempt confirmed EIM}{\Ves{i}, \Veim{to}}
\label{line:leo-reduction-last}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Leo reduction operation complexity}
\label{p:leo-op}

Inclusive space at line
\ref{line:leo-reduction-last}
is \Oc{}
as shown in section \ref{p:add-confirmed-eim-complexity}.
Inclusive space at all other lines is zero,
so that total inclusive space is \Oc{}.

For line
\ref{line:leo-reduction-last}
inclusive time is \Oc,
as is shown in section \ref{p:add-confirmed-eim-complexity}.
For the other lines, it is directly clear
that inclusive time is \Oc.
Therefore inclusive time for the
\call{Leo reduction operation}{} is
$\sum_{\var{line}=1}^{\ref{line:leo-reduction-last}} \Oc{} = \Oc{}$.

{ \bf Summary }:
For
\call{Leo reduction operation}{} \Oc{} time
and \Oc{} space
is charged to \Veim{to}.

\begin{algorithm}[h]
\caption{Attempt confirmed EIM}\label{a:confirmed}
\begin{algorithmic}[1]
\Procedure{Attempt confirmed EIM}{$\Ves{i},\Veim{attempt}$}
\If{\Veim{attempt} is new in \Ves{i}}
\label{line:attempt-confirmed-40}
\State Add \Veim{attempt} to \Vtable{i}
\label{line:attempt-confirmed-45}
\State $[\Vdr{dr}, \Vloc{origin}] \gets \Veim{attempt}$
\State $\Vsym{postdot} \gets \Postdot{\Vdr{dr}}$
\If{$\Vsym{postdot} \neq \Lambda$}
\State \Call{Add predictions}{\Ves{i}, \Vsym{postdot}}
\label{line:attempt-confirmed-85}
\label{line:attempt-confirmed-last}
\EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Complexity of attempting a confirmed Earley item}
\label{p:add-confirmed-eim-complexity}

This operation attempts to add a confirmed EIM
item as well as the EIMs for the predictions which
result from it.

If \Veim{attempt} is new,
\Oc{} space is used directly at line
\ref{line:attempt-confirmed-45}; and
from section \ref{p:add-predictions-complexity},
we know that \Oc{} space is used indirectly at line
\ref{line:attempt-confirmed-85}.
Therefore,
the inclusive space used is
$\Oc + \Oc = \Oc$ per call.
If \Veim{attempt} is not new,
inclusive space used is zero.

Inclusive time used by each line is \Oc{},
so that inclusive time for the call
of
\call{Add Confirmed EIM}{} is
$\sum_{\var{line}=1}^{\ref{line:attempt-confirmed-last}} \Oc{} = \Oc{}$.
In the case of lines
\ref{line:attempt-confirmed-40},
\ref{line:attempt-confirmed-45}, and
\ref{line:attempt-confirmed-85}
the claim that time is \Oc{}
is non-trivial.

At line \ref{line:attempt-confirmed-40},
\Marpa{} checks whether an Earley item is new.
The time used is \Oc{}
because Marpa
uses a data structure called a PSL.
PSL's are the subject of Section \ref{s:per-set-lists}.

At line
\ref{line:attempt-confirmed-45},
an Earley item is added to the current
set.
This can be done in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

At line \ref{line:attempt-confirmed-85}.
\call{Add predictions}{} is called.
We know that the inclusive time is
\Oc{},
from section \ref{p:add-predictions-complexity}{}.

{\bf Summary}:
Inclusive time is always \Oc{}.
When a \Veim{attempt} is new,
inclusive space is \Oc{}.
When \Veim{attempt} is not new,
inclusive space is zero.
Time and space are caller included.

\begin{algorithm}[h]
\caption{Add predictions}\label{a:predictions}
\begin{algorithmic}[1]
\Procedure{Add Predictions}{$\Ves{i},\Vsym{lhs}$}
\State Note: \var{rules} is the set of grammar rules
\State Set $\var{lhs-rules} \gets \left \lbrace \Vrule{r} \mid \var{r} \in \var{rules} \land \LHS{\var{r}} = \Vsym{lhs} \right \rbrace$
\For{ $[ \Vsym{lhs} \de \Vstr{rhs} ] \in \var{lhs-rules}$ }
\label{line:predictions-35}
\State $\Vdr{predict} \gets [ \Vsym{lhs} \de \mydot \Vstr{rhs} ]$
\State $\Veim{predict} \gets [ \Vdr{predict}, \Ves{i} ]$
\If{\Veim{predict} is new in \Ves{i}}
\label{line:predictions-50}
\State Add \Veim{predict} to \Vtable{i}
\label{line:predictions-60}
\State \Call{Add predictions}{\Ves{i}, $\Postdot{\Vdr{predict}}$}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Complexity of Adding Earley predictions}
\label{p:add-predictions-complexity}

Time and space used by
\call{Add predictions}{} is caller-included.
\Oc{} space is used in line
\ref{line:predictions-60}.
The number of passes in the loop beginning at
line
\ref{line:predictions-35}
is limited by \Vsize{lhs-rules},
which is less than or equal to the number of rules
in the grammar.
The number of rules in the grammar is a constant,
so the exclusive time and space used by
\call{Add predictions}{} is \Oc{}.

To find the inclusive time and space consumed
by
\call{Add predictions}{},
we must take into account that it is called recursively.
Therefore inclusive time and space must take into
account all calls of
\call{Add predictions}{}
in the call tree below the current call of
\call{Add predictions}{}.

From line
\ref{line:predictions-50}
it can be seen that the number of child calls
in the call tree
below any call
of \call{Add predictions}{}
is limited
to the number of predictions in \Ves{i}.
Predictions, like any EIM, must be unique in \Ves{i},
and the number of possible predictions is
the same as the number of rules in the grammar \var{g}.
The number of rules in \var{g}
is a constant depending on \var{g}.
Therefore the number of caller-included child calls
of \call{Add predictions}{}
whose time and space
must be included in this call of
\call{Add predictions}{}
is \Oc{}.
From this we can conclude that inclusive time and space
charged by
any call
of \call{Add predictions}{}
is $\Oc{} \times \Oc{} = \Oc{}$.

{\bf Summary}:
Time and space is \Oc{},
and is caller-included.

\subsection{Complexity summary}

For convenience, we collect and summarize here
some of the observations of this section.

\begin{observation}
The time and space charged to an Earley item
which is actually added to the Earley sets
is \Oc.
\end{observation}

\begin{observation}
The time charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc.
\end{observation}

For evaluation purposes, \Marpa{} adds a link to
each EIM that records each attempt to
add that EIM,
whether originally or as a duplicate.
Traditionally, complexity results treat parsers
as recognizers, and such costs are ignored.
This will be an issue when the space complexity
for unambiguous grammars is considered.

\begin{observation}
The space charged to an attempt
to add a duplicate Earley item to the Earley sets
is \Oc{} if links are included,
zero otherwise.
\end{observation}

As noted in Section \ref{p:add-confirmed-eim-complexity},
the time and space used by predicted Earley items
and attempts to add them is charged elsewhere.

\begin{observation}
No space or time is charged to predicted Earley items,
or to attempts to add predicted Earley items.
\end{observation}

\section{Per-set lists}
\label{s:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, \var{x}]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a link direct to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to,
or very similar to,
PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this \doc{}.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
a PSL is kept for every previous Earley set ---
that is, for every
$\Vloc{i} < \Vloc{j}$.

The PSL for \Ves{j}
tracks the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{g}.
\Vsize{g}
is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's
for all the Earley sets
every time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

Let $\ID{\Vdr{x}}$ be the integer ID of a dotted rule.
One way to assign integer values
to $\ID{\Vdr{x}}$ to list the rules
in some arbitrary order,
and then traverse the dot positions in
the list, left-to-right within each rule,
then top-to-bottom within the list.
The number of dot positions
within a rule \Vrule{r} is \Vsize{r}.

Intuitively, the idea is avoid most re-computation
of the PSL's by keeping timestamps with the PSL data.
To illustrate the use of PSL's,
we will consider the case
where Marpa is building \Ves{j}
and wants to check whether Earley item,
\begin{equation*}
\Veim{x} = [ \Vdr{x}, \Vorig{x} ],
\end{equation*}
is new,
or if \Veim{x} needs to be added to the Earley sets.

Let
\begin{equation*}
\var{psl-entry} = \PSL{\Ves{i}}{\var{y}}
\end{equation*}
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}, such that
\begin{equation*}
\begin{alignedat}{3}
& \var{psl-entry} && \defined && \quad
\begin{cases}
\Lambda, \quad \text{if the entry at $\var{psl-entry}$ has never been used,} \\
[\var{time-stamp}, \Vbool{z}], \text{otherwise} \\
\end{cases} \\
%
\end{alignedat}
\end{equation*}
Here \var{time-stamp} is a time stamp which will be used to
avoid unnecessary re-computation,
and \Vbool{z} is \var{true} if \Veim{x}
is already present in \Ves{j},
and \var{false} otherwise.

If
\begin{multline*}
\PSL{\Ves{i}}{\var{y}} = [\var{time-stamp}, \Vbool{exists}] \\
 \text{such that} \quad
 \var{time-stamp} \neq \Vloc{j} \\
 \lor \; \Vbool{exists} = \var{false},
\end{multline*}
the \Veim{x} already exists,
and should not be added to \Ves{j}.
The psl entry for \Ves{i} and \Vdr{x},
$\PSL{\Ves{x}}{\ID{\Vdr{x}}}$,
is left as is.

If \Veim{x} does not already exist,
then  \Veim{x} is new.
In this case,
\Veim{x} is added to \Ves{j}, and
the PSL entry is re-set, as follows:
\begin{equation*}
\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets [\Vloc{j}, \var{true}].
\end{equation*}

The reader may notice that the above example could be simplified,
because the data to be kept for EIM's is a single boolean.
Further, this boolean is never set to \var{false}, so that
if a PSL entry exists, its boolean may be assumed to be \var{true}.
This means that an explicit boolean is not actually needed.
A simpler approach, which omits the boolean,
is the one that
the Marpa implementation uses.

It will not always be the case that the data that the PSL's need
to track is limited to a single boolean.
It is for this reason that
the more complicated approach is illustrated above.

\section{Preliminaries to the theoretical results}
\label{s:proof-preliminaries}

\subsection{Nulling symbols}
\label{s:nulling}

Recall that Marpa grammars,
without loss of generality,
contain neither empty rules or
properly nullable symbols.
This corresponds directly
to a grammar rewrite in the \Marpa{} implementation,
and its reversal during \Marpa's evaluation phase.
For the correctness and complexity proofs in this \doc{},
we assume an additional rewrite,
this time to eliminate nulling symbols.

Elimination of nulling symbols is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether a grammar \Cg{} accepts
an input \Cw{}
will not depend on the nulling symbols in its rules.

In its implementation,
\Marpa{} does not directly rewrite the grammar
to eliminate nulling symbols.
But nulling symbols are ignored in
creating the AHFA states,
and must be restored during \Marpa's evaluation phase,
so that the implementation and
this simplification for theory purposes
track each other closely.

\subsection{Comparing Earley items}

\begin{definition}
A Marpa Earley item \dfn{corresponds}
to a traditional Earley item
$\Veimt{x} = [\Vdr{x}, \Vorig{x}]$
if and only if the Marpa Earley item is a
$\Veim{y} = [\Vah{y}, \Vorig{x}]$
such that $\Vdr{x} \in \Vah{y}$.
A traditional Earley item, \Veimt{x}, corresponds to a
Marpa Earley item, \Veim{y}, if and only if
\Veim{y} corresponds to \Veimt{x}.
\end{definition}

\begin{definition}
A set of EIM's is \dfn{consistent} with respect to
a set of EIMT's,
if and only if each of the EIM's in the first set
corresponds to at least one of the
EIMT's in the second set.
A Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{consistent} if and only if
all of its EIM's correspond to
EIMT's in
\EVtable{\Leo}{i}.
\end{definition}

\begin{sloppypar}
\begin{definition}
A set of EIM's is \dfn{complete} with respect to
a set of EIMT's,
if and only if for every EIMT in the second set,
there is a corresponding EIM in the first set.
A Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{complete} if and only if for every
traditional Earley item in \EVtable{\Leo}{i}
there is a corresponding Earley item in
\EVtable{\Marpa}{i}.
\end{definition}
\end{sloppypar}

\begin{definition}
A Marpa Earley set is \dfn{correct}
if and only that Marpa Earley set is complete
and consistent.
\end{definition}

\subsection{About AHFA states}

Several facts from \cite{AH2002}
will be heavily used in the following proofs.
For convenience, they are restated here.

\begin{observation}
Every dotted rule is an element of one
or more AHFA states, that is,
\begin{equation*}
\forall \, \Vdr{x} \, \exists \, \Vah{y} \; \mid \; \Vdr{x} \in \Vah{y}.
\end{equation*}
\end{observation}

\begin{observation}
\label{o:confirmed-AHFA-consistent}
AHFA confirmation is consistent with respect to the dotted rules.
That is,
for all \Vah{from}, \Vsym{t}, \Vah{to}, \Vdr{to} such that
\begin{equation*}
\begin{split}
& \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to} \\
\qquad \qquad \land \quad & \Vdr{to} \in \Vah{to}, \\
\intertext{there exists \Vdr{from} such that}
& \Vdr{from} \in \Vah{from} \\
\qquad \qquad \land \quad & \Vsym{t} = \Postdot{\Vdr{from}} \\
\qquad \qquad \land \quad & \Next(\Vdr{from}) = \Vdr{to}. \\
\end{split}
\end{equation*}
\end{observation}

\begin{observation}
\label{o:confirmed-AHFA-complete}
AHFA confirmation is complete with respect to the dotted rules.
That is,
for all \Vah{from}, \Vsym{t}, \Vdr{from}, \Vdr{to} if
\begin{equation*}
\begin{split}
& \Vdr{from} \in \Vah{from} \\
\qquad \land \quad & \Postdot{\Vdr{from}} = \Vsym{t}, \\
\qquad \land \quad & \Next(\Vdr{from}) = \Vdr{to} \\
\intertext{then there exists \Vah{to} such that }
& \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to}  \\
\qquad \land \quad & \Vdr{to} \in \Vah{to}. \\
\end{split}
\end{equation*}
\end{observation}

\begin{observation}
\label{o:predicted-AHFA-consistent}
AHFA prediction is consistent with respect to the dotted rules.
That is,
for all \Vah{from}, \Vah{to}, \Vdr{to} such that
\begin{equation*}
 \GOTO(\Vah{from}, \epsilon) = \Vah{to}
\, \land  \,
 \Vdr{to} \in \Vah{to},
\end{equation*}
there exists \Vdr{from} such that
\begin{equation*}
 \Vdr{from} \in \Vah{from}
\, \land  \,
 \Vdr{to} \in \Predict{\Vdr{from}}.
\end{equation*}
\end{observation}

\begin{observation}
\label{o:predicted-AHFA-complete}
AHFA prediction is complete with respect to the dotted rules.
That is,
for all \Vah{from}, \Vdr{from}, \Vdr{to}, if
\begin{equation*}
 \Vdr{from} \in \Vah{from}
\, \land \,
\Vdr{to} \in \Predict{\Vdr{from}},
\end{equation*}
then there exists \Vah{to} such that
\begin{equation*}
\Vdr{to} \in \Vah{to}
\, \land  \,
\GOTO(\Vah{from}, \epsilon) = \Vah{to}
\end{equation*}
\end{observation}

\section{Marpa is correct}
\label{s:correct}

\subsection{Marpa's Earley sets grow at worst linearly}

\begin{theorem}\label{t:es-count}
For a context-free grammar,
and a parse location \Vloc{i},
\begin{equation*}
\textup{
    $\bigsize{\EVtable{\Marpa}{i}} = \order{\var{i}}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
EIM's have the form $[\Vah{x}, \Vorig{x}]$.
\Vorig{x} is the origin of the EIM,
which in Marpa cannot be after the current
Earley set  at \Vloc{i},
so that
\begin{equation*}
0 \le \Vorig{x} \le \Vloc{i}.
\end{equation*}
The possibilities for \Vah{x} are finite,
since the number of AHFA states is a constant,
$\size{\Cfa}$,
which depends on \Cg{}.
Since duplicate EIM's are never added to an Earley set,
the maximum size of Earley set \Vloc{i} is therefore
\begin{equation*}
\Vloc{i} \times \size{\Cfa} = \order{\Vloc{i}}.\qedhere
\end{equation*}
\end{proof}

\subsection{Marpa's Earley sets are correct}

\begin{theorem}\label{t:table-correct}
Marpa's Earley sets are correct.
\end{theorem}

The proof
is by triple induction,
that is, induction with a depth down to 3 levels.
We number the levels of induction
0, 1 and 2,
starting with the outermost.
The level 0 induction is usually called the outer induction.
The level 1 induction is usually called the inner induction.
Level 2 induction is referred to by number.

The outer induction is on the Earley sets.
The outer induction hypothesis is that all Earley sets
\EVtable{\Marpa}{i},
$0 \le \Vloc{i} \le \Vloc{n}$,
are complete and consistent,
and therefore correct.
We leave it as an exercise to show, as the
basis of the induction, that
\EEtable{\Marpa}{0} is complete and consistent.

To show the outer induction step, we show first
consistency, then completeness.
We show consistency by
an inner induction on the Marpa operations.
The inner induction hypothesis is that
\EVtable{\Marpa}{i},
as so far built,
is consistent with respect to
\EVtable{\Leo}{i}.

As the basis of the inner induction,
an empty Marpa Earley set is
consistent, trivially.
We show the step of the inner induction by cases:
\begin{itemize}
\item \Marpa{} scanning operations;
\item \Marpa{} reductions when there are no Leo reductions; and
\item \Marpa{}'s Leo reductions
\end{itemize}

\subsubsection{Marpa scanning is consistent}
\label{s:scan-consistent}

For Marpa's scanning operation, we know
that the predecessor EIM is correct
by the outer induction hypothesis,
and that the token is correct
by the definitions in the preliminaries.
We know, from Section \ref{p:scan-op},
that at most two EIM's will be added.
We now examine them in detail.

Let
\begin{equation*}
    \Vah{confirmed} = \GOTO(\Vah{predecessor}, \Vsym{token})
\end{equation*}
If $\Vah{confirmed} = \Lambda$,
the pseudocode of Section \ref{p:scan-op} shows
that we do nothing.
If we do nothing,
since \EVtable{\Marpa}{i} is consistent by the inner
induction hypothesis,
it remains consistent, trivially.

Otherwise, let
$\Veim{confirmed} = [\Vah{confirmed}, \Vloc{i}]$.
We see that \Veim{confirmed} is consistent with respect
to \EVtable{\Leo}{i},
by the definition of Earley scanning (Section~\ref{d:scan})
and Observation~\ref{o:confirmed-AHFA-consistent}.
Consistency is invariant under union,
and since \EVtable{\Marpa}{i} is consistent by the inner induction,
\EVtable{\Marpa}{i} remains consistent after
\Veim{confirmed} is added.

For predictions,
if $\Vah{confirmed} \ne \Lambda$, let
\begin{equation*}
\Vah{predicted} = \GOTO(\Vah{confirmed}, \epsilon)
\end{equation*}
If $\Vah{predicted} = \Lambda$,
the pseudocode of Section \ref{p:add-confirmed-eim-complexity} shows
that we do nothing.
If we do nothing,
since \EVtable{\Marpa}{i} is consistent by the inner
induction hypothesis,
it remains consistent, trivially.
Otherwise, let
\begin{equation*}
\Veim{predicted} = [\Vah{predicted}, \Vloc{i}].
\end{equation*}
We see that \Veim{predicted} is consistent with respect
to \EVtable{\Leo}{i},
by the definition of Earley prediction (Section~\ref{d:prediction}) and
Observation ~\ref{o:predicted-AHFA-consistent}.
Consistency is invariant under union and,
since \EVtable{\Marpa}{i} is consistent by the inner induction,
\EVtable{\Marpa}{i} remains consistent after
\Veim{predicted} is added.

\subsubsection{Earley reduction is consistent}
\label{s:reduction-consistent}

\begin{sloppypar}
Next,
we show that \Marpa{}'s reduction operation
is consistent,
in the case where there is no Leo reduction.
There will be two cause EIM's, \Veim{predecessor}
and \Veim{component}.
\Veim{predecessor} will be correct by the outer induction
hypothesis
and \Veim{component}
will be consistent by the inner induction hypothesis.
From \Veim{component}, we will find zero or more transition
symbols, \Vsym{lhs}.
From this point,  the argument is very similar to
that for the case of the scanning operation.
\end{sloppypar}

Let
\begin{equation*}
\Vah{confirmed} = \GOTO(\Vah{predecessor}, \Vsym{lhs})
\end{equation*}
If $\Vah{confirmed} = \Lambda$, we do nothing,
and \EVtable{\Marpa}{i} remains consistent, trivially.
Otherwise, let
\begin{equation*}
\Veim{confirmed} = [\Vah{confirmed}, \Vloc{i}].
\end{equation*}
We see that \Veim{confirmed} is consistent with respect
to \EVtable{\Leo}{i}
by the definition of Earley reduction (Section~\ref{d:reduction}),
and Observation~\ref{o:confirmed-AHFA-consistent}.
By the invariance of consistency under union,
\EVtable{\Marpa}{i} remains consistent after
\Veim{confirmed} is added.

For predictions, the argument exactly repeats that of
Section \ref{s:scan-consistent}.
\EVtable{\Marpa}{i} remains consistent,
whether or not a \Veim{predicted} is added.

\subsubsection{Leo reduction is consistent}
\label{s:leo-consistent}

\begin{sloppypar}
We now show consistency for \Marpa{}'s
reduction operation,
in the case where there is a Leo reduction.
If there is a Leo reduction, it is signaled by the
presence of \Vlim{predecessor},
\begin{equation*}
\Vlim{predecessor} = [ \Vah{top}, \Vsym{lhs}, \Vorig{top} ]
\end{equation*}
in the Earley set where we would look
for the \Veim{predecessor}.
We treat
the logic to create \Vlim{predecessor} as a matter of memoization
of the previous Earley sets,
and its correctness follows from
the outer induction hypothesis.
\end{sloppypar}

As the result of a Leo reduction,
\Leo{} will add
$[\Vdr{top}, \Vorig{top}]$
to \EVtable{\Leo}{j}.
Because the \Marpa{} LIM is correct,
using Observations \ref{o:confirmed-AHFA-consistent}
and \ref{o:confirmed-AHFA-complete}
and Theorem
% TODO DELETE
DELETED,
we see that \Vah{top} is the singleton set
$\set{ \Vdr{top} }$.
From Section \ref{p:leo-op}, we see
that, as the result of the Leo reduction,
\Marpa{} will add
\begin{equation*}
\Veim{leo} = [\Vah{top}, \Vorig{top}]
\end{equation*}
to \EVtable{\Marpa}{j}.
The consistency of \Veim{leo} follows from the definition
of EIM consistency.
The consistency of
\EVtable{\Marpa}{i},
once \Veim{leo} is added,
follows by the invariance
of consistency under union.

\subsubsection{Marpa's Earley sets are consistent}
\label{s:sets-consistent}

Sections
\ref{s:scan-consistent},
\ref{s:reduction-consistent}
and
\ref{s:leo-consistent}
show the cases for the step of the inner induction,
which shows the induction.
It was the purpose of the inner induction to show
that consistency of \EVtable{\Marpa}{i} is invariant
under Marpa's operations.

\subsubsection{The inner induction for completeness}

It remains to show that,
when Marpa's operations are run as described
in the pseudocode of Section \ref{s:pseudocode},
that
\EVtable{\Marpa}{i} is complete.
To do this,
we show that
at least one EIM in \EVtable{\Marpa}{i}
corresponds to every EIMT in
\EVtable{\Leo}{i}.
We will proceed by cases,
where the cases are \Leo{} operations.
For every operation that \Leo{} would perform,
we show that
\Marpa{} performs an operation that
produces a corresponding Earley item.
Our cases for the operations of \Leo{} are
Earley scanning operations;
Earley reductions;
Leo reductions;
and Earley predictions.

\subsubsection{Scanning is complete}
\label{s:scan-complete}

For scanning, the Marpa pseudocode shows
that a scan is attempted for every
pair
\begin{equation*}
[\Veim{predecessor}, \Vsym{token}],
\end{equation*}
where \Veim{predecessor} is an EIM in the previous
Earley set,
and \Vsym{token} is the token scanned at \Vloc{i}.
(The pseudocode actually finds
\Veim{predecessor} in a set
returned by $\mymathop{transitions}()$.
This is a memoization for efficiency
and we will ignore it.)

By the preliminary definitions, we know that \Vsym{token}
is the same in both \Earley{} and \Leo.
By the outer induction hypothesis we know that,
for every traditional Earley item in the previous
Earley set,
there is at least one corresponding Marpa Earley item.
Therefore, \Marpa{} performs its scan operation on a complete set
of correct operands.

Comparing the Marpa pseudocode (section \ref{p:scan-op}),
with the Earley scanning operation (section \ref{d:scan})
and using
Observations~\ref{o:confirmed-AHFA-complete}
and \ref{o:predicted-AHFA-complete},
we see that a Earley item will be added to
\EVtable{\Marpa}{i} corresponding to every scanned Earley item
of \EVtable{\Leo}{i}.
We also see,
from the pseudocode of Section \ref{p:add-confirmed-eim-complexity},
that the \Marpa{} scanning operation will
add to \EVtable{\Marpa}{i}
an Earley item for
every prediction that results from
a scanned Earley item in \EVtable{\Leo}{i}.

\subsubsection{Earley reduction is complete}
\label{s:reduction-complete}

We now examine Earley reduction,
under the assumption that there is
no Leo transition.
The Marpa pseudocode shows that the Earley items
in \EVtable{\Marpa}{i}
are traversed in a single pass for reduction.

To show that we traverse a complete and consistent
series of component Earley items,
we stipulate that
the Earley set is an ordered set,
and that new Earley items are added at the end.
From Theorem \ref{t:es-count}, we know
that
the number of Earley items is finite,
so a traversal of them must terminate.

Consider, for the purposes of the level 2 induction,
the reductions of \Leo{} to occur in generations.
Let the scanned Earley items be generation 0.
An EIMT produced by a reduction is generation $\var{n} + 1$
if its component Earley item was generation was \var{n}.
Predicted Earley items do not need to be assigned generations.
In Marpa grammars they can never contain completions,
and therefore can never act as the component of a reduction.

The induction hypothesis for the level 2 induction
is that for some \var{n},
the Earley items of \EVtable{\Marpa}{i} for generations 0 through \var{n}
are complete and consistent.
From Section \ref{s:sets-consistent},
we know that all Earley items in Marpa's sets are consistent.
In Section \ref{s:scan-complete},
we showed that generation 0 is complete --
it contains Earley items
corresponding to all of the generation 0 EIMT's of \Leo.
This is the basis of the level 2 induction.

Since we stipulated that \Marpa{} adds Earley items
at the end of each set,
we know that they occur in generation order.
Therefore \Marpa{},
when creating Earley items of generation $\var{n}+1$
while traversing \EVtable{\Marpa}{i},
can rely
on the level 2 induction hypothesis for
the completeness of Earley items
in generation \var{n}.

Let
$\Veim{working} \in \Ves{i}$
be the Earley item
currently being considered as a potential component for
an Earley reduction operation.
From the pseudocode, we see
that reductions are attempted for every
pair \Veim{predecessor}, \Veim{working}.
(Again, $\mymathop{transitions}()$ is ignored
as a memoization.)
By the outer induction hypothesis we know that,
for every traditional Earley item in the previous
Earley set,
there is at least one corresponding Marpa Earley item.
We see from the pseudocode, therefore,
that for each \Veim{working}
that \Marpa{} performs its reduction operation on a complete set
of correct predecessors.
Therefore \Marpa{} performs its reduction operations on a
complete set of operand pairs.

Comparing the Marpa pseudocode (Section \ref{p:reduction-op})
with the Earley reduction operation (Section \ref{d:reduction})
and using
Observations~\ref{o:confirmed-AHFA-complete}
and \ref{o:predicted-AHFA-complete},
we see that a Earley reduction result of
generation $\var{n}+1$
will be added to
\EVtable{\Marpa}{i} corresponding to every Earley reduction result
in generation $\var{n}+1$
of \EVtable{\Leo}{i},
as well as one corresponding
to every prediction that results from
an Earley reduction result
of generation $\var{n}+1$ in \EVtable{\Leo}{i}.
This shows the level 2 induction
and the case of reduction completeness.

\subsubsection{Leo reduction is complete}
\label{s:leo-complete}

\begin{sloppypar}
We now show completeness for \Marpa{}'s reduction operation,
in the case where there is a Leo reduction.
In Section \ref{s:leo-consistent},
we found that where \Leo{} would create
the EIMT $[\Vdr{top}, \Vorig{top}]$,
Marpa adds
$[\Vah{top}, \Vorig{top}]$
such that $\Vdr{top} \in \Vah{top}$.
Since \Vdr{top} is a completed rule,
there are no predictions.
This shows the case immediately,
by the definition of completeness.
\end{sloppypar}

\subsubsection{Prediction is complete}
\label{s:prediction-complete}

\begin{sloppypar}
Predictions result only from items in the same Earley set.
In Sections \ref{s:scan-complete},
\ref{s:reduction-complete}
and \ref{s:leo-complete},
we showed that,
for every prediction that would result
from an item added to \EVtable{\Leo}{i},
a corresponding prediction
was added to \EVtable{\Marpa}{i}.
\end{sloppypar}

\subsubsection{Finishing the proof}
Having shown the cases in Sections
\ref{s:scan-complete},
\ref{s:reduction-complete},
\ref{s:leo-complete} and
\ref{s:prediction-complete},
we know that Earley set
\EVtable{\Marpa}{i} is complete.
In section \ref{s:sets-consistent}
we showed that \EVtable{\Marpa}{i} is consistent.
It follows that \EVtable{\Marpa}{i} is correct,
which is the step of the outer induction.
Having shown its step, we have the outer induction,
and the theorem.
\qedsymbol

\subsection{Marpa is correct}

We are now is a position to show that Marpa is correct.
\begin{theorem}
\textup{ $\myL{\Marpa,\Cg} = \myL{\Cg}$ }
\end{theorem}

\begin{proof}
From Theorem \ref{t:table-correct},
we know that
\begin{equation*}
[\Vdr{accept},0] \in \EVtable{\Leo}{\Vsize{w}}
\end{equation*}
if and only there is a
\begin{equation*}
[\Vah{accept},0] \in \EVtable{\Marpa}{\Vsize{w}}
\end{equation*}
such that $\Vdr{accept} \in \Vah{accept}$.
From the acceptance criteria in the \Leo{} definitions
and the \Marpa{} pseudocode,
it follows that
\begin{equation*}
\myL{\Marpa,\Cg} = \myL{\Leo,\Cg}.
\end{equation*}
By Theorem 4.1 in \cite{Leo1991}, we know that
\begin{equation*}
\myL{\Leo,\Cg} = \myL{\Cg}.
\end{equation*}
The theorem follows from
the previous two equalities.
\end{proof}

\section{Marpa recognizer complexity}
\label{s:complexity}

\subsection{Complexity of each Earley item}

For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
We showed that this rewrite
is without loss of generality
in Section \ref{s:nulling},
when we examined correctness.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

\begin{lemma}\label{l:nulling-rewrite}
All time and space required
to rewrite the grammar to eliminate nulling
symbols, and to restore those rules afterwards
in the Earley sets,
can be allocated
to the Earley items
in such a way that each Earley item
requires \Oc{} time and space.
\end{lemma}

\begin{proof}
The time and space used in the rewrite is a constant
that depends on the grammar,
and is charged to the parse.
The reversal of the rewrite can be
done in a loop over the Earley items,
which will have time and space costs
per Earley item,
plus a fixed overhead.
The fixed overhead is \Oc{}
and is charged to the parse.
The time and space per Earley item
is \Oc{}
because the number of
rules into which another rule must be rewritten,
and therefore the number of Earley items
into which another Earley item must be rewritten,
is a constant that depends
on the grammar.
\end{proof}

\begin{theorem}\label{t:O1-time-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item,
and each attempt to
add a duplicate Earley item,
requires \Oc{} time.
\end{theorem}

\begin{theorem}\label{t:O1-space-per-eim}
All space in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item
requires \Oc{} space and,
if links are not considered,
each attempt to add a duplicate
Earley item adds no additional space.
\end{theorem}

\begin{theorem}\label{t:O1-links-per-eim}
If links are considered,
all space in \Marpa{} can be allocated
to the Earley items
in such a way that each Earley item
and each attempt to
add a duplicate Earley item
requires \Oc{} space.
\end{theorem}

\begin{proof}[Proof of Theorems
\ref{t:O1-time-per-eim},
\ref{t:O1-space-per-eim},
and \ref{t:O1-links-per-eim}]
These theorems follows from the observations
in Section \ref{s:pseudocode}
and from Lemma \ref{l:nulling-rewrite}.
\end{proof}

\subsection{Duplicate dotted rules}

The same complexity results apply to \Marpa{} as to \Leo,
and the proofs are very similar.
\Leo's complexity results\cite{Leo1991}
are based on charging
resource to Earley items,
as were the results
in Earley's paper\cite{Earley1970}.
But both assume that there is one dotted rule
per Earley item,
which is not the case with \Marpa.

\Marpa's Earley items group dotted rules into AHFA
states, but this is not a partitioning in the strict
sense -- dotted rules can fall into more than one AHFA
state.
This is an optimization,
in that it allows dotted rules,
if they often occur together,
to be grouped together aggressively.
But it opens up the possibility
that, in cases where \Earley{} and \Leo{} disposed
of a dotted rule once and for all,
\Marpa{} might have to deal with it multiple times.

From an efficiency perspective,
\Marpa's duplicate rules
are by all the evidence, a plus.
And they do not change the complexity results,
although the price of showing this is the
theoretical apparatus of this section.

\begin{theorem}\label{t:marpa-O-leo}
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} < \var{c} \times \Rtablesize{\Leo}$,
}
\end{equation*}
where \var{c} is a constant that depends on the grammar.
\end{theorem}

\begin{proof}
We know from Theorem \ref{t:table-correct}
that every Marpa Earley item corresponds to one of
\Leo's traditional Earley items.
If an EIM corresponds to an EIMT,
the AHFA state of the EIM contains the
EIMT's dotted rule,
while their origins are identical.
Even in the worst case, a dotted rule cannot
appear in every AHFA state,
so that
the number of Marpa items corresponding to a single
traditional Earley item must be less
than $\size{\Cfa}$.
Therefore,
\begin{equation*}
    \Rtablesize{\Marpa} < \size{\Cfa} \times \Rtablesize{\Leo}\qedhere
\end{equation*}
\end{proof}

Earley\cite{Earley1970} shows that,
for unambiguous grammars,
every attempt to add
an Earley item will actually add one.
In other words, there will be no attempts to
add duplicate Earley items.
Earley's proof shows that for each attempt
to add a duplicate,
the causation must be different --
that the EIMT's causing the attempt
differ in either their dotted
rules or their origin.
Multiple causations for an Earley item
would mean multiple derivations
for the sentential form that it represents.
That in turn would mean that
the grammar is ambiguous,
contrary to assumption.

In \Marpa, there is an slight complication.
A dotted rule can occur in more than one AHFA
state.
Because of that,
it is possible that two of \Marpa's
operations to add an EIM
will represent identical Earley causations,
and therefore will be
consistent with an unambiguous grammar.
Dealing with this complication requires us
to prove a result that is weaker than that of \cite{Earley1970},
but that is
still sufficient to produce the same complexity results.

\begin{theorem}\label{t:tries-O-eims}
For an unambiguous grammar,
the number of attempts to add
Earley items will be less than or equal to
\begin{equation*}
\textup{
    $\var{c} \times \Rtablesize{\Marpa}$,
}
\end{equation*}
where \var{c} is a constant
that depends on the grammar.
\end{theorem}

\begin{proof}
Let \var{initial-tries} be the number of attempts to add the initial item to
the Earley sets.
For Earley set 0, it is clear from the pseudocode
that there will be no attempts to add duplicate EIM's:
\begin{equation*}
\var{initial-tries} = \bigsize{\Vtable{0}}
\end{equation*}

Let \var{leo-tries} be the number of attempted Leo reductions in
Earley set \Vloc{j}.
For Leo reduction,
we note that by its definition,
duplicate attempts at Leo reduction cannot occur.
Let \var{max-AHFA} be the maximum number of
dotted rules in any AHFA state.
From the pseudo-code of Sections \ref{p:reduce-one-lhs}
and \ref{p:leo-op},
we know there will be at most one Leo reduction for
each each dotted rule in the current Earley set,
\Vloc{j}.
\begin{equation*}
\var{leo-tries} \le \var{max-AHFA} \times \bigsize{\Vtable{j}}
\end{equation*}

Let \var{scan-tries} be the number of attempted scan operations in
Earley set \Vloc{j}.
Marpa attempts a scan operation,
in the worst case,
once for every EIM in the Earley set
at $\Vloc{j} \subtract 1$.
Therefore, the number of attempts
to add scans
must be less than equal to \bigsize{\Etable{\var{j} \subtract 1}},
the number
of actual Earley items at
$\Vloc{j} \subtract 1$.
\begin{equation*}
\var{scan-tries} \le \bigsize{\Etable{\var{j} \subtract 1}}
\end{equation*}

Let \var{predict-tries} be the number of attempted predictions in
Earley set \Vloc{j}.
\Marpa{} includes prediction
in its scan and reduction operations,
and the number of attempts to add duplicate predicted EIM's
must be less than or equal
to the number of attempts
to add duplicate confirmed EIM's
in the scan and reduction operations.
\begin{equation*}
\var{predict-tries} \le \var{reduction-tries} + \var{scan-tries}
\end{equation*}

The final and most complicated case is Earley reduction.
Recall that \Ves{j} is the current Earley set.
Consider the number of reductions attempted.
\Marpa{} attempts to add an Earley reduction result
once for every triple
\begin{equation*}
[\Veim{predecessor}, \Vsym{transition}, \Veim{component}].
\end{equation*}
where
\begin{equation*}
\begin{split}
& \Veim{component} = [ \Vah{component}, \Vloc{component-origin} ]  \\
\land \quad & \Vdr{component} \in \Vah{component} \\
 \land \quad & \Vsym{transition} = \LHS{\Vdr{component}}. \\
\end{split}
\end{equation*}

We now put an upper bound on number of possible values of this triple.
The number of possibilities for \Vsym{transition} is clearly at most
\size{\var{symbols}},
the number of symbols in \Cg{}.
We have $\Veim{component} \in \Ves{j}$,
and therefore there are at most
$\bigsize{\Ves{j}}$ choices for \Veim{component}.

\begin{sloppypar}
We can show that the number of possible choices of
\Veim{predecessor} is at most
the number of AHFA states, \Vsize{fa}, by a reductio.
Suppose, for the reduction,
there were more than \Vsize{fa} possible choices of \Veim{predecessor}.
Then there are two possible choices of \Veim{predecessor} with
the same AHFA state.
Call these \Veim{choice1} and \Veim{choice2}.
We know, by the definition of Earley reduction, that
$\Veim{predecessor} \in \Ves{j}$,
and therefore we have
$\Veim{choice1} \in \Ves{j}$ and
$\Veim{choice2} \in \Ves{j}$.
Since all EIM's in an Earley set must differ,
and
\Veim{choice1} and \Veim{choice2} both have the same
AHFA state,
they must differ in their origin.
But two different origins would produce two different derivations for the
reduction, which would mean that the parse was ambiguous.
This is contrary to the assumption for the theorem
that the grammar is unambiguous.
This shows the reductio
and that the number of choices for \Veim{predecessor},
compatible with \Vorig{component}, is as most \Vsize{fa}.
\end{sloppypar}

\begin{sloppypar}
Collecting the results we see that the possibilities for
each \Veim{component} are
\begin{equation*}
\begin{alignedat}{2}
& \Vsize{fa} &&
\qquad \text{choices of \Veim{predecessor}} \\
\times \; & \Vsize{symbols} &&
\qquad \text{choices of \Vsym{transition}} \\
\times \; & \size{\Ves{j}} &&
\qquad \text{choices of \Veim{component}} \\
\end{alignedat}
\end{equation*}
\end{sloppypar}

The number of reduction attempts will therefore be at most
\begin{equation*}
\var{reduction-tries} \leq \Vsize{fa} \times \Vsize{symbols} \times \bigsize{\Ves{j}}.
\end{equation*}

Summing
\begin{multline*}
\var{tries} =
\var{scan-tries} +
\var{leo-tries} + \\
\var{predict-tries} +
\var{reduction-tries} +
\var{initial-tries},
\end{multline*}
we have,
where $\var{n} = \Vsize{\Cw}$,
the size of the input,
\begin{equation*}
\begin{alignedat}{2}
& \bigsize{\Vtable{0}} & \quad &
\qquad \text{initial EIM's} \\
+ \; & \sum\limits_{i=0}^{n}{
\var{max-AHFA} \times \bigsize{\Vtable{j}}
} &&
\qquad \text{LIM's} \\
+ \; & 2 \times \sum\limits_{i=1}^{n}{
\bigsize{\Etable{\var{j} \subtract 1}}
} &&
\qquad \text{scanned EIM's} \\
+ \; & 2 \times \sum\limits_{i=0}^{n}{\Vsize{fa} \times \Vsize{symbols} \times \bigsize{\Ves{j}}} &&
\qquad \text{reduction EIM's}.
\end{alignedat}
\end{equation*}
In this summation,
\var{prediction-tries} was accounted for by counting the scanned and predicted
EIM attempts twice.
Since \var{max-AHFA} and \Vsize{symbols} are both constants
that depend only on \Cg{},
if we collect the terms of the summation,
we will find a constant \var{c}
such that
\begin{equation*}
\var{tries} \leq \var{c} \times \sum\limits_{i=0}^{n}{\bigsize{\Vtable{j}}},
\end{equation*}
and
\begin{equation*}
\var{tries} \leq \var{c} \times \Rtablesize{\Marpa},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.\qedhere
\end{proof}

As a reminder,
we follow tradition by
stating complexity results in terms of \var{n},
setting $\var{n} = \Vsize{\Cw}$,
the length of the input.

\begin{theorem}\label{t:eim-count}
For a context-free grammar,
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} = \order{\var{n}^2}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
By Theorem \ref{t:es-count},
the size of the Earley set at \Vloc{i}
is $\order{\var{i}}$.
Summing over the length of the input,
$\Vsize{\Cw} = \var{n}$,
the number of EIM's in all of \Marpa's Earley sets
is
\begin{equation*}
\sum\limits_{\Vloc{i}=0}^{\var{n}}{\order{\var{i}}}
= \order{\var{n}^2}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:ambiguous-tries}
For a context-free grammar,
the number of attempts to add
Earley items is $\order{\var{n}^3}$.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries-O-eims},
we see that the only bound that required
the assumption that \Cg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Let \var{other-tries}
be attempts to add EIM's other than
as the result of Earley reductions.
By Theorem \ref{t:eim-count},
\begin{equation*}
\Rtablesize{\Marpa} = \order{\var{n}^2},
\end{equation*}
and by Theorem \ref{t:tries-O-eims},
\begin{equation*}
\var{other-tries} \le \var{c} \times \Rtablesize{\Marpa},
\end{equation*}
so that
$\var{other-tries} = \order{\var{n}^2}$.

\begin{sloppypar}
Looking again at \var{reduction-tries}
for the case of ambiguous grammars,
we need to look again at the triple
\begin{equation*}
[\Veim{predecessor}, \Vsym{transition}, \Veim{component}].
\end{equation*}
We did not use the fact that the grammar was unambigous in counting
the possibilities for \Vsym{transition} or \Veim{component}, but
we did make use of it in determining the count of possibilities
for \Veim{predecessor}.
We know still know that
\begin{equation*}
\Veim{predecessor} \in \Ves{component-origin},
\end{equation*}
where
\Vloc{component-origin} is the origin of \Veim{component}.
Worst case, every EIM in \Ves{component-origin} is a possible
match, so that
the number of possibilities for \Veim{predecessor} now grows to
\size{\Ves{component-origin}}, and
\begin{equation*}
\var{reduction-tries} =
\bigsize{\Ves{component-origin}} \times \Vsize{symbols} \times \bigsize{\Ves{j}}.
\end{equation*}
\end{sloppypar}

In the worst case $\var{component-origin} \simeq \var{j}$,
so that by Theorem \ref{t:es-count},
\begin{equation*}
\size{\Ves{component-origin}} \times \size{\Ves{j}} = \order{\var{j}^2}.
\end{equation*}
Adding \var{other-tries}
and summing over the Earley sets,
we have
\begin{equation*}
\order{\var{n}^2} +
\! \sum\limits_{\Vloc{j}=0}^{n}{\order{\var{j}^2}} = \order{\var{n}^3}.
\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:leo-right-recursion}
Either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let the constant \var{c} be the number
of symbols.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
(Since for the purposes of these
complexity results we ignore nulling symbols,
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.)
So part of the rightmost derivation must take the form
\begin{equation*}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation*}
But the first step of this derivation sequence must use a rule of the
form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

\subsection{The complexity results}
We are now in a position to show
specific time and space complexity results.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
the number of traditional Earley items produced by
\Leo{} when parsing input \Cw{} with an LR-regular grammar \Cg{} is
\begin{equation*}
\order{\Vsize{\Cw}} = \order{\var{n}}.
\end{equation*}
\Marpa{} may produce more Earley items than \Leo{}
for two reasons:
First, \Marpa{} does not apply Leo memoization to Leo sequences
which do not contain right recursion.
Second, \Marpa{}'s Earley items group dotted rules into
states and this has the potential to increase the number
of Earley items.

By theorem TODO-DELETED,
the definition of an EIMT,
and the construction of a Leo sequence,
it can be seen that a Leo sequence
corresponds step-for-step with a
right derivation.
It can therefore be seen that
the number of EIMT's in the Leo sequence
and the number of right derivation steps
in its corresponding right derivation
will be the same.

Consider one EIMT that is memoized in \Leo{}.
By theorem DELETED it corresponds to
a single dotted rule, and therefore a single rule.
If not memoized because it is not a right recursion,
this EIMT will be expanded to a sequence
of EIMT's.
How long will this sequence of non-memoized EIMT's
be, if we still continue to memoize EIMT's
which correspond to right recursive rules?
The EIMT sequence, which was formerly a memoized Leo sequence,
will correspond to a right
derivation that does not include
any steps that use right recursive rules.
By Theorem \ref{t:leo-right-recursion},
such a
right derivation can be
of length at most \var{c1},
where \var{c1} is a constant that depends on \Cg{}.
As noted, this right derivation has
the same length as its corresponding EIMT sequence,
so that each EIMT not memoized in \Marpa{} will expand
to at most \var{c1} EIMT's.

By Theorem \ref{t:marpa-O-leo},
when EIMT's are replaced with EIM's,
the number of EIM's \Marpa{} requires is at worst,
$\var{c2}$ times the number of EIMT's,
where \var{c2} is a constant that depends on \Cg{}.
Therefore the number of EIM's per Earley set
for an LR-regular grammar in a \Marpa{} parse
is less than
\begin{equation*}
    \var{c1} \times \var{c2} \times \order{\var{n}} = \order{\var{n}}.
\end{equation*}

LR-regular grammar are unambiguous, so that
by Theorem \ref{t:tries-O-eims},
the number of attempts that \Marpa{} will make to add
EIM's is less than or equal to
\var{c3} times the number of EIM's,
where \var{c3} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is
\begin{equation*}
    \var{c3} \times \order{\var{n}}
    = \order{\var{n}}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
By assumption, \Cg{} is unambiguous, so that
by Theorem \ref{t:tries-O-eims},
and Theorem \ref{t:eim-count},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{}
for unambiguous grammars is \order{\var{n}^2}.
\end{proof}

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-time-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\begin{theorem}\label{t:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-space-per-eim}
and Theorem \ref{t:eim-count}.
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{t:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-links-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\section{The Marpa Input Model}
\label{s:input}

In this \doc{},
up to this point,
the traditional input stream model
has been assumed.
As implemented,
Marpa generalizes the idea of
input streams beyond the traditional
model.

Marpa's generalized input model
replaces the input \Cw{}
with a set of tokens,
\var{tokens},
whose elements are triples of symbol,
start location and length:
\begin{equation*}
    [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
$\var{length} \ge 1$
and
$\Vloc{start} \ge 0$.
The size of the input, \size{\Cw},
is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

Multiple tokens can start at a single location.
(This is how \Marpa{} supports ambiguous tokens.)
The variable-length,
ambiguous and overlapping tokens
of \Marpa{}
bend the conceptual framework of ``parse location''
beyond its breaking point,
and a new term for parse location is needed.
Start and end of tokens are described in terms
of \dfn{earleme} locations,
or simply \dfn{earlemes}.
Token length is also measured in earlemes.

Like standard parse locations, earlemes start at 0,
and run up to \size{\Cw}.
Unlike standard parse locations,
there is not necessarily a token ``at'' any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
properly inside at least one token.
Here ``properly inside'' means after the token's start earleme
and before the token's end earleme.

In the Marpa input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is, for all \Vloc{i} such
that $0 \le \Vloc{i} < \size{\Cw}$,
there must exist
\begin{equation*}
	 \var{token} = [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
\begin{gather*}
	 \var{token} \in \var{tokens} \quad \text{and} \\
	 \Vloc{start} \le \Vloc{i} < \Vloc{start}+\var{length}.
\end{gather*}

The intent of Marpa's generalized input model is to allow
users to define alternative input models for special
applications.
An example that arises in current practice is natural
language, features of which are most
naturally expressed with ambiguous tokens.
The traditional input stream can be seen as the special case of
the Marpa input model where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength},
if we have both of
\begin{align*}
    [\Vsym{x}, \Vloc{x}, \var{xlength}] & \in \var{tokens} \quad \text{and} \\
    [\Vsym{y}, \Vloc{y}, \var{ylength}] & \in \var{tokens},
\end{align*}
then we have both of
\begin{gather*}
\var{xlength} = \var{ylength} = 1 \quad \text{and} \\
     \Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}.
\end{gather*}

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
restrictions must be imposed.
In stating them,
let it be understood that
\begin{equation*}
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \in \var{tokens}
\end{equation*}
We require that,
for some constant \var{c},
possibly dependent on the grammar \Cg{},
that every token length be less than \var{c},
\begin{equation}
\label{e:restriction1}
\forall \, \Vtoken{[\Vsym{x}, \Vloc{x}, \var{length}]},
\; \var{length} < \var{c},
\end{equation}
and that
the cardinality of the set of tokens starting at any
one location
be less than \var{c},
\begin{equation}
\label{e:restriction2}
 \forall \Vloc{i}, \;
 \Bigl|
 \bigl \lbrace
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \bigm|
	\Vloc{x} = \Vloc{i}
  \bigr \rbrace
  \Bigr| < \var{c}
\end{equation}
Restrictions \ref{e:restriction1}
and \ref{e:restriction2}
impose little or no obstacle
to the practical use
of Marpa's generalized input model.
And with them,
the complexity results for \Marpa{} stand.

\section{Acknowledgments}
\label{p:Acknowledgments}

Ruslan Shvedov
and
Ruslan Zakirov
made many useful suggestions
that are incorporated in this paper.
Many members of the Marpa community
have helped me in many ways,
and it is risky
to single out one of them.
But Ron Savage
has been unstinting in
his support.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Culik1973}
{\v{C}}ulik, Karel and Cohen, Rina.
\newblock LR-Regular grammarsan extension of LR (k) grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Timeline}
Jeffrey~Kegler.
\newblock Parsing:~a~timeline.
\newblock Version 3.1, 2019.
\newblock \url{https://jeffreykegler.github.io/personal/timeline_v3}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}
