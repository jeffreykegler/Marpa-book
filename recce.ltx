% Copyright 2013 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{amssymb}

\delimitershortfall=0pt
\delimiterfactor=1100

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\var{#1}}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

\newcommand{\Aref}[1]{(a\ref{#1}:\pageref{#1})}
\newcommand{\Eref}[1]{(e\ref{#1}:\pageref{#1})}
\newcommand{\Lref}[1]{(L\ref{#1}:\pageref{#1})}
\newcommand{\Sref}[1]{(s\ref{#1}:\pageref{#1})}
\newcommand{\Tref}[1]{(t\ref{#1}:\pageref{#1})}
\newcommand{\Thref}[1]{(Th\ref{#1}:\pageref{#1})}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mapsto}
\newcommand{\nderives}[1]{
\mathrel{%
  \ifthenelse{\equal{#1}{}}{%
    {\not\mapsto}%
  }{%
    {\mbox{$\:\stackrel{\!{#1}}{\not\mapsto\!}\:$}}%
  }%
}}%
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\mapsto\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\mapsto\!}\:$}}}
\newcommand{\ndestar}{\nderives{\ast}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\mapsto\!}\:$}}}
\newcommand{\ndeplus}{\nderives{+}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\mapsto\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\mapsto\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\Tvar}[2]{\ensuremath{\var{#1}_{\var{#2}}}}
\newcommand{\Tvarset}[2]{\ensuremath{\var{#1}_{\set{\var{#2}}}}}
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }

\newcommand{\Vbool}[1]{\Tvar{#1}{BOOL}}
\newcommand{\Vcfg}[1]{\Tvar{#1}{CFG}}
\newcommand{\Vdr}[1]{\Tvar{#1}{DR}}
\newcommand{\Vdrset}[1]{\Tvarset{#1}{DR}}
\newcommand{\Veim}[1]{\Tvar{#1}{EIM}}
\newcommand{\Veimset}[1]{\Tvarset{#1}{EIM}}
\newcommand{\Ves}[1]{\Tvar{#1}{ES}}
\newcommand{\Vesset}[1]{\Tvarset{#1}{ES}}
\newcommand{\Vext}[1]{\Tvar{#1}{EXT}}
\newcommand{\Vint}[1]{\Tvar{#1}{INT}}
\newcommand{\Vloc}[1]{\Tvar{#1}{LOC}}
\newcommand{\Vlim}[1]{\Tvar{#1}{LIM}}
\newcommand{\Vpim}[1]{\Tvar{#1}{PIM}}
\newcommand{\Vpimset}[1]{\Tvarset{#1}{PIM}}
\newcommand{\Vrule}[1]{\Tvar{#1}{RULE}}
\newcommand{\Vruleset}[1]{\Tvarset{#1}{RULE}}
\newcommand{\str}[1]{\ensuremath{\langle\langle{#1}\rangle\rangle}}
\newcommand{\Vstr}[1]{\str{\var{#1}}}
\newcommand{\Vstrset}[1]{\Tvarset{#1}{STR}}
\newcommand{\sym}[1]{\ensuremath{\langle{#1}\rangle}}
\newcommand{\Vsym}[1]{\sym{\var{#1}}}
\newcommand{\Vsymset}[1]{\Tvarset{#1}{SYM}}
\newcommand{\Vorig}[1]{\Tvar{#1}{ORIG}}
\newcommand{\Vtoken}[1]{\Tvar{#1}{TOKEN}}

\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}

% types to be deleted with A&H LR(0) stuff
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}

% TODO to be deleted with A&H LR(0) stuff
\newcommand{\AH}{\ensuremath{\alg{AH}}}

% TODO to be deleted with A&H LR(0) stuff
\newcommand{\Cfa}{\var{fa}}

\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\Vsym{\Cw[\var{#1}]}}}
\newcommand{\NT}[1]{\ensuremath{\mymathop{NT}(#1)}}
\newcommand{\Term}[1]{\ensuremath{\mymathop{Term}(#1)}}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}
\newcommand{\GOTO}{\ensuremath{\mymathop{GOTO}}}
\newcommand{\Next}[1]{\ensuremath{\mymathop{Next}(#1)}}
\newcommand{\Predict}[1]{\ensuremath{\mymathop{Predict}(#1)}}
\newcommand{\Postdot}[1]{\ensuremath{\mymathop{Postdot}(#1)}}
\newcommand{\Penult}[1]{\ensuremath{\mymathop{Penult}(#1)}}
\newcommand{\LHS}[1]{\ensuremath{\mymathop{LHS}(#1)}}
\newcommand{\RHS}[1]{\ensuremath{\mymathop{RHS}(#1)}}
\newcommand{\RightRecursive}[1]{\ensuremath{\mymathop{Right-Recursive}(#1)}}
\newcommand{\Rightmost}[1]{\ensuremath{\mymathop{Rightmost}(#1)}}
\newcommand{\LeoEligible}[1]{\ensuremath{\mymathop{Leo-Eligible}(#1)}}
\newcommand{\LeoUnique}[1]{\ensuremath{\mymathop{Leo-Unique}(#1)}}
\newcommand{\LIMPredecessor}[1]{\ensuremath{\mymathop{LIM-Predecessor}(#1)}}
\newcommand{\ID}[1]{\ensuremath{\mymathop{ID}(#1)}}
\newcommand{\PSL}[2]{\ensuremath{\mymathop{PSL}[#1][#2]}}
\newcommand{\myL}[1]{\ensuremath{\mymathop{L}(#1)}}
\newcommand\Etable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\Rtable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\Rtablesize[1]{\ensuremath{\left| \mymathop{table}[#1] \right|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\mymathop{table}[#1,#2]}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}[theorem]{Observation}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

\begin{document}

\date{\today}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2019 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
The Marpa recognizer is described.
Marpa is
a fully implemented
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa is an Earley parser
which includes the first
practical implementation
of the improvements
in Joop Leo's 1991 paper.
In addition,
Marpa tracks the full state of the parse,
at run-time,
in a form accessible by the application.
This greatly improves error detection;
enables event-driven parsing;
and allows techniques such as
``Ruby Slippers'' parsing,
in which
the input is altered in response
to the parser's expectations.
\end{abstract}

\maketitle

\section{Introduction}

The Marpa project was intended to create
a practical and highly available tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite an encouraging academic literature,
no such tool had existed for context-free parsing.

The first stable version of Marpa was uploaded to
a public archive on Solstice Day 2011.
This revision of this paper
describes the algorithm used
in the most recent version of Marpa,
Marpa::R2~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in the first version of this paper.

While the presentation in this paper is theoretical,
the approach is practical.
The Marpa::R2 implementation has been widely available
for some time,
and has seen considerable use.

In this paper,
internal references will be of the form
({\it Xn:p}),
where {\it X} is the reference type,
{\it n} is the number of the referent,
and
{\it p} is a page reference.
Reference types are
``a'' (algorithm),
``e'' (equation),
``s'' (section)
``L'' (a location within a section,
where {\it n} is the section number),
``t'' (table), and
``Th'' (theorem).
For example
\Eref{e:restriction1} refers to equation
\ref{e:restriction1} on page \pageref{e:restriction1};
\Sref{s:features} refers to
section \ref{s:features}
on page \pageref{s:features};
and
\Lref{loc:brick-depth} refers
to page \pageref{loc:brick-depth}
inside section \ref{loc:brick-depth}.

\Sref{s:features}
describes the important features of Marpa
and
\Sref{s:using-features}
goes into detail about their application.
\Sref{s:preliminaries} describes the notation and conventions
of this paper.
\Sref{s:CFGs} defines context-free grammars.
\Sref{s:EXTs} introduces Marpa's external grammars ---
the grammars which the users see.
\Sref{s:improper} describes Marpa's
implementation of improper context free grammars.
\Sref{s:rewrite} explain how Marpa rewrites its external
grammars into internal grammars, and
\Sref{s:INTs} describes Marpa's internal grammars.
\Sref{s:earley-table} describes the data structures used
in parsing Marpa internal grammars.
\Sref{s:earley}
describes Earley's algorithm.
\Sref{s:leo} describes Leo's modification
to Earley's algorithm.
\Sref{s:algorithm}
describes the Marpa algorithm.
\Sref{s:per-set-lists} describes Marpa's
per-set lists.
while \Sref{s:complexity} contains
its complexity results.
Finally,
section \Sref{s:input}
generalizes Marpa's input model.

A full description of Marpa's relationship to
prior work in parsing can be found in \cite{Timeline}.
Readers may find it helpful to read
\cite{Timeline}
before this paper.

\section{Features}
\label{s:features}

\subsection{General context-free parsing}
The Marpa algorithm is capable of parsing all
context-free grammars.
As implemented,
Marpa parses
all cycle-free context-free
grammars.\footnote{
Earlier implementations of Marpa allowed cycles,
but support for them
was removed because
practical interest seems to be non-existent.
For more detail, see section
\ref{s:improper}.
}
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},
and therefore never worse than $\order{\var{n}^3}$.

\subsection{Linear time for practical grammars}
Currently, the grammars suitable for practical
use are thought to be a subset
of the deterministic context-free grammars (DCFG's).
\Marpa{} uses a technique discovered by
Leo~\cite{Leo1991}
to run in
\On{} time for LR-regular grammars,
a superset of the the DCFG's.
\Marpa{}
also parses many ambiguous grammars in linear
time.

\subsection{Left-eidetic}
The original Earley algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
\Marpa{}'s parse engine makes available
its
information about the state of the parse so far.
This information is
in useful form,
and can be accessed efficiently.

\subsection{Recoverable from read errors}
When
\Marpa{} reads a token which it cannot accept,
the error is fully recoverable.
An application can attempt to read another
token,
repeatedly if that is desirable.
Once the application provides
a token that is accepted by the parser,
parsing will continue
as if the unsuccessful read attempts had never been made.

\subsection{Ambiguous tokens}
\Marpa{} allows ambiguous tokens.
These are often useful in natural language processing
where, for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
recovery from rejected tokens so that,
for example, an application could react to the
rejection of a token by reading two others.

\section{Using the features}
\label{s:using-features}

\subsection{Error reporting}
An obvious application of left-eideticism is error
reporting.
\Marpa{}'s abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar.
\Marpa{}, as currently implemented,
can detect an ambiguity and report
specifically where it occurred
and what the alternatives were.

\subsection{Event driven parsing}
As implemented,
Marpa::R2~\cite{Marpa-R2}
allows the user to define ``events''.
Events can be defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\subsection{Ruby slippers parsing}
Left-eideticism, efficient error recovery,
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
In traditional parser practice,
error detection is an act of desperation.
In contrast,
\Marpa{}'s error detection is so painless
that it can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of Marpa::R2~\cite{Marpa-R2},
the author has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\subsection{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very different from the practice in the languages humans choose
when communicating with each other.
For example,
the language of this paper, English, is notoriously
ambiguous.
Human languages exploit ambiguity in order to allow expression
to be more flexible and powerful.

Ambiguity of course can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Consider, for comparison purposes,
the standard treatment of syntax errors.
Syntax errors are undesired, but nobody tries
to design languages that make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be not just cumbersome, but dangerous.
A parser would never warn the user about typos,
because all typos in such a language would have unintended
meanings.

With \Marpa{}, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
A \Marpa{}-powered
language can be designed to allow ambiguity,
and any actual ambiguity will be detected
and reported at parse time.
This exploits \Marpa{}'s ability
to report exactly where
and what the ambiguity is.
Marpa::R2's own parser description language, the SLIF,
uses ambiguity in this way.

\subsection{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LR-regular languages
opens the way to auto-generated languages.
In particular,
\cite{Culik1973} notes that a parser which
can parse any LR-regular language will be
able to parse a language generated using syntax macros.

\subsection{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
which are useful for second-order programming.
True second-order languages --- languages which
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With Marpa, this barrier is raised.
As an example,
Marpa::R2's own parser description language, the SLIF,
allows ``precedenced rules''.
Precedenced rules are specified in an extended BNF.
The BNF extensions allow precedence and associativity
to be specified for each RHS.

Marpa::R2's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to create a BNF grammar which is equivalent,
and which has the desired precedence.
Essentially,
the SLIF does a standard parsing textbook transformation.
The transformation starts
with a set of rules,
each of which has a precedence and
an associativity specified.
The result of the transformation is a set of
rules in pure BNF.
The SLIF's advantage is that it is powered by Marpa,
and therefore the SLIF can ensure that the grammar
that it auto-generates will
be parsed in linear time.

Notationally, Marpa's precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison.
In the SLIF,
there are two important differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity greater than three.

Second, and more important, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies.
The user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\section{Preliminaries}
\label{s:preliminaries}

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.

This paper will
use subscripts to indicate commonly occurring types,
as shown in
\Tref{tab:type-notation}
\begin{table}[tb]
\centering
\caption{Type notations}
\begin{tabular}{ll}
\hline
$\var{X}_\var{T}$ & The variable \var{X} of type \type{T} \\
$\var{set-one}_\set{\var{T}}$ & The variable \var{set-one} of type ``set of \type{T}'' \\
\type{SYM} & The type for a symbol \\
\type{STR} & The type for a string \\
\type{EIM} & The type for an Earley item \\
\type{RULE} & The type for the rule of a grammar\\
\Tvar{a}{SYM} & A variable \var{a} of type \type{SYM}, subscripted form \\
\Vsym{a} & A variable \var{a} of type \type{SYM}, angle bracket form \\
\Tvar{a}{STR} & A variable \var{a} of type \type{STR}, subscripted form \\
\Vstr{a} & A variable \var{a} of type \type{STR}, angle bracket form \\
\Veim{a} & A variable \var{a} of type \type{EIM} \\
\Vrule{a} & A variable \var{a} of type \type{RULE} \\
\Vsymset{set-two} & The variable \var{set-two}, a set of strings \\
\Vstrset{strings} & The variable \var{strings}, a set of strings \\
\Veimset{set-three} & The variable \var{set-three}, a set of Earley items \\
\Vruleset{set-four} & The variable \var{set-four}, a set of grammar rules \\
\hline
\end{tabular}
\label{tab:type-notation}
\end{table}
Subscripts may be omitted when the type
is obvious from the context.
Symbols and strings
have an alternate type notation,
which uses angle bracket as an alternative to
the subscripting.
Type names are often used in the text
as a convenient way to refer to
their type.

Multi-character variable names will be common.
When the angle bracket notation is used,
concatenation of strings and symbols may be implicit.
Otherwise, operations are never implicit
\Tref{tab:op-notation}.
\begin{table}[tb]
\centering
\caption{Selected operator notations}
\begin{tabular}{ll}
\hline
Multiplication &  $\var{a} \times \var{b}$ \\
Concatenation & $\var{a} \cat \var{b}$ \\
Concatenation & $\Vstr{a} \Vsym{b}$ \\
Subtraction & $\var{symbol-count} \subtract \var{terminal-count}$ \\
\hline
\end{tabular}
\label{tab:op-notation}.
\end{table}

Where \var{set} is a set,
\Vsize{set} is the size of the set.
Where \var{seq} is a sequence (ordered set),
$\var{seq}[\var{i}]$ is its \var{i}'th element,
$0 \le \var{i} \le \Vsize{seq}$.
We will write
$\var{s}[\var{a} \ldots \var{z}]$
for the subsequence
\[
    \var{s}[\var{a}], \;\;
    \var{s}[\var{a}+1], \;\;
    \ldots \;\;
    \var{s}[\var{z}].
\]

Let \Vsymset{vocab} be a non-empty set of symbols.
Symbols are of type \type{SYM}.
Let \Tvar{x}{SYM} be a symbol.
More commonly we write
\Tvar{x}{SYM} as \Vsym{x}.

let $\var{vocab}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Let \Tvar{s}{STR} be a string.
More commonly we write
\Tvar{s}{STR} as \Vstr{s}.
\size{\Vstr{s}} is the length of \Vstr{s},
counted in symbols.
Let $\var{vocab}^+$ be
\begin{equation*}
\left\{ \Vstr{x}
\; \middle| \;
\Vstr{x} \in \var{vocab}^\ast
\; \land \;
\Vsize{\Vstr{x}} > 0
\right\}.
\end{equation*}

Typing in this paper is not strict.
As one example,
we will move freely between strings, and their equivalent sequences of symbols.
As another example,
when it is convenient,
we will treat strings of length 1 as symbols,
and symbols as strings of length 1.

\section{Context-free grammars}
\label{s:CFGs}

A \dfn{context free grammar} is type \type{CFG}.
Let
\begin{equation}
\var{g} = [\Vsymset{nt}, \Vsymset{term}, \Vruleset{rules}, \Vsym{accept}]
\end{equation}
be a 4-tuple.
\var{g} is a CFG, \Vcfg{g},
if and only if
\begin{gather*}
\Vsymset{nt} \cup \Vsymset{term} = \emptyset, \quad \text{and} \\
\Accept{\var{accept}} \in \Vsymset{nt}, \quad \text{and} \\
\text{\Vruleset{rules} is a set of elements of type \type{RULE}.}
\end{gather*}
We will define type \type{RULE} shortly.
For convenience,
for a CFG, \Vcfg{g},
we write
\Vcfg{g} (type \type{CFG}) is a 4-tuple
\begin{equation*}
\begin{alignedat}{3}
& \NT{\Vcfg{g}} && \defined && \quad \Vsymset{nt}, \\
& \Term{\Vcfg{g}} && \defined && \quad \Vsymset{term}, \\
& \Accept{\Vcfg{g}} && \defined && \quad \Vsym{accept}, \\
& \Rules{\Vcfg{g}} && \defined && \quad \Vruleset{rules}, \quad \text{and} \\
& \Vocab{\Vcfg{g}} && \defined && \quad \Vsymset{nt} \cup \Vsymset{term}.
\end{alignedat}
\end{equation*}
The elements of the set \NT{\var{g}}, are non-terminal symbols,
or simply \dfn{non-terminals}.
The elements of the set \Term{\var{g}}, are terminal symbols,
or simply \dfn{terminals}.
\Accept{\var{g}} is the \dfn{accept symbol},
and in the parsing literature is often called
the ``start symbol''.
\Vocab{\var{g}} is
the \dfn{vocabulary} of \Vext{g}.

A string of symbols from \Vocab{\var{g}}
is called a \dfn{sentential form}.
A string of symbols from \Term{\var{g}}
is called a \dfn{sentence}.

A rule
(type \type{RULE})
is a duple
\begin{equation*}
\left[ \Vsym{lhs} \de \Vstr{rhs} \right]
\end{equation*}
such that
\begin{equation*}
\Vsym{lhs} \in \NT{\var{g}} \; \land \;
\Vstr{rhs} \in \Vocab{\var{g}}^\ast.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\var{r}}$ and $\RHS{\var{r}}$, respectively.
The size of \var{r}, $\size{\Vrule{r}}$,
is the number of symbols on its RHS,
so that
\begin{equation*}
\size{\Vrule{r}} = \size{\RHS{\var{r}}}.
\end{equation*}

The rules imply the traditional rewriting system,
in which
\begin{align*}
\Vstr{x} \derives & \; \Vstr{y} \quad \defined &&
\text{\Vstr{x} derives \Vstr{y} in exactly one step.} \\
\Vstr{x} \deplus & \; \Vstr{y} \quad \defined &&
\text{\Vstr{x} derives \Vstr{y} in one or more steps.} \\
\Vstr{x} \destar & \; \Vstr{y} \quad \defined &&
\text{\Vstr{x} derives \Vstr{y} in zero or more steps.} \\
\Vstr{x} \xderives{\var{n}} & \; \Vstr{y} \quad \defined &&
\text{\Vstr{x} derives \Vstr{y} in exactly \var{n} steps.} \\
\Vstr{x} \xderives{0} & \; \Vstr{y} \quad \defined &&
\text{\Vstr{x} derives \Vstr{y} in exactly 0 steps.}
\end{align*}

Every symbol derives itself in zero steps.
The zero step derivation is called a \dfn{trivial derivation}.
A derivation of more than zero steps
is called a \dfn{non-trivial derivation}.

We regard a derivation as equivalent to a sequence of strings,
called its \dfn{derivation sequence}.
Where
\begin{equation*}
\var{d} = \Vstr{first} \destar \Vstr{last}
\end{equation*}
is a derivation and
\Vstrset{dseq} is the derivation sequence of \var{d},
then saying that
\begin{equation*}
\var{dseq}[\var{i}] = \Vstr{s}
\end{equation*}
is equivalent to saying that
\begin{equation*}
\var{d} = \Vstr{first} \xderives{\var{i}} \Vstr{s} \destar \Vstr{last}.
\end{equation*}

We say that symbol \Vsym{x} is \dfn{nullable} if and only if
$\Vsym{x} \destar \epsilon$.
\Vsym{x} is \dfn{nulling} if and only if it always derives the null
string, that is, for all \Vstr{y}
\begin{equation*}
\Vsym{x} \destar \Vstr{y} \implies \Vstr{y} \destar \epsilon.
\end{equation*}
\Vsym{x} is \dfn{non-nullable} if and only if it is not nullable.
\Vsym{x} is a \dfn{proper nullable} if and only if it is nullable,
but not nulling.

By tradition, the input to the parse is written as \Cw{}.
\Cw{} must be a sentence of \Vcfg{g},
that is,
$\Cw \in \Term{\var{g}}^\ast$.
\Vsize{w} will be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.

The language of \Vcfg{g},
\myL{\var{g}},
is the set of sentences that
that \Accept{\Vcfg{g}} derives.
That is,
\begin{equation*}
\myL{\Vcfg{g}} \defined
\set{
\quad
\begin{aligned}
& \Vstr{sentence} \quad \text{such that} \\
& \qquad \begin{aligned}
               & \Accept{\Vcfg{g}} \destar \Vstr{sentence} \\
      \land \quad & \Vstr{sentence} \in \Term{\Vcfg{g}}
      \end{aligned}
\end{aligned}
\quad
}
\end{equation*}

Above we defined ``sentence'' and ``sentential form''
more broadly than in some of the tradition.
When we wish to refer only to the sentences and sentential forms
allowed by a grammar, \Vcfg{g}, we will refer specifically to that
grammar.
Thus a ``sentence of \Vcfg{g}'' will be some \Vstr{sentence}
such that
\begin{equation*}
\Accept{\Vcfg{g}} \destar \Vstr{sentence} \; \land \; \Vstr{sentence} \in \Term{\Vcfg{g}}^\ast,
\end{equation*}
and a
``sentential form of \Vcfg{g}''.
will be some \Vstr{sf}
such that
\begin{equation*}
\Accept{\Vcfg{g}} \destar \Vstr{sf} \; \land \; \Vstr{sf} \in \Vocab{\Vcfg{g}}^\ast,
\end{equation*}

Locations in the input will be of type \type{LOC}.
We represent
the \var{i}'th character of the input,
$0 \le \Vloc{i} < \Vsize{w}$,
by \sym{\var{w}[\var{i}]},
and
\begin{equation*}
\str{\var{w}[\var{a} \ldots \var{z}]} \defined
\str{\lbrace
    \var{w}[\var{a}], \;\;
    \var{w}[\var{a}+1], \;\;
    \ldots \;\;
    \var{w}[\var{z}].
\rbrace}
\end{equation*}

\section{External grammars}
\label{s:EXTs}

An \dfn{external grammar} (type \type{EXT}) is cycle-free CFG.
A CFG is \dfn{cycle-free} if it contains no cycles.
A \dfn{cycle string}, or \dfn{cycle} is a sentential form of length 1
which non-trivially derives itself.
Intuitively, a cycle is the parsing equivalent of an infinite loop.

If \Vstr{cycle} is a cycle,
then $\size{\Vstr{cycle}} = 1$,
so that $\Vstr{cycle} = \Vsym{cycle}$.
The symbol \Vsym{cycle} is a \dfn{cycle symbol}.
A \dfn{cycle derivation} is any derivation of the form
\begin{equation*}
\label{eq:cycle}
 \Vsym{cycle} \deplus \Vsym{cycle}.
\end{equation*}
Cycle symbols and cycle derivations are also often called
simply ``cycles''.

Earlier implementations of Marpa have supported cycles,
but the current implementation of Marpa does not.
When the current implementation finds a cycle in a grammar,
it print a message describing the cycle and throws
a fatal error.
This seems to be exactly the behavior users desire.

Cycles are recursions but most recursions
are not cycles.
In a recursion, a symbol \Vsym{recurse} derives itself as part of a string.
That is, \Vsym{recurse} is recursive if and only if
\begin{equation*}
 \Vsym{recurse} \deplus \Vstr{pre} \cat \Vsym{recurse} \cat \Vstr{post}.
\end{equation*}
\Vsym{recurse} is a cycle only if
both \Vstr{pre} and \Vstr{post} are the null string:
\begin{equation*}
 \Vstr{pre} = \Vstr{post} = \epsilon.
\end{equation*}

There is no indication that cycles are of practical use.
Nor does the theoretical literature suggest potential uses for
them.
Support for cycles did require complicated extra code,
and removing the support of cycles seems to be cost-free.

When Marpa did support cycles,
it did so by cutting the cycle short at cycle depth 1.
Intuitively, the cycle depth is the number of times the cycle
string returns to itself.
More formally, if \Vstrset{d} is the derivation sequence
of the cycle derivation
$\Vsym{cycle} \xderives{\var{n}} \Vsym{cycle}$
then the cycle depth is
\begin{equation}
\left| \;
\left\lbrace \;
\var{i}
\quad \middle| \quad
\begin{gathered}
1 \le \var{i} \le \var{n} \quad \text{and} \; \\
\var{d}[\var{i}] = \Vsym{cycle}
\end{gathered}
\; \right\rbrace
\; \right|
\end{equation}
It follows that
a cycle of cycle depth 1 begins and ends with the
cycle string, but does not derive the cycle string
in any of its intermediate steps.

\section{Improper context free grammars}
\label{s:improper}

A \dfn{proper CFG} is a CFG with no
\begin{itemize}
\item no unproductive symbols;
\item no inaccessible symbols; and
\item no cycles.
\end{itemize}
A \dfn{improper CFG} is a CFG which is
not proper.
We discussed Marpa's handling of cycles earlier
(section \ref{s:EXTs}, p. \pageref{s:EXTs}).

\subsection{Unproductive symbols}

The current implementation of Marpa supports
unproductive symbols.
By default, Marpa treats an unproductive symbols
as a fatal error,
but the user may bypass the fatal error
by marking the unproductive symbol as a ``unicorn''.

A \dfn{productive symbol}
is one which derives a sentence.
That is, \Vsym{prod} is productive if
and only if, for some \Vstr{sentence},
\begin{equation*}
\Vstr{sentence} \in \Term{\var{g}}^\ast
\;  \land \;
  \Vsym{prod} \destar \Vstr{sentence}
\end{equation*}
The empty string is a sentence,
and therefore all nullable
symbols are productive.

A symbol is \dfn{unproductive} if it is not
productive.
In the current version of Marpa,
unproductive symbols can be implemented by
defining \dfn{unicorn} terminals.
A unicorn terminal is a terminal symbol
which can never be
found in the input.

In the Marpa implementation,
a terminal can be made into a unicorn by
requiring that
a terminal match a regular expression that does not match
any string in $\Vocab{\Vint{g}}^\ast$.
For example,
the POSIX expression
\texttt{[\char`^ \char`\\ D\char`\\ d]}
indicates that the terminal
must match a single character which
is both a digit and a non-digit.
This is, of course, not possible
and therefore any terminal required to match that POSIX expression
will never match anything in the input.

Unproductive symbols often prove useful in Marpa
applications.
One example of the utility of unproductive symbols
is interaction with custom lexers.
Marpa allows custom lexers, and these can feed lexemes
to the parser under application control,
observe the result,
and react accordingly.
The custom lexer can hand control back to Marpa's
own lexer at any point.

It is often convenient for Marpa to use its internal lexer
in most situations,
switching over to custom lexer only at certain points.
To do this, unicorn terminals can
be defined and placed in the grammar at points where the
custom lexer should take over.
Marpa will never recognize a unicorn in the input,
but it does keep track of when unicorns are expected.
Marpa can be configured to trigger a run-time event
when the unicorn is expected.
The application will receive control when the event occurs,
and it can then invoke the custom lexer.

\subsection{Inaccessible symbols}

The current implementation of Marpa supports inaccessible symbols.
The user can specify \Marpa{}'s treatment
of inaccessible symbols:
inaccessible symbols can be silently ignored,
they can be reported as warnings,
or they can be treated as fatal errors.
Treatment of inaccessible symbols can be specified
symbol by symbol,
and the default treatment can also be specified.
The ``factory default'' is
to report inaccessible symbols as warnings.

An \dfn{inaccessible symbol} is one which can never be derived from
the accept symbol.
That is, \Vsym{noacc} is inaccessible in \Vext{g} if and only if
\begin{equation*}
\neg \; \exists \; \Vstr{pre}, \Vstr{post} \; \mid \; \Accept{g} \destar \Vstr{pre} \Vsym{noacc} \Vstr{post}.
\end{equation*}

Inaccessible symbols have been found useful in Marpa applications
that use higher-level languages ---
languages which generate other languages.
Often the generated grammar consists of ``custom''
rules which vary,
and ``boilerplate'' rules,
which are the same in every generated grammar.
It can be useful to have sections of the ``boilerplate''
be optional,
so that some of the ``boilerplate'' rules are only used
if the ``custom'' rules call for them.
If the custom rules do not call for a set of optional rules
within the boilerplate,
those boilerplate rules become inaccessible.

Optional boilerplace can be implemented
by silencing inaccessibility errors for the symbols in the optional boilerplate.
Usually it is possible to locate a topmost symbol for
each section of optional boilerplace,
in which case the warning need only be silenced for those
topmost symbols.

\section{Rewriting the grammar}
\label{s:rewrite}

\subsection{The Aycock-Horspool parser}

A parser that claims to be practical must accept
grammars with nullable and nulling symbols
and empty rules.
But difficulties in handling these correctly
have bedeviled previous attempts
at Earley parsing.

Marpa follows Aycock and Horspool\cite{AH2002} in using
grammar rewrites to handle these issues.
Marpa's recognizer works using
Marpa internal grammars, but
Marpa's grammar rewriting allows the user to specify
their parse in terms of an external grammar.
In Marpa, parsing, evaluation and
run-time events are performed
in such a way that the internal grammar
is invisible to
the user.

We defined Marpa's external grammars in
\Sref{s:EXTs}.
Recall that an external grammar may be any cycle-free grammar.
Marpa internal grammars are described by construction
in this section.
They will be defined more carefully
in \Sref{s:INTs}.
This section describes the rewrite
from an external grammar
to an internal grammar.

Grammar rewriting was not the focus
of \cite{AH2002} ---
instead that paper centered on revising Earley's algorithm
to use LR(0) states.
The first version of Marpa \cite{Kegler2019}
used the LR(0) states of \cite{AH2002},
combining them with Joop Leo's improvements \cite{Leo1991}.

Adding the LR(0) states to the first version of Marpa
made its theory much more complex,
and the change offered
no complexity gains in terms of Landau notation.
The hope was that benefits would be seen in practice.

Unfortunately, these benefits did not emerge.
Marpa's experience in using, developing and maintaining
LR(0) states in \Marpa{}'s early versions showed
that LR(0) states added
greatly to the complexity of the code;
were an obstacle to the implementation of run-time features
in the parser;
and produced no real improvement in speed or in space requirements.
On the other hand,
Aycock and Horspool's idea
of using grammar rewriting
to deal with
with nullable symbols became
fundamental to the Marpa algorithm.

Rewriting grammars is a common
technique
in the parsing literature,
but many of these rewrites are almost totally without
usefulness in practice.
In the academic literature,
a rewritten grammar may recognize the same language,
but often it will not preserve the semantics
of the grammar.

Very few users of CFG's are
content with recognizing a language.
Almost always the user of a grammar-driven parser
formulated the rules and symbols of that grammar
in terms of a semantics,
and that user wants the parser
to preserve their semantics.
If a rewrite scrambles the user's semantics,
then that rewrite cannot play an essential role
in a general-purpose practical parsing tool.

Marpa restricts itself to using rewrites that
are \dfn{semantics-safe}.
The intent behind
semantics-safe rewrites
is that they
allow the rules and symbols of the external grammar
to be reconstructed from a parse using the internal grammar,
not just at evaluation time,
but at run-time as well.
\Sref{s:augmentation},
\Sref{s:eliminating-proper_nullables},
\Sref{s:removing-nulling-symbols},
and
\Sref{s:trivial-grammars}
describe the rewrites
applied by Marpa.
We will state Marpa's definition
of semantics-safe more precisely
in section \Sref{semantics-safe}.

\subsection{Augmenting the grammar}
\label{s:augmentation}

Let \Vint{g} be a Marpa internal grammar
such that $\Vsym{accept} = \Accept{\Vint{g}}$.
There must be
a dedicated acceptance rule for \Vint{g},
\begin{gather*}
\Vrule{accept} = [\Vsym{accept} \de \Vsym{top}], \\
\intertext{such that}
\Vsym{top} \in \NT{\Vint{g}}, \\
\Vrule{accept} \in \Rules{\Vint{g}}, \\
\intertext{and for all \Vrule{r}, $\var{r} \in \Rules{\Vint{g}}$,}
\LHS{\Vrule{r}} = \Vsym{accept} \implies \Vrule{r} = \Vrule{accept} \\
\text{and} \quad \Vsym{accept} \notin \RHS{\Vrule{r}}.
\end{gather*}

The dedicated acceptance rule is also called
the \dfn{augment rule}.
The semantics for the augment rule are simple ---
the semantics of \Vsym{top} are passed up to
\Vsym{accept}.
This is obviously semantics-safe.

In the Marpa implementation,
the general method
described in section \Sref{semantics-safe}
for dealing with the semantics of rewritten rules
is not applied to the augmented rule.
Instead, the augmented rule is
dealt with as a special case.

\subsection{Eliminating proper nullables}
\label{s:eliminating-proper_nullables}

Recall that a proper nullable
is a nullable symbol which is not nulling.
Marpa eliminates proper nullables with an improved
version of the rewrite in~\cite{AH2002}.
In \Marpa{}'s rewrite,
rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
This is similar to a rewrite into Chomsky form.

Every proper nullable symbol is then cloned into two others:
one nulling and one non-nullable.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols.
New rules are created as necessary to ensure that all possible combinations
of nulling and non-nullable symbols are accounted for.

The rewrite in~\cite{AH2002} was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the post-rewrite grammar of \cite{AH2002}
could be an exponential function of the rule count
in the pre-rewrite grammar of \cite{AH2002}.
In our version, the worst case growth in the number of rules is linear.

\subsection{Removing nulling symbols}
\label{s:removing-nulling-symbols}

After the rewrite of
\Sref{s:eliminating-proper_nullables},
all the nullable symbols in the grammar are also
nulling symbols.
Note that, if \Vext{g} is an external grammar,
and
if \var{g'} is \Vext{g}
revised to eliminate
all nulling symbols,
that
the language of the two grammars is the same: $\myL{\Vext{g}} = \myL{\var{g'}}$.
Similarly,
other than
the presence and absence of the nodes for the nulling symbols themselves,
the parse trees generated from \Vext{g} are the same as those generated from \var{g'}.

Thus, Marpa may rewrite its grammars to eliminate nulling symbols.
For the purposes of evaluation and run-time events,
Marpa records the locations of the eliminated nulling symbols.

A corner case occurs when eliminating nulling symbols from two distinct
rules makes them identical.
In this paper,
we assume that a rewrite that involves creating
a new LHS symbol for each rule that must remain distinct;
and adding new rules to derive the new LHS symbols from the original ones.
Less clean theoretically,
but easier in practice,
is to have the implementation treat rules as distinct
even if they differ only in their recorded nulling symbols.
This is what the Marpa implementation does.

\subsection{Trivial grammars and null parses}
\label{s:trivial-grammars}

Null parses are parses of zero length inputs.
The Marpa implementation
handles null parses by treating them
as a special case.

\dfn{Degenerate grammars} are grammars which do not accept any string.
\dfn{Trivial grammars} are grammars which only accept
zero length inputs.
No Marpa internal grammar can be a trivial grammar.
A degenerate grammar is treated as a fatal error.
Trivial grammars are handled as a special case.

\subsection{Semantics-safe rewriting}
\label{semantics-safe}

This subsection describes the restrictions
that rewrites must follow in order to be semantics-safe.
Consider an input \Cw{} and
an external grammar, \Vext{g}.
Let \Vint{g} be the internal grammar that is
the rewritten version of \Vext{g}.

Let
\begin{equation}
\label{e:parse-tree}
\var{pt} = \var{tree}(\var{g},\Vstr{w})
\end{equation}
be the parse tree that results from a parse of \Vstr{w}
by a grammar \var{g}.
If \var{g} is an internal grammar,
\var{pt} is an \dfn{internal parse}.
If \var{g} is an external grammar,
\var{pt} is an \dfn{external parse}.

Every node of \var{pt} is a \dfn{symbol instance}.
The symbol instances of the parse correspond one-to-one with 4-tuples
of the form
\begin{equation*}
[ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ].
\end{equation*}
where
\begin{itemize}
\item \Vsym{s} is the node's symbol.
\item \Vloc{i} is the location
of the first character in \Cw{} that is derived from \Vsym{s}.
derived from \Vsym{s}.
\item \var{depth}
is the ``brick depth''~\Lref{loc:brick-depth}.
\end{itemize}

In an internal grammar, there are
brick and mortar symbols.
Internal \dfn{brick} symbols correspond, many-to-one, to
external symbols.
Internal \dfn{mortar} symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.
All terminal symbols are bricks.

Recall \var{pt} from \Eref{e:parse-tree}.
If the symbol of an instance in \var{pt} is a brick,
the instance is a \dfn{brick instance}.
If the symbol of an instance in \var{pt} is a mortar symbol,
the instance is a \dfn{mortar instance}.

The
\dfn{brick depth}~\label{loc:brick-depth}
of an instance in a external grammar is
the depth of the node of the instance measured in the traditional way --
the distance in nodes to the root.
In an internal grammar, the brick depth is more complicated.
Intuitively, it is the distance of the node
in brick nodes.
More formally,
\begin{itemize}
\item The brick depth of the root instance of an internal grammar is undefined.
\item Since an internal grammar is augmented, the root instance must
have only one child, a brick instance whose brick depth is defined to be zero.
\item For all other brick nodes,
the brick depth is one plus the brick depth of its parent node.
\item For mortar nodes, the brick depth is the brick depth of its parent node.
\end{itemize}

For evaluation, it is useful to convert the internal parse tree to
a \dfn{brick tree}.
To create a brick tree, we traverse the internal parse tree in pre-order,
ignoring internal nodes,
and creating instances for the brick nodes.
Note that this implies that the root node (the node
whose symbol is \Accept{\Vint{g}}) is ignored.

When, while constructing a brick tree,
a brick node is encountered that is marked with memoized
nulling symbols,
new instances must be created for the nulling symbols.
The new instances are called \dfn{nulling instances}.

Let \var{marked} be a node in a parse tree.
Assume that \var{marked} has
one or more recorded parsing instances.
Recorded with each nulling instance will be whether it occurs
before or after \var{marked}.
In the brick tree, each nulling instance takes the form
\begin{equation*}
[ \Vsym{nulling}, \Vloc{bound}, 0, \var{depth} ]
\end{equation*}
where
\begin{itemize}
\item \Vsym{nulling} is the memoized nulling symbol.
\item \Vloc{bound} is the location of the nulling symbol.
If the nulling instance occurs before \var{marked},
\var{bound} is the start location of \var{marked}.
If the nulling instance occurs after \var{marked},
\var{bound}
is the location
immediately after
\var{marked}.
\item \var{depth} is the same as the depth of \var{marked}.
\end{itemize}
Nulling instances are brick instances.

We are now in a position to define a semantics-safe
rewrite more formally.
In what follows, we will want to match instances from an internal
parse
to instances of an external parse of the same input.
An internal symbol instance \textbf{matches}
an external symbol instance, and vice versa,
if and only if their 4-tuples match.
More formally,
let
$\var{etree} = \var{parse}(\Vext{g},\Vstr{w})$
be the parse from
an external grammar,
let $\var{itree} = \var{parse}(\Vint{g},\Vstr{w})$
be the parse from
an internal grammar,
where \Vint{g} is a rewrite of \Vext{g},
and let \var{btree} be the brick tree
produced from \var{itree}.
Let
\begin{equation*}
\var{enode} = [ \Vsym{esym}, \Vloc{estart}, \var{elength}, \var{edepth} ]
\end{equation*}
be an instance in \var{etree}, and let
\begin{equation*}
\var{bnode} = [ \Vsym{bsym}, \Vloc{bstart}, \var{blength}, \var{bdepth} ]
\end{equation*}
be an instance in \var{btree}.
Let \var{Instance-matches} be the relation defined by the set
\begin{equation*}
\left\lbrace \;
\begin{aligned}
& [ \var{enode},\var{bnode} ] \quad \text{such that} \\
& \qquad \Vsym{esym} = \Vsym{bsym} \\
& \qquad \text{and} \quad \Vloc{estart} = \Vloc{bstart} \\
& \qquad \text{and} \quad \var{elength} = \var{blength} \\
& \qquad \text{and} \quad \var{edepth} = \var{bdepth}
\end{aligned}
\; \right\rbrace
\end{equation*}

\begin{sloppypar}
A rewrite is semantics-safe, if and only if the relation
defined by
\var{Instance-matches}
is a one-to-one correspondence
(bijection)
between
the instances in \var{btree}
and the instances in \var{etree}.
In other words,
a rewrite is semantics-safe,
if \var{Instance-matches}
matches every instance of \var{btree} with exactly one
in \var{etree}
and every instance of \var{etree} with exactly one
in \var{btree}.
\end{sloppypar}

\section{Marpa internal grammars}
\label{s:INTs}

The formal definition of
a Marpa internal grammar is that of
a Marpa external grammar,
with two additional restrictions.

\begin{itemize}
\item Marpa internal grammars are augmented.
For details of grammar augmentation,
see \Sref{s:augmentation}.
\item Marpa internal grammars do not allow nullable symbols.
\end{itemize}


The notations for CFGs \Sref{s:CFGs}
carry over to internal grammars,
although with the changes imposed
by the restricted form of INT's.
Noteworthy among these changes:

\begin{itemize}
\item Marpa internal grammars do not allow nulling symbols.
\item The input to a parse must be of length greater than one.
\item The RHS of a rule can never be empty.
\end{itemize}

In the rest of this paper,
unless otherwise stated,
we will be assuming that the grammar
we are discussing
is a Marpa internal grammar.
The restrictions imposed on INT's allow us
to state some
definitions more simply than we could if they had to apply
to EXT's, or to CFG's.

Let \Vint{g} be an internal grammar.
A rule \Vrule{x},
$\Vrule{x} \in \Rules{\Vint{g}}$,
is \dfn{right-recursive},
$\RightRecursive{\Vrule{x}}$,
if and only if, for some \Vstr{y}
\begin{equation}
\label{eq:right-recursive}
\begin{gathered}
\Rightmost{\Vrule{x}} \deplus \Vstr{y} \quad \text{and} \\
\Rightmost{\Vstr{y}} = \LHS{\Vrule{x}}.
\end{gathered}
\end{equation}
\Vrule{x} is \dfn{directly right-recursive}
if the derivation of \Vstr{y} in
\Eref{eq:right-recursive}
is of length 1,
in which case
\Eref{eq:right-recursive}
is equivalent to
\begin{equation*}
\label{eq:direct-right-recursive}
\LHS{\Vrule{x}}  = \Rightmost{\RHS{\Vrule{x}}}.
\end{equation*}

In this paper,
\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the recognizer described in
this paper, and implemented in
Marpa::R2~\cite{Marpa-R2}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Cg}$ will be the language accepted by $\alg{Recce}$
when parsing \Cg{}.

\section{Earley tables}
\label{s:earley-table}

The definitions of this section
assume that \Vint{g} is a Marpa internal grammar.
Definitions of Earley's for more general CFG's
can be found in many places,
including
\cite[pp. 320-321]{AU1972},
\cite{AH2002},
\cite{Earley1968},
\cite{Earley1970},
\cite{GJ2008}, and
\cite{Leo1991}.

Let $\Vrule{r} \in \Rules{\Vint{g}}$
be a rule.
Recall that $\Vsize{r}$
is the length of the RHS of \var{r}.
A dotted rule (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.

We consider the size of a grammar, \Vsize{\var{g}}, to be the
number of distinct dotted rules it allows.
\Vsize{g} is a constant that depends on \var{g}.

If we let \Vdr{x} be a dotted rule, such that
\begin{gather*}
\Vdr{x} = \left[ \Vrule{x}, \var{pos} \right], \\
\intertext{then} \\
\begin{alignedat}{3}
& \LHS{\Vdr{x}} && \defined && \quad
\LHS{\Vrule{x}} \\
& \Postdot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\var{pos}], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
%
& \Next{\Vdr{x}} && \defined && \quad
\begin{cases}
[\Vrule{x}, \var{pos}+1], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
%
& \Penult{\Vdr{x}} && \defined && \quad
\begin{cases}
\Postdot{\Vdr{x}}, \; \text{if $\var{pos} = \size{\Vrule{x}}\subtract 1$} \\
\Lambda, \quad \text{otherwise}
\end{cases}
%
\end{alignedat}
\end{gather*}

A dotted rule, \Vdr{d},
is a \dfn{penult} if it is such that $\Penult{\var{d}} \neq \Lambda$.
The \dfn{initial dotted rule} is
\begin{equation*}
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\end{equation*}
A \dfn{predicted dotted rule} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{rhs} ].
\end{equation*}
A \dfn{confirmed dotted rule}
is the initial dotted rule,
or a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule} is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{rhs} \mydot ].
\end{equation*}
Predicted, confirmed and completed dotted rules
are also called, respectively,
\dfn{predictions}, \dfn{confirmations} and \dfn{completions}.
A dotted rule which is not a completion is called an
\dfn{incompletion}.

A Earley item (type \type{EIM}) is a duple
\[
    [\Vdr{dotted}, \Vorig{x}]
\]
of dotted rule and origin.
(The origin is the location where recognition of the rule
started.
It is sometimes called the ``parent''.)
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin element of an Earley item.

When a dotted rule concept is applied to an EIM,
it is the same as applying that concept to the EIM's
dotted rule.
When a rule concept is applied to an dotted rule
it is the same as applying that concept to the rule
of the dotted rule.
When a rule concept is applied to an EIM
it is the same as applying that concept to the rule
of the dotted rule of the EIM.

For example, let
\begin{equation*}
\Veim{x} = [\Vdr{x}, \Vorig{x}] =
\left[[\Vrule{x}, \var{pos}], \Vorig{x} \right].
\end{equation*}
Then
\begin{equation*}
\begin{gathered}
\Penult{\Veim{x}} = \Penult{\Vdr{x}} \\
\Postdot{\Veim{x}} = \Postdot{\Vdr{x}} \\
\LHS{\Veim{x}} = \LHS{\Vdr{x}} = \LHS{\Vrule{x}} \\
\RHS{\Veim{x}} = \RHS{\Vdr{x}} = \RHS{\Vrule{x}} \\
\end{gathered}
\end{equation*}
and so forth.

Similarly, we say that
an EIM is a \dfn{penult} if its dotted rule is a penult,
an EIM is a \dfn{prediction} if its dotted rule is a prediction,
and so on.

A \dfn{parse item} is either an Earley or a Leo item.
Leo items will be defined in \Sref{s:leo}.
Parse items are of type \type{PIM}.

Within Marpa,
an Earley set is defined as a set of \dfn{parse items}.
Earley sets are of type \type{ES}.

An Earley parser builds a table of Earley sets,
which is called
a \dfn{parse table},
or an \dfn{Earley table}.
An Earley table
is a sequence of Earley sets.
If \Vesset{table} is an Earley table, then
\begin{equation*}
\size{\Vesset{table}} = \size{\Vstr{w}},
\end{equation*}
where \Vstr{w} is the input of the parse.
and \Vesset{table} is the parse table.
Earley set 0 is the initial Earley set.
Earley set $\Vloc{i}+1$ describes the state of
the parse after processing the input at location \Vloc{i}.

Earley sets are often named by their location,
so that \Ves{i} means the Earley set at \Vloc{i}.
Where \Vloc{i} is an integer location,
the notation \Ves{i} represents an Earley set, so that
if \Vesset{table}
is the parse table,
\begin{equation*}
\Ves{i} = \Vesset{table}[\Vloc{i}].
\end{equation*}

Whenever helpful,
we will allow ourselves
to go back and forth between integer location,
\Vloc{i},
and Earley set,
\Ves{i}.\footnote{
For a discussion of this from the implementation
point of view, see \Sref{s:es-indexing}.
}

\EVtable{\alg{Recce}}{i} will be
the Earley set at \Vloc{i}
in the table of Earley sets of
the \alg{Recce} recognizer.
For example,
\EVtable{\Marpa}{j} will be Earley set \Vloc{j}
in \Marpa's table of Earley sets.
In contexts where it is clear which recognizer is
intended,
\Vtable{k}, or \Ves{k}, will symbolize Earley set \Vloc{k}
in that recognizer's table of Earley sets.
If \var{working} is an Earley set,
$\size{\Ves{working}}$ is the number of Earley items
in \Ves{working}.

\Rtablesize{\alg{Recce}} is the total number
of Earley items in all Earley sets for \alg{Recce},
\begin{equation*}
\Rtablesize{\alg{Recce}} =
     \sum\limits_{\Vloc{i}=0}^{\size{\Cw}}
	{\size{\EVtable{\alg{Recce}}{i}}}.
\end{equation*}
For example,
\Rtablesize{\Marpa} is the total number
of Earley items in all the Earley sets of
a \Marpa{} parse.

Recall that
there was a unique acceptance symbol,
\Accept{\Vint{g}}, in \Vint{g}.
The input \Cw{} is accepted if and only if,
for some \Vstr{rhs},
\begin{gather*}
\Vdr{accept} = [\Accept{\Vint{g}} \de \Vstr{rhs} \mydot] \quad \text{and} \\
\left[\Vdr{accept}, 0\right] \in \Etable{\Vsize{\Cw}}.
\end{gather*}

\section{The Leo algorithm}
\label{s:leo-items}

\subsection{The history of a conjecture}

Jay Earley~\cite[p. 60]{Earley1968}
conjectured that \Earley{} could be
modified to be \On{} for
all deterministic context-free grammars (DCFG's).
(The DCFG's are the union of the
\var{LR}(\var{k})
grammars for all \var{k}.)
He gave no details of the method he had in mind.
Earley left the field shortly thereafter ---
by 1973 he had earned a second Ph.D. and
was a practicing psychotherapist in California.

In the late 1980's, Joop Leo
also conjectured that
Earley's algorithm could be modified to be linear for all
DCFG's.
Leo was initially unaware of Earley's earlier conjecture ---
he discovered it in the course of writing up his
discovery.
Leo published his method in~\cite{Leo1991}.

The problem both investigators noticed was that,
while \Earley{} is \On{}
for left recursion,
and in fact very efficient,
it is $\order{\var{n}^2}$ for right recursion.
This is because all the EIM's necessary for a right
recursion are created at every parse location
where the right recursion might end.

For simplicity,
in the discussion of this subsection,
we will assume that the grammar is unambiguous.
Consider one right recursion,
and the set of EIM's in it.
We will the subset of these EIM's that are part of a single Earley set,
a ``right recursion edge'', or \dfn{RR edge}.
All of the EIM's in an RR edge will have the same end location.

As an Earley parse proceeds through a right recursion,
the potential length of the recursion grows.
The actual length of the right recursion will not be known
until the Earley parser finds the end of the right recursion,
which can be arbitrarily long.

At the first RR edge, the potential length is 1,
and the RR edge contains 1 EIM.
at the second RR edge, the potential length is 2,
and the RR edge contains 2 EIM's.
If the potential length of the right
recursion at an RR edge is \var{n},
then the RR edge contains \var{n} EIM's.
The total number of EIM's in the RR edges
needed for a right recursion of length \var{n} is
\[
  \sum_{\var{i} = 1}^\var{n} \var{i} = \order{\var{n}^2}.
\]
Whne the end of the right recursion is found,
and its actual length is known,
only the EIM's in the last RR edge will be used.
Therefore, only \var{n} EIM's are actually used in the parse ---
the other EIM's are useless.

Leo's idea was to memoize the right recursions.
With Leo memoization, each RR edge is represented by
a pair of EIM's and a memo.
There are \Oc{} Leo memos per Earley set,
so the time and space complexity
of an RR edge is \Oc{}.
The time and space complexity of all the RR edges
in a right recursion of length \var{n} will
be
\[
  \sum_{\var{i} = 1}^\var{n} 1 = \On.
\]

If, at evaluation time,
it is desirable to expand the Leo memoizations,
only the RR edge actually used in the parse
needs to be expanded.
The number of memoized
EIM's that need to be expanded
will be \On{},
where \var{n} is the length of the recursion.
As a result,
even if the time and space
required to expand Leo memoization
during evaluation
are taken into account,
the time and space complexity of
a right recursion become
$\On{} + \On{} = \On{}$.

In \cite{Leo1991}, Joop Leo
showed that,
with his modification, Earley's algorithm
is \On{} for all LR-regular grammars.
LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.
Earley did not claim \On{} for LR-regular
because LR-regular was not introduced
to the literature until 1973~\cite{Culik1973}.
Even in 1991, LR-regular was not well-known,
which is why Leo only claims
the weaker
\var{LR}(\var{k})
bound in
his title.

\subsection{Leo items}
\label{s:LIMs}

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this paper Leo's ``transitive items''
will be called Leo items.
Leo items
will have type \type{LIM}.
Every parse item (\type{PIM})
is either
an Earley item or a Leo item.

In each Earley set, there is at most one Leo item per symbol.
A Leo item (LIM) is the triple
\begin{equation*}
[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]
\end{equation*}
where \Vsym{transition} is the transition symbol,
and
\begin{equation*}
\Veim{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley item to be added on reductions over
\Vsym{transition}.

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
so that the top of the sequence can represent
the entire sequence --
the only role the other EIM's in the sequence
play in the parse is to derive the top EIM.
We will call these memoized sequences, Leo sequences.

\subsection{Leo uniqueness}
\label{s:leo-uniqueness}

To quarantee that a Leo sequence is deterministic,
\Leo{} enforced \dfn{Leo uniqueness}.
An EIM \Veim{uniq} such that
\begin{equation*}
\Veim{uniq} \in \Ves{i}
\quad \text{and} \quad \Penult{\Veim{uniq}} \neq \Lambda,
\end{equation*}
is \dfn{Leo unique} in \Ves{i},
\begin{equation*}
\LeoUnique{\Veim{uniq}, \Ves{i}},
\end{equation*}
if and only if,
for all \Veim{other},
such that
\begin{equation*}
\quad \Veim{other} \in \Ves{i}
\end{equation*}
we have
\begin{equation*}
\Penult{\Veim{uniq}} = \Postdot{\Veim{other}}
\implies \Veim{uniq} = \Veim{other}.
\end{equation*}
It is important to notice that \Veim{other} ranges over all the
EIMs of Earley set \Ves{i},
even those which are not penults.

\subsection{Leo predecessor}
\label{s:leo-predecessor}

Tracking the Leo chains requires that there
be an analog to the concept of predecessor EIM
for LIMs.
Let EIM \Veim{successor} and \Ves{i} be such that
\begin{equation*}
\begin{gathered}
\Veim{successor} = [ \Vdr{successor}, \Vloc{origin} ] \\
\quad \text{and} \quad \Veim{successor} \in \Ves{i} \\
\quad \text{and} \quad \Penult{\Veim{successor}} \neq \Lambda.
\end{gathered}
\end{equation*}
We say that \Vlim{predecessor}
is the \dfn{Leo predcessor} of \Veim{successor} in \Ves{i},
\begin{equation*}
\Vlim{predecessor} = \LIMPredecessor{\Veim{successor}, \Ves{i}},
\end{equation*}
if and only if, for some \Vdr{top}, \Vloc{top},
\begin{equation*}
\begin{gathered}
\Vlim{predecessor} = [\Vdr{top}, \LHS{\Veim{successor}}, \Vloc{top}] \\
\quad \text{and} \quad \Vlim{predecessor} \in \Ves{origin}.
\end{gathered}
\end{equation*}

\subsection{The Leo algorithm}
\label{s:leo}

Implementing the Leo logic requires
adding Leo reduction as a new basic operation,
adding a new premise to the Earley reduction
operation,
and extending the Earley sets to memoize Earley
items as LIM's.

\subsection{Leo reduction}

\begin{equation*}
\inference{
    \begin{array}{c}
    \Veim{component} = \bigl[
     [ \Vsym{lhs} \de \Vstr{rhs} \mydot ]
    , \Vloc{component-origin} \bigr] \\
    \Veim{component} \in \Ves{current} \\[3pt]
	\Vlim{predecessor} = [ \Vdr{top}, \Vsym{lhs}, \Vorig{top} ] \\
	\Vlim{predecessor} \in \Ves{component-orig} \\
    \end{array}
}{
    \bigset{ [ \Vdr{top}, \Vorig{top} ] }
}
\end{equation*}
The new Leo reduction operation resembles the Earley reduction
operation, except that it looks for an LIM,
instead of a predecessor EIM.
\Vlim{predecessor} and
\Veim{component} are the operands of the Leo reduction
operation.
\Vsym{lhs} is the transition symbol
of the Leo reduction.
As with Earley reduction,
it may be convenient to treat the transition
symbol as an operand,
so that the operands
are \Vlim{predecessor} and \Vsym{lhs}.

\subsection{Changes to Earley reduction}

Earley reduction still applies, with an additional premise:
\begin{equation*}
\begin{split}
& \neg \exists \, \Vlim{x} \; \mid \\
&  \qquad \Vlim{x} \in \Ves{component-orig} \\
&	\qquad \qquad  \land \Vlim{x} = [ \Vdr{x}, \Vsym{lhs}, \Vorig{x} ]
\end{split}
\end{equation*}
The additional premise
prevents Earley reduction from being applied
where there is an LIM with \Vsym{lhs} as its transition symbol.
This reflects the fact that
Leo reduction replaces Earley reduction if and only if
there is a Leo memoization.

\subsection{Leo memoization operations}

Finally, we define two new operations for
the creation of LIM's or,
in other words,
for Leo memoization.
We define uniqueness of a penult in an Earley set as
\begin{equation*}
\begin{split}
& \mymathop{Penult-Unique}(\Vsym{penult},\Ves{i}) \defined \\
& \qquad \forall \, \Vdr{y} \bigl( \mymathop{Contains}(\Ves{current}, \Vdr{y})
\land \Vsym{penult} = \Penult{\Vdr{y}} \bigr) \\
& \qquad \qquad \implies \Vdr{x} = \Vdr{y}.
\end{split}
\end{equation*}
We define the
Leo uniqueness of a dotted rule within an Earley set as
\begin{equation*}
\begin{split}
& \LeoUnique{\Vdr{x},\Vloc{current}} \defined \\
& \qquad \mymathop{Contains}(\Ves{current}, \Vdr{x})  \\
& \qquad \qquad \land \Penult{\Vdr{x}} \neq \Lambda \\
& \qquad \qquad \land \mymathop{Penult-Unique}({\Penult{\Vdr{x}}}, \Ves{current})
\end{split}
\end{equation*}
and defined the Leo eligibility
of a dotted rule within an Earley set as
\begin{equation*}
\begin{split}
& \LeoEligible{\Vdr{x},\Vloc{current}} \defined \\
& \qquad \exists \, \Vrule{x}, \Vorig{x} \; \mid \\
& \qquad \qquad \Vdr{x} = [ \Vrule{x}, \Vorig{i} ] \\
& \qquad \qquad \quad \land \RightRecursive{\Vrule{x}} \\
& \qquad \qquad \quad \land \LeoUnique{\Ves{current},\Vdr{x}}.
\end{split}
\end{equation*}
We define
the LIM predecessor of \Veim{eim} as follows:
\begin{multline*}
\mymathop{LIM-Predecessor}({\Veim{bottom}}) = \Vlim{pred} \quad \text{such that} \\
\text{there exists} \qquad \Ves{bottom-origin}, \Vdr{bottom}, \\
\qquad \Vdr{pred}, \Vloc{pred-origin}, \Vloc{bottom-origin} \\
\end{multline*}
where
\begin{gather}
\Veim{bottom} = [ \Vdr{bottom}, \Vloc{bottom-origin} ] \\
\text{and} \quad \Vlim{pred} = [ \Vdr{pred}, \LHS{\Vdr{bottom}}, \Vloc{pred-origin} ] \\
\text{and} \quad \Vlim{pred} \in \Ves{bottom-origin}
\end{gather}
There may not be a LIM predecessor for \Veim{bottom},
in which case
$\mymathop{LIM-Predecessor}({\Veim{bottom}}) = \Lambda$.

We are now ready to define an inference rule which holds if
a LIM predecessor can be found for an EIM \Veim{bottom}
\begin{equation*}
\inference{
    \begin{array}{c}
    \mymathop{LIM-Predecessor}({\Vlim{pred},\Veim{bottom}}) \\
    \Vlim{pred} = [ \Vdr{pred}, \LHS{\Vdr{bottom}}, \Vorig{pred} ] \\
    \Veim{bottom} = [ \Vdr{bottom}, \Vloc{bottom} ] \\
    \Veim{bottom} \in \Ves{current} \\
    \LeoEligible{\Vdr{bottom}}
    \end{array}
}{
    \left \lbrace [ \Vdr{pred}, \Penult{\Vdr{bottom}}, \Vorig{pred} ] \right \rbrace
}
\end{equation*}
and another, which holds if
\Veim{bottom} has no predecessor LIMT,
\begin{equation*}
\inference{
    \begin{array}{c}
    \neg \mymathop{LIMT-Predecessor}({\Vlim{pred},\Veim{bottom}}) \\
    \Veim{bottom} = [ \Vdr{bottom}, \Vorig{bottom} ] \\
    \Veim{bottom} \in \Ves{current} \\
    \LeoEligible{\Vdr{bottom}}
    \end{array}
}{
    \bigset{ [ \Next(\Vdr{bottom}), \Penult{\Vdr{bottom}}, \Vorig{bottom} ] }
}
\end{equation*}

\section{The Marpa algorithm}
\label{s:algorithm}

For the purpose of defining the Marpa algorithm,
let \Vint{g} be a Marpa internal grammar used in the parse,
let \Vstr{w} be the input of the parse,
and let \Vesset{esets} be the Earley table.

The top-level view of the Earley algorithm is
show in \Aref{a:marpa}.

We let
\begin{align*}
& \var{PredictOne}(\Vsym{top}, \Vloc{i}) \defined \\
& \qquad \left\lbrace
\begin{aligned}
& \left[
\left[
\Vsym{predicted}
\de
\mydot
\Vstr{rhs}
\right],
\Vloc{i}
\right] \\
& \quad \text{such that for some \Vstr{suffix},} \\
& \qquad \left[\Vsym{predicted} \de \Vstr{rhs} \right] \in \Rules{\Vint{g}} \\
& \qquad \quad \land \quad
\Vsym{top} \destar \Vsym{predicted} \Vstr{suffix} \\
\end{aligned}
\right\rbrace
\end{align*}

\subsection{Initializer operation}
\label{s:initializer}

\begin{equation*}
\var{Initializer}() \defined \var{PredictOne}(\Accept{\Vint{g}}, 0)
\end{equation*}

\subsection{Scanner operation}
\label{d:scanner}

\begin{align*}
& \var{Scanner}(\Ves{i}) \defined \\
& \qquad \Bigl\lbrace
\left [
\left[
\Vsym{lhs}
\de
\Vstr{prefix}
\Vsym{token}
\mydot
\Vstr{suffix}
\right],
\Vloc{origin}
\right]
\Bigr\rbrace
\end{align*}
such that
\begin{equation*}
\begin{aligned}
& \Vsym{token} = \Vstr{w}[\Vloc{i} \subtract 1] \\
\land \quad & \left[ \left[ \Vsym{lhs} \de \Vstr{prefix} \mydot \Vsym{token} \Vstr{suffix}
\right], \Vloc{origin} \right]\\
& \qquad \in \Ves{(\Vloc{i}\subtract 1)}.
\end{aligned}
\end{equation*}

\subsection{Reducer operation}
\label{d:reducer}

Let
\begin{align*}
& \var{Earley-reducer}(\Ves{i}) \defined \\
& \qquad \Bigl\lbrace
\bigl [
\left[
\Vsym{lhs}
\de
\Vstr{prefix}
\Vsym{completed}
\mydot
\Vstr{suffix}
\right],
\Vloc{origin}
\bigr]
\Bigr\rbrace
\end{align*}
such that
\begin{equation*}
\begin{aligned}
& \bigl[ \left[ \Vsym{completed} \de \Vstr{rhs} \mydot
\right], \Vloc{predecessor} \bigr]
\in \Ves{i} \\
\land \quad & \bigl[ \left[ \Vsym{lhs} \de \Vstr{prefix} \mydot \Vsym{completed} \Vstr{suffix}
\right], \Vloc{origin} \bigr]\\
& \qquad \in \Ves{predecessor}.
\end{aligned}
\end{equation*}

Also, let
\begin{equation*}
\var{Leo-reducer}(\Ves{i}) \defined
\Bigl\lbrace
\bigl(
\left[ \Vdr{top}, \Vloc{top} \right]
\bigr)_\var{EIM}
\Bigr\rbrace
\end{equation*}
such that
\begin{equation*}
\begin{gathered}
\Bigl(\bigl[ \left[ \Vsym{completed} \de \Vstr{rhs} \mydot
\right], \Vloc{predecessor} \bigr]\Bigr)_\var{EIM}
\in \Ves{i} \\
\land \quad \bigl([ \Vdr{top}, \Vsym{completed}, \Vloc{top} ]\bigr)_\var{LIM}
\in \Ves{predecessor}.
\end{gathered}
\end{equation*}

\begin{algorithm}[tb]
\caption{The Marpa Algorithm}
\label{a:marpa}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State $\Ves{(0)} \gets \Call{Initializer}{{}}$
\For{ $\Ves{i}, 1 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\Ves{x}$ is complete, for $0 \le \var{x} < \var{i}$
\State $\Ves{i} \gets \Call{Scanner}{\Ves{i}}$
\If{$\size{\Ves{i}} = 0$}
\State return \Comment \Cw{} will be rejected
\EndIf
\State $\Ves{i} \gets \Ves{i} \cup \Call{Reducer}{\Ves{i}}$
\State $\Ves{i} \gets \Ves{i} \cup \Call{Predicter}{\Ves{i}}$
\State $\Ves{i} \gets \Ves{i} \cup \Call{Memoizer}{\Ves{i}}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{The Earley algorithm}
\label{s:earley}

\subsection{Prediction}
\label{d:prediction}

Where
\begin{equation}
          \Veim{predecessor} = [ \Vdr{predecessor}, \Vorig{predecessor} ]
\end{equation}
is an incompletion in the input at \Vloc{i}, and
there is a
\begin{equation}
\Vrule{predict} \in \Rules{\Vint{g}}
\end{equation}
such that
\begin{equation}
\LHS{\Vrule{predict}} = \Postdot{\Vdr{predecessor}}
\end{equation}
then union
\begin{gather*}
\begin{align*}
& \; \Bigset{ \bigl[ [ \Vrule{predict}, 0 ], \Vloc{i} \bigr] } \\
\intertext{into $\Ves{i}$.}
\end{align*}
\end{gather*}

\Veim{predecessor} is called the \dfn{predecessor} of the prediction operation.

\section{Marpa Changes}
\label{s:changes}

\begin{sloppypar}
Recall that we
call a dotted rule \Vdr{d} a \dfn{penult} if $\Penult{\var{d}} \neq \Lambda$.
In Leo's original algorithm, any penult
was treated as a potential right-recursion.
\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\begin{equation*}
\Vdr{candidate} = [\Vrule{candidate}, \var{i}]
\end{equation*}
for Leo memoization,
\Vdr{candidate} must be a penult and
\Vrule{candidate} must be right-recursive.
\end{sloppypar}

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be infinitely
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the longest potential Leo sequence is short,
and the payoff is therefore very limited.

A future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
justify inclusion in the Leo memoizations.
Such cases would be unusual, but may occur.

Omission of a memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
Later in this paper we will
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

\section{Implementation}
\label{s:implementation}

\subsection{Earley set indexing}
\label{s:es-indexing}

In our theoretical discussions,
we assumed that we could efficiently convert between
an Earley set location as an integer
(\Vloc{i} or \Ves{i}),
and an Earley set as data structure including the PIM's.
Marpa::R2\cite{Marpa-R2}
builds Earley sets as a linked list of structures,
and stores the \Vloc{i} in an integer field.
An array for indexing is created on an ``as needed''
basis.

Earley sets are stored internally as pointers.
These pointers are sufficient even for the run-time
event mechanism.
In practice,
indexing by \Vloc{i} only becomes necessary
during run-time for run-time tracing and debugging.
In normal usage, there creation of the array of
Earley sets on an ``as needed'' basis means that it
is created once,
after the parse is complete.
The Earley sets will never be changed after this point
and therefore the array of Earley sets will never need to be resized.\footnote{
In this discussion a parse is said to be ``complete'' when it is ready to be evaluated.
This becomes relevant when an application takes advantage of
Marpa::R2's ability to resume an already evaluated
parse, reading more input.
Each of these is, in fact, a separate parse,
and we take this point of view when discussing per-parse resource.
}

\section{Per-set lists}
\label{s:per-set-lists}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\Ves{i}, \var{x}]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a link direct to the Earley set \Ves{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to,
or very similar to,
PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this paper.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of each Earley set.
While \Marpa{} is building a new Earley set,
\Ves{j},
a PSL is kept for every previous Earley set ---
that is, for every
$\Vloc{i} < \Vloc{j}$.

The PSL for \Ves{j}
tracks the Earley items in \Ves{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{g}.
\Vsize{g}
is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's
for all the Earley sets
every time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

Let $\ID{\Vdr{x}}$ be the integer ID of a dotted rule.
One way to assign integer values
to $\ID{\Vdr{x}}$ to list the rules
in some arbitrary order,
and then traverse the dot positions in
the list, left-to-right within each rule,
then top-to-bottom within the list.
The number of dot positions
within a rule \Vrule{r} is \Vsize{r}.

Intuitively, the idea is avoid most re-computation
of the PSL's by keeping timestamps with the PSL data.
To illustrate the use of PSL's,
we will consider the case
where Marpa is building \Ves{j}
and wants to check whether Earley item,
\begin{equation*}
\Veim{x} = [ \Vdr{x}, \Vorig{x} ],
\end{equation*}
is new,
or if \Veim{x} needs to be added to the Earley sets.

Let
\begin{equation*}
\var{psl-entry} = \PSL{\Ves{i}}{\var{y}}
\end{equation*}
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}, such that
\begin{equation*}
\begin{alignedat}{3}
& \var{psl-entry} && \defined && \quad
\begin{cases}
\Lambda, \quad \text{if the entry at $\var{psl-entry}$ has never been used,} \\
[\var{time-stamp}, \Vbool{z}], \text{otherwise} \\
\end{cases} \\
%
\end{alignedat}
\end{equation*}
Here \var{time-stamp} is a time stamp which will be used to
avoid unnecessary re-computation,
and \Vbool{z} is \var{true} if \Veim{x}
is already present in \Ves{j},
and \var{false} otherwise.

If
\begin{multline*}
\PSL{\Ves{i}}{\var{y}} = [\var{time-stamp}, \Vbool{exists}] \\
 \text{such that} \quad
 \var{time-stamp} \neq \Vloc{j} \\
 \lor \; \Vbool{exists} = \var{false},
\end{multline*}
the \Veim{x} already exists,
and should not be added to \Ves{j}.
The psl entry for \Ves{i} and \Vdr{x},
$\PSL{\Ves{x}}{\ID{\Vdr{x}}}$,
is left as is.

If \Veim{x} does not already exist,
then  \Veim{x} is new.
In this case,
\Veim{x} is added to \Ves{j}, and
the PSL entry is re-set, as follows:
\begin{equation*}
\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets [\Vloc{j}, \var{true}].
\end{equation*}

The reader may notice that the above example could be simplified,
because the data to be kept for EIM's is a single boolean.
Further, this boolean is never set to \var{false}, so that
if a PSL entry exists, its boolean may be assumed to be \var{true}.
This means that an explicit boolean is not actually needed.
A simpler approach, which omits the boolean,
is the one that
the Marpa implementation uses.

It will not always be the case that the data that the PSL's need
to track is limited to a single boolean.
It is for this reason that
the more complicated approach is illustrated above.

\section{Marpa recognizer correctness}
\label{s:correctness}

\section{Marpa recognizer complexity}
\label{s:complexity}

The following theorem will prove useful.

\begin{theorem}\label{t:es-size}
The maximum size of an Earley set at {\upshape $\Vloc{i}$}
is
{\upshape $\order{\var{i}}$}.
\end{theorem}

\begin{proof}
Each EIM is of the form
$[ \Vdr{x}, \Vloc{x} ]$.
Recall that
the number of dotted rules is a \Vsize{g},
a constant depending on the grammar.
The possible values of \Vloc{x} range from
0 to \Vloc{i}.
Therefore the number of possible Earley items
in \Ves{i} is
is $\Vloc{i} \times \Oc{}$.
Recall that the size of an Earley set is the number of EIM's
that it contains.
It follows
that the maximum size of the Earley
set at \Vloc{i} is
\begin{equation*}
\Vloc{i} \times \Oc{} = \order{\var{i}} \times \Oc{} = \order{\var{i}}.
\end{equation*}
\end{proof}

\subsection{Complexity of each Earley item}

\begin{theorem}\label{t:O1-time-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item,
and each attempt to
add a duplicate Earley item,
requires \Oc{} time.
\end{theorem}

\begin{theorem}\label{t:O1-space-per-eim}
All space in \Marpa{} can be allocated
to the Earley items,
in such a way that each Earley item
requires \Oc{} space and,
if links are not considered,
each attempt to add a duplicate
Earley item adds no additional space.
\end{theorem}

\begin{theorem}\label{t:O1-links-per-eim}
If links are considered,
all space in \Marpa{} can be allocated
to the Earley items
in such a way that each Earley item
and each attempt to
add a duplicate Earley item
requires \Oc{} space.
\end{theorem}

\begin{proof}[Proof of Theorems
\ref{t:O1-time-per-eim},
\ref{t:O1-space-per-eim},
and \ref{t:O1-links-per-eim}]
Omitted.
\end{proof}

\subsection{Delete this?}

\begin{theorem}\label{t:tries-O-eims}
For an unambiguous grammar,
the number of attempts to add
Earley items will be less than or equal to
\begin{equation*}
\textup{
    $\var{c} \times \Rtablesize{\Marpa}$,
}
\end{equation*}
where \var{c} is a constant
that depends on the grammar.
\end{theorem}

\begin{proof}
Omitted.
\end{proof}

As a reminder,
we follow tradition by
stating complexity results in terms of \var{n},
setting $\var{n} = \Vsize{\Cw}$,
the length of the input.

\begin{theorem}\label{t:eim-count}
For a context-free grammar,
\begin{equation*}
\textup{
    $\Rtablesize{\Marpa} = \order{\var{n}^2}$.
}
\end{equation*}
\end{theorem}

\begin{proof}
By Theorem \ref{t:es-size},
the size of the Earley set at \Vloc{i}
is $\order{\var{i}}$.
Summing over the length of the input,
$\Vsize{\Cw} = \var{n}$,
the number of EIM's in all of \Marpa's Earley sets
is
\begin{equation*}
\sum\limits_{\Vloc{i}=0}^{\var{n}}{\order{\var{i}}}
= \order{\var{n}^2}.\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:ambiguous-tries}
For a context-free grammar,
the number of attempts to add
Earley items is $\order{\var{n}^3}$.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries-O-eims},
we see that the only bound that required
the assumption that \Cg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Let \var{other-tries}
be attempts to add EIM's other than
as the result of Earley reductions.
By Theorem \ref{t:eim-count},
\begin{equation*}
\Rtablesize{\Marpa} = \order{\var{n}^2},
\end{equation*}
and by Theorem \ref{t:tries-O-eims},
\begin{equation*}
\var{other-tries} \le \var{c} \times \Rtablesize{\Marpa},
\end{equation*}
so that
$\var{other-tries} = \order{\var{n}^2}$.

\begin{sloppypar}
Looking again at \var{reduction-tries}
for the case of ambiguous grammars,
we need to look again at the triple
\begin{equation*}
[\Veim{predecessor}, \Vsym{transition}, \Veim{component}].
\end{equation*}
We did not use the fact that the grammar was unambigous in counting
the possibilities for \Vsym{transition} or \Veim{component}, but
we did make use of it in determining the count of possibilities
for \Veim{predecessor}.
We know still know that
\begin{equation*}
\Veim{predecessor} \in \Ves{component-origin},
\end{equation*}
where
\Vloc{component-origin} is the origin of \Veim{component}.
Worst case, every EIM in \Ves{component-origin} is a possible
match, so that
the number of possibilities for \Veim{predecessor} now grows to
\size{\Ves{component-origin}}, and
\begin{equation*}
\var{reduction-tries} =
\bigsize{\Ves{component-origin}} \times \Vsize{symbols} \times \bigsize{\Ves{j}}.
\end{equation*}
\end{sloppypar}

In the worst case $\var{component-origin} \simeq \var{j}$,
so that by Theorem \ref{t:es-size},
\begin{equation*}
\size{\Ves{component-origin}} \times \size{\Ves{j}} = \order{\var{j}^2}.
\end{equation*}
Adding \var{other-tries}
and summing over the Earley sets,
we have
\begin{equation*}
\order{\var{n}^2} +
\! \sum\limits_{\Vloc{j}=0}^{n}{\order{\var{j}^2}} = \order{\var{n}^3}.
\qedhere
\end{equation*}
\end{proof}

\begin{theorem}\label{t:leo-right-recursion}
Either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let the constant \var{c} be the number
of symbols.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
(Since for the purposes of these
complexity results we ignore nulling symbols,
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.)
So part of the rightmost derivation must take the form
\begin{equation*}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation*}
But the first step of this derivation sequence must use a rule of the
form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\end{proof}

\subsection{The complexity results}
We are now in a position to show
specific time and space complexity results.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
Omitted.
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
By assumption, \Cg{} is unambiguous, so that
by Theorem \ref{t:tries-O-eims},
and Theorem \ref{t:eim-count},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant that depends on \Cg{}.
Therefore,
by Theorems \ref{t:O1-time-per-eim}
and \ref{t:O1-links-per-eim},
the time and space complexity of \Marpa{}
for unambiguous grammars is \order{\var{n}^2}.
\end{proof}

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-time-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\begin{theorem}\label{t:cfg-space}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-space-per-eim}
and Theorem \ref{t:eim-count}.
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in \ref{t:cfg-space}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
By Theorem \ref{t:O1-links-per-eim},
and Theorem \ref{t:ambiguous-tries}.
\end{proof}

\section{The Marpa Input Model}
\label{s:input}

In this paper,
up to this point,
the traditional input stream model
has been assumed.
As implemented,
Marpa generalizes the idea of
input streams beyond the traditional
model.

Marpa's generalized input model
replaces the input \Cw{}
with a set of tokens,
\var{tokens},
whose elements are triples of symbol,
start location and length:
\begin{equation*}
    [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
$\var{length} \ge 1$
and
$\Vloc{start} \ge 0$.
The size of the input, \size{\Cw},
is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

Multiple tokens can start at a single location.
(This is how \Marpa{} supports ambiguous tokens.)
The variable-length,
ambiguous and overlapping tokens
of \Marpa{}
bend the conceptual framework of ``parse location''
beyond its breaking point,
and a new term for parse location is needed.
Start and end of tokens are described in terms
of \dfn{earleme} locations,
or simply \dfn{earlemes}.
Token length is also measured in earlemes.

Like standard parse locations, earlemes start at 0,
and run up to \size{\Cw}.
Unlike standard parse locations,
there is not necessarily a token ``at'' any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
properly inside at least one token.
Here ``properly inside'' means after the token's start earleme
and before the token's end earleme.

In the Marpa input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is, for all \Vloc{i} such
that $0 \le \Vloc{i} < \size{\Cw}$,
there must exist
\begin{equation*}
	 \var{token} = [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
\begin{gather*}
	 \var{token} \in \var{tokens} \quad \text{and} \\
	 \Vloc{start} \le \Vloc{i} < \Vloc{start}+\var{length}.
\end{gather*}

The intent of Marpa's generalized input model is to allow
users to define alternative input models for special
applications.
An example that arises in current practice is natural
language, features of which are most
naturally expressed with ambiguous tokens.
The traditional input stream can be seen as the special case of
the Marpa input model where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength},
if we have both of
\begin{align*}
    [\Vsym{x}, \Vloc{x}, \var{xlength}] & \in \var{tokens} \quad \text{and} \\
    [\Vsym{y}, \Vloc{y}, \var{ylength}] & \in \var{tokens},
\end{align*}
then we have both of
\begin{gather*}
\var{xlength} = \var{ylength} = 1 \quad \text{and} \\
     \Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}.
\end{gather*}

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
restrictions must be imposed.
In stating them,
let it be understood that
\begin{equation*}
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \in \var{tokens}
\end{equation*}
We require that,
for some constant \var{c},
possibly dependent on the grammar \Cg{},
that every token length be less than \var{c},
\begin{equation}
\label{e:restriction1}
\forall \, \Vtoken{[\Vsym{x}, \Vloc{x}, \var{length}]},
\; \var{length} < \var{c},
\end{equation}
and that
the cardinality of the set of tokens starting at any
one location
be less than \var{c},
\begin{equation}
\label{e:restriction2}
 \forall \Vloc{i}, \;
 \Bigl|
 \bigl \lbrace
	\Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \bigm|
	\Vloc{x} = \Vloc{i}
  \bigr \rbrace
  \Bigr| < \var{c}
\end{equation}
Restrictions \Eref{e:restriction1}
and \Eref{e:restriction2}
impose little or no obstacle
to the practical use
of Marpa's generalized input model.
And with them,
the complexity results for \Marpa{} stand.

\section{Acknowledgments}
\label{p:Acknowledgments}

Ruslan Shvedov
and
Ruslan Zakirov
made many useful suggestions
that are incorporated in this paper.
Many members of the Marpa community
have helped me in many ways,
and it is risky
to single out one of them.
But Ron Savage
has been unstinting in
his support.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Culik1973}
{\v{C}}ulik, Karel and Cohen, Rina.
\newblock LR-Regular grammarsan extension of LR (k) grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Kegler2019}
Jeffrey~Kegler.
\newblock Marpa, A practical general parser: the recognizer.
\newblock arXiv:1910.08129v1
\newblock \url{https://arxiv.org/abs/1910.08129v1}

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Timeline}
Jeffrey~Kegler.
\newblock Parsing:~a~timeline.
\newblock Version 3.1, 2019.
\newblock \url{https://jeffreykegler.github.io/personal/timeline_v3}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}
