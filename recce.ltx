% Copyright 2021 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[draft,openany,12pt]{amsbook}
\usepackage{calc} % calc is required by todonotes
\usepackage{xstring}
\usepackage{xspace}
\usepackage{needspace}
\usepackage{mfirstuc}
\usepackage{ifdraft}
\usepackage[obeyDraft]{todonotes}
\ifdraft{
    \usepackage{datetime2}
    \usepackage{draftwatermark}
}{}


% Fix conflict between todonotes and amsart packages
% See http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/todonotes/todonotes.pdf,
% p. 10.
\makeatletter
\providecommand\@dotsep{5}
\def\listtodoname{List of TODOs}
\def\listoftodos{\@starttoc{tdo}\listtodoname}
\makeatother

% The \ifhack commands are intended for last minute "hacks", adjustments
% done when the document is final.  These are things like
% workarounds for over/underfull hboxes, which require hack-ish
% adjustments that easily break (or become unnecessary) when
% minor changes are made to the text.

% The "Hack" is the first argument of the command, the non-hack text
% is the 2nd.  Often there is no "non-hack text".  To turn off hacks
% and go to a pre-hack state, redefine if \ifhack command.
%
% In the event of a major change, we may want to turn off the current
% hacks and start a new "generation" of hacks.  For a new generation,
% we create a new \ifhackN command, where N is the generation, so that
% the commands are \ifhackA, \ifhackB, etc.
\newcommand{\ifhack}[2]{#1}

% Capitalization conventions.  We prefer to follow "sentence case" --
% capitalization of first words only. In math and computer topics,
% capitalization is often significant, and sentence case makes it hard
% to distinquish between words capitalized as a matter of style,
% and words capitalized because they are special technical terms
% (for example, LaTeX).
%
% We do make exceptions.  For readability, we often want references
% to theoremoids to stand out, and we selectively capitalize them
% for this purpose.
\emergencystretch=3em

\usepackage{amsmidx}
% for escaping in index commands ...
%    the quote sign does not seem to work in amsmidx
\newcommand{\Pipe}{\charâ€˜\|}
\newcommand{\See}[2]{\emph{see} #1}
\makeindex{recce-general}
\makeindex{recce-defienda}
\makeindex{recce-symbol}

% This is used to find the font size if need be.
% Its uses are usually commented out.
\makeatletter
\newcommand\thefontsize[1]{{#1 The current font size is: \f@size pt\par}}
\makeatother

\usepackage{epigraph}
% sets a wider default
\newcommand{\myepi}[3][.64\textwidth]{
    \setlength{\epigraphwidth}{#1}%
    \epigraph{#2}{#3}%
}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{phonetic} % For Polish accent in bibliography
\usepackage{wasysym} % Used for \Circle
\usepackage{stmaryrd} % Used for \llbracket, \rrbracket
\usepackage{mathtools}
\usepackage{fancybox}
\usepackage{url}
\usepackage[final]{hyperref} % ignore draft status
\hypersetup{pdfborder=0 0 .1}

\usepackage{ragged2e}
\usepackage{placeins}

\usepackage{thmtools}
\usepackage{multirow}
% \usepackage[notcite,notref]{showkeys} % for diagnostics

% \usepackage{glossaries} % must after hyperref
% \makeglossaries

\newcommand{\COL}{\ensuremath{\mathbb{COL}}\xspace}
\newcommand{\MtL}{\ensuremath{\mathbb{M}\text{t}\mathbb{L}}\xspace}

%% From Chiron 2012 paper

\newcommand{\sets}{\ensuremath{\mathbb{V}}}
\newcommand{\classes}{\ensuremath{\mathbb{C}}}
\newcommand{\Dv}{\ensuremath{\mathbb{D}{\rm v}}}
\newcommand{\Dc}{\ensuremath{\mathbb{D}{\rm c}}}
\newcommand{\Ds}{\ensuremath{\mathbb{D}{\rm s}}}
\newcommand{\Df}{\ensuremath{\mathbb{D}{\rm f}}}
\newcommand{\Dill}{\ensuremath{\mathbb{D}{\rm ill}}}
\newcommand{\Dbool}{\ensuremath{\mathbb{D}{\rm bool}}}
\newcommand{\Do}{\ensuremath{\mathbb{D}{\rm o}}}
\newcommand{\De}{\ensuremath{\mathbb{D}{\rm e}}}
\newcommand{\Don}{\ensuremath{\mathbb{D}{\rm on}}}
\newcommand{\Dsy}{\ensuremath{\mathbb{D}{\rm sy}}}
\newcommand{\Dop}{\ensuremath{\mathbb{D}{\rm op}}}
\newcommand{\Dty}{\ensuremath{\mathbb{D}{\rm ty}}}
\newcommand{\Dte}{\ensuremath{\mathbb{D}{\rm te}}}
\newcommand{\Dfo}{\ensuremath{\mathbb{D}{\rm fo}}}

\newcommand{\TypeLE}{\ll}
\newcommand{\TypeEqual}{=_{\rm ty}}

\newcommand{\cnbg}{\mbox{\sc cnbg}}
\newcommand{\subvaluation}{\sqsubseteq}
% \newcommand{\qmname}[1]{\synbrack{\mname{#1}}}
% \newcommand{\provesast}[2]{#1 \vdash^\ast #2}

% \renewcommand{\phi}{\varphi}
% \newcommand{\seq}[1]{{\langle #1 \rangle}}
% \newcommand{\set}[1]{{\{ #1 \}}}
% \newcommand{\tuple}[1]{{( #1 )}}
% \newcommand{\mlist}[1]{{[ #1 ]}}
\newcommand{\sembrack}[1]{\llbracket#1\rrbracket}
%\newcommand{\sembrack}[1]{[\![#1]\!]}
\newcommand{\synbrack}[1]{\ulcorner#1\urcorner}
\newcommand{\commabrack}[1]{\lfloor#1\rfloor}
\newcommand{\bsynbrack}[1]{\lceil#1\rceil}
\newcommand{\bsembrack}[1]{\lceil\!\!\lceil#1\rceil\!\!\rceil}
\newcommand{\mname}[1]{\mbox{\sf #1}}
\newcommand{\mcolon}{\mathrel:}
\newcommand{\mcoloncolon}{\mathrel{\vcenter{\hbox{$::$}}}}

\newcommand{\mdot}{\mathrel.}
% \newcommand{\modpar}{\models_{\rm par}}
% \newcommand{\modreg}{\models_{\rm reg}}
% \newcommand{\proves}[2]{#1 \vdash #2}
% \newcommand{\notproves}[2]{#1 \not\vdash #2}
% \newcommand{\provesin}[3]{#1 \vdash_{#2} #3}
% \newcommand{\notprovesin}[3]{#1 \not\vdash_{#2} #3}
%\newcommand{\leqq}[1]{\mathrel{\preceq_{#1}}}

\newcommand{\fnMap}{\rightharpoonup}
\newcommand{\fnMMap}{\mathbin{\ni\joinrel\rightharpoonup}}
\newcommand{\tfnMap}{\rightarrow}
\newcommand{\tfnMMap}{\mathbin{\ni\joinrel\rightarrow}}

\newcommand{\tarrow}{\rightarrow}
% \newcommand{\term}{\seq}
\newcommand{\lub}{\sqcup}
\newcommand{\subfun}{\sqsubseteq}
\newcommand{\subpred}{\subseteq}
\newcommand{\BoxApp}{\Box\,}
\newcommand{\BOX}{\mathrel{\Box}}
\newcommand{\StarApp}{\star\,}

\newcommand{\com}{\mname{complement}}
\newcommand{\dom}{\mname{domain}}
\newcommand{\sumcl}{\mname{sum}}
\newcommand{\pow}{\mname{power}}
\newcommand{\pair}{\mname{pair}}
\newcommand{\opair}{\mname{ordered-pair}}
\newcommand{\inters}{\mname{intersection}}
\newcommand{\emp}{\mname{empty}}
\newcommand{\uni}{\mname{univocal}}
\newcommand{\fun}{\mname{function}}
\newcommand{\card}{\mname{card}}
\newcommand{\monotone}{\mname{monotone}}
\newcommand{\continuous}{\mname{continuous}}
\newcommand{\chain}{\mname{chain}}
\newcommand{\emptyfun}{\triangle}
\newcommand{\emptylist}{[\,]}

\newcommand{\True}{\mbox{\sf T}}
\newcommand{\False}{\mbox{\sf F}}
\newcommand{\Trueword}{\sf true}
\newcommand{\Falseword}{\sf false}
\newcommand{\Neg}{\neg}
\newcommand{\Implies}{\supset}
\newcommand{\ImpliesAlt}{\Rightarrow}
% \newcommand{\Iff}{\equiv}
% \newcommand{\IffAlt}{\Leftrightarrow}
\newcommand{\Sheffer}{\mathrel|}
\newcommand{\Forall}{\forall}
\newcommand{\ForallApp}{\forall\,}
\newcommand{\Forsome}{\exists}
\newcommand{\ForsomeApp}{\exists\,}
\newcommand{\ForsomeUniqueApp}{\exists\,!\,}

\newcommand{\isWellDef}{\mathop{\downarrow}}
\newcommand{\isIllDef}{\mathop{\uparrow}}

\newcommand{\qeq}{\simeq}
\newcommand{\unicorn}{\bot}
\newcommand{\TRUE}{\ensuremath{\mathbb T}}
\newcommand{\FALSE}{\ensuremath{\mathbb F}}

\allowdisplaybreaks[3]

\makeatletter
\renewcommand{\listofalgorithms}{\@starttoc{loa}{List of algorithms}}
\let\l@algorithm=\l@figure
\makeatother

% In this document, we have many multi-line displays throughout.  Tex often
% tries to deal with these by stretching vertical space to an annoying
% degree.  In any case, good page breaks for flush bottoms are just
% not to be found for many of our pages.  We therefore simply give up,
% and let page bottoms be ragged.
\raggedbottom

% Effectively ban the splitting of footnotes across pages.
% If this causes problems, we may have to try something less brutal.
\interfootnotelinepenalty=10000

% Re hbox's, see
% https://tex.stackexchange.com/questions/241343/what-is-the-meaning-of-fussy-sloppy-emergencystretch-tolerance-hbadness

% * Reference meta tags ("classifying prefixes") use in this document:
% * From fancyref:
% Chapter chap:
% Section sec:
% Equation eq:
% Figure fig:
% Table tab:
% Enumeration enum:
% Footnote fn:
%
% * Not in fancyref
% Definition def:
% Epigraph epi:
% Theorem th:
% Lemma lem:
% Remark rem:
% Observation obs:
% Example ex:
% Page page:

% The "page:" metatag is for labels intended as the target of
% page references.

% The auto-placement of qed's fails a lot.
% Semi-manual replacement using \qedhere often fails as well.
% I went with % 100% manual placement,
% which in fact is not any more trouble,
% and which is the only way to guarantee the QED's wind up where I
% want them.
\renewcommand{\qedsymbol}{}
\newcommand{\myqed}{%
  \ifmmode\square
  \else$\square$
  \fi%
}

% Hack the algorithmicx package
\algdef{S}[FOR]{ForNoDo}[1]{\algorithmicfor\ #1}%
\algtext*{EndIf}
\algtext*{EndFor}
\algtext*{EndProcedure}
\algtext*{EndWhile}

\delimitershortfall=0pt
\delimiterfactor=1100

% Shorten the distance between equations.  This is not
% to save space for its own sake, but because
% we often group equations into series, and these
% do not look good with too much space.
\newlength{\myDisplayskip}
\setlength{\abovedisplayskip}{\myDisplayskip}
\setlength{\belowdisplayskip}{\myDisplayskip}
\setlength\abovedisplayshortskip{\myDisplayskip}
\setlength\belowdisplayshortskip{\myDisplayskip}

% Some ``temporary variables'' for custom uses
\newcommand{\basis}[1]{}
\newcommand{\step}[1]{}

% Eliminate deffn someday
\newcommand{\deffn}[3]{\ensuremath{%
    \Vtyped{#1}{#3\fnMap#2}%
}}

% function composition operator
\newcommand{\afterOp}{\mathbin{\circ}}
\newcommand{\fnAfterOp}{\afterOp}
\newcommand{\relAfterOp}{\mathbin{\circ_R}}
\newcommand{\caret}{\mathord{\char`\^}}

\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\bigorder}[1]{\ensuremath{{\mathcal O}\left(#1\right)}}
\newcommand{\powerset}[1]{\ensuremath{\mathcal{P}(#1)}}
\newcommand{\Vpowerset}[1]{\ensuremath{\mathcal{P}(\V{#1})}}

% \newcommand{\TODO}[1]{\par{\bf TODO}: #1\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a realm name, when it occurs in text
\newcommand{\realm}[1]{\ensuremath{\mathtt{#1}}}

\newcommand{\dfn}[2][]{\rawdfn{#2}%
    \ifthenelse{\equal{#1}{}}{%
            \index{recce-defienda}{#2@\makefirstuc{#2}}%
        }%
        {\index{recce-defienda}{#1}}%
}
\newcommand{\rawdfn}[1]{{\bf #1}}
\newcommand{\op}[2][]{%
  \ifmmode\texttt{#2}(#1)%
  \else\texttt{#2}(#1)%
  \fi%
}
\newcommand{\proc}[2][]{%
  \ifmmode\texttt{#2}(#1)%
  \else\texttt{#2}(#1)%
  \fi%
}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\fixedsize}[1]{\ensuremath{|{#1}|}}
\newcommand{\size}[1]{\ensuremath{\left| {#1} \right|}}
\newcommand{\enRange}[2]{\ensuremath{#1\text{--}#2}}
\newcommand{\enRefs}[2]{\enRange{\ref{#1}}{\ref{#2}}}
\newcommand{\overlap}{\mathrlap{\raisebox{.5ex}{$\vee$}}\cap}
\newcommand{\plusplus}{\mathord{++}}
\newcommand{\mult}{\mathbin{\ast}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\V{n}}}
\newcommand{\lastix}[1]{\ensuremath{\##1}}

% The "Vxxx" macros wrap one of their arguments in \V{}.
% A "VVxxx" macros wraps two of its arguments in \V{}.

\newcommand{\Vlastix}[1]{\lastix{\V{#1}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\Vlastix{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\Velement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\VVelement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[ \V{#2}
    \right]\mathclose{} }}

% Rename as Felement for fixed element, if needed
% \newcommand{\element}[2]{\ensuremath{\mathop{#1}[#2]}}
% \newcommand{\Velement}[2]{\element{\V{#1}}{#2}}
% \newcommand{\VVelement}[2]{\Velement{#1}{\V{#2}}}

% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\tupelem}[2]{\ensuremath{\mathop{#1}%
    \mathopen{}\left\llbracket #2 \right\rrbracket\mathclose{} }}
\newcommand{\Vtupelem}[2]{\tupelem{\V{#1}}{#2}}
\newcommand{\VVtupelem}[2]{\Vtupelem{#1}{\V{#2}}}

\newcommand{\cuz}{%
  \ifmmode%
    \mathrel{\because}%
  \else%
    $\because$%
  \fi%
}
\newcommand{\bcuz}{%
    \linebreak[1]\cuz{}%
}
% \newcommand{\tightMath}[1]{%
%     {%
%     \medmuskip=0mu
%     \thinmuskip=0mu
%     \thickmuskip=0mu
%     \ensuremath{#1}%
%     }%
% }

% Macros for references

\newcommand{\labBase}{UNSET}
\newcommand{\mylab}[2][null]{\label{#1\labBase-#2}}
\newcommand{\Elab}[1]{\mylab[eq:]{#1}}
\newcommand{\E}[1]{\Eref{\labBase-#1}}

\newcommand{\colonpageref}[1]{%
 \ifthenelse{\equal{\pageref{#1}}{\thepage}}%
  {}{{:}\pageref*{#1}}%
}

% Pre-declare these temporary values
\newcommand{\refChapter}{UNUSED}
\newcommand{\refEqno}{UNUSED}

% "Full refs" (with 'F' in name) exist because if, for example, both refs
% are to floats the question of whether both are on the same page becomes
% difficult for Tex, and it is easiest to just force use of
% the page number.

\newcommand{\Aref}[1]{(a\ref{#1}\colonpageref{#1})}
% "full" ref -- that is, always include page
\newcommand{\AFref}[1]{(a\ref{#1}:\pageref{#1})}

% "full" ref -- that is, always include page and chapter
\newcommand{\EFref}[1]{(e\ref{eq:#1}:\pageref{eq:#1})}
\newcommand{\Eref}[2][]{%
    \ifthenelse{\equal{#1}{}}{}{%
         \ifmmode \text{``#1''}%
         \else``#1'' %
         \fi%
    }%
    \StrBefore{\getrefnumber{eq:#2}}{.}[\refChapter]%
    \StrBehind{\getrefnumber{eq:#2}}{.}[\refEqno]%
    (\ensuremath{% To cover both cases, force math mode
        \hyperref[eq:#2]{%
            \text{e}%
            \ifthenelse{\equal{\refChapter}{\thechapter}}%
            {% Do nothing
            }{% Otherwise, add chapter ref
                \refChapter{.}%
            }%
            \refEqno%
            \colonpageref{eq:#2}%
         }%
    })%
}

% genref -- generic reference
\newcommand{\genref}[3][]{%
    \ifthenelse{\equal{#1}{}}{}{%
      \ifmmode \text{``#1''}%
      \else``#1'' %
      \fi%
    }%
    (\ensuremath{% To cover both cases, force math mode
        \hyperref[#2]{%
            \textrm{#3}\ref*{#2}\colonpageref{#2}%
        }%
    })%
}
\newcommand{\Lref}[1]{(L\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Sref}[1]{(S\ref{sec:#1}:\pageref{sec:#1})} % always include page
\newcommand{\Chref}[1]{(Ch\ref{chap:#1}:\pageref{chap:#1})} % always include page
\newcommand{\Tbref}[1]{(Tb\ref{#1}\colonpageref{#1})}
\newcommand{\Pgref}[1]{(page \pageref{#1})}
\newcommand{\asideRef}[1]{Aside \ref{sec:aside-#1}, page \pageref{sec:aside-#1}}

% Try to gather all the stuff to do with theoremoids here
% Start off with theorems, then the other theoremoids,
% mostly alphabetically.

\theoremstyle{definition}

% theoremoids: Th

\newtheorem{theorem}{Theorem}
\newcommand{\rotatedSquare}{\mathop{\rotatebox[origin=c]{45}{$\square$}}}
\newcommand{\Thref}[2][]{%
    \genref[#1]{th:#2}{Th}%
}
% some day delete \longThref
\newcommand{\longThref}[2]{%
  \ifmmode \text{``#1''\Thref{#2}}%
  \else``#1'' \Thref{#2}%
  \fi%
}
% "full" ref -- always includes page
\newcommand{\ThFref}[1]{(\textrm{Th}\ref{#1}:\pageref{#1})}
\newcommand{\thEnd}{%
  \ifmmode\rotatedSquare
  \else$\rotatedSquare$
  \fi%
}

% theoremoids: Df

\newtheorem{definition}[theorem]{Definition}
\newcommand{\Dfref}[2][]{%
    \genref[#1]{def:#2}{Df}%
}
% some day delete \longDfref
\newcommand{\longDfref}[2]{%
  \ifmmode \text{``#1''\Dfref{#2}}%
  \else``#1'' \Dfref{#2}%
  \fi%
}
\newcommand{\dfEnd}{\Circle}

% theoremoids: Ex

\newtheorem{example}[theorem]{Example}
\newcommand{\Exref}[1]{\genref{ex:#1}{Ex}}
\newcommand{\exEnd}{\Circle}

% theoremoids: Ob

\newtheorem{observation}[theorem]{Observation}
\newcommand{\Obref}[1]{\genref{obs:#1}{Ob}}
\newcommand{\obEnd}{\Circle}

% theoremoids: Rm

\newtheorem{remark}[theorem]{Remark}
\newcommand{\Rmref}[2][]{\genref[#1]{rem:#2}{Rm}}
\newcommand{\rmEnd}{\Circle}

% local theoremoids: Lm

\newtheorem{lemma}{Lemma}[chapter]
\newcommand{\Lmref}[2][]{\genref[#1]{lem:#2}{Lm}}
% some day delete \longLmref
\newcommand{\longLmref}[2]{%
  \ifmmode \text{``#1''\Lmref{#2}}%
  \else``#1'' \Lmref{#2}%
  \fi%
}

\newcommand{\lmEnd}{\thEnd}

% local theoremoids: Ld (local definition)

\newtheorem{ldef}{Local Definition}[chapter]
\newcommand{\Ldref}[2][]{\genref[#1]{ldef:#2}{df}}
% some day delete \longLdref
\newcommand{\longLdref}[2]{%
  \ifmmode \text{``#1''\Ldref{#2}}%
  \else``#1'' \Ldref{#2}%
  \fi%
}

\newcommand{\ldEnd}{\dfEnd}

%% End of theoremoid section

% I once used hyphens in variable names,
% so I needed to ensure that subtraction was
% clearly distinguished by the typography.
% I now need to remove this macro.
\newcommand{\subtract}[2]{\ensuremath{#1-#2}}

\newcommand{\eqWClOp}[1]{\mathrel{=:\joinrel{#1}}}
\newcommand{\eqClOp}[1]{\ensuremath{=_{#1}}}
\newcommand{\eqWTyOp}{\ensuremath{\eqWClOp{\Ds}}}
\newcommand{\eqTyOp}{\ensuremath{\eqClOp{\Ds}}}

\newcommand{\rawDRel}[1]{\raise-.3pt\hbox{$\diamondsuit$}\mkern-1mu{#1}}
\newcommand{\DRel}[1]{\ensuremath{\mathrel{\rawDRel{#1}}}}
\newcommand{\DefOp}{\ensuremath{\mathrel{\rawDRel{}}}}
\newcommand{\rawDDRel}[1]{\raise-.3pt\hbox{${\diamondsuit}\mkern-11mu{\diamondsuit}$}%
  \mkern-1mu{#1}}
\newcommand{\DDRel}[1]{\ensuremath{\mathrel{\rawDDRel{#1}}}}
\newcommand{\LDefOp}{\ensuremath{\mathrel{\rawDDRel{}}}}
\newcommand{\DEqWClOp}[1]{\DRel{\mathrel{=:\joinrel{#1}}}}
\newcommand{\DEqClOp}[1]{\DRel{\ensuremath{=_{#1}}}}
\newcommand{\DEqWTyOp}{\DRel{\ensuremath{\eqWClOp{\Ds}}}}
\newcommand{\DEqTyOp}{\DRel{\ensuremath{\eqClOp{\Ds}}}}
\newcommand{\DDEqWClOp}[1]{\DDRel{\mathrel{=:\joinrel{#1}}}}
\newcommand{\DDEqClOp}[1]{\DDRel{\ensuremath{=_{#1}}}}
\newcommand{\DDEqWTyOp}{\DDRel{\ensuremath{\eqWClOp{\Ds}}}}
\newcommand{\DDEqTyOp}{\DDRel{\ensuremath{\eqClOp{\Ds}}}}

\newcommand{\defined}{\DRel{\mathrel{=}}}
\newcommand{\ldefined}{\DDRel{\mathrel{=}}}

\newcommand{\condOpA}{\mathbin{?}}
\newcommand{\condOpB}{\mathbin{:}}
\newcommand{\cond}[3]{\ensuremath{#1\condOpA#2\condOpB#3}}

\newcommand{\V}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\var}[1]{\V{#1}}
\newcommand{\varprime}[2]{\ensuremath{\texttt{#1}#2}}
\newcommand{\boldvar}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mapsto}
\newcommand{\nderives}[1]{
\mathrel{%
  \ifthenelse{\equal{#1}{}}{%
    {\not\mapsto}%
  }{%
    {\mbox{$\:\stackrel{\!{#1}}{\not\mapsto\!}\:$}}%
  }%
}}%
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\mapsto\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\mapsto\!}\:$}}}
\newcommand{\ndestar}{\nderives{\ast}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\mapsto\!}\:$}}}
\newcommand{\ndeplus}{\nderives{+}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\mapsto\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\mapsto\!}\:$}}}

\newcommand{\inr}{\mathrel{\overline{\in}}}
% scale plus signs down further?
% \newcommand{\plusplus}{\mathbin{++}}
% \newcommand{\minusminus}{\mathbin{--}}
\newcommand{\incr}[1]{\ensuremath{#1+1}}
\newcommand{\Vincr}[1]{\incr{\V{#1}}}
% \newcommand{\decr}[1]{\ensuremath{#1\text{\kern .04em{-}\kern .07em{-}}}}
\newcommand{\decr}[1]{\ensuremath{#1-1}}
\newcommand{\Vdecr}[1]{\decr{\V{#1}}}
\newcommand{\Rseq}[1]{{\left[ #1 \right]} }
\newcommand{\set}[1]{%
    \ensuremath{{\left\lbrace #1 \right\rbrace}}%
}
\newcommand{\midOp}{|}
\newcommand{\collB}[2]{% collection builder, with mid op
    \ensuremath{\left\lbrace #1 \;\middle\midOp\; #2 \right\rbrace}%
}
\newcommand{\famB}[2]{% family builder, with mid op
    \ensuremath{\left[ #1 \;\middle\midOp\; #2 \right]}%
}
\newcommand{\quantify}[3]{% quantifier, variable, predicte
    \ensuremath{#1#2 \mcoloncolon #3}%
}


\newcommand{\rIota}{\mathop{\scalebox{1.5}{\rotatebox[origin=c]{180}{$\iota$}}}}
\newcommand{\iotaQOp}{\rIota}
\newcommand{\iotaQ}[2]{\quantify{\rIota}{#1}{#2}}
\newcommand{\existUniqQOp}{\mathop{\exists{!}}}
\newcommand{\existUniqQ}[2]{\quantify{\existUniqQOp}{#1}{#2}}
% extra {} in existQOP and uniqQOp to raise to baseline
\newcommand{\existQOp}{\mathop{\exists{}}}
\newcommand{\existQ}[2]{\quantify{\existQOp}{#1}{#2}}
% extra {} in existQOP and univQOp to raise to baseline
\newcommand{\univQOp}{\mathop{\forall{}}}
\newcommand{\univQ}[2]{\quantify{\univQOp}{#1}{#2}}

\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\typed}[2]{\ensuremath{#1\mcolon#2}}
\newcommand{\Vtyped}[2]{\ensuremath{\var{#1}\mcolon#2}}
\newcommand{\VVtyped}[2]{\ensuremath{\var{#1}\mcolon\var{#2}}}

% Eliminate this, now that I follow Chiron notation?
\newcommand{\family}[1]{\ensuremath{\left[ #1 \right]}}

\newcommand{\seq}[1]{\ensuremath{\left[ #1 \right]}}
\newcommand{\Dom}[1]{\ensuremath{\mymathop{Dom}(#1)}}
\newcommand{\Cod}[1]{\ensuremath{\mymathop{Cod}(#1)}}
\newcommand{\Ran}[1]{\ensuremath{\mymathop{Ran}(#1)}}
\newcommand{\Pairs}[1]{\ensuremath{\mymathop{Pairs}(#1)}}
\newcommand{\frontier}[1]{{\left\lfloor #1 \right\rfloor} }
\newcommand{\Vfrontier}[1]{\frontier{\boldvar{#1}}}
\newcommand{\boldElement}[2]{\textbf{#1[#2]}}
\newcommand{\boldRange}[3]{\textbf{#1[#2 \ldots #3]}}
\newcommand{\boldRangeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\textbf{#3}-1$)}]}%
}
\newcommand{\boldRangeSizeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\left|\textbf{#3}\right|-1$)}%
]} }

% Because I use multi-character variables, I need special typography for them.
% The V{} macro does this.  Many other macros are prefaced with a "V" to indicate
% the first appropriate argument is to be have the V{} macro applied.
% A "VV" indicates the first two appropiate arguments should have V{} applied.
% Macros prefaced with a "R" apply realm information.

\newcommand{\Rvar}[2]{\ensuremath{\V{#1}_{\V{#2}}}}
\newcommand{\Rvarseq}[2]{\ensuremath{\V{#1}_{\Rseq{\V{#2}}}}}
\newcommand{\Rvarset}[2]{\ensuremath{\V{#1}_{\set{\V{#2}}}}}
\newcommand{\wRange}[2]{\boldRange{w}{#1}{#2}}

\newcommand{\eim}[1]{\ensuremath{#1_\realm{EIM}}}
\newcommand{\es}[1]{\ensuremath{#1_\realm{ES}}}
\newcommand{\et}[1]{\ensuremath{#1_\realm{ET}}}
\newcommand{\mylim}[1]{\ensuremath{#1_\realm{LIM}}}
\newcommand{\lss}[1]{\ensuremath{#1_\realm{LSS}}}
\newcommand{\les}[1]{\ensuremath{#1_\realm{LES}}}
\newcommand{\mes}[1]{\ensuremath{#1_\realm{MES}}}
% Delete NSS?
\newcommand{\nss}[1]{\ensuremath{#1_\realm{NSS}}}

% change to tagged form?  Or drop macro for "raw" str entirely?
\newcommand{\str}[1]{\ensuremath{\langle\langle{#1}\rangle\rangle}}

% change to tagged form?  Or drop macro for "raw" term entirely?
\newcommand{\term}[1]{\sym{#1}}

% change to tagged form?  Or drop macro for "raw" sym entirely?
\newcommand{\sym}[1]{\ensuremath{\langle{#1}\rangle}}
\newcommand{\Vbool}[1]{\Rvar{#1}{BOOL}}
\newcommand{\Vcfg}[1]{\Rvar{#1}{CFG}}
\newcommand{\dr}[1]{\ensuremath{#1_\realm{DR}}}
\newcommand{\Vdr}[1]{\dr{\V{#1}}}
\newcommand{\Vdrset}[1]{\Rvarset{#1}{DR}}
\newcommand{\Veim}[1]{\eim{\V{#1}}}
\newcommand{\Veimseq}[1]{\Rvarseq{#1}{EIM}}
\newcommand{\Veimset}[1]{\Rvarset{#1}{EIM}}
\newcommand{\ves}[1]{\ensuremath{#1_\realm{VES}}}
\newcommand{\Ves}[1]{\es{\V{#1}}}
\newcommand{\Vesseq}[1]{\Rvarseq{#1}{ES}}
\newcommand{\Vet}[1]{\et{\V{#1}}}
\newcommand{\Vext}[1]{\Rvar{#1}{EXT}}
\newcommand{\Vint}[1]{\Rvar{#1}{INT}}
\newcommand{\Vlim}[1]{\mylim{\V{#1}}}
\newcommand{\Vlss}[1]{\lss{\V{#1}}}
\newcommand{\Vlssset}[1]{\Rvarset{#1}{LSS}}
\newcommand{\Vlp}[1]{\Rvar{#1}{LP}}
\newcommand{\Vlimset}[1]{\Rvarset{#1}{LIM}}
\newcommand{\Vloc}[1]{\Rvar{#1}{LOC}}
\newcommand{\Vles}[1]{\les{\V{#1}}}
\newcommand{\Vmes}[1]{\mes{\V{#1}}}
% Delete Vnss?
\newcommand{\Vnss}[1]{\nss{\V{#1}}}
\newcommand{\Vcurr}[1]{\Rvar{#1}{CURR}}
\newcommand{\Vorig}[1]{\Rvar{#1}{ORIG}}
\newcommand{\Vpim}[1]{\Rvar{#1}{PIM}}
\newcommand{\Vpimset}[1]{\Rvarset{#1}{PIM}}
\newcommand{\Vrule}[1]{\Rvar{#1}{RULE}}
\newcommand{\Vruleset}[1]{\Rvarset{#1}{RULE}}

\newcommand{\Vstr}[1]{\doublebox{\ensuremath{\mathstrut\V{#1}}}}

\newcommand{\Vstrset}[1]{\Rvarset{#1}{STR}}
\newcommand{\Vterm}[1]{\Vsym{\boldvar{#1}}}
\newcommand{\Vtermset}[1]{\Rvarset{\boldvar{#1}}{TERM}}

\newcommand{\Vsym}[1]{\ensuremath{\boxed{\vphantom{\hbox{\mathstrut}} \V{#1}}}}

\newcommand{\Vsymset}[1]{\Rvarset{#1}{SYM}}
\newcommand{\Vtoken}[1]{\Rvar{#1}{TOKEN}}
\newcommand{\Vves}[1]{\ves{\V{#1}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\V{#1}}}}
\newcommand{\nat}[1]{\ensuremath{#1_\realm{\naturals}}}
\newcommand{\Vnat}[1]{\nat{\V{#1}}}
\newcommand{\z}[1]{\ensuremath{#1_\realm{\integers}}}
\newcommand{\Vz}[1]{\z{\V{#1}}}

\newcommand{\naturals}{\ensuremath{\mathbb{N}}}
\newcommand{\integers}{\ensuremath{\mathbb{Z}}}
\newcommand{\algo}[1]{\texorpdfstring{\ensuremath{\mathbb{\uppercase{#1}}}}{#1}}
\newcommand{\Earley}{\algo{Earley}\xspace}
\newcommand{\Leo}{\algo{Leo}\xspace}
\newcommand{\Marpa}{\algo{Marpa}\xspace}
\newcommand{\Rtwo}{\texttt{Marpa::R2}\xspace}

% I want to use 'call' outside of pseudocode
\newcommand\mycallname[1]{\textproc{#1}}
\newcommand\call[2]{\mycallname{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newcommand{\Cg}{\V{g}}
\newcommand{\Cw}{\V{w}}
\newcommand{\CVw}[1]{\ensuremath{\Vsym{\Cw[\V{#1}]}}}

\newcommand{\rawfn}[2]{\ensuremath{#1(#2)}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\bigrawfn}[2]{\ensuremath{#1\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\fname}[1]{\V{#1}}
\newcommand{\bigfn}[2]{\bigrawfn{\fname{#1}}{#2}}
\newcommand{\myfn}[2]{\rawfn{\fname{#1}}{#2}}
\newcommand{\Vmyfn}[2]{\myfn{#1}{\V{#2}}}

\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\Cause}[1]{\ensuremath{\mymathop{Cause}(#1)}}
\newcommand{\Current}[1]{\ensuremath{\mymathop{Current}(#1)}}
\newcommand{\DotPos}[1]{\ensuremath{\mymathop{DotPos}(#1)}}
\newcommand{\DottedRules}[1]{\ensuremath{\mymathop{DottedRules}(#1)}}
\newcommand{\DR}[1]{\ensuremath{\mymathop{DR}(#1)}}
\newcommand{\GOTO}{\ensuremath{\mymathop{GOTO}}}
\newcommand{\ID}[1]{\ensuremath{\mymathop{ID}(#1)}}
\newcommand{\Invocs}[1]{\ensuremath{\mymathop{Invocs}(#1)}}
\newcommand{\LeoEligible}[1]{\ensuremath{\mymathop{LeoEligible}(#1)}}
\newcommand{\LeoUnique}[1]{\ensuremath{\mymathop{LeoUnique}(#1)}}
\newcommand{\LHS}[1]{\ensuremath{\mymathop{LHS}(#1)}}
\newcommand{\Lim}[1]{\ensuremath{\mymathop{Lim}(#1)}}
\newcommand{\LIMPredecessor}[1]{\ensuremath{\mymathop{LIMPredecessor}(#1)}}
\newcommand{\LinkPairs}[1]{\ensuremath{\mymathop{LinkPairs}(#1)}}
\newcommand{\Matches}[1]{\ensuremath{\mymathop{Matches}(#1)}}
\newcommand{\Maximal}[1]{\ensuremath{\mymathop{Maximal}(#1)}}
\newcommand{\myL}[1]{\ensuremath{\mymathop{L}(#1)}}
\newcommand{\Next}[1]{\ensuremath{\mymathop{Next}(#1)}}
\newcommand{\NextSource}[1]{\ensuremath{\mymathop{NextSource}(#1)}}
\newcommand{\NT}[1]{\ensuremath{\mymathop{NT}(#1)}}
\newcommand{\Origin}[1]{\ensuremath{\mymathop{Origin}(#1)}}
\newcommand{\Passes}[1]{\ensuremath{\mymathop{Passes}(#1)}}
\newcommand{\Penult}[1]{\ensuremath{\mymathop{Penult}(#1)}}
\newcommand{\Pfx}[1]{\ensuremath{\mymathop{Pfx}(#1)}}
\newcommand{\Postdot}[1]{\ensuremath{\mymathop{Postdot}(#1)}}
\newcommand{\Predecessor}[1]{\ensuremath{\mymathop{Predecessor}(#1)}}
\newcommand{\Predict}[1]{\ensuremath{\mymathop{Predict}(#1)}}
\newcommand{\Predot}[1]{\ensuremath{\mymathop{Predot}(#1)}}
\newcommand{\PreviousLink}[1]{\ensuremath{\mymathop{PreviousLink}(#1)}}
\newcommand{\PrPfx}[1]{\ensuremath{\mymathop{PrPfx}(#1)}}
\newcommand{\PrSfx}[1]{\ensuremath{\mymathop{PrSfx}(#1)}}
\newcommand{\PSL}[2]{\ensuremath{\mymathop{PSL}[#1][#2]}}
\newcommand{\RHS}[1]{\ensuremath{\mymathop{RHS}(#1)}}
\newcommand{\Rightmost}[1]{\ensuremath{\bigfn{Rightmost}{#1}}}
\newcommand{\RightRecursive}[1]{\ensuremath{\mymathop{RightRecursive}(#1)}}
\newcommand{\Rule}[1]{\ensuremath{\mymathop{Rule}(#1)}}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Seen}[1]{\ensuremath{\mymathop{Seen}(#1)}}
\newcommand{\SetSeen}[1]{\ensuremath{\mymathop{SetSeen}(#1)}}
\newcommand{\Sfx}[1]{\ensuremath{\mymathop{Sfx}(#1)}}
\newcommand{\Source}[1]{\ensuremath{\mymathop{Source}(#1)}}
\newcommand{\Space}[1]{\ensuremath{\mymathop{Space}(#1)}}
\newcommand{\Symbol}[1]{\ensuremath{\mymathop{Symbol}(#1)}}
\newcommand{\Term}[1]{\ensuremath{\mymathop{Term}(#1)}}
\newcommand{\Time}[1]{\ensuremath{\mymathop{Time}(#1)}}
\newcommand{\Transition}[1]{\ensuremath{\mymathop{Transition}(#1)}}
\newcommand{\transitions}[2]{\ensuremath{%
    \element{\element{\V{transitions}}{#1}}{#2}}}
\newcommand{\Valid}[1]{\ensuremath{\mymathop{Valid}(#1)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}

\numberwithin{equation}{chapter}

% I use parboxes in equations.  This sets a useful width for them.
\newlength{\mathparwidth}
\newlength{\longtagwidth}
\settowidth{\longtagwidth}{(99999)\quad}
\setlength{\mathparwidth}{\dimexpr\textwidth-\longtagwidth}

% The following set of macros is for putting paragraphs in cells of tables.

% \cellRule takes the string and makes sure there is enough
% space above and below for parentheses, etc.  It need only be called
% once per row, and sometimes is used for an empty cell ("\cellrule{}")
% just in order to set the vertical spacing for the row.
\newcommand{\cellRule}[1]{%
        \rule{0em}{2.3ex}%
        #1\rule[-1.05ex]{0em}{0in}%
}

\newcommand{\cellboxcWidth}[1]{1in}
\newcommand{\cellboxc}[2][\cellboxcWidth]{%
    \parbox{#1}{\centering\cellRule{#2}}%
}
\newcommand{\cellboxrWidth}[1]{4.5in}
\newcommand{\cellboxr}[2][\cellboxrWidth]{%
    \parbox{#1}{\raggedright\cellRule{\ignorespaces#2}}%
}
\newcommand{\mylegend}[1]{%
    \parbox{\textwidth}{\leavevmode\\[1ex]#1}}

\newcommand{\myparbox}[2][\mathparwidth]{%
  \parbox[t]{#1}{%
  % \linespread{1.15}\selectfont
  %\raggedright#2\par
  \centering#2\par
  % \vspace{-\prevdepth} % remove the depth of the last line
  % \vspace{1ex} % add a fixed vertical space
  }%
}
\newcommand{\cuzbox}[1]{%
   \myparbox{\cuz{} #1}
}
% A "My paragraph-based equation"
\newcommand{\eqpar}[2]{\mypareq{eq:\labBase-#1}{#2}}
\newcommand{\mypareq}[2]{%
   \ifthenelse{\equal{#1}{}}%
   {%
       \begin{equation*}
   }{%
       \begin{equation}%
       \label{#1}
   }%
   \begin{gathered}%
   \myparbox{#2}%
   \end{gathered}%
   \ifthenelse{\equal{#1}{}}%
   {%
       \end{equation*}
   }{%
       \end{equation}
   }%
}
\newcommand{\rawparcomment}[2][\mathparwidth]{%
  \myparbox[#1]{#2}%
}
\newcommand{\parcomment}[2][\mathparwidth]{%
  \myparbox[#1]{$\triangleright$ #2}%
}
\newcommand{\lcomment}[1]{\hspace\algorithmicindent$\triangleright$ #1}

\newsavebox{\myBox}
\newlength{\myHt}
\newlength{\myDp}
\newlength{\myWd}
\newcommand{\padBoxC}[3]{%
    \savebox{\myBox}{\ignorespaces#3}%
    \usebox{\myBox} \myHt=\ht\myBox \myDp=\dp\myBox%
    \vrule height\dimexpr\myHt+#1 depth\dimexpr\myDp+#2 width0pt%
}
\newcommand{\padBox}[2]{\padBoxC{#1}{#1}{#2}}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}
\hyphenation{TeXbook}

\begin{document}

\title{The Marpa Book}
\author{Jeffrey Kegler}
\ifdraft{
    % legacy draftwatermark options -- update?
    \SetWatermarkLightness{0}
    \SetWatermarkText{DRAFT \DTMnow{} DRAFT}
    \SetWatermarkAngle{0}
    % \SetWatermarkScale{1}
    % \SetWatermarkHorCenter{1.5in}
    \SetWatermarkVerCenter{.5in}
}{}
\thanks{\ifdraft{%
    DRAFT as of \DTMnow%
}{%
    TODO: No date set
}}
\thanks{%
Copyright \copyright\ 2021 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

% \begin{abstract}
% The \Marpa{} recognizer is described.
% \Marpa{} is
% a fully implemented
% algorithm for the recognition,
% parsing and evaluation of context-free grammars.
% \Marpa{} is an Earley parser
% which includes the first
% practical implementation
% of the improvements
% in Joop Leo's 1991 paper.
% In addition,
% \Marpa{} tracks the full state of the parse,
% at run-time,
% in a form accessible by the application.
% This greatly improves error detection;
% enables event-driven parsing;
% and allows techniques such as
% ``Ruby Slippers'' parsing,
% in which
% the input is altered in response
% to the parser's expectations.
% \end{abstract}

\maketitle

\chapter{Introduction}

\myepi[.83\textwidth]{%
\label{epi:dewey}
[W]ith
the development of speech (usually in the second year) adaptation of the
baby's activities to and with those of other persons gives the keynote
of mental life. His range of possible activities is indefinitely widened
as he watches what other persons do, and as he tries to understand and
to do what they encourage him to attempt. The outline pattern of mental
life is thus set in the first four or five years. Years, centuries,
generations of invention and planning, may have gone to the development of
the performances and occupations of the adults surrounding the child. [...]
Were it not for this process by which the achievements of one
generation form the stimuli that direct the activities of the next,
the story of civilization would be writ in water, and each
generation would have laboriously to make for itself, if it could,
its way out of savagery.}
{John~Dewey~\footnotemark}
\footnotetext{Sources and other details for the epigraphs are given
starting at page \pageref{chap:epigraphs}.}

\myepi{%
\label{epi:goedel}
The more I think about language,
the more it amazes me that people ever understand each other at all.}
{Kurt G{\"o}del}

The \Marpa project was intended to create
a practical and highly available tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite an encouraging academic literature,
no such tool had existed for context-free parsing.

The first stable version of \Marpa was uploaded to
a public archive on Solstice Day 2011%
\index{recce-general}{Solstice Day 2011}.
This book
describes the algorithm used
in the most recent version of \Marpa,
\Rtwo~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in the paper which originally described Marpa~\cite{Kegler2019}.

The presentation in this book is theoretical,
but the approach is practical.
The \Rtwo{} implementation is widely available
and has seen considerable use.

\Chref{features}
describes the important features of \Marpa.
\Chref{using-features}
goes into detail about their application.
The remaining chapters are incomplete and
inconsistent and the material in them is
subject to deletion.

\todo{Uncomment this summary of chapters, and correct it.}

% Section \Chref{math-conventions}
% describes our conventions for presenting
% the mathematics, and includes
% remarks on methodology and presentation.
% Section \Chref{preceding-disciplines}
% contains definitions which are ``general'',
% that is, for the mathematics which we use as
% a basis for our parsing discussions.
% Section \Chref{CFGs} defines context-free grammars.
% Section \Chref{lr-grammars} defines LR grammars.
% Section \Chref{EXTs} defines \Marpa{}'s external grammars.
% Section \Chref{useless-symbols} describes \Marpa{}'s handling of ``useless''
% symbols.
%
% Section \Chref{rewrite} describes the way in which \Marpa{}
% rewrites its external grammars into internal grammars.
% Section \Chref{INTs} defines \Marpa{}'s internal grammars.
% Sections \Chref{earley-item} and
% \Chref{earley-table} introduce Earley parsing.
% Section \Chref{leo} describes Joop Leo's modification
% of Earley's algorithm.
% Section \Chref{ancestry} describes the tracking
% of the ``ancestry'' of Earley items.
% \Chref{complexity-preliminaries} lays our strategy
% for showing the time and speed complexity of \Marpa{}.
%
% \Chref{algorithm} describes the \Marpa{} algorithm.
% It includes pseudocode, as well as the low-level complexity analysis.
%
% Section \Chref{ambiguity} contains a treatment of parsing ambiguity,
% with particular relevance to \Marpa{}.
% Section \Chref{per-set-lists} describes \Marpa{}'s
% per-set lists.
% Section \Chref{correctness} contains
% a correctness proof for \Marpa{}.
% Section \Chref{reverse-leo} describes our method
% for reversing the Leo memoizations.
% Section \Chref{lrr-complexity} shows that Marpa
% has linear complexity for for LRR grammars.
% Section \Chref{complexity} lays out Marpa's non-linear
% complexity results, which match those of \Earley{}.
% Finally,
% section \Chref{input}
% generalizes \Marpa{}'s input model.

We\footnote{%
    In the math literature it is traditional,
    to refer to the author in the third person ---
    as a ``we''.
    This is done even when, as in this case,
    there is only one author.
    We do not know the origins of this convention,
    but we like to see it
    as inviting the reader to join with us in a collaboration.
    (Compare \cite[p. 492]{Cummings2021}.)}
assume familiarity with the theory of parsing,
as well as Earley's algorithm.
\cite{Timeline}
contains a full description of \Marpa's relationship to
prior work.
Readers may find it helpful to read
\cite{Timeline}
before this book.

\chapter{Features}
\label{chap:features}

\section{General context-free parsing}
The \Marpa algorithm is capable of parsing all
context-free grammars.
\Marpa's \Rtwo{} implementation~\cite{Marpa-R2}
parses all cycle-free context-free
grammars.\footnote{
Earlier implementations of \Marpa allowed cycles,
but support for them
was removed because
practical interest seems to be non-existent.
For more detail, see \Chref{EXTs}.
}
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},%
\index{recce-general}{Earley, Jay}
that is, never worse than $\order{\V{n}^3}$.

\section{Linear time for practical grammars}
Currently, the grammars suitable for practical
use are thought to be a subset
of the deterministic context-free grammars (DCFG's).
\Marpa uses a technique discovered by
Leo~\cite{Leo1991}
to run in
\On{} time for LR-regular (LRR) grammars,
a superset of the DCFG's.
\Marpa
also parses many ambiguous grammars in linear
time.

\section{Left-eidetic}
The original \Earley{} algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
\Marpa's parse engine makes available
its
information about the current state of the parse.
This information is available
in a useful form
and can be accessed efficiently during the parse.

\section{Recoverable from read errors}
When
\Marpa reads a token that it cannot accept,
the error is fully recoverable.
An application can attempt to read another
token,
repeatedly if that is desirable.
Once the application provides
a token that is accepted by the parser,
parsing will continue
as if the unsuccessful attempts had never been made.

\section{Ambiguous tokens}
\Marpa allows ambiguous tokens.
These are often useful. In natural language processing,
for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
recovery from rejected tokens,
so that an application could react to the
rejection of a token by reading two others.

\chapter{Using the features}
\label{chap:using-features}

\myepi{%
\label{epi:duhem}
But translation is treacherous: {\it traduttore,
it traditore} (to translate
is to betray). There is never a complete equivalence between two
texts when one is a translated version of the other.}
{Pierre Duhem}

\section{Error reporting}
An obvious application of left-eideticism is error
reporting.
\Marpa's abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar.
\Marpa, as implemented in \Rtwo{}~\cite{Marpa-R2},
can detect an ambiguity and report
specifically where it occurred
and what the alternative parses were.

\section{Event driven parsing}
As implemented, \Rtwo{}
allows the user to define ``events''.
Events can be defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\section{Ruby slippers parsing}
Left-eideticism, efficient error recovery,
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
In traditional parser practice,
error detection is an act of desperation.
In contrast,
\Marpa's error detection is so painless
that it can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
that matches the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of \Rtwo{}, we
% \footnote{
% The use of ``we''
% follows the tradition in which a mathematician
% speaks as a representative of a community of inquiry,
% one which includes the reader.
% }
have implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple to describe
even perfectly
standard-conformant HTML but,
using the Ruby Slippers,
the lexical analyzer can
supply start and end tags as required by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\section{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very different from the practice in the languages humans choose
when communicating with each other.
The language of this book, English, is notoriously
ambiguous.
Human languages exploit ambiguity in order to allow expression
to be more flexible and powerful.

Ambiguity, of course, can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Consider, for comparison purposes,
the standard treatment of syntax errors.
Syntax errors are undesirable, but nobody tries
to design languages that make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be not just cumbersome, but dangerous.
A parser would never warn the user about typos,
because all typos in such a language would have unintended
meanings.

With \Marpa, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
A \Marpa-powered
language can be designed to allow ambiguity,
and any actual ambiguity will be detected
and reported at parse time.
This exploits \Marpa's ability
to report exactly where
and what the ambiguity is.
\Rtwo's own parser description language, the SLIF,
uses ambiguity in this way.

\section{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LRR languages
opens the way to auto-generated languages.
Language auto-generation, in turn, opens the
way for true second-order languages.

\section{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
that are useful for second-order programming.
True second-order languages --- languages that
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With \Marpa, this barrier is lifted.
As an example,
\Rtwo's own parser description language, the SLIF,
allows ``precedenced rules''.
Precedenced rules are specified in an extended BNF.
The BNF extensions allow precedence and associativity
to be specified for each RHS.

\Rtwo's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to auto-generate a pure BNF grammar which is equivalent,
and which has the desired precedence and associativity.

In creating the auto-generated BNF grammar,
the SLIF does a standard parsing textbook transformation,
translating explicitly specified precedence and associativity
into pure BNF that implicitly implements
that precedence and associativity.
Since the SLIF is powered by \Marpa, which is
linear for LRR,
the SLIF can easily ensure that the grammar
that it auto-generates will
be parsed in linear time.

Notationally, the SLIF's precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison.
There are two important differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity greater than three.

Second, and more importantly, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies.
The user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\chapter{Mathematical conventions}
\label{chap:math-conventions}

\todo[prepend, caption={``Conventions'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{In following chapters, implement changes in
    LaTeX comment following this TODO.}
% TODO as I move forward through the chapters:
%
% 1.) Change captialization of theorems, lemmas.
% 2.) \thEnd after each theorem, \lmEnd after each lemma statement.
% 3.) Convert theorems to lemmas, as appropriate.
% 4.) Create remarks, as appropriate.
% 5.) Convert definitions to local definitions, as appropriate.
% 6.) Replace subtraction of one with \decr{}?
% 7.) Reserve V for characteristic function of class of all sets.

\myepi{You should optimize your writing for the reader,
     not for the word-counter.}
{Jay Cumming \cite[p. 491]{Cummings2021}}

\section{The long form}

\todo{Describe "long form" style.}

This is the first of the mathematical chapters.
This chapter contains considerations that introduce and
describe the deductive reasoning,
but which are outside of it.

Often mathematics is presented very tersely.
In fact, mathematics is often written so tersely
that a longer presentation would be significantly
faster and easier to read and understand.
Our presentation is ``long form''\footnote{
    The term ``long form'' is from Cummings~\cite{Cummings2021}.
    However, while our method of presentation is in the same spirit
    as Cummings, it has little else in common.}.
This is a break from previous
parsing literature, which almost universally closely adheres to
a ``terse form''.

The traditional parsing literature did not adopt the
``terse form'' gratuitously.
Its conventions were formed
when the dissemination of mathematics relied on printed media.
The economics of print made
mathematical publishing a zero-sum game:
every line printed came at the expense of other mathematics.
Mathematics that did reach printed form
might well be lost forever.
And the participants realized that contemporary mathematical judgement
is an infallible measure of ultimate value.
Some of the greatest mathematics barely survived to see the light of day.

To mitigate these losses, mathematicians enforced a
ruthless brevity on themselves.
Many of the details of the reasoning
were left out,
and the reader was expected to reconstruct them.
Compact notations were accepted even when
hard on the eyes.
Helpful explanations were left implicit.
Historical narrative and motivation were almost
entirely excluded.

Narrative and motivation may be seen as expendable,
but with the passage of time their omission has produced
an irrecoverable loss to history of the field.%
\footnote{%
   An important adversary of this disastrous tend was Donald Knuth.
   Knuth went to considerable lengths to preserve this material
   relevant to the motivations,
   not only of his own work,
   but of the work of others.%
}
In every field, judgments must be made as
to what is important and what is not.
To the extent a new investigator lacks an explanation of why
her predecessors
made the judgments they made,
she faces an unhappy choice.
She can suppress her skepticism,
and accept previous judgments uncritically,
or she can risk rejecting those judgments
out of ignorance.
We believe this erasure of the field's history is much
of the explanation for its slowed progress
after the 1970s.

The dissemination of this book is likely to
be electronic, primarily.
This liberates us from the cruel trade-offs of
printed dissemination.
We detail our arguments as long as the extra depth seems
to add insight into,
or increase confidence in the correctness of,
our reasoning.
We stop when additional detail seems to degenerate
into mere symbol-pushing.

We will be willing to include explanations
and adopt conventions
that increase the length of this book,
if they shorten the time needed for a careful reading.
We include narrative and motivation if they
seem likely to be helpful in the preservation of
useful history.
And we are careful with our use of superscripts
and subscripts to avoid those traditional uses
which pose legibility and ophthalmological challenges.

We make heavy use of multi-character variable names.
This makes implicit operations inconvenient so that,
with one exception,
operations are never implicit.
The exception is the concatenation of strings and symbols ---
as will be explained in \Chref{foundation},
these have a special notation which makes implicit operations
unambiguous.

\section{Base theories}

\epigraph{%
\label{epi:tarski}
[Mathematical] statements are generally accompanied by arguments whose
purpose is to demonstrate their truth.}{Tarski~\cite[p. 3]{Tarski1994}}

\todo{Move this section to foundations}

The algorithmic notions are, in concept,
ultimately reducible to sequences of the primitive operations
of a random access machine (RAM), like the one described by Earley%
\index{recce-general}{Earley, Jay}
but with two additional understandings.%
\footnote{Jay Earley%
  \index{recce-general}{Earley, Jay}
  describes his random access machine model in
  \cite[p. 61]{Earley1970}.
}
First, the word size of our machine must be less than
$\V{c}\mult\V{n}$,
where \V{c} is a constant and \V{n} is the size of the problem under
discussion.
Second, the primitive operations of our RAM
are understood to be those typical of a modern computer.%
\footnote{Jay Earley's%
\index{recce-general}{Earley, Jay}
primitive operations only included addition --
  his algorithm and his data structure (linked lists) did not need multiplication.
  Our changes bring us in line with the more modern
  approach of Cormen et al. 2009~\cite[p. 23--24]{Cormen2009}.
  In their RAM, multiplication and bit shifts are primitive operations,
  but their RAM does not have an exponentiation instruction.%
}
In fact, we never break any algorithm down even close
to the primitive operation level.
Once a piece is small enough that we can conveniently
characterize its complexity by other means,
we will break it down no further.\footnote{%
  Those who want more implementation details will find
  most of the procedures in this book,
  including all of
  the procedures required for a practical implementation,
  broken further down in our C language
  implementation
  \cite{Marpa-R2}.%
}

\Earley{} will refer to the Earley's%
\index{recce-general}{Earley, Jay}
original
recognizer~\cite{Earley1970}
with zero characters of lookahead.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa will refer to the recognizer described in
this book, and implemented in
\Rtwo~\cite{Marpa-R2}.

In this book,
a \dfn{function} is never an algorithm ---
it always refers to the set theoretic notion.
Algorithms are also referred to as \dfn{procedures}.
The procedures which are major building blocks
of \Marpa will be called \dfn{ops}.
A second level of \Marpa building blocks will
be called \dfn{sub-ops}.

The building blocks of \Earley{} will be
called \dfn{operations}.
This overloads a term with a mathematical meaning
but we feel that being able to follow Jay Earley's%
\index{recce-general}{Earley, Jay}
terminology in \cite{Earley1970}
outweighs the risk of confusion.

\section{Presentation of the deductive logic}

\begin{remark}[Theoremoids]
\label{rem:theoremoid}
Much of the deductive presentation of this paper
is made in the form of labelable text blocks with
headers,
like this one.

Because the archetypal example of these are theorems,
labelable text blocks with headers are called
\dfn[Theoremoid]{theoremoids}.
Another example of a theoremoid is a remark,
like this one \Rmref{theoremoid}.

Theoremoids fall into two major groups:
global and local.
If a theoremoid is global, it is considered
to be visible in this
entire book.
If a theoremoid is local, it is treated
as visible only in part of this book.

Every global theoremoid is a theorem, a definition,
an example,
a remark, or an observation.

A local theoremoid is either a lemma,
or a local definition.
Local theoremoids are visible
either in the current scope
and in an explicitly stated scope.
Scopes are described in more detail
below \Sref{scope-and-realm}.
\rmEnd
\end{remark}

\begin{remark}[Remarks]
Remarks are parts of our presentation
that are outside of the deductive logic,
which we have chosen to label,
either to highlight them,
or for convenient reference.
Remarks do not have proofs.
\rmEnd
\end{remark}

\begin{remark}[Observations]
Observations are statements which form part of the deductive logic,
together with a brief indication of why the statement is considered true.
Sometimes our reason to use an observation is to save clutter.
This is the case when the observation states a fact that is
obvious from previous results in this book,
or from one or more of the preceding disciplines.

In other cases, our
indication of the truthfulness of our observation,
may fall short of being a careful proof.
As an example of this,
we will need to go from pseudocode
to statements in a from we can use in our deductive logic.
We will do this by making statements which we consider clearly obvious
from the pseudocode in the form of observations.
Many of us will be familiar with programming
logic, and will have a high degree of confidence
in their ability to check the truth of our observations,
without resort to careful proof.

In fact, little additional confidence might be gained from
more rigorous proofs.
To lay out a full proof apparatus for
drawing conclusions from pseudocode would require another book
of this size and complexity.
The time-effectiveness of this effort aside,
checking this apparatus, and
the proofs based on it, would require a long chain of reasoning,
reasoning which itself would be open to doubt at every step.
Ad hoc reasoning based on programming experience would be less rigorous,
but the reasoning involved is more familiar,
it reaches its results in fewer steps,
and, since it can be performed more quickly,
can be repeatedly checked.
As a result, careful proofs may not noticeably increase
our confidence in the observation's truth.\footnote{%
    Some readers might wonder, given the advantages of ad hoc
    programmer's reasoning, why so much effort is given to proofs
    in this book.
    The answer is that, while some facts about pseudocode are
    reliably evident to the experienced programmer using ad hoc reasoning,
    others usually are not.
    Facts about pseudocode that do not reliably yield themselves
    up to ad hoc programming reasoning are, in our case,
    essential to having confidence in claims for correctness and complexity.}
\rmEnd
\end{remark}

\begin{remark}[Labelable Text]
\label{rem:labelables}
A \dfn[Labelable]{labelable text},
or \dfn[Labelable]{labelable},
is either a a theoremoid or a
displayed equation.
\rmEnd
\end{remark}

We follow current practice%
\footnote{%
    See, for example, Chapter~19, ``Displayed Equations'',
    p.~185,
    in Knuth's {\em TeXbook}~\cite{Knuth1986}.%
}
in overloading
the term ``equation'' with a typographic sense,
to mean a special kind of displayed expression,
one which frequently is not an equation in the
traditional mathematical sense.
Equations are often followed by a ``hint''.
The ``hint'' is a list of considerations,
such as theorems, definitions
and other equations,
intended to justify the assertion of the equation
in that context.
The ``hint'' is separated
from the body of the equation by a ``because'' symbol (``$\because$'').

References internal to this book are often of the form
({\it Xn:p}),
where {\it X} is the reference type,
{\it n} is the number of the referent,
and
{\it p} is a page reference.
Reference types are
``a'' (algorithm),
``df'' (local definition),
``Df'' (definition),
``e'' (equation),
``Ex'' (example),
``L'' (a location within a section,
where {\it n} is the section number),
``Lm'' (lemma),
``Ob'' (observation),
``Rm'' (remark),
``S'' (section),
``t'' (table), and
``Th'' (theorem).

For example
\Eref{restriction1} refers to equation
\ref{eq:restriction1} on page \pageref{eq:restriction1};
\Chref{features} refers to
chapter \ref{chap:features}
on page \pageref{chap:features};
and
\Lref{loc-brick-depth} refers
to page \pageref{loc-brick-depth}
inside section \ref{loc-brick-depth}.
Where a page reference would be to the current page,
the page reference is omitted.
Where a labelable text is numbered globally,
instead of within each chapter,
no chapter reference is provided.
Where a chapter reference would be to the current chapter,
the chapter reference is omitted.

Definitions occur in equations, theoremoids
and even in ordinary text.
If the defiendum of a definition
is of global scope, then it,
or one or more its key words,
is usually in boldface.
If the defiendum of a definition
is of local scope, then it,
or one or more its key words,
is usually in quotes.
The end of a theoremoid definition
is indicated with a circle (\dfEnd).

Sometimes one or more shorter defienda are defined as synonyms of a longer one.
For example, \Dfref{parse} states ``full parse'' and ``parse'' mean
the same thing.
Shorter synonyms are intended to be used for brevity,
and are preferred when they are not expected to cause confusion.

Often a definition requires a proof to be useful.
For example,
if no function satisfying a definition exists,
the function could never be used.
Proving the existence of a function can be non-trivial.
In these cases, the definition is stated as a theorem
and a lemma.

A definition may also be stated as part of a theorem if it
is closely tied in with that theorem,
in order to avoid unnecessary cross-referencing.
This is often done for definitions which are of details
of the \Marpa algorithm,
or which are a matter of tactical choice within a proof strategy,
such as the details of resource charging in our complexity
analysis.

\rawdfn{AF}%
\index{recce-defienda}{AF@AF (``assumed for'')}
abbreviates ``assumed for''.
For example, ``AF theorem'' abbreviates ``assumed for the theorem''.
\rawdfn{Def}%
\index{recce-defienda}{Def@Def (``definition'')}
abbreviates ``definition''.
\rawdfn{iff}%
\index{recce-defienda}{iff@iff (``if and only if'')}
abbreviates ``if and only if''.
\rawdfn{EI}%
\index{recce-defienda}{EI@EI (``existential instantiation'')}
abbreviates ``existential instantiation''.
\rawdfn{EG}%
\index{recce-defienda}{EG@EG (``existential generalization'')}
abbreviates ``existential generalization''.
\rawdfn{UI}%
\index{recce-defienda}{UI@UI (``universal instantiation'')}
abbreviates ``universal instantiation''.
\rawdfn{UG}%
\index{recce-defienda}{UG@UG (``universal generalization'')}
abbreviates ``universal generalization''.
\rawdfn{WLOG}%
\index{recce-defienda}{WLOG@WLOG (``without loss of generality'')}
abbreviates ``without loss of generality''.
\rawdfn{wrt}%
\index{recce-defienda}{wrt@wrt (``with respect to'')}
abbreviates ``with respect to''.
\rawdfn{ZF}%
\index{recce-defienda}{ZF@ZF (``Zermolo-Fraenkel set theory'')}
(for Zermolo-Fraenkel) abbreviates ``set theory''.
For our purposes, number theory is consider to be part of set theory.

\section{Scope and realms}
\label{sec:scope-and-realm}

\FloatBarrier

\label{page:binding-discussion}
Parsing theory examples usually require a cumbersome ``setup''
during which the various variables are introduced and related to
each other.
These setups are repetitive, tedious,
and usually obvious in their details.
In this book we try to avoid them in favor of
a system of ``contextual binding''.
We believe
that the reader will find this system intuitive,
to the degree that the reader is able to ignore it.

Variables in this book,
as in most of the parsing literature,
have a ``scope''.
Usually scope conventions are left implicit,
but we will find it helpful to make some of our
conventions explicit.
When a new variable is introduced,
unless otherwise stated,
it is bound to,
and visible only within,
the most recently opened scope.

Every definition is a scope,
called a \dfn{definition scope}.
The defiens is visible only within a definition's
scope, but the defiendum is visible outside it.

Every theorem is a scope,
called a \dfn{theorem scope}.
The proof associated with a theorem
is within the scope of the theorem.
Only the theorem itself is visible outside
that scope.
An observation is a scope, with only the observation
itself being visible outside the scope.
An example is a scope with nothing visible outside the scope.

Scopes may be introduced explicitly.
The entire book is the \dfn{global scope}.

% In the parsing literature,
% the concept analogous to our ``realms''
% was called the "domain" for each variable%
% \footnote{
%     The use of term ``domain'' for the set
%     from which a variable may take its value
%     is well-established.
%     But we choose to depart from it because the
%     term ``domain'' is heavily overloaded with other
%     even more well-established uses.
%     For example,
%     in the traditional terminology,
%     a function variable has a ``domain of the variable''
%     but each of the variable's values also has a ``domain of the function''.
%     So if \V{f} is a variable representing a boolean function of the integers,
%     the domain of the function variable \V{f} is $2\tfnMap\naturals$, while
%     the domain of the function represented by the variable \V{f} is \naturals.
%     This distinction arises in many of our contexts,
%     and we thought it would be exasperatingly confusing.
% },
% and it was represented by a scheme
% in which the domain
% was suggested by a combination
% of the language of the alphabet used,
% the typeface,
% and whether the single character of the variable
% was taken from the end,
% middle or beginning of the alphabet.
% For single-character variables,
% these schemes were awkward and restrictive.
% They also pushed the legibility envelope to its limits.
% The multi-character variables used in this paper
% worsen all of these problems.

\section{Arbitrary and convenience variables}

We often, especially in proofs,
find it helpful to notate the point at which we introduce a free variable.
In that introductory note,
a new variable is labeled ``ARB'' if it is chosen arbitrarily from a set
of values.
If the ``ARB'' introduction is made via a displayed equation,
the new variable is subject to the restrictions of that equation,
which often states the realm of the variable.
The realm of a new variable may also be stated in its introductory note.

If, in its introductory note, a new variable is labeled ``CV'',
it is a convenience variable, introduced to abbreviate an expression
and avoid repetition.
An instance of a convenience variable
can always be eliminated by expanding it back into
the expression that it abbreviates.
Convenience variables are similar to definitions.
In fact,
especially for functions and macros,
a variable is sometimes introduced via a local definition
as an alternative to treating it as a convenience variable.

Context variables are arbitrary variables unless otherwise stated.
New variables in the statements of theorems and of lemmas are arbitrary variables
unless otherwise stated.
When a proof ends,
a universal generalization over all the relevant arbitrary
variables in the current scope is implied.
\todo{Discuss relevant and irrelevant variables and placeholders.}

\section{Preceding disciplines}

Our preceding disciplines are
\begin{itemize}
\item logic,
\item set theory,
\item procedures and algorithms,
\item graph theory,
\item the analysis of algorithms, and
\item parsing theory.
\end{itemize}

Arithmetic comes to us as part of set theory.\footnote{
    Unlike most math papers, most readers coming to this one will have
    no training in advanced mathematics.
    We make few concessions to these readers.
    One do make is that, at points where someone new to advanced mathematics might
    be perplexed, if it seems that a brief note will clarify the matter,
    we provide it.
    This is one such note.
    We hope that readers more familiar with the traditions will encounter these
    with patience.}
Together, logic and set theory are our foundation.

Our treatment of preceding disciplines is selective.
We use notations and conventions that are basic
to preceding disciplines without explanation.
We treat results basic to preceding disciplines
as ``facts'', and use them without proof.
We do make a point of describing notations, conventions
and results
\begin{itemize}
\item that are non-standard;
\item that we do not consider basic; or
\item that are the subject of conflicting or ambiguous standards.
\end{itemize}

\chapter{Foundation}
\label{chap:foundation}

\todo[prepend, caption={``Foundation'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\myepi{\label{epi:looking-glass}%
    `When I use a word,' Humpty Dumpty said in rather a scornful tone,
    `it means just what I choose it to mean â€” neither more nor less.'
    \\[.3\baselineskip]
    `The question is,' said Alice, `whether you can make words mean so many different things.'
    \\[.3\baselineskip]
    `The question is,' said Humpty Dumpty, `which is to be master â€” thatâ€™s all.'}
    {Lewis Carroll, {\em Through the Looking Glass}}
\myepi[.75\textwidth]{\label{epi:corcoran}
    [S]tipulated technical usage is powerless to completely cancel
    metaphorical implications, overtones and connotations
    accompanying ordinary usage.}
    {John Corcoran~\cite[p. 223]{Corcoran2005}}

In this section, we often assume knowledge of the preceding disciplines.
We make explicit
those matters in which we are not following
tradition, or in which we are making a choice
among traditions.

The foundation of the mathematics in this article is Chiron.\footnote{%
    For sources, see \asideRef{chiron}.}
Chiron is based on von-Neumann-G{\"o}del-Bernays set theory (NBG),\footnote{
    G{\"o}del 1940 \cite{Goedel1940} is the original article on NBG.
    The chapter in Mendelson's logic text \cite{Mendelson1997} is a good introduction.}
which in turn is based on Zermolo-Fraenkel set theory and first order logic (ZF).
Chiron is a conservative extension of NBG~\cite[p. 7]{Farmer2012}.
NBG, in turn, is a conservative extensive of ZF~\cite{Halmos1960}.
This implies that Chiron is inconsistent iff ZF is.
~\cite[p. 289]{Mendelson1997}
Most modern mathematics is based on ZF,
so that Chiron is as ``safe''
as ZF.\footnote{%
    Chiron does not require the Axiom of Choice,
    and the Axiom of Choice is not used in this book.
    For details, see \asideRef{chiron}.}

Chiron is intended for mechanizing mathematics,
but that is not the reason for our use of it.
Chiron extends NBG is several ways that will be very
useful for us.
Chiron allows us the matter of ill-defined terms carefully.
And Chiron's type system is well thought out and more than
sufficient for our needs.

\section{Meta-language and object-language}

\myepi[.83\textwidth]{\label{epi:meta-language}[W]e have to use two different languages
[...] The first of these languages is
the language which is "talked about" and which is the subject matter
of the whole discussion; the definition of truth which we are seeking
applies to the sentences of this language. The second is the language
in which we "talk about" the first language, and in terms of which we
wish, in particular, to construct the definition of truth for the first
language. We shall refer to the first language as "the object language,"
and to the second as "the meta-language."}
{Tarski~\cite[pp. 349-350]{Tarski1944}}

In 1933, Tarski~\cite{Tarski1935a},
building on a result by G{\"o}del of the same year~\cite{Goedel1931},
published his famous result on the ``undefinability of truth''.
Informally, this states that no consistent formal system containing basic
arithmetic can define its own truth.
Tarski's ``undefinability'' result was dismaying,
since the most important goal of formalizing mathematics is
precisely to enhance our confidence in its truth.

The method Chiron uses to get around this problem
is also due to Tarski, who describes it
in the epigraph for this section.
Chiron contains a fully formalized object language,
and an extensible meta-language, with formal components.
Our meta-language is informal English,
and incorporates Chiron's meta-languge.
Our object language is Chiron's.
We use the abbreviation \dfn[COL, ``Chiron object language'']{\COL} to refer to Chiron's object
language.
MT use the abbreviation \dfn[MtL, ``the meta language of this presentation'']{\MtL} to refer to Chiron's object
language.

\COL cannot reason about its own truth.
But, within our meta-language,
we can reason about the truth of statements
in \COL.
Like \COL,
our meta-language cannot reason about its own truth.
For our conviction regarding the truth of assertions
we make in our meta-language, we must look to our intuition.

\section{Collections}

In our meta-language, we use the informal term \dfn{collection}.
Every collections contain elements,
and therefore has a notion of membership.
The collections that we will use will be sets,
classes, superclasses or metadomains.
Sets, classes and superclasses are objects in the Chiron object language.
Metadomains exists only in the meta-language.

In this book, our \dfn[Set]{sets} are the sets of ZF.
Sets are our ``workhorse'' objects.
Sets contain other sets,
within the restrictions imposed by the
axioms of ZF.
One of the more notable of these restrictions is that there is no
set of all sets.
Sets are first class objects in the Chiron object language.

Our \dfn[Class]{classes}
are the classes of NBG.
Classes contain sets.
Every set is also a class,
but not every class is a set.
A class that is not a set
is a \dfn[Class!proper]{proper class}.

An example of a class that is a proper class
is the class of all sets.
Since there is not set of all sets,
the class of all sets cannot be a set.

The existence of a proper class means that there is
no class of all classes.
This is because every element of a class must be a set,
and the class of all sets is not a set.

Classes make it convenient to make statements about
all sets.
In this paper, classes allow us to avoid the use of infinite
lists of theorems and axioms, described by schemas.
Without classes, schemas would be necessary, for example,
to define sequences of arbitrary first-class value in the natural way.
Schemas are widely used in the literature,
usually implicitly,
but have no generally recognized definition.\footnote{%
    Corcoran, in his 2015 survey, states that
    ``no generally recognized
    definition or characterization of a concept of schema is known to the
    author.''~\cite{Corcoran2005}.}

Classes are Chiron's first class objects.
Earlier we mentioned sets as first class objects.
Sets are first class objects of Chiron because they are also classes.
That an object in Chiron is a first class object means that
it can be quantified over,
and that it has a type.

A \dfn{type} in Chiron is a collection of the possible values that
can be taken by variables
of that type.
A type, therefore, is a collection of classes,
Because every type is a collection of classes,
types are also called \dfn[Superclass]{superclasses}.

Superclasses contain classes.
Every class is also a superclass, but not every
superclass is a class.
There is a superclass of all classes.
A superclass is a \dfn[Superclass!proper]{proper superclass}
iff it is not a class.
Since there is no class of all classes,
the superclass of all classes is a proper superclass.
No proper superclass has a type,
and a proper superclass is not a first-class object.

Skipping ahead, our notation to indicate the \var{x}
is a term of type $\alpha$ is \Vtyped{x}{\alpha}.
We will often abbreviate this by indicating the type
with a subscript: $\var{x}_\alpha$.

\section{Values}

To describe values, we introduce the idea of
\dfn[Metadomain]{metadomains}.
A metadomain is a meta-language concept.
A metadomain is a collection of object-language objects,
and therefore not metadomain can contain another metadomain.
To deal with metadomains,
we use the terms and notation we use for classes.\footnote{%
    See \asideRef{meta-language-notation}.
}

\todo{TO HERE}

All object-language values in Chiron are members of one
of the following disjoin metadomains:
\begin{itemize}
\item \Ds{}, the domain of superclasses.
\Ds{} properly includes \Dc{}, the domain of classes,
which in turn properly includes \Dv{},
the domain of sets.
That is, $\Dv \subset \Dc \subset \Ds$.
\item $\Dill{} = \set{ \unicorn }$, the singleton metadomain containing the
``unicorn'' constant, which is used for handling ill-definedness.
For more on ill-definedness, see Section \ref{sec:ill-definedness}
on page \pageref{sec:ill-definedness}.
\item $\Dbool{} = \set{ \TRUE, \FALSE }$, the metadomain of truth-values.
\item \Don{}, the domain of built-in operators;
\end{itemize}

The operators of Chiron include the built-in operators and
functions.
\dfn[Function]{Functions} in Chiron are defined as classes.
Some built-in operators can be redefined as functions,
but functions are more restricted than operators.
The domain of a functional class must be a class,
and therefore arguments of functions must be sets.
Similarly, the range of a function must be class,
and therefore the value of each function application
must be a set.

\section{Ill-definedness}
\label{sec:ill-definedness}

While ``undefined'' terms occur frequently in mathematics,
and in essential contexts,
their handling has not been well documented.
This vagueness in approach is reflected in the terminology:
The term ``undefined'', in the tradition, can mean
\begin{trivlist}
\centering
\item ``not having a definition'' (sense 1) or
\item ``not having a defined value'' (sense 2).
\end{trivlist}
For example, we can define of \Vtyped{goldbach}{\naturals\tfnMap\naturals}
to be the \V{n}'th number that it is even
and not the sum of two primes.
But Goldbach's conjecture is that no such numbers
exist.
If Goldbach was right, our definition of
\Vtyped{goldbach}{\naturals\tfnMap\naturals}
is of a function that does not exist.
That is,
\begin{trivlist}
\centering
\item we have ``defined'' (in sense 1)
\item an object that is not ``defined'' (in sense 2).
\end{trivlist}

In this book, we use the term ``defined'' only in sense 1 above.
An object is \dfn{defined} iff it is the defiendum of a definition,
either explicitly or because it is assumed to be so in a preceding discipline.
For sense 2 above, we use the term ``well-defined''.
An object is \dfn{well-defined} iff it has a defined value.
An object that is not \dfn{well-defined} is \dfn{ill-defined}.

The traditional approach to well-definedness is to propagate ill-definedness
until it reaches a boolean context at which point the ill-defined value is
treated as a boolean ``false''.
This is not fully rigorous: the distinction between an ill-defined value
and a well-defined boolean value of ``false''
can be essential.\footnote{
    Farmer 2004~\cite{Farmer2004} contains
    a more precise description of the traditional approach.
}
But in the tradition, caution was usually exercised
and disasters mostly avoided.

While, this traditional approach to well-definedness has certainly been around
for a long time,
the first people to document it seem to have been
Farmer and Guttman.~\cite{FG2000}.
They also put forward an rigorous alternate in the form of PFOL,
and it is their approach that is used in this book.

Sometimes it will be convenient to treat ill-definedness as a value.
For this, our object-language has a special dedicated value, the
\hbox{unicorn: $\unicorn$.}%
\index{recce-symbol}{Unicorn@$\unicorn$ (the ``unicorn'', the ill-defined constant)}%
\index{recce-defienda}{Unicorn@Unicorn, $\unicorn$ (ill-defined constant)}%
\footnote{%
    Our use of the term ``unicorn'' is derived from the usage of that term
    in the philosophy literature.
    It is not clear to us why it must be impossible for
    an equine species with a single horn in the middle of its forehead
    to lurk in some corner of one of the world's barely explored forests.
    But philosophers, who can agree on nothing else,
    are unanimous on this point of cryptozoology.}

\todo{Finish this}

\section{Precedence}

Our toolbox of operators is large enough that
we would find it useful to state explicitly our order
of precedence, simply as a reminder.
And in fact, tradition does not fully determine
our order of precedence.
For example, if two operators are drawn
from multiple traditions, no source will state
their precedence
relative to each other.

In the following boxed displays,
operators are given in non-tightening order of precedence.
The operator of a row following a double-line separator has
looser precedence than the operator in the preceding rows.
The operator of a row following a single-line separator has
the same precedence as the operator in the preceding row.
Unless specifically stated, all operators and pseudo-operators
associate left to right.

First come the various ``atomic'' elements:
\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\cellboxr{Constants, such as $\classes$,
    $\naturals$, $\sets$, and $\unicorn$} \\
\hline
\cellboxr{Variables, such as (possibly) \V{x}, $\mathbb{ABC}$, $\alpha$ and \V{multichar}} \\
\hline
\cellboxr{Literals, such as 0, -12, and 42} \\
\hline
\end{tabular}
\end{table}%
\index{recce-symbol}{C@$\classes$ (the universal type)}%
\index{recce-defienda}{C@$\classes$ (the universal type)}%
\index{recce-symbol}{N@$\naturals$ (set of natural numbers)}%
\index{recce-defienda}{N@$\naturals$ (set of natural numbers)}%
\index{recce-symbol}{V@$\sets$ (the class of all sets)}%
\index{recce-defienda}{V@$\sets$ (the class of all sets)}%
\index{recce-symbol}{Unicorn@$\unicorn$ (the ``unicorn'', the ill-defined constant)}%
\index{recce-defienda}{Unicorn@Unicorn, $\unicorn$ (ill-defined constant)}%
\FloatBarrier

The bracketing operators bind as tightly as the atomic operators:
\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\cellboxr{Parenthesized expressions like $(42)$} \\
\hline
\cellboxr{Tuples like $\tuple{42, 43}$} \\
\hline
\cellboxr{Sequences like $\seq{43, 1729, 1491, 1493 }$} \\
\hline
\cellboxr{Families like $\family{ \V{i} \in \V{S} : \Vmyfn{P}{i} }$} \\
\hline
\cellboxr{Cardinalty of a collection, \rule{0em}{2.7ex}$\Vsize{Col}$\rule[-1.5ex]{0em}{0in}} \\
\hline
\end{tabular}
\end{table}%
\index{recce-symbol}{cardinality@\Vsize{C}, cardinality of collection \V{C}}

The next tightest level of precedence includes the application operators:
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Operator application, such as \powerset{\V{A}},
  \Cod{\V{A}}, \Dom{\V{A}}, and \Ran{\V{A}}} \\
\hline
\cellboxr{Function application, such as \Vmyfn{f}{x},
    $\Vmyfn{multicharFn}{v1}$, \Vmyfn{g}{x}, and \Vmyfn{f2}{maxV}} \\
\hline
\end{tabular}
\end{table}%
\index{recce-defienda}{\Cod{\V{r}}, codomain of function \V{r}}%
\index{recce-defienda}{\Dom{\V{r}}, domain of relation \V{r}}%
\index{recce-defienda}{\Ran{\V{r}}, range of relation \V{r}}

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Arithmetic negation of \var{x}: $-\var{x}$} \\
\hline
\cellboxr{Logical negation of \var{x}: $\neg\var{x}$} \\
\hline
\hline
{Unary set operators:
  $\textstyle {\sum}$, $\textstyle {\bigcap}$, $\textstyle {\bigcup}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Next in precedence are binary operators whose results are relations.
Their associativity is right to left.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{\V{f} repeated \V{n} times: $\fname{f}\caret\V{n}$} \\
\hline
  \cellboxr{Relational composition, ``\V{g} after \V{f}'':
  $\fname{g}\relAfterOp\fname{f}$} \\
\hline
    \cellboxr{Functional composition ``\V{g} after \V{f}''
    $\fname{g}\fnAfterOp\fname{f}$ } \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

At the next loosest priority is exponentiation,
which associates right to left.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Exponentiation: ${*}{*}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Next come a set of precedences containing binary operations
on numbers.
Much more convenient for our purposes is to make
string concatenation our only implicit operation.

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Multiplication: $\mult$} \\
\hline
\cellboxr{Division: $/$} \\
\hline
\cellboxr{Modulo: $\bmod$} \\
\hline
\hline
\cellboxr{Addition: $+$} \\
\hline
\cellboxr{Subtraction: $-$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next looser precedence is string concatendation.
String concatenation is usually implicit.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Implicit concatenation of string \V{b} after string \V{a}:
        \padBox{2pt}{$\Vstr{a} \Vstr{b}$}}\\
\hline
\cellboxr{Explicit concatenation of \V{b} after \V{a}: $\V{a} \cat \V{b}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Next looser are a set of precedences
containing binary operations on sets.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Cartesian product: $\times$} \\
\hline
\hline
\cellboxr{Set difference: $\setminus$} \\
\hline
\hline
\cellboxr{Set union: $\cup$} \\
\hline
\hline
\cellboxr{Set intersection: $\cap$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next looser precedence contains operations which
produce functions types.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Type for functions from domain \V{D} to codomain \V{C}: $\V{D} \fnMap \V{C}$} \\
\hline
\cellboxr{Type for functions from some domain of type \V{Dt} to codomain \V{C}: $\V{Dt} \fnMMap \V{C}$} \\
\hline
\cellboxr{Type for total functions from domain \V{D} to codomain \V{C}: $\V{D} \tfnMap \V{C}$} \\
\hline
\cellboxr{Type for total functions from some domain of type \V{Dt} to codomain \V{C}: $\V{Dt} \tfnMMap \V{C}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Next in precedence is a pseudo-operation for declaring type.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{$\Vtyped{x}{\alpha}$ states that expression \V{x}
    is of type $\alpha$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next precedence contains boolean-result operations
dealing with well-definedness.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{$\V{x}\isWellDef\alpha$ means that \V{x} is of type $\alpha$ and is well-defined} \\
\hline
\cellboxr{$\V{x}\isIllDef\alpha$ means $\neg(\V{x} \isWellDef \alpha)$} \\
\hline
\cellboxr{$\V{x}\isWellDef$ means $\V{x} \isWellDef \classes$} \\
\hline
\cellboxr{$\V{x}\isIllDef$ means $\neg(\V{x} \isWellDef)$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next loosest precedence contains boolean-result
operations on collections.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Subcollection: $\subset$} \\
\hline
\cellboxr{Proper subcollection: $\subseteq$} \\
\hline
\cellboxr{Supercollection: $\supset$} \\
\hline
\cellboxr{Proper supercollection: $\supseteq$} \\
\hline
\cellboxr{Collection membership: $\in$} \\
\hline
\cellboxr{Collection non-membership: $\notin$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Equivalences form the next precedence group.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{The operation for equality of types is $\eqWTyOp$} \\
\hline
\cellboxr{$\eqTyOp$ is a synonym for $\eqWTyOp$} \\
\hline
\cellboxr{The operation for equality of classes of type $\alpha$ is $\eqWClOp{\alpha}$} \\
\hline
\cellboxr{$\eqClOp{\alpha}$ is a synonym for $\eqWClOp{\alpha}$} \\
\hline
\cellboxr{$\eqClOp{\classes}$ is a synonym for $\eqWClOp{\classes}$} \\
\hline
\cellboxr{$=$ is a synonym for $\eqClOp{\classes}$} \\
\hline
\cellboxr{$\V{a}\qeq\V{b}$ means $(\V{a} \isWellDef \mathrel{\land} \V{b} \isWellDef) \implies \V{a} = \V{b}$ : $\qeq$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Non-equivalence comparisons come next.
This precedence group does not assoicate.
Instead, in case of an ambiguity about order of precedence involving
two of these relations, the expression is rewritten into
a conjunction of two comparisons.
For example, $0 < \V{x} \le 42$ is interpreted as
\[ 0 < \V{x} \land \V{x} \le 42. \]

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Lax less than: $\le$} \\
\hline
\cellboxr{Lax greater than: $\ge$} \\
\hline
\cellboxr{Strict less than: $<$} \\
\hline
\cellboxr{Strict greater than: $>$} \\
\hline
\cellboxr{$(\V{a} \neq \V{b})$ means $\neg(\V{a} = \V{b})$: $\neq$} \\
\hline
\end{tabular}
\end{table}
\FloatBarrier

At the next precedence are the operators on truth values.

\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline \hline
\cellboxr{Logical and: $\land$} \\
\hline \hline
\cellboxr{Logical or: $\lor$} \\
\hline \hline
%
% We use \implies, but to get the spacing right
% in the cell we need to use the symbol directly.
\cellboxr{Implication: ${\Longrightarrow}$} \\
% {${\implies}$} & \cellboxr{Implication} \\
\hline \hline
\cellboxr{Logical equivalence: ${\equiv}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The conditional operator has its own level of precedence.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{Conditional operator: $\cond{\V{bool}}{\V{v1}}{\V{v2}}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next looser level of precedence is occupied by the definite
descriptors, and the quanitifiers.

\FloatBarrier

\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
    \hline
    \hline
\cellboxr{Definite descriptor: the unique \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}: $\iotaQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} \\
    \hline
\cellboxr{Existential quantifer: true if there exists some \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}: $\existQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} \\
    \hline
\cellboxr{Uniqueness quantifier: true if there exists a unique \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}: $\existUniqQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} \\
    \hline
\cellboxr{Universal quantifier: true if, for every \Vtyped{x}{\alpha}, \Vmyfn{P}{x} is true: $\univQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The next loosest level of precedence is occupied by the diamond and double-diamond variants
of the equivalence operations.
In the \COL these function identically to their underlying equivalences
while in the \MtL these signify definitions.

\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\hline
\cellboxr{The diamond variants of equivalence operations function as global definitions.
    They include $\DEqWTyOp$, $\DEqTyOp$, $\DEqWClOp{\alpha}$, $\DEqClOp{\alpha}$, $\DEqClOp{\classes}$,
    and $\defined$.} \\
\hline
\cellboxr{The double-diamond variants of equivalence operations function as global definitions.
    They include $\DDEqWTyOp$, $\DDEqTyOp$, $\DDEqWClOp{\alpha}$, $\DDEqClOp{\alpha}$, $\DDEqClOp{\classes}$,
    and $\ldefined$.} \\
\hline
\end{tabular}
\end{table}
\FloatBarrier

The verbal forms of the logical operators have the same meaning
as their equivalents in notation,
but looser priority.

\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline \hline
\cellboxr{``and'' means $\land$} \\
\hline \hline
\cellboxr{``or'' means $\lor$} \\
\hline \hline
\cellboxr{``implies'' means $\implies$} \\
\hline \hline
\cellboxr{``iff'' means $\iff$} \\
\hline
\end{tabular}
\end{table}

The hint separator is not actually an operator and part of the \COL.
It serves as syntax to separate a mathematical statement from
the ``hints'' that justify it.

\FloatBarrier
\renewcommand{\cellboxrWidth}{4.5in}
\begin{table}[H]
\begin{tabular}{|l|}
\hline
\cellboxr{Hint separator: $\because$} \\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\section{Operators}
\todo{Finish this section.}

Associativity of operators follows tradition.
We include constants in the table entries of the section,
for this purpose treating them as nullary operators.
In fact, where the constant is a Chiron built-in,
it is a nullary operator as a matter of definition.

Unless otherwise stated, the operator meaning in
each row is the \COL meaning.
The \MtL includes a copy of the \COL,
so every \COL operator is a \MtL operator.
In addition, the \MtL overloads many of the \COL
operators for use with \MtL variables.
Often, the overloading is intuitively obvious,
as when two \MtL variables are added using the $+$
operator,
or when the element operator ($\in$) is used
with metadomains.
In cases where the meaning of an operator overloaded
for the \MtL is not obvious,
we state it explicitly.

\FloatBarrier
\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\classes$} & \cellboxr{The universal type} \\
\hline
{$\naturals$} & \cellboxr{The set of natural numbers} \\
\hline
{$\sets$} & \cellboxr{The class of all sets} \\
\hline
{$\unicorn$} & \cellboxr{The ``unicorn'', the ill-defined constant} \\
\hline
\end{tabular}
\end{table}%
\index{recce-symbol}{C@$\classes$ (the universal type)}%
\index{recce-defienda}{C@$\classes$ (the universal type)}%
\index{recce-symbol}{N@$\naturals$ (set of natural numbers)}%
\index{recce-defienda}{N@$\naturals$ (set of natural numbers)}%
\index{recce-symbol}{V@$\sets$ (the class of all sets)}%
\index{recce-defienda}{V@$\sets$ (the class of all sets)}%
\index{recce-symbol}{Unicorn@$\unicorn$ (the ``unicorn'', the ill-defined constant)}%
\index{recce-defienda}{Unicorn@Unicorn, $\unicorn$ (ill-defined constant)}%
\FloatBarrier

\classes{}, the universal type, is also the superclass of
all classes.
Since every set or class is also a type,
\naturals{} is the type for a natural number,
and \sets{} is the type for a sets.

\myepi{\label{epi:naturals-discomfort}%
    The slight feeling of discomfort the reader
    may experience with the definition of natural number
    is quite common and in most cases temporary.
    The trouble is [...] some irrelevant structure,
    which seems to get in the way.
    We want to be told the the successor of 7 is 8,
    but to be told that 7 is a subset of 8
    or that 7 is an element of 8 is disturbing.}
    {{\em Naive Set Theory}, Halmos~\cite[p. 45]{Halmos1960}}

In this book, the natural numbers begin with 0,
and the set of natural numbers is written \naturals{},
whether it is used as a cardinal or an ordinal.
We use the von Neumann convention,
according to which every natural number is the set of
all preceding natural numbers
so that, for example, $0 = \emptyset = \set{}$
and $3 = \set{0,1,2}$.
We will often write $\Vnat{i} \in 3$ instead of $\Vnat{i} < 3$.
The von Neumann convention extends to \naturals,
so that $\V{i} \in \naturals$ is a way of saying that \V{i}
is any natural number.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\V{x}_\alpha$} & \cellboxr{Variable \V{x} of type $\alpha$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The atomic expression $\V{x}_\alpha$
indicates a first class COL variable of type $\alpha$
and abbreviates the expression of looser precedence,
$\Vtyped{x}{\alpha}$.
We prefer to avoid subscripts,
and $\Vtyped{x}{\alpha}$ is used only when the type is readable
in a subscript font,
and only to remind ourselves
of a previously stated type,
or for stating types which that are suggested
by the context.

We overload the typing notation in \MtL to allow
pseudo-types after the colon.
If the pseudo-type is a meta-domain, it indicates that
the variable is in that meta-domain.
For example, $\Vtyped{metaX}{\Ds}$ indicates that \V{metaX}
is in the metadomain \Ds{}. 
\Ds{} is not a \COL value.
Unless specified more narrowly elsewhere,
\V{metaX} is not a first-class COL variable;
and \V{metaX} does not have a \COL type.

If the pseudo-type is \MtL, it indicates that
the variable is a variable of the meta-language.
For example,
\begin{equation}
\label{eq:pseudo-type-notation2}
\Vtyped{metaY}{\MtL}
\end{equation}
indicates that \V{metaY} is a variable in the meta-language.
As a pseudo-type, \MtL is a special notation,
and is not a collection.
Since all variables are in the metalanguage,
\EFref{pseudo-type-notation2} tells us very little about \V{metaY},
and we may expect
that \V{metaY} will be specified more narrowly elsewhere.
Based on \EFref{pseudo-type-notation2} by itself,
\V{metaY} is not necessarily a part of the \COL,
and therefore will not necessarily be
a first-class \COL with a type.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{\powerset{\V{A}}} & \cellboxr{Powerset of the collection \V{A}} \\
\hline
{\Cod{\V{A}}} & \cellboxr{Codomain of the relation \V{A}} \\
\hline
{\Dom{\V{A}}} & \cellboxr{Domain of the relation \V{A}} \\
\hline
{\Ran{\V{A}}} & \cellboxr{Range of the relation \V{A}} \\
\hline
{\rule{0em}{2.7ex}$\Vsize{Col}$\rule[-1.5ex]{0em}{0in}} &
    \cellboxr{Cardinality of the collection \V{Col}} \\
        % \rule{0em}{2.3ex}%
        % \rule[-1.05ex]{0em}{0in}%
\hline
\end{tabular}
\end{table}%
\index{recce-defienda}{\Cod{\V{r}}, codomain of function \V{r}}%
\index{recce-defienda}{\Dom{\V{r}}, domain of relation \V{r}}%
\index{recce-defienda}{\Ran{\V{r}}, range of relation \V{r}}%
\index{recce-symbol}{cardinality@\Vsize{C}, cardinality of collection \V{C}}

\FloatBarrier

The only transfinite cardinality we use directly is that
of the natural numbers, $\size{\naturals}$.
For example, the notation $\Vsize{set} = \naturals$
indicates that \V{set} is countably infinite.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\neg\var{x}$} & \cellboxr{Logical negation of \var{x}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\textstyle {\sum}$} & \cellboxr{Summation} \\
\hline
{$\textstyle {\bigcap}$} & \cellboxr{Unary union} \\
\hline
{$\textstyle {\bigcup}$} & \cellboxr{Unary intersection} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

In keeping with our avoidance of sub- and superscripts,
we usually avoid the use of ``large operators'',
instead writing summations and similar operations
using unary prefix operators on sets.
For example, our preferred notation
for the union of the elements of the set \V{S} is $\bigcup \V{S}$.
Similarly, we write
$\bigcap \V{S}$ for the intersection of the elements of the set \V{S}; and
$\sum \V{S}$ for the sum of the elements of the set \V{S}.

Where the set is a family,
unless otherwise stated,
the operation is on the range of the family.
That is, where \V{fam} is a placeholder for a family,
\[
    {\textstyle \sum \V{fam}} = \sum\limits_{\textstyle \V{ix}\in\V{fam}} \VVelement{fam}{ix},
\]
and, where the family is a sequence,
\[
    {\textstyle \sum \V{fam}} = \VVelement{fam}{0} +
        \VVelement{fam}{1} + \ldots + \VVelement{fam}{\Vlastix{fam}}.
\]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\fname{f}\caret\V{n}$} & \cellboxr{
    \V{f} repeated \V{n} times
} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Where \V{rel} is a relation,
$\fname{f}\caret{}\V{n}$ is the
composition of \V{rel} repeated \V{n} times.
We note that the composition of a function zero
times is the identity function:
$\rawfn{(\fname{f}\caret0)}{\V{x}} = \V{x}$.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
$\fname{g}\relAfterOp\fname{f}$ &
    \cellboxr{Relational composition: ``\V{g} after \V{f}''} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

For relations,
$\fname{g}\relAfterOp{}\fname{f}$ is
the \dfn[Relational composition (of relations)]{relational composition},
of \fname{g} and \fname{f}:
\fname{g} ``after'' \fname{f}.
That is,
\[
\fname{g}\relAfterOp{}\fname{f} =
     \set{ \tuple{\V{x}, \V{z}} \in \Dom{\V{f}}\times\Ran{\V{g}}:
         \tuple{\V{x}, \V{y}} \in \V{f} \land
         \tuple{\V{y}, \V{z}} \in \V{g} }.
\]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
$\fname{g}\fnAfterOp\fname{f}$ &
    \cellboxr{ Functional composition ``\V{g} after \V{f}''} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Where \fname{f} are \fname{g} are functions,
$\fname{g}\fnAfterOp{}\fname{f}$ is
the \dfn[Functional composition (of functions)]{functional composition},
or simply \dfn[Composition (of functions)]{composition}
of \fname{g} and \fname{f}:
\fname{g} ``after'' \fname{f}.
That is,
\[
\fname{g}\fnAfterOp{}\fname{f} =
     \set{ \begin{gathered}
         \tuple{\V{x}, \V{z}} \in \Dom{\V{f}}\times\Cod{\V{g}}:
         \\ \Cod{\V{f}} = \Dom{\V{g}} \land
             \tuple{\V{x}, \V{y}} \in \V{f} \land
             \tuple{\V{y}, \V{z}} \in \V{g}
     \end{gathered} }.
\]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\mult$} & \cellboxr{Multiplication} \\
\hline
{$/$} & \cellboxr{Division} \\
\hline
{$\bmod$} & \cellboxr{Modulo} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier
Arithmetic in this book is always cardinal arithmetic.
Multiplication is never implied in this book.
Our only implied operator is string concatenation,
a convention that is much more convenient for our purposes.
Division of natural numbers and integers is
``integer division'', so that
for natural number or integer placeholders
\V{x}, $\V{y}>0$,
\[
     (\V{x} / \V{y}) \mult \V{y} + \V{x} \bmod \V{y} = \V{x}.
\]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$+$} & \cellboxr{Addition} \\
\hline
{$-$} & \cellboxr{Subtraction} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Since arithmetic in this book is always cardinal arithmetic,
$\naturals+1 = 1+\naturals = 1$.

\todo{Distinguish numerical equality from term equivalence?}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
\rule{0em}{5.1ex}\raisebox{1.5ex}{$\Vstr{a} \Vstr{b}$} &
    \raisebox{1.4ex}
        {\cellboxr{Implicit concatenation of string \V{b} after string \V{a}}} \\
\hline
{$\V{a} \cat \V{b}$} & \cellboxr{Explicit concatenation of \V{b} after \V{a}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Concatenation is the only implied operator in this paper:
multiplication is always explicit.
Concatenation may also be made explicit.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\times$} & \cellboxr{Cartesian product} \\
\hline
\hline
{$\setminus$} & \cellboxr{Set difference} \\
\hline
\hline
{$\cup$} & \cellboxr{Set union} \\
\hline
\hline
{$\cap$} & \cellboxr{Set intersection} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\V{D} \fnMap \V{C}$} &
    \cellboxr{Type for functions from domain \V{D} to codomain \V{C}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

In this book a function is a partial function, unless otherwise stated.
An onto function is a surjection.
A one-to-one function is an injection.
A function that is both one-to-one and
onto is a bijection.

We recall that we regard any two collections as equivalent
iff their extensions are identical, and
that equivalent class objects may have different types.
The following is true of functions:
\begin{itemize}
\item Two functions are never equivalent unless both have the same range.
\item Two equivalent total functions must have the same domain.
\item Two functions may have different domains
and still be equivalent, if the functions are non-total.
\item Two equivalent functions may have different codomains,
even if both functions are total.
\end{itemize}

We write $\V{D}\fnMap\V{C}$%
\index{recce-symbol}{function map 1@$\fnMap$ ($\V{D}\fnMap\V{C}$
    is the type for functions from \V{D} to \V{C})}
for the type of
functions with \V{D} as their domain
and \V{C} as their codomain.
Therefore we can write
\[
    \Vtyped{f}{\V{D}\fnMap\V{C}}
\]
to say that \V{f} is a function variable
whose type is the superclass of all
functions such that $\Dom{\V{f}} = \V{D}$
and $\Cod{\V{f}} = \V{C}$.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\V{Dt} \fnMMap \V{C}$} &
    \cellboxr{Type for functions from some domain of type \V{Dt} to codomain \V{C}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

We may wish to specify the domain of a function type by type,
instead of explicitly.
That is,
we may wish to specify that \V{f} is a function variable whose
type is $\V{D1}\fnMap\V{C}$ for some
$\VVtyped{D1}{Dt}$.
For this we write
$\V{Dt}\fnMMap\V{C}$%
\index{recce-symbol}{function map 2@$\fnMMap$ ($\V{Dt}\fnMMap\V{C}$
    is the type for functions from an element of \V{Dt} to \V{C})}%
,
\todo{Add reference if use of unary union is cataphoric}
so that
\[ \V{Dt}\fnMMap\V{C} = \bigcup \collB{ \V{D}\fnMap\V{C} }{ \V{D} \in \V{Dt} }. \]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\V{D} \tfnMap \V{C}$} &
    \cellboxr{Type for total functions from domain \V{D} to codomain \V{C}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

We write $\V{D}\tfnMap\V{C}$%
\index{recce-symbol}{total function map 1@$\tfnMap$ ($\V{D}\tfnMap\V{C}$
    is the type for functions from \V{D} to \V{C})}
for the type of
total functions with \V{D} as their domain
and \V{C} as their codomain.
Therefore we write
\[
    \Vtyped{f}{\V{D}\tfnMap\V{C}}
\]
to say that \V{f} is a function
such that $\Dom{\V{f}} = \V{X}$
and $\Cod{\V{f}} = \V{C}$.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\V{D} \tfnMMap \V{C}$} &
    \cellboxr{Type for total functions from some domain in class \V{D} to codomain \V{C}} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Where \V{Dt} is a type to which the domains belong
and \V{C} is a codomain,
$\V{Dt}\tfnMMap\V{C}$%
\index{recce-symbol}{total function map 2@$\tfnMMap$ ($\V{Dt}\tfnMMap\V{C}$
    is the type for total functions from a domain of type \V{Dt} to \V{C})}%
, so that
\[ \V{Dt}\tfnMMap\V{C} = \bigcup \collB{ \V{D}\tfnMap\V{C} }{ \V{D} \in \V{Dt} }. \]

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\Vtyped{x}{\alpha}$} & \cellboxr{Variable \V{x} of type $\alpha$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\V{x}\isWellDef\alpha$} & \cellboxr{\V{x} is of type $\alpha$ and is well-defined} \\
\hline
{$\V{x}\isIllDef\alpha$} & \cellboxr{$\neg(\V{x} \isWellDef \alpha)$} \\
\hline
{$\V{x}\isWellDef$} & \cellboxr{$\V{x} \isWellDef \classes$} \\
\hline
{$\V{x}\isIllDef$} & \cellboxr{$\neg(\V{x} \isWellDef)$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\subset$} & \cellboxr{Subcollection} \\
\hline
{$\subseteq$} & \cellboxr{Proper subcollection} \\
\hline
{$\supset$} & \cellboxr{Supercollection} \\
\hline
{$\supseteq$} & \cellboxr{Proper supercollection} \\
\hline
{$\in$} & \cellboxr{Collection membership} \\
\hline
{$\notin$} & \cellboxr{Collection non-membership} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\le$} & \cellboxr{Lax less than} \\
\hline
{$\ge$} & \cellboxr{Lax greater than} \\
\hline
{$<$} & \cellboxr{Strict less than} \\
\hline
{$>$} & \cellboxr{Strict greater than} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\eqWTyOp$} & \cellboxr{Equality of types} \\
\hline
{$\eqTyOp$} & \cellboxr{A synonym for $\eqWTyOp$} \\
\hline
{$\eqWClOp{\alpha}$} & \cellboxr{Equality of classes of type $\alpha$} \\
\hline
{$\eqClOp{\alpha}$} & \cellboxr{A synonym for $\eqWClOp{\alpha}$} \\
\hline
{$\eqClOp{\classes}$} & \cellboxr{A synonym for $\eqWClOp{\classes}$} \\
\hline
{$=$} & \cellboxr{A synonym for $\eqClOp{\classes}$} \\
\hline
{$\qeq$} & \cellboxr{$\V{a}\qeq\V{b}$ means
    $(\V{a} \isWellDef \mathrel{\land} \V{b} \isWellDef) \implies \V{a} = \V{b}$ } \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

Our notation for the equivalence operators ``puns'' our notation for
typing first-class variables.  The equivalent operators are built-in
operators or derived from them, and do not actually have types.
We feel that the virtues as a mnemonic of pseudo-typing the equivalence operands,
justifies the overloading of the typing notation.

The equivalence operators all have variants in which they are prefixed with a single
diamond ($\rawDRel{}$) or a double diamond ($\rawDDRel{}$).
These ``diamond variants'', in addition to their COL significance,
act as definitions in the \MtL.
They are described at their precedence level.
\todo{Add ref}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\neq$} & \cellboxr{$(\V{a} \neq \V{b})$ means $\neg(\V{a} = \V{b})$} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The operator general equality operator $=$
represents equality in the meta-language,
as well as $\eqWClOp{\classes}$ in the COL.
From a foundational point of view, within the COL,
$\eqWClOp{\classes}$ and $\eqWTyOp$
are represented by two distinct
built-in operators.
\mname{type-equal} for \classes{} represents
$\eqWClOp{\classes}$;
and \mname{type-equal} represents $\eqWTyOp$.
The operator $\eqWTyOp$ is equality of types,
which are not necessarily first-class variables.
The operator $\eqWClOp{\alpha}$,
where $\alpha$ is a type,
is equality between first-class variables of type $\alpha$.
Since $\classes$ is the universal type,
$\eqWClOp{\classes}$ is the operator for equality between Chiron's
first class variables.
$\eqWClOp{\sets}$ is the operator for equality between sets.

Note that types which are first class objects are equivalent
iff they are equivalent as first class objects,
and in this case we have both
$\alpha \eqTyOp \beta$ and $\alpha = \beta$.
If $\alpha$ and $\beta$ are types which are not first class objects, however,
we may have both $\alpha \eqTyOp \beta$ and $\alpha \neq \beta$.
For example, let the types $\alpha$ and $\beta$ both be the superclass of all
classes which are not natural numbers.
That is, let $\gamma \eqTyOp \classes \setminus \naturals$
and $\delta \eqTyOp \classes \setminus \naturals$.
Then $\gamma \eqTyOp \delta$, as expected.
But, if either $\gamma$ or $\delta$ is not well-defined in $\classes$,
$\gamma \neq \delta$.
In fact we have both
$\gamma \isIllDef \classes$,
and $\delta \isIllDef \classes$,
so that $\gamma \neq \delta$.


\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline \hline
{$\land$} & \cellboxr{Logical and} \\
\hline \hline
{$\lor$} & \cellboxr{Logical or} \\
\hline \hline
%
% We use \implies, but to get the spacing right
% in the cell we need to use the symbol directly.
{${\Longrightarrow}$} & \cellboxr{Implication} \\
% {${\implies}$} & \cellboxr{Implication} \\
\hline \hline
{${\equiv}$} & \cellboxr{Logical equivalence} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\cond{\V{bool}}{\V{v1}}{\V{v2}}$} & \cellboxr{Conditional operator} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The ternary conditional operator
is a notation adopted
from programming languges.
Where \V{test} is a formula,
\[ \cond{\V{test}}{\Vtyped{a}{\alpha}}{\Vtyped{b}{\beta}} \text{ is} \]
\begin{itemize}
\item[] \Vtyped{a}{\alpha} iff \V{test} is true and $\alpha = \beta$;
\item[] \Vtyped{b}{\beta} iff \V{test} is false and $\alpha = \beta$;
\item[] \Vtyped{a}{\classes} iff \V{test} is true and $\alpha \neq \beta$; and
\item[] \Vtyped{b}{\classes} iff \V{test} is false and $\alpha \neq \beta$.
\end{itemize}
For example,
$\cond{\Vnat{x} \ge 0}{\Vnat{x}}{-\Vnat{x}}$ is the absolute value
of \Vnat{x}.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
    \hline
    \hline
{$\iotaQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} &
    \cellboxr{Definite descriptor:
        the unique \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}} \\
    \hline
{$\existQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} &
    \cellboxr{Existential quantifer:
        true if there exists some \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}} \\
    \hline
{$\existUniqQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} &
    \cellboxr{Uniqueness quantifier:
        true if there exists a unique \Vtyped{x}{\alpha} such that \Vmyfn{P}{x}} \\
    \hline
{$\univQ{\Vtyped{x}{\alpha}}{\Vmyfn{P}{x}}$} &
    \cellboxr{Universal quantifier:
        true if, for every \Vtyped{x}{\alpha}, \Vmyfn{P}{x} is true} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The quantifiers
$\iotaQOp$, $\existQOp$, $\existUniqQOp$, and $\univQOp$
have very loose precedence,
and, in the absense of parentheses,
their scope is to the end of the expression.

\begin{remark}
\label{rem:number-quantification}
When quantification is over a number variable,
we may express its type using a formula,
so that, for example,
\begin{equation}
\label{eq:number-quantification-example}
\univQ{\typed{\Vnat{j}}{42 < \Vnat{j} \le \Vnat{k}}}{\Vmyfn{P}{j}}
\end{equation}
is equivalent to
\[ \univQ{\Vtyped{i}{\collB{\Vtyped{j}{\naturals}}{42 < \V{j} \le \V{k}}}}{\Vmyfn{P}{i}}. \]
When the quantification variable is first in the formula,
we often treat the name operand of a type expression as redundant,
and abbreviate the typed quantification variable to the formula,
so that \Eref{number-quantification-example} becomes
\[ \univQ{42 < \Vnat{j} \le \Vnat{k}}{\Vmyfn{P}{j}}. \quad \rmEnd \]
\end{remark}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\DefOp$} & \cellboxr{Definition} \\
\hline
{$\LDefOp$} & \cellboxr{Local definition} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\DEqWTyOp$} & \cellboxr{Definition of a type; \COL synonym of $\eqWTyOp$} \\
\hline
{$\DEqTyOp$} & \cellboxr{A synonym for $\DEqWTyOp$} \\
\hline
{$\DEqWClOp{\alpha}$} & \cellboxr{Definition of a class of type $\alpha$;
    \COL synonym of $\eqWClOp{\alpha}$} \\
\hline
{$\DEqClOp{\alpha}$} & \cellboxr{A synonym for $\DEqWClOp{\alpha}$} \\
\hline
{$\DEqClOp{\classes}$} & \cellboxr{A synonym for $\DEqWClOp{\classes}$} \\
\hline
{$\defined$} & \cellboxr{A synonym for $\DEqClOp{\classes}$} \\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\todo{Finish and rewrite}
The single-diamond variants of the equivalence operators
have the same effect as their corresponding equivalence opertor.
In addition,
equivalences prefixed with a single diamond
act as global definitions in the meta-language.

For example,
\[ \V{ANSWER} \DEqTyOp  \set{42} \]
states that the \COL type \V{ANSWER} is a
singleton superclass containing the number 42,
that this is the definition of \V{ANSWER},
and that this definition has global scope.
And
\[ \V{myAnswer} \defined 42 \]
states that \V{myanswer} is a first-class \COL variable whose value
is 42 and that, in addition to establishing this in the local scope in the \COL,
this is a global definition.

\FloatBarrier
\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
\hline
{$\DDEqWTyOp$} & \cellboxr{Local definition of a type; \COL synonym of $\eqWTyOp$} \\
\hline
{$\DDEqTyOp$} & \cellboxr{A synonym for $\DDEqWTyOp$} \\
\hline
{$\DDEqWClOp{\alpha}$} & \cellboxr{Local definition of a class of type $\alpha$;
    \COL synonym of $\eqWClOp{\alpha}$} \\
\hline
{$\DDEqClOp{\alpha}$} & \cellboxr{A synonym for $\DDEqWClOp{\alpha}$} \\
\hline
{$\DDEqClOp{\classes}$} & \cellboxr{A synonym for $\DDEqWClOp{\classes}$} \\
\hline
{$\ldefined$} & \cellboxr{A synonym for $\DDEqClOp{\classes}$} \\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\todo{Finish and rewrite}
The double-diamond variants of equivalences are like the single-diamong equivalences,
except that the definitions they establish are local instead of global.
The scope of a local definition is the current context, unless otherwise stated.
For example,
\[ \V{yourAnswer} \ldefined 42 \]
states that \V{myanswer} is a first-class \COL variable whose value
is 42 and that, in addition to establishing this in the local scope in the \COL,
this is a local definition.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\begin{tabular}{|p{1.5in}|l|}
\hline
{$\because$} & \cellboxr{Hint separator} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

The hint separator ($\because$) is informal notion,
and not a real operator.
We often follow a mathematical statement in a line
of deduction with a list of some of the justifications
for that statement.
The hint separator serves to separate a statement
from its justifications.

\section{Tuples}

We define a tuple recursively:
\begin{itemize}
\item The empty set is a 0-tuple: $\emptyset = \tuple{}$.
\item The ordered pair consisting of a element, call it \V{h}, followed by an
\V{n}-tuple, call it \V{before}, is a ($\V{n}+1$)-tuple, call it \V{after},
so that $\V{after} = \set{\V{h}}\times\V{before}$.
We say that \V{h} is the \dfn{head} of \V{after}.
We say that \V{before} is the \dfn{tail} of \V{after}.
\end{itemize}

The head of a tuple \V{S} can be written \Vmyfn{hd}{S}.
The tail of a tuple \V{S} can be written \Vmyfn{tl}{S}.
We write tuples using angle brackets.
For example, where
\mypareq{eq:tuple-example}{%
    \V{S} = \tuple{ 42, 1729, 42 },
}
\V{S} is a 3-tuple, or triple.
The head of \V{S} is $\Vmyfn{hd}{S} = 42$.
The tail of \V{S} is $\Vmyfn{tl}{S} = \set{1729, 42}$.
Using the $\fname{hd}$ and $\fname{tl}$ operators,
tuples can be treated as lists.
We occasionally refer to tuple element by numeric indices where,
if \V{S1} is a tuple,%
\footnote{%
   Our use of \VVelement{S}{i} instead of the more traditional
   $\V{S}_\V{i}$ is another of the choices that we
   made hoping to save
   eyesight as well as reading time.}
$$\VVtupelem{S1}{n} = \rawfn{\left(
        \fname{hd}\afterOp\fname{tl}\caret\V{n}
    \right)}{
        \V{S1}
    }$$
so that, where $\V{S} = \tuple{ 42, 1729, 42 }$,
\begin{gather*}
\Vtupelem{S}{0}=42, \\
\Vtupelem{S}{1}=1729, \\
\Vtupelem{S}{2}=42, \text{ and} \\
\Vtupelem{S}{3}\isIllDef.
\end{gather*}
\todo{Move footnote to first use of element macro?}

\section{Extended uses of type notation}

Our rule is that types do not share variable names,
so that, in every scope,
we can uniquely determine the type of a variable from its name.
However, there are two cases,
where breaking our rule seemed to promote readability
much more than it risked confusion.
We therefore make local, sparing departures
from our rule, in these two cases.

In the first case, sometimes the most natural way to describe two
tuples is to use the same name for every element of a tuple,
and disambiguate them by type.
For example,
when comparing
two tuples, we might compare
$\tuple{\Rvar{a}{R}, \Rvar{a}{S}, \Rvar{a}{T}}$ with
$\tuple{\Rvar{b}{R}, \Rvar{b}{S}, \Rvar{b}{T}}$,
where the name of each variable indicates its tuple,
and the type of each variable is sufficient to
determine its position
within its tuple.

\todo{What to do about this?}
As another example,
it seems very intuitive
to represent a natural number as \Vnat{x}
and its canonical embedding in the integers
as \Vz{x}.
Here again,
\Vnat{x} and \Vz{x} are two distinct variables distinguished
only by their type.%
\footnote{In set theory, the different classes of number are quite distinct.
    The classes of numbers are also, however, quite distinct in their implementation
    in software, and ignoring this distinction has not been without incident.
    Among the many problems that have resulted,
    the explosion of the Ariane 5 rocket
    is particularly well-documented.~\cite{Lyons1996}}

\section{Relations}

Multi-character variables introduce ambiguity into the standard
notation for relations
so that,
where \V{R} is a binary relation between \V{x} and \V{y},
we avoid representing that fact in the form
\begin{equation}
\label{eq:relation-example}
\V{x} \V{R} \V{y}.
\end{equation}
We sometimes use parentheses to disambiguate, for
example, writing
\Eref{relation-example} as $\V{x}(\V{R})\V{y}$.
But we prefer to deal with relations in the form of the equivalent
set,
or via the boolean function they induce.
For instance, we might discuss
\Eref{relation-example} via the boolean binary function
\myfn{R}{\V{x}, \V{y}}.

Recall that an relation
$\V{R}$ is \dfn{anti-symmetric} if
\[
    \V{a}\;(\V{R})\;\V{b} \;\; \land \;\; \V{b}\;(\V{R})\;\V{a} \implies \V{a} = \V{b}.
\]
A relation \V{R} is \dfn{asymmetric}
if
\[
    \V{a}\;(\V{R})\;\V{b} \implies \neg (\V{b}\;(\V{R})\;\V{a}).
\]
A \dfn{lax partial order} is a relation which
is reflexive, transitive and anti-symmetric.\footnote{%
Halmos~\cite{Halmos1960}
refers to ``lax'' orders as ``weak'' orders.
Unfortunately, modern usage often reappropriates the term
``weak order'' for another purpose -- orders which allow
ties.
We felt that this forces us to depart from Halmos.
}
A \dfn{strict partial order} is a relation which
is irreflexive, transitive and asymmetric.
If a partial order is connected,
then it is also a \dfn{total order}.

With respect to a domain \V{V},
let the identity relation be $\V{I} = \set{ \tuple{ \V{a}, \V{a} } : \V{a} \in \V{V} }$.
The lax relation $\preccurlyeq$ corresponding to a strict relation $\prec$ is the strict
relation's adjoin with the identity relation: $(\preccurlyeq) = (\prec)\cup\V{I}$.
The strict relation $\prec$ corresponding to a lax relation $\preccurlyeq$ is
the set difference of the lax relation and
the identity relation: $(\prec) = \subtract{(\preccurlyeq)}{\V{I}}$.

For example, where the domain is the natural numbers,
$<$ is the strict total relation corresponding to the lax total relation $\le$.
Where the domain is a class of sets,
$\subseteq$ is the lax partial relation corresponding to the strict partial relation $\subset$.

\section{Families}

\begin{definition}[Family Constructor]
\label{def:family-builder}
\todo{Change this to follow Chiron conventions?}
To describe families,
we sometimes use a family builder of the form
\begin{equation}
\label{eq:def-family-builder-050}
    \family{ \V{i} \in \V{S} : \Vmyfn{P}{i} }.
\end{equation}
If \V{fam} is the family described by
\Eref{def-family-builder-050},
then $\V{S}=\Dom{\V{fam}}$ is the index set,
and the value of \VVelement{fam}{i} is
\Vmyfn{P}{i} for every \V{i} in the set
\V{S}.
\dfEnd
\end{definition}

\begin{definition}[Family Indexing]
\label{def:family-indexing}
Let \V{I} and \V{X} be placeholder sets.
For arbitrary family $\V{fam} \in \V{X}\tfnMap\V{I}$,
arbitrary $\V{i} \in \V{I}$,
and arbitrary $\V{x} \in \V{X}$,
\[ \VVelement{fam}{i} \defined \rIota \V{y} \in \V{X} : \tuple{\V{i}, \V{y}} \in \V{fam}.
    \quad \dfEnd \]
\end{definition}

\chapter{Sequences}
\label{chap:sequences}

\todo[prepend, caption={``Sequences'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\myepi{%
\label{epi:range-vs-function}
There are occasions when the range of a function
is deemed to be more important than the function itself.
When that is the case, both the terminology
and the notation undergo radical alterations.}
{{\em Naive Set Theory},
    \\ Halmos~\cite[p.~34]{Halmos1960}}

We assume that we are familiar with
sequences from basic set theory.
In this chapter, we clarify our terminology
and introduce additional terminology and results which
we will use in this book.

\begin{definition}[Sequence]
\label{def:sequence}
A sequence is a family (and therefore a function)
whose domain is either a natural number or \naturals.
\dfEnd
\end{definition}

In most of this book, we will need only finite sequences.
But in this chapter, we want to be more general,
and we allow sequences to be countably infinite.
This means that codomain of the cardinality operator for sequences
is the countable numbers.
That is, where \V{seq} is a sequence,
$\Vsize{seq} \in \naturals\cup\set{\naturals}$.

When \V{seq} is of length zero,
that is, when it has an empty domain,
we say that it is the
\dfn{empty sequence}.%
\index{recce-defienda}{sequence@Sequence!empty}
We often write the empty sequence as $\epsilon$.%
\index{recce-symbol}{e@$\epsilon$ (empty sequence)}

Since a sequence is a function,
we can repurpose function notations to deal with sequences.
If \V{seq} is a sequence,
\Dom{\V{seq}} is the set of valid
\dfn[Index (of a sequence)]{indexes}
of \V{seq}.
The length of \V{seq}, seen as a location,
is called its
\dfn[Endpoint (of a sequence)]{endpoint}.
The set of
\dfn[Location (of a sequence)]{locations} of \V{seq} is the set
containing its indexes
and its endpoint:
$\Dom{\V{seq}} \cup \Vsize{seq}$.
\Cod{\V{seq}} is the set of valid values of \V{seq},
and \Ran{\V{seq}} is the set of actual values of \V{seq}.

For example, $\sets{} \tfnMap 5$ is the set of all sequences
of length 5 whose codomain is \sets{}.
(We recall that $5 = \set{0,1,2,3,4}$.)
We write $\sets{}\tfnMMap\naturals$
for the set of all finite sequences
whose codomain is \sets{}.
We write $\sets{}\tfnMap\naturals$
for the infinite sequence whose codomain is \sets{}.
We write $\sets{}\tfnMMap(\naturals\cup\set{\naturals})$
for the set of all sequences
whose codomain is \sets{}.

Recall
\todo{Fix this to be consistent with Chiron}
that we defined 0 as boolean false and
1 as boolean true;
and that therefore $2 = \set{0,1}$ is the set of boolean values.
This means that
$2 \tfnMap \V{V}$ is the set of boolean functions whose domain
is \V{V}.

We specify a sequence by specifying its elements explicitly,
via a sequence builder,
or inductively.

\begin{remark}[Explicitly specified sequence]
\label{rem:explicitly-specified-sequence}
An example of a
\dfn[Sequence@Sequence!specified explicitly]{sequence
    with explicitly specified elements}
is \seq{43, 1729, 1491, 1493 }.
In explicitly specified sequences, ellipses specify continuation
in some fashion which is understood in context so that,
for example,
\seq{0, 1, 2, \ldots 99 }
is the sequence of the first 100 natural numbers
in order, and
\seq{1, 2, \ldots }
is the infinite sequence containing, in order,
all the counting numbers.
\rmEnd
\end{remark}

\begin{remark}[Sequence builder]
\label{rem:sequence-builder}
A \dfn{sequence builder}
follows the form already described for
use in quantification over numbers \Rmref{number-quantification},
so that
\begin{equation}
\label{eq:rem-sequence-builder-10}
    \famB{\typed{\Vnat{j}}{42 < \Vnat{j} < 45}}{\V{j}} = \seq{42, 43, 44}.
\end{equation}
As with quantifiers, the variable name may be omitted
if the variable bound within the sequence builder
is the first variable of the formula, so that
\Eref{rem-sequence-builder-10} may be abbreviated to
\[ \famB{42 < \Vnat{j} < 45}{\V{j}} = \seq{42, 43, 44}.
    \quad \rmEnd \]
\end{remark}

\begin{example}[Sequence builder]
\label{ex:sequence-builder}
The following are examples of sequence builders.
Where \Vnat{hi} and the function $\V{f} \in \V{V}\tfnMap\naturals$
are bound in context,
\begin{gather*}
    \begin{gathered}
        \seq{ \Vnat{ix},\; \V{ix} \le \Vnat{hi} : \Vmyfn{f}{ix} }
        = \seq{ \Vmyfn{f}{0}, \Vmyfn{f}{1}, \ldots  \Vmyfn{f}{hi}}
            \\ = \set{ \begin{gathered}
                \V{element} \in \naturals\times\V{V} : \exists\; \Vnat{ix} :
                \\ \V{ix} \le \Vnat{hi} \land
                \V{element} = \tuple {\V{ix}, \Vmyfn{f}{ix} }
            \end{gathered} };
    \end{gathered}
\\[2ex]
    \begin{gathered}
        \seq{ \Vnat{ix} < 42 : \Vmyfn{f}{ix+1} }
        = \seq{ \Vmyfn{f}{1} \ldots \Vmyfn{f}{42} }
            \\ = \set{ \begin{gathered}
                \V{element} \in \naturals\times\V{V} : \exists\; \Vnat{ix} :
                \\ \V{ix} < 42 \land
                \V{element} = \tuple {\V{ix}, \Vmyfn{f}{ix+1} }
            \end{gathered} };
    \end{gathered}
\end{gather*}
and
\[
    \begin{gathered}
    \seq{ \Vnat{ix} : \Vmyfn{f}{ix+1} }
    = \seq{ \Vmyfn{f}{1}, \Vmyfn{f}{2}, \Vmyfn{f}{3}, \ldots }
            \\ = \set{ \begin{gathered}
                \V{element} \in \naturals\times\V{V} : \exists\; \Vnat{ix} :
                \V{element} = \tuple {\V{ix}, \Vmyfn{f}{ix+1} }
            \end{gathered} }.
                \quad \exEnd
    \end{gathered}
\]
\end{example}

\begin{remark}[Sequence by induction]
\label{rem:sequence-by-induction}
The sequence \V{seq} is
specified by induction by stating
an \dfn{initial value} from the codomain of \V{seq},
and a function on the codomain of \V{seq},
called the \dfn{successor function}.
Let $\V{init} \in \Cod{\V{seq}}$ be the initial value,
and let \deffn{succ}{\Cod{\V{seq}}}{\Cod{\V{seq}}} be the successor function.
Then \Velement{seq}{0} = \V{init}
and
\[
    \forall\; \V{ix} \in \Dom{\V{seq}} :
        \V{ix} > 0 \implies \VVelement{seq}{ix} = \myfn{succ}{\Velement{seq}{\Vdecr{ix}}}.
        \quad \rmEnd
\]
\end{remark}

\begin{definition}[First and last sequence index]
\label{def:last-index}
Where \V{seq} is a non-empty sequence,
the first element of \V{seq} is \Velement{seq}{0}.
Where \V{seq} is a finite non-empty sequence,
the last index is \Vlastix{seq},
so that the last element is \Velement{seq}{\Vlastix{seq}}.
This means that, when \V{seq} is well-defined,
$\Vlastix{seq} = \Vdecr{seq} = \decr{\Vsize{seq}}$.
\dfEnd
\end{definition}

When we apply a unary family operator to a sequence,
we often use a sequence builder
to represent the sequence,
so that
\[
\sum \seq{\V{ix} < 100 : \V{ix} }
\]
is the sum of the first 100 natural numbers and
\[
\prod \seq{\V{ix} < 100 : \V{ix}+100 }
\]
is the product of the next 100.

\begin{definition}[Sequence Predecessor and Successor]
\label{def:sequence-predecessor}
In the context of sequences,
we overload the terms ``predecessor'' and ``successor''.
\Velement{seq}{\V{n}+1} is the \dfn{successor}
of \VVelement{seq}{n} with respect to \V{seq}.
\Velement{seq}{\Vdecr{n}} is the \dfn{predecessor}
of \VVelement{seq}{n} with respect to \V{seq}.
Usually the respective sequence
is clear in context and left implicit.
\dfEnd
\end{definition}


\begin{definition}[Sequence Pairs]
\label{def:sequence-pairs}
Let \V{V} be a non-empty set of objects,
and let $\Rvarseq{seq}{V}$ be a finite sequence.
The set of \dfn{pairs} of \V{seq}, \Pairs{\V{seq}},
is the set of duples containing the values
of consecutive elements of \V{seq}.
That is,
\[
   \Pairs{\V{seq}} =
       \set{
       \begin{gathered}
       \tuple{\V{from},\V{to}} \in \V{V}\times\V{V} :
       \\ \exists\; \V{i} \in \Dom{\V{seq}} :
           \exists\; \V{j} \in \Dom{\V{seq}} :
       \\ \V{j} = \V{i}+1
       \land \V{from} = \VVelement{seq}{i}
       \land \V{to} = \VVelement{seq}{j}
       \end{gathered} }. \quad \dfEnd
\]
\end{definition}

\begin{lemma}[Sequence Pairs Set 1]
\label{lem:sequence-pairs-set-1}
Let \V{V} be a placeholder
for a variable whose value is a non-empty set.
Let $\V{seq} \in \naturals\cup\set{\naturals}$
be a sequence.
Then
\begin{gather}
\label{eq:th-sequence-pairs-set-1-010}
   \Pairs{\V{seq}}
\\ \label{eq:th-sequence-pairs-set-1-020}
   \subseteq \set{ \begin{gathered}
       \V{pair} \in \V{V}\times\V{V} : \exists\; \Vnat{i} :
       \\ \Vnat{i} + 1 < \Vsize{seq}
       \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
   \end{gathered} }. \quad \thEnd
\end{gather}
\end{lemma}

\begin{proof}
The proof is direct, assuming an arbitrary element of
\Pairs{\V{seq}} \Eref{th-sequence-pairs-set-1-010}
to show that it is an element of the set in
\Eref{th-sequence-pairs-set-1-020}.
\mypareq{eq:th-sequence-pairs-set-1-100}{%
    $\V{pair} \in \Pairs{\V{seq}}$
       \cuz{} AF implication;
       ARB $\V{pair} \in \V{V}\times\V{V}$.}
\begin{gather}
\label{eq:th-sequence-pairs-set-1-105}
       \V{pair} = \tuple{\V{from},\V{to}}
\\ \label{eq:th-sequence-pairs-set-1-110}
       \land\; \V{from} = \VVelement{seq}{i}
           \land\ \V{to} = \VVelement{seq}{j}
\\ \label{eq:th-sequence-pairs-set-1-130}
       \land\; \V{i} \in \Dom{\V{seq}}
           \land\ \V{j} \in \Dom{\V{seq}}
\\ \label{eq:th-sequence-pairs-set-1-150}
       \land\; \V{j} = \V{i}+1
\\ \nonumber \myparbox{\cuz{}
       \Eref{th-sequence-pairs-set-1-100},
       \longDfref{\myfn{Pairs}{}}{sequence-pairs};
       \linebreak[1] CV $\V{from} \in \V{V}$; CV $\V{to} \in \V{V}$;
       CV \Vnat{i}; CV \Vnat{j}.}
\end{gather}
\mypareq{eq:th-sequence-pairs-set-1-160}{%
    $\forall\;\Vnat{x} :
        \Vnat{x} \in \Dom{\V{seq}} \iff \V{x} < \Vsize{seq}$
        \cuz{} ZF.}
\mypareq{eq:th-sequence-pairs-set-1-170}{%
    $\Vnat{i} + 1 < \Vsize{seq}$
    \cuz{} \Eref{th-sequence-pairs-set-1-130},
        \Eref{th-sequence-pairs-set-1-150},
        \Eref{th-sequence-pairs-set-1-160}.}
\mypareq{eq:th-sequence-pairs-set-1-180}{%
    $\V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}$
        \cuz{} \Eref{th-sequence-pairs-set-1-105},
        \Eref{th-sequence-pairs-set-1-110}.}
\mypareq{eq:th-sequence-pairs-set-1-185}{%
    $\V{pair} \in \V{V}\times\V{V}$
        \cuz{} \Eref{th-sequence-pairs-set-1-100}.}
\mypareq{eq:th-sequence-pairs-set-1-190}{%
    $\exists\; \Vnat{i} :
        \Vnat{i} + 1 < \Vsize{seq} \land
        \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}$
        \bcuz{} EG of \Vnat{i} in
            \Eref{th-sequence-pairs-set-1-170}
            and \Eref{th-sequence-pairs-set-1-180}.}
\begin{equation}
\label{eq:th-sequence-pairs-set-1-200}
   \begin{gathered}
       \V{pair} \in \set{ \begin{gathered}
           \V{pair} \in \V{V}\times\V{V} : \exists\; \Vnat{i} :
           \\ \Vnat{i} + 1 < \Vsize{seq}
           \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
       \end{gathered} }
       \\ \myparbox{\cuz{}
               \Eref{th-sequence-pairs-set-1-185},
               \Eref{th-sequence-pairs-set-1-190}.}
   \end{gathered}
\end{equation}
\Eref{th-sequence-pairs-set-1-200} states that \V{pair}
is an element of the set in
\Eref{th-sequence-pairs-set-1-020},
so that the implication from \Eref{th-sequence-pairs-set-1-100}
to \Eref{th-sequence-pairs-set-1-200} is what we sought
to show the theorem.
\myqed
\end{proof}

\begin{lemma}[Sequence Pairs Set 2]
\label{lem:sequence-pairs-set-2}
Let \V{V} be a placeholder
for a variable whose value is a non-empty set.
Let $\V{seq} \in \naturals\cup\set{\naturals}$
be a sequence.   Then
\begin{gather}
\label{eq:th-sequence-pairs-set-2-010}
   \set{ \begin{gathered}
       \V{pair} \in \V{V}\times\V{V} : \exists\; \Vnat{i} :
       \\ \Vnat{i} + 1 < \Vsize{seq}
       \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
   \end{gathered} }
\\ \label{eq:th-sequence-pairs-set-2-020}
       \subseteq \Pairs{\V{seq}}. \quad \thEnd
\end{gather}
\end{lemma}

\begin{proof}
The proof is direct, assuming an arbitrary element of the set in
\Eref{th-sequence-pairs-set-2-010}
to show that it is an element of
\Pairs{\V{seq}} \Eref{th-sequence-pairs-set-2-020}.
\begin{gather}
\label{eq:th-sequence-pairs-set-2-100}
       \V{pair} \in \V{V}\times\V{V}
\\ \label{eq:th-sequence-pairs-set-2-110}
       \land\; \left( \begin{gathered}
           \exists\; \Vnat{i} < \Vlastix{seq} :
           \\ \Vnat{i} + 1 < \Vsize{seq}
           \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
       \end{gathered} \right)
\\ \nonumber
    \myparbox{\cuz{} AF implication; ARB \V{pair}.}
\end{gather}
\begin{gather}
\label{eq:th-sequence-pairs-set-2-120}
       \Vnat{i} + 1 < \Vsize{seq}
\\ \label{eq:th-sequence-pairs-set-2-130}
       \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
\\ \nonumber
    \myparbox{\cuz{} EI of \Eref{th-sequence-pairs-set-2-110}.}
\end{gather}
\mypareq{eq:th-sequence-pairs-set-2-140}{%
    $\forall\;\Vnat{x} :
        \Vnat{x} \in \Dom{\V{seq}} \iff \V{x} < \Vsize{seq}$
        \cuz{} ZF.}
\mypareq{eq:th-sequence-pairs-set-2-150}{%
       $\Vnat{i} + 1 \in \Dom{\V{seq}}$ \cuz{}
           \Eref{th-sequence-pairs-set-2-120},
           \Eref{th-sequence-pairs-set-2-140}.}
\mypareq{eq:th-sequence-pairs-set-2-160}{%
       $\V{j} = \V{i}+1$ \cuz{} ARB \Vnat{j}.}
\mypareq{eq:th-sequence-pairs-set-2-170}{%
       $\Vnat{j} \in \Dom{\V{seq}}$ \cuz{}
           \Eref{th-sequence-pairs-set-2-120},
           \Eref{th-sequence-pairs-set-2-140},
           \Eref{th-sequence-pairs-set-2-160}.}
\mypareq{eq:th-sequence-pairs-set-2-175}{%
       $\Vnat{i} \in \Dom{\V{seq}}$ \cuz{}
           \Eref{th-sequence-pairs-set-2-150}.}
\mypareq{eq:th-sequence-pairs-set-2-180}{%
       $\V{from} = \VVelement{seq}{i}$ \cuz{} CV $\V{from} \in \V{V}$.}
\mypareq{eq:th-sequence-pairs-set-2-190}{%
       $\V{to} = \VVelement{seq}{j}$ \cuz{} CV $\V{to} \in \V{V}$.}
\mypareq{eq:th-sequence-pairs-set-2-200}{%
       $\V{pair} = \tuple{\V{from}, \V{to}}$
           \cuz{} \Eref{th-sequence-pairs-set-2-130},
               \Eref{th-sequence-pairs-set-2-160},
               \Eref{th-sequence-pairs-set-2-180},
               \Eref{th-sequence-pairs-set-2-190}.}
\begin{equation}
\label{eq:th-sequence-pairs-set-2-210}
\begin{gathered}
   \exists\; \V{i} \in \Dom{\V{seq}} :
       \exists\; \V{j} \in \Dom{\V{seq}} :
   \\ \V{j} = \V{i}+1
       \land \V{from} = \VVelement{seq}{i}
       \land \V{to} = \VVelement{seq}{j}
   \\ \myparbox{\cuz{} EI of \V{i}, \V{j};
       \Eref{th-sequence-pairs-set-2-160},
       \Eref{th-sequence-pairs-set-2-170},
       \Eref{th-sequence-pairs-set-2-175},
       \Eref{th-sequence-pairs-set-2-180},
       \Eref{th-sequence-pairs-set-2-190}.
   }
\end{gathered}
\end{equation}
\mypareq{eq:th-sequence-pairs-set-2-220}{%
    $\V{pair} \in \Pairs{\V{seq}}$ \cuz{}
        \longDfref{\myfn{Pairs}{}}{sequence-pairs},
        \Eref{th-sequence-pairs-set-2-200},
        \Eref{th-sequence-pairs-set-2-210}.}
Equation \Eref{th-sequence-pairs-set-2-220} is what we needed to
show the theorem.
\myqed
\end{proof}

\begin{theorem}[Sequence Pairs Set]
\label{th:sequence-pairs-set}
Let \V{V} be a placeholder
for a variable whose value is a non-empty set.
Let $\V{seq} \in \naturals\cup\set{\naturals}$
be a sequence.
Then
\begin{equation}
\label{eq:th-sequence-pairs-set-010}
   \Pairs{\V{seq}} =
   \set{ \begin{gathered}
       \V{pair} \in \V{V}\times\V{V} : \exists\; \Vnat{i} :
       \\ \Vnat{i} + 1 < \Vsize{seq}
       \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i}, \VVelement{seq}{\V{i}+1}}
   \end{gathered} }. \quad \thEnd
\end{equation}
\end{theorem}

\begin{proof}
The current theorem follows immediately from
Sequence Pairs Lemma 1 \Lmref{sequence-pairs-set-1}
and Sequence Pairs Lemma 2 \Lmref{sequence-pairs-set-2}.
\myqed
\end{proof}

\section{Slices}

\renewcommand{\labBase}{def-slice-mapping}
\begin{definition}[Slice mapping]
\label{def:slice-mapping}
We say that a relation is a
\dfn[Slice mapping]{slice mapping}
for
domain $\V{len}\inr\naturals\cup\set{\naturals}$
iff
it maps \V{len} to the natural numbers,
preserving
the successor relationships among all of the elements of \V{len},
as well as the uniqueness of the first element of \V{len}.
That is, \fname{f},
is a slice mapping iff
\begin{gather}
\Elab{010} \V{f} \inr \powerset{\V{len}\times\naturals}
\\ \Elab{020} \land\; \Dom{\V{f}} = \V{len}
\\ \Elab{040} \land\; \left( \begin{gathered}
        \forall\; \tuple{ \V{i1}, \V{v1} } \in \V{f} :
        \forall\; \tuple{ \V{i2}, \V{v2} } \in \V{f} :
        \\ \Vincr{i1} = \V{i2} \iff \Vincr{v1} = \V{v2}
    \end{gathered} \right)
\\ \Elab{050} \land\; \left( \begin{gathered}
        \forall\; \tuple{ 0, \V{v1} } \in \V{f} :
        \forall\; \tuple{ 0, \V{v2} } \in \V{f} :
        \\ \V{v1} = \V{v2}
    \end{gathered} \right).
        \quad \dfEnd
\end{gather}
\end{definition}

\renewcommand{\labBase}{lem-slice-mapping}
\begin{lemma}[Slice mapping]
\label{lem:slice-mapping}
Let $\V{f}\inr\powerset{\V{len}\times\naturals}$
be a slice mapping.
Then
\begin{equation}
\Elab{050}
\begin{gathered}
     \forall\; \tuple{\V{i}, \V{v}} \in \V{f} :
         \forall\; \V{offset} \in \naturals :
     \\ \tuple{0,\V{offset}}\in\V{f}
         \implies \V{v} = \V{offset}+\V{i}.
         \quad \lmEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
\todo{Revise and finish this proof.}
The proof is by minimal counter-example.
We first define a macro for a ``bad'' instance of \V{f}:
\begin{equation}
\Elab{110}
\begin{gathered}
\Vmyfn{BAD}{\Vnat{inst}} \ldefined
\left( \begin{gathered}
    \exists\; \tuple{\V{inst}, \V{v}} \in \V{f} :
    \exists\; \V{offset} \in \naturals :
    \\ \tuple{0,\V{offset}}\in\V{f}
    \\ \land\; \V{v} \neq \V{offset}+\V{inst}
\end{gathered} \right)
    \\ \myparbox{\cuz{}
        Macro for EI of negation of \E{050}, setting $\Vnat{inst}=\V{i}$.}
\end{gathered}
\end{equation}
We then assume, for a reductio,
that there is a minimal ``bad'' instance, \Vnat{bad}, such that
\Vmyfn{BAD}{bad} is true:
\begin{gather}
\Elab{120}
    \Vmyfn{BAD}{bad}
\\ \Elab{130}
    \land\; \forall\; \Vnat{i} < \V{bad} : \neg \Vmyfn{BAD}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
For there to be a ``bad'' instance, \V{f} must be non-empty,
that is,
\eqpar{140}{$\V{len} > 0$ \cuz{} \E{120}--\E{130}.}
\eqpar{150}{$\exists\;\Vnat{v0} : \tuple{0, \V{v0}} \in \V{f}$
    \cuz{} \E{140}.}
And from the definition of a slice mapping,
\eqpar{160}{$\existUniqQOp\;\Vnat{v0} : \tuple{0, \V{v0}} \in \V{f}$
    \bcuz{} \Eref{def-slice-mapping-050} in \Dfref{slice-mapping}, \E{150}.}
We have now shown that the ``offset'' of \V{f} is well-defined:
\eqpar{170}{$\Vnat{offset} = \rIota\; \Vnat{offset}:\tuple{0,\V{offset}}\in\V{f}$
    \bcuz{} \E{160}, CV $\V{offset}\inr\naturals$.}
\eqpar{180}{$\tuple{0,\V{v0}}\in\V{f} \implies \V{v0} = \V{offset}$
    \bcuz{} \E{170}.}
\eqpar{190}{$\forall\; \tuple{0, \V{v}} \in \V{f} : \V{v} = \V{offset}$
    \bcuz{} \E{180}.}
\eqpar{200}{$\neg \Vmyfn{BAD}{0}$ \cuz{} \E{110}, \E{190}.}
\eqpar{210}{$\V{bad}>0$ \cuz{} \E{120}, \E{200}.}
\eqpar{220}{$\neg \Vmyfn{BAD}{\Vdecr{bad}}$ \cuz{} \E{130}, \E{210}.}
\myqed
\end{proof}

\todo{Revise from here}

\renewcommand{\labBase}{def-slice}
\begin{definition}[Slice]
\label{def:slice}
A \dfn{slice relation} is the relational composition
of an offset mapping and a sequence.
That is,
\begin{itemize}
\item let $\V{len} \inr \naturals\cup\set{\naturals}$
    be a countable number,
\item let $\Vnat{offset} \inr \naturals$,
\item let $\V{offmap}\inr \naturals\tfnMap\V{len}$
    be the offset mapping
    which \V{offset} induces
    for the domain \V{len}, and
\item let $\V{seq} \inr \V{V}\tfnMMap\naturals\cup\set{\naturals}$
    be a sequence.
\end{itemize}
Then the relational composition of \V{offMap} and \V{seq},
\begin{gather}
    \V{sliceRel} \inr \powerset{\V{len}\times\V{V}} \text{ such that}
\\ \Elab{040}
    \V{sliceRel} =
    \set{ \begin{gathered}
        \tuple{\V{ix},\V{v}} \in \V{len}\times\V{V} :
        \\ \tuple{\V{offset}+\V{ix}, \V{v}} \in \V{seq}
    \end{gathered} },
\end{gather}
is a \dfn{slice relation}.

If a slice relation is a function whose domain is the same as \V{offmap},
we say that that slice relation
is a \dfn{slice function}.
More often, we call a slice function a \dfn{slice}.
We say that the
\dfn[Offset (of a slice relation)]{offset}%
\index{recce-defienda}{Offset (of a slice)}
of a slice relation is
the offset of its offset mapping,
in other words,
that the offset of \V{sliceRel} is \V{offset}.
We say that \V{seq} is the \dfn{container sequence}%
\index{recce-defienda}{sequence@Sequence!container}
of \V{sliceRel}.

We say the slice relation is
\dfn[From (of a slice relation)!from an index of the container sequence]{from the index}
\V{offset} of \V{seq}.
If \V{sliceRel} is a slice and \V{len} is finite,
we say the \V{sliceRel} is
\dfn[To (of a slice)!to an index of the container sequence]{to the index}
$\V{offset}+\Vdecr{len}$ of \V{seq}.
\dfEnd
\end{definition}

\renewcommand{\labBase}{lem-slice-relation-existence}
\begin{lemma}[Slice relation existence]
\label{lem:slice-relation-existence}
For each offset, length and container sequence,
a slice relation exists.
\obEnd
\end{lemma}

\begin{proof}
\todo{Fix this}
The proof follows from the hypothesized existence of
the offset, length and container sequence;
the observation about the offset function TODO!;
and \Eref{def-slice-040} in the definition of slice \Dfref{slice}.
Note that the slice relation may be empty.
\myqed
\end{proof}

\renewcommand{\labBase}{rem-cxt-sec-slice}
\begin{remark}[Slice Section Context]
\label{rem:cxt-sec-slice}
This section, beginning after this remark, is a scope.
This ``slice section scope''%
\index{recce-defienda}{Scope, by section, slice@Scope!Of section!Slice}
contains%
\index{recce-defienda}{Context, Scope, section, slice@Context!Scope!Of section!Slice}
the variables
\begin{itemize}
\item $\V{offset}\inr\naturals,$
\item $\V{len}\inr\naturals\cup\set{\naturals},$
\item $\V{seq}\inr\V{V}\tfnMMap(\naturals\cup\set{\naturals}),$ and
\item $\V{sliceRel} \inr \powerset{\V{len}\times\V{V}}$,
    where \V{sliceRel} is a slice relation
    at offset \V{offset} of \V{seq}.
\end{itemize}
Also, the scope of every theoremoid within the slice section scope
contains its own instances of
\V{offset}, \V{len}, \V{seq}, and \V{sliceRel},
specified as just stated.
\rmEnd
\end{remark}

\renewcommand{\labBase}{lem-slice-value}
\begin{lemma}[Slice value]
\label{lem:slice-value}
\begin{equation}
\Elab{010}
    \begin{gathered}
        \forall\; \V{ix} \in \naturals :
            \forall\; \V{v} \in \V{V} :
        \\ \tuple{ \V{ix},\V{v} } \in \V{sliceRel}
            \implies \V{v} = \Velement{seq}{\V{offset}+\V{i}}
            \quad \lmEnd
    \end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The lemma is restatement of
\Eref{def-slice-040}
in the definition of slice \Dfref{slice}.
\myqed
\end{proof}

\renewcommand{\labBase}{lem-slice-functional}
\begin{lemma}[Slice functional]
\label{lem:slice-functional}
Every slice relation is functional.  That is,
\[
    \begin{gathered}
    \forall\; \V{ix} \in \naturals :
        \forall\; \V{v1} \in \V{V} :
        \forall\; \V{v2} \in \V{V} :
    \\ \tuple{\V{ix}, \V{v1}} \in \V{sliceRel}
        \land \tuple{\V{ix}, \V{v2}} \in \V{sliceRel}
        \implies \V{v1} = \V{v2}.  \quad\lmEnd
    \end{gathered}
\]
\end{lemma}

\begin{proof}
The proof is by reductio,
assuming for the reductio two elements of the slice relation
with the same index but different values.
\begin{gather}
\Elab{100} \tuple{\Vnat{ix}, \V{v1}} \in \V{sliceRel}
\\ \Elab{110} \land\; \tuple{\V{ix}, \V{v2}} \in \V{sliceRel}
\\ \Elab{120} \land\; \V{v1} \neq \V{v2}
\\ \nonumber \myparbox{\cuz{} AF reductio,
    ARB $\V{ix} \inr \naturals$,
    ARB $\V{v1} \inr \V{V}$,
    ARB $\V{v2} \inr \V{V}$.}
\end{gather}
\eqpar{130}{$\V{v1} = \Velement{seq}{\V{offset}+\V{ix}}$
    \bcuz{} \Lmref[Slice Value Lemma]{slice-value}, \E{100}.}
\eqpar{140}{$\V{v2} = \Velement{seq}{\V{offset}+\V{ix}}$
    \cuz{} \Lmref{slice-value}, \E{110}.}
\eqpar{150}{$\V{v1}=\V{v2}$ \cuz{} \E{130}, \E{140}.}
\E{150} is contrary to \E{120}, which shows the reductio
and the proof.
\myqed
\end{proof}

\renewcommand{\labBase}{lem-slice-domain}
\begin{lemma}[Slice domain]
\label{lem:slice-domain}
If the offset plus the length of a slice
relation is less than or equal to the size
of the container sequence, then
the length of the slice relation is the domain
of the slice relation.
That is,
\begin{gather}
\Elab{020} \V{offset}+\V{len} \le \Dom{\V{seq}}
\\ \Elab{040} \implies \Dom{\V{sliceRel}} = \V{len}.  \quad \lmEnd
\end{gather}
\end{lemma}

\begin{proof}
The proof is direct, assuming the hypothesis \E{020}
to show the consequent \E{040}.
\eqpar{100}{$\Vnat{ix}\in\Dom{\V{sliceRel}} \implies \V{ix}\in\V{len}$
    \bcuz{} \Eref{def-slice-040} in \Dfref[Slice]{slice},
    ARB $\V{ix} \inr \naturals$.}
\eqpar{110}{$\V{ix}\in\V{len} \land \V{offset}+\V{ix}\in\Dom{\V{seq}}
    \linebreak \implies \V{ix} \in \Dom{\V{sliceRel}}$
    \bcuz{} \Eref{def-slice-040} in \Dfref{slice}.}
\eqpar{120}{$\V{ix}\in\V{len} \implies \V{offset}+\V{ix}\in\Dom{\V{seq}}$
    \bcuz{} hypothesis of theorem E{020}.}
\eqpar{130}{$\V{ix}\in\V{len} \implies \V{ix} \in \Dom{\V{sliceRel}}$
    \cuz{} \E{110}, \E{120}.}
\eqpar{140}{$\forall\; \Vnat{i} : \V{i}\in\V{len} \implies \V{i} \in \Dom{\V{sliceRel}}$
        \cuz{} UG of \E{130}.}
\eqpar{150}{$\forall\; \Vnat{i} :
        \V{i} \in \Dom{\V{sliceRel}} \implies \V{i}\in\V{len}$
        \cuz{} UG of \E{100}.}
And, because in ZF all sets are equivalent iff their extensions are the same,
\eqpar{160}{$\V{len} = \Dom{\V{sliceRel}}$ \cuz{} ZF, \E{140}, \E{150}.}
\E{160} is the consequent \E{040}, which is what we needed for the proof.
\myqed
\end{proof}

\todo{Revise from here}

\todo{Add proofs of slice uniqueness, codomain, pairs.}

\renewcommand{\labBase}{th-slice-sequence-uniqueness}
\begin{theorem}[Slice sequence uniqueness]
\label{th:slice-sequence-uniqueness}
For every \V{offset}, \V{len} and \V{seq},
such that $\V{offset}+\V{len} \le \Dom{\V{seq}}$,
\V{sliceRel} exists and is unique.
\thEnd
\end{theorem}

\begin{proof}
% Proof template
\todo{Finish this proof.}
\myqed
\end{proof}

\renewcommand{\labBase}{th-slice-sequence}
\begin{theorem}[Slice sequence]
\label{th:slice-sequence}
If
\eqpar{040}{$\V{offset}+\V{len} \le \Dom{\V{seq}}$}
then \V{sliceRel} is a sequence.
\thEnd
\end{theorem}

\begin{proof}
\todo{Finish this proof}
By the Slice Functional Lemma \Lmref{slice-functional},
E{040} and the Slice Domain Lemma \Lmref{slice-domain},
\V{sliceRel} is a function with domain \V{len}.
A function whose domain is countable is a
sequence, by definition.
\myqed
\end{proof}

\renewcommand{\labBase}{th-slice-range}
\begin{theorem}[Slice range]
\label{th:slice-range}
A slice is a sequence whose range is a subset
of the range of its container sequence.
That is,
\[ \Ran{\V{slice}} \subseteq \Ran{\V{seq}}.
    \quad \thEnd \]
\end{theorem}

\begin{proof}
\todo{Revise this proof}
The proof is direct.
We note that where $\Ran{\V{slice}} = 0$,
we have the proof trivially.
For the non-empty case,
$\Ran{\V{slice}} \neq 0$,
we know that there is at least one element in \Ran{\V{slice}}.
We assume an arbitrary element of \Ran{\V{slice}},
to show that it must also be an element of
\Ran{\V{seq}}.
\eqpar{100}{$\V{v} \in \Ran{\V{slice}}$ \cuz{} AF non-empty case, ARB $\V{v}\in\V{V}$.}
\eqpar{110}{$\exists\; \Vnat{ix} : \tuple{\V{ix}, \V{v}} \in \V{slice}$ \cuz{} \E{100}.}
\eqpar{120}{$\tuple{\V{ix}, \V{v}} \in \V{slice}$ \cuz{} EI of \E{110}.}
\eqpar{130}{$\V{v} = \Velement{seq}{\V{offset}+\V{ix}}$
    \bcuz{} TODO, \E{120}.}
\eqpar{140}{$\V{v} \in \Ran{\V{seq}}$ \cuz{} \E{130}.}
\E{140} gives us the non-empty subcase and the proof.
\myqed
\end{proof}

\renewcommand{\labBase}{th-slice-indexing}
\begin{theorem}[Slice indexing]
\label{th:slice-indexing}
\eqpar{020}{$\forall\; \V{ix}\in\V{len} : \VVelement{slice}{ix} =
 \Velement{seq}{\V{offset}+\V{ix}}. \quad \thEnd$}
\end{theorem}

\begin{proof}
\todo{Revise this proof}
The proof is direct.
It follows trivially where $\V{len} = 0$.
For the non-empty case, $\V{len} > 0$,
we assume an arbitrary \V{ix}, to show
the quantification predicate in \E{020}.
\eqpar{100}{$\Vnat{ix} \in \V{len}$ \cuz{} ARB $\V{ix} \in \naturals$.}
\eqpar{110}{$\tuple{\V{ix}, \Velement{seq}{\V{offset}+\V{ix}}} \in \V{slice}$
    \bcuz{} TODO, \E{100}.}
\eqpar{120}{$\VVelement{slice}{ix} = \Velement{seq}{\V{offset}+\V{ix}}$
    \cuz{} \E{110}. \myqed}
\end{proof}

\renewcommand{\labBase}{th-slice-pairs}
\begin{theorem}[Slice pairs]
\label{th:slice-pairs}
The pairs of a slice are a subset of
the pairs of the container sequence.
That is,
\[ \Pairs{\V{slice}} \subseteq \Pairs{\V{seq}}.
            \quad \thEnd \]
\end{theorem}

\begin{proof}
\todo{Revise this proof}
For the case where there are no pairs in \V{slice},
$\size{\Pairs{\V{slice}}} = 0$,
we have the proof trivially.
We show the non-empty case,
$\size{\Pairs{\V{slice}}} > 0$,
by reductio.
For the reductio,
we assume the existence of a pair for a slice,
which is not a pair of the container sequence.
\begin{gather}
\Elab{100} \V{pair} \in \Pairs{\V{slice}}
\\ \Elab{110} \land\; \V{pair} \notin \Pairs{\V{seq}}
\\ \nonumber \myparbox{\cuz{} AF reductio, ARB $\V{pair} \in \V{V}\times\V{V}$.}
\end{gather}
\begin{equation}
\Elab{120}
\begin{gathered}
       \exists\; \Vnat{i} : \Vnat{i} + 1 < \Vsize{slice}
       \\ \land\; \V{pair} = \tuple{\VVelement{slice}{i}, \VVelement{slice}{\V{i}+1}}
       \\ \myparbox{\cuz{} \Thref[Sequence Pairs Set]{sequence-pairs-set}, \E{100}.}
\end{gathered}
\end{equation}
\begin{gather}
\Elab{140} \Vnat{i} + 1 < \Vsize{slice}
\\ \Elab{150} \land\; \V{pair} = \tuple{\VVelement{slice}{i}, \VVelement{slice}{\V{i}+1}}
\\ \nonumber \myparbox{\cuz{} EI of \E{120}, ARB $\V{i} \in \naturals$.}
\end{gather}
\begin{equation}
\Elab{160}
\begin{gathered}
       \nexists\; \Vnat{i1} : \Vnat{i1} + 1 < \Vsize{seq}
       \\ \land\; \V{pair} = \tuple{\VVelement{seq}{i1}, \VVelement{seq}{\V{i1}+1}}
       \\ \myparbox{\cuz{} \Thref{sequence-pairs-set}, \E{110}.}
\end{gathered}
\end{equation}
\begin{equation}
\Elab{170}
\begin{gathered}
       \forall\; \Vnat{i1} : \Vnat{i1} + 1 \ge \Vsize{seq}
       \\ \lor\; \V{pair} \neq \tuple{\VVelement{seq}{i1}, \VVelement{seq}{\V{i1}+1}}
       \\ \myparbox{\cuz{} \E{160}.}
\end{gathered}
\end{equation}
\begin{equation}
\Elab{180}
\begin{gathered}
       \V{offset} + \Vnat{i} + 1 \ge \Vsize{seq}
       \\ \lor\; \V{pair} \neq \tuple{\Velement{seq}{\V{offset}+\V{i}}, \Velement{seq}{\V{offset}+\V{i}+1}}
       \\ \myparbox{\cuz{} EI of \V{i1} with \V{offset}+\V{i} in \E{170}.}
\end{gathered}
\end{equation}
\eqpar{190}{$\V{offset}+\V{len} \le \Dom{\V{seq}}$
    \bcuz{} \Dfref[Slice]{slice}, \Rmref[Slice Section Context]{cxt-sec-slice}.}
\eqpar{200}{$\V{offset}+\Vsize{slice} \le \Vsize{seq}$
    \bcuz{} \Dfref[Sequence]{sequence},
        TODO, \E{190}.}
\eqpar{210}{$\V{offset}+\V{i}+1 < \Vsize{seq}$ \cuz{} \E{140}, \E{190}.}
\eqpar{220}{$\V{pair} \neq \tuple{\Velement{seq}{\V{offset}+\V{i}}, \Velement{seq}{\V{offset}+\V{i}+1}}$
     \bcuz{} \E{180}, \E{210}.}
\eqpar{230}{$\V{i}+1 \in \V{len}$ \cuz{} TODO, \E{140}.}
\eqpar{240}{$\V{pair} \neq \tuple{\VVelement{slice}{i}, \VVelement{slice}{\V{i}+1}}$
     \bcuz{} TODO, \E{220}, \E{230}.}
\E{240} is contrary to \E{150} which shows the reductio, the non-empty subcase and the proof.
\myqed
\end{proof}

\todo{Eliminate slice indexing by integer?}
\begin{remark}[Slice natural range]
\label{rem:slice-natural-range}
The notation
\[ \V{subseq} = \V{s}[\Vnat{a} \ldots \Vnat{z}], \]
where \Vnat{a} and \Vnat{z} are natural numbers,
is interpreted by applying the following rules,
in order:
\begin{itemize}
\item
If $\V{a} \notin \Dom{\V{s}}$
or $\V{z} \notin \Dom{\V{s}}$,
then \V{subseq} is not well-defined.
\item
If \V{subseq} is well-defined,
and $\V{a} \le \V{z}$, then
\V{subset} is
the slice of \V{s} from \V{a} of size $\V{z}+1-\V{a}$,
in other words, the slice of \V{s} from \V{a} to \V{z}.
\item
If \V{subseq} is well-defined
and $\V{a} > \V{z}$,
then $\V{subseq} = \epsilon.$
\rmEnd
\end{itemize}
\end{remark}

\begin{remark}[Slice integer range]
\label{rem:slice-integer-range}
We recall that, when \Vz{x} is a non-negative integer, \Vnat{x} is
the representation of \Vz{x} in the canonical embedding from \naturals.
The notation
\[ \V{subseq} = \V{s}[\Vz{a} \ldots \Vz{z}], \]
where \Vz{a} and \Vz{z}
are integers,
is interpreted by applying the following rules,
in order:
\begin{itemize}
\item If \Vz{a} is negative, then $\V{subseq} = \epsilon.$
\item If \Vz{z} is negative, then $\V{subseq} = \epsilon.$
\item If both \Vz{a} and \Vz{z} are non-negative,
then
$\V{subseq} = \Velement{s}{\Vnat{a}\ldots\Vnat{z}}$.
\rmEnd
\end{itemize}
\end{remark}

\begin{example}[Slice range]
\label{ex:slice-range}
Here are some examples of
the slice notation of
\Rmref{slice-natural-range} and
\Rmref{slice-integer-range}.
Let
\[
    \V{s} = \seq{1, 2, 3, 4, 5}.
\]
\begin{itemize}
\item
    $\Velement{s}{0 \ldots {-1}} = \epsilon.$
\item
    $\Velement{s}{1 \ldots 2} = \seq{2, 3}.$
\item
    $\Velement{s}{2 \ldots 2} = \seq{3}.$
\item
    $\Velement{s}{3 \ldots 2} = \epsilon.$
\item
    $\Velement{s}{0 \ldots 4} = \seq{1, 2, 3, 4, 5}.$
\item
    \Velement{s}{0 \ldots 5} is not well-defined
        because $5 \notin \Dom{\V{s}}.$
\end{itemize}
We avoid corner-cases in our presentation.
But, for the sake of completeness, here are examples:
\begin{itemize}
\item $\Velement{s}{{-1} \ldots 5} = \epsilon.$
        The use of a negative number as one of the
        limits of the range is considered first.
        Therefore, that $5 \notin \Dom{\V{s}}$ does not become relevant.
\item $\Velement{s}{{-1} \ldots 4} = \epsilon.$
\item \Velement{s}{42 \ldots 3} is not well-defined
        because $42 \notin \Dom{\V{s}}.$
\item $\Velement{s}{42 \ldots {-3}} = \epsilon.$
        That $-3$ is negative is
        considered first and the fact that
        $42 \notin \Dom{\V{s}}$
        is not relevant.
\item $\Velement{s}{{-42} \ldots {-7}} = \epsilon.$ \exEnd
\end{itemize}
\end{example}

\todo{Rewrite from here}
\todo{Define offsets of a slice?}

\begin{definition}[Prefix]
\label{def:prefix}
\V{slice}
is a \dfn[Prefix@Prefix (of a sequence)]{prefix}
of \V{seq} iff
there is no indexable location before the first offset
of \V{slice} within \V{seq}.
\V{slice} is a
\dfn[Prefix@Prefix (of a sequence)!proper]{proper prefix}
iff $\V{slice} \neq \V{seq}$.
We write \Pfx{\V{slice},\V{seq}} to say
that \V{slice} is a prefix of \V{seq}.
We write \PrPfx{\V{slice},\V{seq}} to say
that \V{slice} is a proper prefix of \V{seq}.
\dfEnd
\end{definition}

\renewcommand{\labBase}{th-prefix}
\begin{theorem}[Prefix]
\label{th:prefix}
\V{slice} is a prefix of \V{seq} iff $\V{offset} = 0$.
\thEnd
\end{theorem}

\begin{proof}
For the ``if'' direction we note that no indexable
location is before location zero.
So if the first offset of a slice is at location zero,
no indexable location is before the first offset of
that slice, and the definition of prefix \Dfref{prefix} is satisfied.

For the ``only if'' direction we assume, for a reductio,
that there is a prefix, call it \V{prefix},
whose first offset is greater than zero.
Then the container sequence must have a length greater than zero,
and zero must be an indexable location.
If we have an indexable location at zero,
we have an indexable location less than the first offset.
If we have an indexable location less than the first offset,
then \V{prefix} does not satisfy the definition of prefix \Dfref{prefix},
contrary to assumption for the reductio.
With the reductio, we have both directions and the proof.
\myqed
\end{proof}

\begin{definition}[Suffix]
\label{def:suffix}
\V{slice}
is a \dfn[Suffix@Suffix (of a sequence)]{suffix}
of \V{seq} iff
there is no indexable location at or after the last offset
of \V{slice} in \V{seq}.
The suffix is a
\dfn[Suffix@Suffix (of a sequence)!proper]{proper suffix}
iff it is not identical to its container sequence.
We write \Sfx{\V{slice},\V{seq}} to say
that \V{slice} is a suffix of \V{seq}.
We write \PrSfx{\V{slice},\V{seq}} to say
that \V{slice} is a proper suffix of \V{seq}.
\dfEnd
\end{definition}

\renewcommand{\labBase}{th-suffix}
\begin{theorem}[Suffix]
\label{th:suffix}
\V{slice} is a suffix iff
$\V{len} = \Vsize{seq}-\V{offset}$.
\thEnd
\end{theorem}

\begin{proof}
% Proof template
\todo{Finish this proof.}
\myqed
\end{proof}

\section{Concatenations}

\section{Fusions}

\begin{definition}[Fusion]
\label{def:fusion}
We say that a container sequence is the
\dfn[Fusion (of two sequences)]{fusion}
of a prefix and a suffix iff
\begin{itemize}
\item the suffix and the prefix share that container sequence, and
\item the size of the prefix is equal to the offset of the suffix.  \dfEnd
\end{itemize}
\end{definition}

\begin{observation}
The prefix of a fusion must be finite, because
the size of the prefix is equal to the offset
of the suffix by the definition a fusion \Dfref{fusion},
and the offset
of a slice is finite by the definition of a slice \Dfref{slice},
\obEnd
\end{observation}

\begin{observation}
A prefix of a fusion may be the empty sequence,
in which case the matching suffix of the fusion is identical
to the fusion.
A suffix of a fusion may be the empty sequence,
in which case the matching prefix of the fusion is identical
to the fusion.
\obEnd
\end{observation}

\todo{Prove well-definedness, length, domain,
    codomain, and slice pairs for fusions}

\chapter{Directed graphs}
\label{ch:directed-graphs}

\todo[prepend, caption={``Directed Graphs'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

Graph theory is one of our preceding disciplines,
and the graph theory results we use are standard.
Ordinarily, we would simply refer to them.
Unfortunately,
our authorities differ in details of terminology
and derivation.\footnote{%
    The authority we follow most closely is
    is \cite{HU1979}.}

In order to be careful, therefore,
we are forced to
make explicit the choices
we have made for the fragments of graph theory that we use.
This chapter lays out our definitions and basic results
for directed graphs.

\begin{definition}[Directed graph]
\label{def:directed-graph}
A \dfn{directed graph} is a pair
$\tuple{\V{V}, \V{E}}$,
where \V{V} is a finite set of \dfn{vertices},
and $\V{E} \in \powerset{\V{V}\times\V{V}}$.
\dfEnd
\end{definition}

This chapter is a new context scope, which contains an arbitrary
directed graph called the â€œcontext directed graphâ€ or ``context graph''.
The \dfn{context graph} is
$\V{G} = \tuple{\V{V}, \V{E}}$.

\begin{definition}[Parents]
\label{def:parents}
\noindent
\begin{itemize}
\item \V{parent} is the \dfn{parent} of \V{child} iff $\tuple{\V{parent},\V{child}} \in \V{E}$.
\item \V{child} is the \dfn{child} of \V{parent} iff $\tuple{\V{parent},\V{child}} \in \V{E}$.
\item The \dfn{in-degree} of a vertex is the count of its parents.
\item The \dfn{out-degree} of a vertex is the count of its children.
    \dfEnd
\end{itemize}
\end{definition}

\begin{ldef}[\myfn{parents}{} and \myfn{children}{}]
\label{ldef:parents}
\begin{gather}
\label{eq:ldef-parents-150}
\Vmyfn{parents}{child} \ldefined
    \set{
        \begin{gathered}
        \V{parent} \in \V{V} :
        \\  \tuple{\V{parent},\V{child}} \in \V{E}
        \end{gathered}
    }.
\\ \label{eq:ldef-parents-250}
\Vmyfn{children}{parent} \ldefined
    \set{
        \begin{gathered}
        \V{child} \in \V{V} :
        \\  \tuple{\V{parent},\V{child}} \in \V{E}
        \end{gathered}
    }. \quad \dfEnd
\end{gather}
\end{ldef}

\section{Paths}

\begin{definition}[Path]
\label{def:path}
We say that a finite non-empty sequence of vertices is a \dfn{path}
of a directed graph
iff each successive pair of  values is one
of the edges of the directed graph.
More specifically,
\V{pth} is a \dfn{path}
iff it is an element of \Vmyfn{paths}{G},
where
\begin{equation}
\label{eq:def-path}
\begin{gathered}
    \Vmyfn{paths}{G} \defined \set{ \begin{gathered}
        \V{pth} \in \powerset{\naturals\times\V{V}} :
        \\ \exists\; \Vnat{length} > 0 : \Dom{\V{pth}} = \V{length}
        \\ \land\; \left( \begin{gathered}
                \forall\; \Vnat{ix} < \Vlastix{pth} :
                \\ \family{ \VVelement{pth}{ix}, \Velement{pth}{\V{ix}+1} } \in \V{E}
            \end{gathered} \right)
    \end{gathered} }.
\end{gathered}
\end{equation}
If $\Vsize{pth}=1$, we say that \V{pth} is \dfn{trivial}.
\dfEnd
\end{definition}

\begin{theorem}[Edge To-node]
\label{th:edge-to-node}
If a vertex is not at location 0,
there must be an edge to it.
This is,
\[
\begin{gathered}
    \forall\; \V{p} \in \Vmyfn{paths}{G} :
    \forall\; \Vnat{ix} \le \Vlastix{p} :
    \\ \V{ix} \ne 0 \implies
        \left(
            \exists\; \V{from} \in \V{V} :
                 \tuple{ \Velement{p}{\Vdecr{ix}}, \VVelement{p}{ix}}
                 \in \V{E}
        \right).
        \quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
This theorem follows immediately from the definition of a path \Dfref{path}.
\myqed
\end{proof}

\begin{definition}[Node-induced Sequence]
\label{def:node-induced}
A node \dfn{induces} a sequence iff that sequence is
the singleton whose only element is that node.
That is, the node \V{nd} induces the sequence \V{seq}
iff $\V{seq} = \seq{\V{nd}}$.
\dfEnd
\end{definition}

\begin{theorem}[Node-induced Path]
\label{th:node-induced-path}
Every node in a directed graph \dfn{induces} a unique sequence,
and that sequence is a path of length 1.
That is, if
\mypareq{eq:th-node-induced-path-010}{%
    $\V{nd} \in \V{V}$}
then
\begin{gather}
\label{eq:th-node-induced-path-020}
    \existUniqQOp\; \V{induced} \in \Vmyfn{paths}{G} :
\\ \label{eq:th-node-induced-path-030}
    \V{induced} = \seq{\V{nd}}
\\ \label{eq:th-node-induced-path-040}
    \land\; \Vsize{induced} = 1.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct,
assuming the hypothesis
\Eref{th-node-induced-path-010}
to show the consequent
\Eref{th-node-induced-path-020}--\Eref{th-node-induced-path-040}.
For this proof we define a boolean function
$\fname{Induced} \in 2 \tfnMap (\V{V} \tfnMMap \naturals)$
such that
\mypareq{eq:th-node-induced-path-100}{%
    for all \Rvarseq{seq}{V}, \Vmyfn{Induced}{\V{seq}} is true iff
    \V{seq} is the sequence induced by \V{nd} according to
    the definition \Dfref{node-induced}.}
We show the existence of at least one \V{nd}-induced
sequence by constructing a new free sequence, and proving that our construction
is, in fact, a sequence induced by \V{nd}.
\mypareq{eq:th-node-induced-path-110}{%
    $\V{singleton} = \seq{\V{nd}}$
        \cuz{} CV \Rvarseq{singleton}{V}.}
\mypareq{eq:th-node-induced-path-120}{%
    $\V{singleton} \in \ \V{V} \tfnMap 1$
        \cuz{} \Eref{th-node-induced-path-110}.}
\mypareq{eq:th-node-induced-path-130}{%
    $\Vmyfn{Induced}{singleton}$
    \cuz{} \Dfref{node-induced}, \Eref{th-node-induced-path-110}.}
We introduce a second, arbitrarily chosen, sequence induced by \V{nd},
in order to show that the sequence induced by \V{nd} is unique:
\mypareq{eq:th-node-induced-path-140}{%
    $\Vmyfn{Induced}{singleton2}$
        \cuz{} ARB \Rvarseq{singleton2}{V}.}
\mypareq{eq:th-node-induced-path-150}{%
    $\V{singleton2} = \seq{\V{nd}}$
    \cuz{} \longDfref{Node-induced sequence}{node-induced}.}
Since \V{singleton2} is an arbitrarily chosen \V{nd}-induced sequence,
we may universally generalize the implication from
\Eref{th-node-induced-path-140} to \Eref{th-node-induced-path-150}:
\mypareq{eq:th-node-induced-path-160}{%
    $\forall\; \Rvarseq{induced}{V} :
    \Vmyfn{Induced}{induced} \implies \V{induced} = \seq{\V{nd}}$
    \bcuz{} \Eref{th-node-induced-path-150}.}
And from this we see that the sequence induced by \V{nd} is unique:
\begin{equation}
\label{eq:th-node-induced-path-170}
\begin{gathered}
    \forall\; \Rvarseq{induced1}{V} : \forall\; \Rvarseq{induced2}{V} :
    \\ \Vmyfn{Induced}{induced1} \land \Vmyfn{Induced}{induced2}
    \\ \implies \V{induced1} = \V{induced2}
    \\ \myparbox{\cuz{} \Eref{th-node-induced-path-160}.}
\end{gathered}
\end{equation}
It remains to show that \V{induced} is a path,
and that it is of length 1.
\mypareq{eq:th-node-induced-path-180}{%
    $\Vsize{singleton} = 1$
        \cuz{} \Eref{th-node-induced-path-120}.}
From \Eref{th-node-induced-path-120}
it follows that
that \V{singleton} is a finite, non-empty sequence of vertices.
From this and a vacuous use of the definition of path,
it follows that \V{singleton} is a path:
\mypareq{eq:th-node-induced-path-190}{%
    $\V{singleton} \in \Vmyfn{paths}{G}$
    \cuz{} \longDfref{Path}{path},
        \Eref{th-node-induced-path-120}.}
The consequent of the theorem follows from equations
\Eref{th-node-induced-path-110},
\Eref{th-node-induced-path-170},
\Eref{th-node-induced-path-180},
and \Eref{th-node-induced-path-190},
followed by the unique existential generalization of \V{singleton}.
\myqed
\end{proof}

\begin{definition}[Edge-induced Sequence]
\label{def:edge-induced}
Let $\V{edge} \in \V{E}$ be an edge in \V{G} such that
$\V{edge} = \tuple{\V{from},\V{to}})$.
We say that \V{edge} \dfn{induces} a sequence iff
that sequence is \seq{\V{from},\V{to}}.
\dfEnd
\end{definition}

\begin{theorem}[Edge-induced Path]
\label{th:edge-induced-path}
Every edge in a directed graph induces a unique path
of length 2.
That is, if
\mypareq{eq:th-edge-induced-path-010}{%
    $\V{edge} \in \V{E} \land \V{edge} = \tuple{\V{from},\V{to}}$}
then
\begin{gather}
\label{eq:th-edge-induced-path-020}
    \existUniqQOp\; \V{induced} \in \Vmyfn{paths}{G} :
\\ \label{eq:th-edge-induced-path-030}
    \V{induced} = \seq{\V{from},\V{to}}
\\ \label{eq:th-edge-induced-path-040}
    \land\; \Vsize{induced} = 2.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct,
assuming the hypothesis
\Eref{th-edge-induced-path-010}
to show the consequent
\Eref{th-edge-induced-path-020}--\Eref{th-edge-induced-path-040}.
For this proof we define a boolean function
$\fname{Induced} \in 2 \tfnMap (\V{V} \tfnMMap \naturals)$
such that
\mypareq{eq:th-edge-induced-path-100}{%
    for all \Rvarseq{seq}{V}, \Vmyfn{Induced}{\V{seq}} is true iff
    \V{seq} is the sequence induced by
    \V{edge} according to
    the definition \Dfref{edge-induced}.}
We show the existence of at least one \V{nd}-induced
sequence by constructing a new free sequence, and proving that our construction
is, in fact, a sequence induced by \V{edge}.
\mypareq{eq:th-edge-induced-path-110}{%
    $\V{duple} = \seq{\V{from},\V{to}}$
        \cuz{} CV \Rvarseq{duple}{V}.}
\mypareq{eq:th-edge-induced-path-120}{%
    $\V{duple} \in \ \V{V} \tfnMap 2$
        \cuz{} \Eref{th-edge-induced-path-110}.}
\mypareq{eq:th-edge-induced-path-130}{%
    $\Vmyfn{Induced}{duple}$
    \cuz{} \Dfref{edge-induced}, \Eref{th-edge-induced-path-110}.}
We introduce a second, arbitrarily chosen, sequence induced by \V{edge},
in order to show that the sequence induced by \V{edge} is unique:
\mypareq{eq:th-edge-induced-path-140}{%
    $\Vmyfn{Induced}{duple2}$
        \cuz{} ARB \Rvarseq{duple2}{V}.}
\mypareq{eq:th-edge-induced-path-150}{%
    $\V{duple2} = \seq{\V{from},\V{to}}$
    \cuz{} \longDfref{Node-induced sequence}{edge-induced}.}
Since \V{duple2} is an arbitrarily chosen \V{edge}-induced sequence,
we may universally generalize the implication from
\Eref{th-edge-induced-path-140} to \Eref{th-edge-induced-path-150}.
\mypareq{eq:th-edge-induced-path-160}{%
    $\forall\; \Rvarseq{induced}{V} :
    \Vmyfn{Induced}{induced} \implies \V{induced} = \seq{\V{from},\V{to}}$
    \bcuz{} \Eref{th-edge-induced-path-150}.}
And from this we see that the sequence induced by \V{edge} is unique:
\begin{equation}
\label{eq:th-edge-induced-path-170}
\begin{gathered}
    \forall\; \Rvarseq{induced1}{V} : \forall\; \Rvarseq{induced2}{V} :
    \\ \Vmyfn{Induced}{induced1} \land \Vmyfn{Induced}{induced2}
    \\ \implies \V{induced1} = \V{induced2}
    \\ \myparbox{\cuz{} \Eref{th-edge-induced-path-160}.}
\end{gathered}
\end{equation}
It remains to show that \V{induced} is a path,
and that it is of length 2.
\mypareq{eq:th-edge-induced-path-180}{%
    $\Vsize{duple} = 2$
        \cuz{} \Eref{th-edge-induced-path-120}.}
\mypareq{eq:th-edge-induced-path-190}{%
    \V{duple} is a finite, non-empty sequence of vertices.
      \bcuz{} \Eref{th-edge-induced-path-120}.}
\mypareq{eq:th-edge-induced-path-200}{%
    $\tuple{\V{from},\V{to}} \in \V{E}$
        \cuz{} \Eref{th-edge-induced-path-010}.}
\mypareq{eq:th-edge-induced-path-210}{%
    $\V{duple} \in \Vmyfn{paths}{G}$
    \bcuz{} \longDfref{Path}{path},
        \Eref{th-edge-induced-path-110},
        \Eref{th-edge-induced-path-190},
        \Eref{th-edge-induced-path-200}.}
The consequent of the theorem follows from equations
\Eref{th-edge-induced-path-110},
\Eref{th-edge-induced-path-170},
\Eref{th-edge-induced-path-180},
and \Eref{th-edge-induced-path-210},
followed by the unique existential generalization of \V{duple}.
\myqed
\end{proof}

\begin{definition}[Subpath]
\label{def:subpath}
Let \V{p} be a path in \V{G}.
A sequence is a \dfn{subpath} of \V{p}
iff it is a slice of \V{p}.
A subpath of \V{p} is a \dfn{proper subpath} of
\V{p} iff it is not equal to \V{p}.
The triple of path \V{p}, start location \Vnat{lo}
and end location \Vnat{hi}
is said to \dfn{induce} a subpath if the subpath
is $\VVelement{p}{\V{lo} \ldots \V{hi}}$.
\dfEnd
\end{definition}

\begin{theorem}[Subpath]
\label{th:subpath}
If start and end locations are inside a path,
and the end location is at or after the start location,
the path and the locations
induce a unique path
whose length is the one plus the difference of the locations.
That is, if
\begin{gather}
\label{eq:th-subpath-010}
\V{path} \in \Vmyfn{paths}{G}
\\ \label{eq:th-subpath-020}
    \land\; \V{lo} \le \Vlastix{path}
\\ \label{eq:th-subpath-030}
    \land\; \V{lo} \le \V{hi} \le \Vlastix{path}
\end{gather}
then
\begin{gather}
\label{eq:th-subpath-040}
\existUniqQOp\; \V{subpath} \in \Vmyfn{paths}{G}
\\ \label{eq:th-subpath-050}
    \land\; \V{subpath} = \VVelement{path}{\V{lo} \ldots \V{hi}}
\\ \label{eq:th-subpath-060}
    \land\; \Vsize{subpath} = (\subtract{\V{hi}}{\V{lo}}) + 1.
        \quad \dfEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct, assuming the hypothesis
\Eref{th-subpath-010}--\Eref{th-subpath-030}
to show the consequent
For convenient reference, we assign a variable name to the
triple containing the data that induces the subpath:
\mypareq{eq:th-subpath-100}{%
    \V{splice} = \tuple{\V{path},\V{lo},\V{hi}}
    \bcuz{} CV $\V{splice} \in
        (\V{V}\tfnMMap\naturals)
        \times\naturals\times\naturals$.}
For this proof we define a boolean function
$\fname{Induced} \in 2 \tfnMap (\V{V} \tfnMMap \naturals)$
such that
\mypareq{eq:th-subpath-105}{%
    for all \Rvarseq{seq}{V}, \Vmyfn{Induced}{\V{seq}} is true iff
    \V{seq} is the sequence induced by \V{splice} according to
    the definition \Dfref{subpath}.}
We show the existence of at least one \V{splice}-induced
sequence by constructing a new free sequence, and proving that our construction
is, in fact, a sequence induced by \V{splice}.
\mypareq{eq:th-subpath-110}{%
    $\V{subpath} = \Velement{path}{\V{lo}\ldots \V{hi}}$
        \cuz{} CV \Rvarseq{subpath}{V}.}
\mypareq{eq:th-subpath-120}{%
    $\V{subpath} \in \V{V} \tfnMap \left( (\subtract{\V{hi}}{\V{lo}})+1 \right)$
        \cuz{} \Eref{th-subpath-110}.}
\mypareq{eq:th-subpath-130}{%
    $\Vmyfn{Induced}{subpath}$
    \cuz{} \Dfref{subpath}, \Eref{th-subpath-110}.}
We introduce a second, arbitrarily chosen, sequence induced by \V{splice},
in order to show that the sequence induced by \V{splice} is unique:
\mypareq{eq:th-subpath-140}{%
    $\Vmyfn{Induced}{subpath2}$
        \cuz{} ARB \Rvarseq{subpath2}{V}.}
\mypareq{eq:th-subpath-150}{%
    $\V{subpath2} = \seq{\V{lo}\ldots \V{hi}}$
    \cuz{} \longDfref{Subpath}{subpath}.}
Since \V{subpath2} is an arbitrarily chosen \V{splice}-induced sequence,
we may universally generalize the implication from
\Eref{th-subpath-140} to \Eref{th-subpath-150}.
\mypareq{eq:th-subpath-160}{%
    $\forall\; \Rvarseq{induced}{V} :
    \Vmyfn{Induced}{induced} \implies \V{induced} = \seq{\V{lo}\ldots \V{hi}}$
    \bcuz{} \Eref{th-subpath-150}.}
And from this we see that the sequence induced by \V{splice} is unique:
\begin{equation}
\label{eq:th-subpath-170}
\begin{gathered}
    \forall\; \Rvarseq{induced1}{V} : \forall\; \Rvarseq{induced2}{V} :
    \\ \Vmyfn{Induced}{induced1} \land \Vmyfn{Induced}{induced2}
    \\ \implies \V{induced1} = \V{induced2}
    \\ \myparbox{\cuz{} \Eref{th-subpath-160}.}
\end{gathered}
\end{equation}
It remains to show that \V{induced} is a path,
and that it is of the required length.
\mypareq{eq:th-subpath-180}{%
    $\Vsize{subpath} = \left( (\subtract{\V{hi}}{\V{lo}})+1 \right)$
	\cuz{} \Eref{th-subpath-120}.}
\mypareq{eq:th-subpath-190}{%
    \V{subpath} is a finite, non-empty sequence of vertices.
        \bcuz{} \Eref{th-subpath-020}, \Eref{th-subpath-030},
            \Eref{th-subpath-120}, \Eref{th-subpath-180}.}
\mypareq{eq:th-subpath-200}{%
    $ \V{lo} + \Vlastix{subpath} = \V{hi}$
	\cuz{} \Eref{th-subpath-180}.}
\mypareq{eq:th-subpath-250}{%
    $\forall\; \Vnat{ix} \le \Vlastix{subpath} :
        \VVelement{subpath}{ix} = \Velement{path}{\V{lo}+\V{ix}}$
        \bcuz{} \Eref{th-subpath-110}, \Eref{th-subpath-200}.}
\mypareq{eq:th-subpath-260}{%
    $\forall\; \Vnat{ix} < \Vlastix{path} :
        \tuple{ \VVelement{path}{ix}, \Velement{path}{\V{ix}+1} } \in \V{E}$
    \bcuz{} \longDfref{Path}{path}, \Eref{th-subpath-040}.}
\mypareq{eq:th-subpath-270}{%
    $\forall\; \Vnat{ix} < \Vlastix{subpath} :
        \tuple{ \VVelement{subpath}{ix}, \Velement{subpath}{\V{ix}+1} } \in \V{E}$
    \bcuz{} \Eref{th-subpath-250}, \Eref{th-subpath-260}.}
\mypareq{eq:th-subpath-400}{%
    $\V{subpath} \in \Vmyfn{paths}{G}$
    \bcuz{} \longDfref{Path}{path},
        \Eref{th-subpath-110},
        \Eref{th-subpath-190},
        \Eref{th-subpath-270}.
	}
The consequent of the theorem follows from equations
\Eref{th-subpath-110},
\Eref{th-subpath-170},
\Eref{th-subpath-180},
and \Eref{th-subpath-400},
followed by the unique existential generalization of \V{subpath}.
\myqed
\end{proof}

\begin{definition}[Path Fusion]
\label{def:path-fusion}
Let \V{p1} and \V{p2} be paths in \V{G}.
Their \dfn{fusion}
is a path, call it \V{fusion},
such that \V{p1} is a prefix of \V{fusion},
\V{p2} is a suffix of \V{fusion},
and the overlap of the prefix and suffix in \V{fusion}
is a singleton.
That is,
the \dfn{fusion} of
\V{p1} and \V{p2} is the sequence \V{fusion} such that
\begin{equation}
\label{eq:def-fusion}
\begin{gathered}
    \Velement{fusion}{0\ldots \Vlastix{p1}} = \V{p1}
    \\ \land\; \Velement{fusion}{\Vlastix{p1}\ldots\Vlastix{fusion}} = \V{p2}
        \quad \dfEnd
\end{gathered}
\end{equation}
\end{definition}

As an example, if \V{p1} = \seq{\V{nd1}, \V{nd2}, \V{nd3}}
and \V{p2} = \seq{\V{nd3}, \V{nd4}, \V{nd5}}
then their fusion is
\seq{\V{nd1}, \V{nd2}, \V{nd3}, \V{nd4}, \V{nd5}}.

\begin{theorem}[Path Fusion]
\label{th:fusion}
If two paths
in \V{G} are such that the tail of the first
one is equal to the head of the second one,
then they have a unique fusion whose
the length is the sum of the lengths
of those two paths, minus one.

That is, if
\begin{gather}
\label{eq:th-fusion-010}
   \V{path1} \in \Vmyfn{paths}{G}
\\ \label{eq:th-fusion-020}
   \land\; \V{path2} \in \Vmyfn{paths}{G}
\\ \label{eq:th-fusion-030}
   \land\; \Velement{path1}{\Vlastix{path1}} = \Velement{path2}{0}
\end{gather}
then
\begin{gather}
\label{eq:th-fusion-040}
   \existUniqQOp\; \V{fusion} \in \Vmyfn{paths}{G} :
\\ \label{eq:th-fusion-050}
   \Velement{fusion}{0\ldots \Vlastix{path1}} = \V{path1}
\\ \label{eq:th-fusion-060}
   \land\; \Velement{fusion}{\Vlastix{path1}\ldots\Vlastix{fusion}} = \V{path2}
\\ \label{eq:th-fusion-070}
   \land\; \Vsize{fusion} = \subtract{\Vsize{path1} + \Vsize{path2}}{1}.
       \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct,
assuming the hypothesis of the theorem to
show its consequent.
For convenience, we assign a variable name to the
duple containing the data that induces the fusion:
\mypareq{eq:th-fusion-100}{%
    \V{pair} = \tuple{\V{path1},\V{path2}}
    \bcuz{} CV $\V{pair} \in
        (\V{V}\tfnMMap\naturals) \times (\V{V}\tfnMMap\naturals).$}
Also, within the context of this proof, we define a boolean function
$\fname{Induced} \in 2 \tfnMap (\V{V} \tfnMMap \naturals)$
such that
\mypareq{eq:th-fusion-105}{%
    for all \Rvarseq{seq}{V}, \Vmyfn{Induced}{\V{seq}} is true iff
    \V{seq} is the sequence induced by \V{pair} according to
    the definition \Dfref{path-fusion}.}
We show the existence of at least one \V{pair}-induced
sequence by constructing a new free sequence, and proving that our construction
is, in fact, a sequence induced by \V{pair}.
\begin{gather}
\label{eq:th-fusion-110}
    \V{fusion} \in \V{V} \tfnMap
        \decr{\left( \Vsize{path1} + \Vsize{path2} \right) }
\\ \label{eq:th-fusion-112}
    \begin{gathered}
    \forall\; \V{ix} < \subtract{\left( \Vsize{path1} + \Vsize{path2} \right) }{1} :
    \\ \VVelement{fusion}{ix} = \left( \begin{gathered}
            \V{ix}\le\Vlastix{path1} \;\condOpA
            \\ \VVelement{path1}{ix} \;\condOpB
            \\ \Velement{path2}{\subtract{\V{ix}}{\Vlastix{path1}}}
         \end{gathered} \right)
\end{gathered}
\\ \nonumber
    \myparbox{\cuz{} CV \Rvarseq{fusion}{V}.}
\end{gather}
\mypareq{eq:th-fusion-120}{%
   $\Velement{fusion}{0\ldots \Vlastix{path1}} = \V{path1}$
   \cuz{} \Eref{th-fusion-110}, \Eref{th-fusion-112}.}
\mypareq{eq:th-fusion-130}{%
   $\Velement{fusion}{\Vlastix{path1}\ldots\Vlastix{fusion}} = \V{path2}$
   \bcuz{} \Eref{th-fusion-030},
       \Eref{th-fusion-110}, \Eref{th-fusion-112}.}
\mypareq{eq:th-fusion-140}{%
    $\Vsize{fusion} =
        \decr{\left( \Vsize{path1} + \Vsize{path2} \right) }$
        \cuz{} \Eref{th-fusion-110}.}
\mypareq{eq:th-fusion-150}{%
    $\Vmyfn{Induced}{fusion}$ \bcuz{} \longDfref{Fusion}{path-fusion},
        \Eref{th-fusion-105}, \Eref{th-fusion-120},
        \Eref{th-fusion-130}.}
To show that the \V{pair}-induced sequence is unique,
we introduce a second, arbitrarily chosen, sequence induced by \V{pair}:
\mypareq{eq:th-fusion-160}{%
    $\Vmyfn{Induced}{fusion2}$
        \cuz{} ARB \Rvarseq{fusion2}{V}.}
\mypareq{eq:th-fusion-170}{%
   $\Velement{fusion2}{0\ldots \Vlastix{path1}} = \V{path1}$
    \bcuz{} \longDfref{Fusion}{path-fusion},
        \Eref{th-fusion-105}, \Eref{th-fusion-160}.}
\mypareq{eq:th-fusion-180}{%
   $\Velement{fusion2}{\Vlastix{path1}\ldots\Vlastix{fusion2}} = \V{path2}$
    \bcuz{} \longDfref{Fusion}{path-fusion},
        \Eref{th-fusion-105}, \Eref{th-fusion-160}.}
\mypareq{eq:th-fusion-182}{%
    $\Vsize{fusion2} =
        \decr{\left( \Vsize{path1} + \Vsize{path2} \right) }$
            \bcuz{} \longDfref{Fusion}{path-fusion},
            \Eref{th-fusion-105}, \Eref{th-fusion-160}.}
\mypareq{eq:th-fusion-184}{%
    $\Vsize{fusion} = \Vsize{fusion2}$ \cuz{}
        \Eref{th-fusion-140}, \Eref{th-fusion-182}.}
\begin{equation}
\label{eq:th-fusion-190}
\begin{gathered}
    \forall\; \V{ix} < \subtract{\left( \Vsize{path1} + \Vsize{path2} \right) }{1} :
    \\ \VVelement{fusion2}{ix} = \VVelement{fusion}{ix}
    \\ \myparbox{\cuz{} \Eref{th-fusion-110},
        \Eref{th-fusion-170}, \Eref{th-fusion-180}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-fusion-200}{%
    $\V{fusion2} = \V{fusion}$ \cuz{}
        \Eref{th-fusion-184}, \Eref{th-fusion-190}.}
Since \V{fusion2} is an arbitrarily chosen \V{pair}-induced sequence,
we may universally generalize the implication from
\Eref{th-fusion-160} to \Eref{th-fusion-200}.
\mypareq{eq:th-fusion-210}{%
    $\forall\; \Rvarseq{induced}{V} :
    \Vmyfn{Induced}{induced} \implies \V{induced} = \V{fusion}$
    \bcuz{} \Eref{th-fusion-200}.}
And from this we see that the sequence induced by \V{pair} is unique:
\begin{equation}
\label{eq:th-fusion-220}
\begin{gathered}
    \forall\; \Rvarseq{induced1}{V} : \forall\; \Rvarseq{induced2}{V} :
    \\ \Vmyfn{Induced}{induced1} \land \Vmyfn{Induced}{induced2}
    \\ \implies \V{induced1} = \V{induced2}
    \\ \myparbox{\cuz{} \Eref{th-fusion-210}.}
\end{gathered}
\end{equation}
It remains to show that \V{fusion} is a path,
and of the required length.
By definition paths are finite and of non-zero length, so that
\mypareq{eq:th-fusion-232}{%
        $\decr{\left( \Vsize{path1} + \Vsize{path2} \right) } \in \naturals
        \land \decr{\left( \Vsize{path1} + \Vsize{path2} \right) } > 0$
        \bcuz{} \longDfref{Path}{path},
            \Eref{th-fusion-010},
            \Eref{th-fusion-020}.}
\mypareq{eq:th-fusion-234}{%
    \V{fusion} is a finite, non-empty sequence of vertices
    \bcuz{} \Eref{th-fusion-140}, \Eref{th-fusion-140},
            \Eref{th-fusion-232}.}
\mypareq{eq:th-fusion-240}{%
    $\forall\; \Vnat{ix} < \Vlastix{path1} :
        \tuple{ \VVelement{path1}{ix}, \Velement{path1}{\V{ix}+1} } \in \V{E}$
    \bcuz{} \longDfref{Path}{path}, \Eref{th-fusion-010},
         \Eref{th-fusion-120}.}
\mypareq{eq:th-fusion-250}{%
    $\forall\; \Vnat{ix} \le \Vlastix{path1} :
        \VVelement{fusion}{ix} = \VVelement{path1}{ix}$
        \cuz{} \Eref{th-fusion-120}.}
\mypareq{eq:th-fusion-260}{%
    $\forall\; \Vnat{ix} < \Vlastix{path1} :
        \tuple{ \VVelement{fusion}{ix}, \Velement{fusion}{\V{ix}+1} } \in \V{E}$
    \bcuz{} \longDfref{Path}{path}, \Eref{th-fusion-240},
        \Eref{th-fusion-250}.}
\mypareq{eq:th-fusion-270}{%
    $\forall\; \Vnat{ix} < \Vlastix{path2} :
        \tuple{ \VVelement{path2}{ix}, \Velement{path2}{\V{ix}+1} } \in \V{E}$
    \bcuz{} \longDfref{Path}{path}, \Eref{th-fusion-020},
         \Eref{th-fusion-130}.}
\mypareq{eq:th-fusion-280}{%
    $\forall\; \Vnat{ix} < \Vlastix{path2} :
        \VVelement{path2}{ix} =
        \VVelement{fusion}{\V{ix}+\Vlastix{path1}}$
        \bcuz{} \Eref{th-fusion-130}.}
\mypareq{eq:th-fusion-290}{%
    $\forall\; \Vnat{ix} < \Vlastix{path2} :
        \tuple{\Velement{fusion}{\V{ix}+\Vlastix{path1}},
            \Velement{fusion}{\V{ix}+\Vlastix{path1}+1}} \in \V{E}$
    \bcuz{}  \Eref{th-fusion-270}, \Eref{th-fusion-280}.}
\begin{equation}
\label{eq:th-fusion-300}
\begin{gathered}
    \forall\; \Vnat{ix}
        \;(\Vlastix{path1} \le \V{ix} < \Vlastix{path1}+\Vlastix{path2}) :
    \\ \tuple{\Velement{fusion}{\V{ix}}, \Velement{fusion}{\V{ix}+1}} \in \V{E}
    \\ \myparbox{\cuz{}  \Eref{th-fusion-270}, \Eref{th-fusion-280}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-fusion-310}{%
    $\Vlastix{fusion} = \Vlastix{path1} + \Vlastix{path2}$
        \cuz{} \Eref{th-fusion-140}.}
\begin{equation}
\label{eq:th-fusion-320}
\begin{gathered}
    \forall\; \Vnat{ix}
        \;(\Vlastix{path1} \le \V{ix} < \Vlastix{fusion}) :
    \\ \tuple{\Velement{fusion}{\V{ix}}, \Velement{fusion}{\V{ix}+1}} \in \V{E}
    \\ \myparbox{\cuz{}  \Eref{th-fusion-300}, \Eref{th-fusion-310}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-fusion-330}{%
    $\forall\; \Vnat{ix} < \Vlastix{fusion} :
     \tuple{\Velement{fusion}{\V{ix}}, \Velement{fusion}{\V{ix}+1}} \in \V{E}$
    \bcuz{} \Eref{th-fusion-260}, \Eref{th-fusion-320}.}
\mypareq{eq:th-fusion-400}{%
    $\V{fusion} \in \Vmyfn{paths}{G}$
    \bcuz{} \longDfref{Path}{path},
        \Eref{th-fusion-234}, \Eref{th-fusion-330}.}
The consequent of this theorem is
\Eref{th-fusion-120}, \Eref{th-fusion-130},
\Eref{th-fusion-140}, \Eref{th-fusion-220},
and \Eref{th-fusion-400},
followed by the unique existential generalization of \V{fusion}.

\todo{Recheck proof.}
\myqed
\end{proof}

\begin{theorem}[Path Pre-increment]
\label{th:path-pre-increment}
If, in a directed graph,
there is an edge from a node to the head of a path,
then there is a unique new path
that is the old path with the node prepended.
Further,
the size of new path is the size of the old path plus one.
That is,
\[
\begin{gathered}
\forall\; \V{old} \in \Vmyfn{paths}{G} : \forall\; \V{nd} \in \V{V} :
\\ \tuple{\V{nd},\Velement{old}{0}} \in \V{E} \implies
\\ \left(\begin{gathered}
        \existUniqQOp \V{new} \in \Vmyfn{paths}{G} :
        \\ \V{new} = \seq{\V{nd},\Velement{old}{0} \ldots \Velement{old}{\Vlastix{old}}}
        \\ \land\; \Vsize{new} = \Vsize{old}+1
    \end{gathered}\right).
    \quad \dfEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
\todo{Revise this proof.}
\myqed
\end{proof}

\begin{theorem}[Path Post-increment]
\label{th:path-post-increment}
If, in a directed graph,
there is an edge from
the tail of a path to
a node to
then there is a unique new path
that is the old path with the node appended.
Further,
the size of new path is the size of the old path plus one.
That is,
\[
\begin{gathered}
\forall\; \V{old} \in \Vmyfn{paths}{G} : \forall\; \V{nd} \in \V{V} :
\\ \tuple{\Velement{old}{\Vlastix{old}}, \V{nd}} \in \V{E} \implies
\\ \left(\begin{gathered}
        \existUniqQOp \V{new} \in \Vmyfn{paths}{G} :
        \\ \V{new} = \seq{\Velement{old}{0} \ldots \Velement{old}{\Vlastix{old}}, \V{nd}}
        \\ \land\; \Vsize{new} = \Vsize{old}+1
    \end{gathered}\right).
    \quad \dfEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
\todo{Revise this proof.}
\myqed
\end{proof}

\section{Acyclic directed graphs}

\begin{definition}[Cycle]
\label{def:cycle}
In a directed graph,
a cycle is a non-trivial path that starts and ends
with the same node.
That is,
\Rvarseq{cycle}{V} is a cycle iff
\[
\Vsize{cycle} > 1 \land
    \Velement{cycle}{0} = \Velement{cycle}{\Vlastix{cycle}}.
    \quad \dfEnd
\]
\end{definition}

\begin{definition}[Acyclic directed graph]
\label{def:acyclic}
A directed graph is \dfn{acyclic} iff
none of its paths is a cycle.
\dfEnd
\end{definition}

Let from here until the end of this section be
a new context scope, in which
the \dfn{context graph} $\V{G} = \tuple{\V{V}, \V{E}}$
is acyclic.

\begin{theorem}[Acyclic path maximum]
\label{th:acyclic-path-maximum}
The length of a path in an acyclic directed graph is at most
the number of nodes in the graph.
That is,
\begin{equation*}
\forall\; \V{path} \in \Vmyfn{paths}{G} :
    \Vsize{path} \le \Vsize{V}. \quad \thEnd
\end{equation*}
\end{theorem}

\begin{proof}
Let \V{path} be a path in \V{G}.
By definition \Dfref{path},
a path is a sequence whose values are the nodes of \V{G}.
By assumption for the theorem, \V{G} is acyclic,
so that by the definition of acyclic \Dfref{acyclic}
no value occurs twice in \V{path},
and every value of \V{path} is distinct.
The values of \V{path} are the nodes of \V{G},
that is, \V{V}.
Therefore there can be at most one distinct value of \V{path}
for each element of {V}.
The theorem follows.
\myqed
\end{proof}

\begin{theorem}[Acyclic Path Set Finiteness]
\label{th:acyclic-pathset-finiteness}
The count of paths for any acyclic directed graph is finite.
That is, if \V{adg} is an acyclic direct graph, then
\[
     \size{\Vmyfn{path}{adg}} \in \naturals.
     \quad \thEnd
\]
\end{theorem}

\begin{proof}
By the definition of a directed graph \Dfref{directed-graph},
it has a finite set of vertices.
By the definition of a path \Dfref{path},
the values of a path in a directed graph are vertices.
The length of a path has
a fixed maximum which depends on \V{adg} \Thref{acyclic-path-maximum}.

From the foregoing, we see that a path in \V{adg} is a finite sequence
of values chosen from a finite set.
Therefore, the number of possible paths
is a finite constant.
\myqed
\end{proof}

\chapter{Trees}
\label{ch:trees}
%
\todo[prepend, caption={``Trees'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}%
%
\myepi[.8\textwidth]{%
Take but degree away, untune that string,
\linebreak And hark what discord follows! Each thing melts
\linebreak In mere oppugnancy: the bounded waters
\linebreak Should lift their bosoms higher than the shores,
\linebreak And make a sop of all this solid globe;
\linebreak Strength should be lord of imbecility,
\linebreak And the rude son should strike his father dead;
\linebreak Force should be right; or, rather, right and wrongâ€”
\linebreak Between whose endless jar justice residesâ€”
\linebreak Should lose their names, and so should justice too.
\linebreak Then everything includes itself in power,
\linebreak Power into will, will into appetite;
\linebreak And appetite, an universal wolf,
\linebreak So doubly seconded with will and power,
\linebreak Must make perforce an universal prey,
\linebreak And last eat up himself.
}{Shakespeare, {\em Troilus and Cressida}, Act I, Scene 3}

What the Shakespeare author%
\index{recce-general}{Shakespeare}
calls ``degree'' we
would call hierarchy today.
An important means of representing hierarchy in mathematical contexts
is graph theory, specifically trees.

While the effect in our case
is not as dire as Shakespeare%
\index{recce-general}{Shakespeare}
suggests it could be,
an inability to establish a hierarchy among our sources
presents us with a difficulty.
For most of our preceding disciplines, we can cite a single clear
authority on whom, usually with changes and updates, we base
our presentation.
For trees, we did not identify a single authority
to follow on details of terminology and derivation.\footnote{%
    Again, in matters of graph theory,
    our approach is perhap most heavily influenced by \cite{HU1979}.}
Trees will be extremely important for us,
so in this chapter
we make our definitions explicit and
justify the basic results that we will use.

\begin{definition}[Tree]
\label{def:tree}
A \dfn{tree} is a directed graph,
$\V{G} = \tuple{\V{V}, \V{E}}$
with the following three properties:

Exactly one vertex of \V{G}, called the \V{root},
has an in-degree of zero.
In other words, the root of \V{G}
is a distinguished vertex
which has no parents.
That is,
\begin{equation}
\label{eq:def-tree-root}
\begin{gathered}
    \existUniqQOp\; \V{root} \in \V{V} : \forall\; \V{parent} \in \V{V} :
        \nexists\; \tuple{\V{parent}, \V{root}} \in \V{E}.
\end{gathered}
\end{equation}

There is a path from the root to every vertex of
the tree.
That is, where \V{root} is the root of \V{G},
\begin{equation}
\label{eq:def-tree-rootpath}
\begin{gathered}
    \forall\; \V{nd} \in \V{V} :
        \exists\; \V{rootpath} \in \Vmyfn{paths}{G} :
    \\ \Velement{rootpath}{0} = \V{root}
\land \Velement{rootpath}{\Vlastix{rootpath}} = \V{nd}.
\end{gathered}
\end{equation}
Note that there is a trivial path from the root to itself.

Every vertex, other than the root, has an in-degree of
one.
In other words, the parent of a node,
if it exists, is unique.
That is,
\begin{equation}
\label{eq:def-tree-hierarchy}
\begin{gathered}
    \forall\; \V{parent1} \in \V{V} :
        \forall\; \V{parent2} \in \V{V} :
        \forall\; \V{nd} \in \V{V} :
    \\ \tuple{ \V{parent1}, \V{nd} } \in \V{E}
        \land \tuple{ \V{parent2}, \V{nd} } \in \V{E}
    \\ \implies \V{parent1} = \V{parent2}.
\end{gathered}
\end{equation}
When a directed graph is a tree, its vertices are more
often called \dfn{nodes}.
A \dfn{leaf} of a tree is a node with an out-degree of zero ---
in other words, a leaf is a node with no children.
As a matter of convention,
the children of every vertex of a tree
are ordered left-to-right.
\dfEnd
\end{definition}


From here until the end of this chapter
the \dfn{context tree} $\V{G} = \tuple{\V{V}, \V{E}}$
will be in scope.

\begin{definition}[Root path]
\label{def:root-path}
A \dfn{root path} is a path that contains the root of its tree.
    \dfEnd
\end{definition}

\begin{theorem}
\label{th:root-path}
In every root path
a root node can only occur at the first element
of the path.
That is,
\[
\begin{gathered}
\forall\; \V{p} \in \Vmyfn{paths}{G} :
\\ \forall\; \V{root} \in
\set{ \V{to} \in \V{V} :
    \nexists\; \V{from} \in \V{V} :
    \tuple{\V{from}, \V{to}} \in \V{E}
} :
\\
    \left(
        \exists\; \V{ix} \le \Vlastix{p} : \VVelement{p}{ix} = \V{root}
    \right)
\implies \V{ix} = 0.  \quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
By the definition of a path \Dfref{path}
every node of a path except the first is
the to-node of an edge,
and therefore has in-degree at least one.
By the definition of the root of a tree \Dfref{tree}
the root has an in-degree of zero.
Therefore, wherever a path contains the root node,
it must be the first element of the path.
\myqed
\end{proof}

\begin{definition}[Leaf path]
\label{def:leaf-path}
A \dfn{leaf path} is a path which contains a leaf of its tree.
\dfEnd
\end{definition}

\begin{theorem}
\label{th:leaf-path}
In every leaf path,
a leaf can only occur as the last element.
That is,
\[
\begin{gathered}
\forall\; \V{p} \in \Vmyfn{paths}{G} :
\\ \forall\; \V{leaf} \in
\set{ \V{from} \in \V{V} :
    \nexists\; \V{to} \in \V{V} :
    \tuple{\V{from}, \V{to}} \in \V{E}
} :
\\
    \left(
        \exists\; \V{ix} \le \Vlastix{p} : \VVelement{p}{ix} = \V{leaf}
    \right)
\implies \V{ix} = \Vlastix{p}.  \quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
By the definition of a path \Dfref{path}
every node of a path except the last is
the from-node of an edge,
and therefore has out-degree at least one.
By the definition of the leaf of a tree \Dfref{tree}
a leaf has an out-degree of zero.
Therefore, if a path contains a leaf node,
that leaf node must be the last element of the path.
\myqed
\end{proof}

\begin{definition}[Prefix Path]
\label{def:prefix-path}
A path \V{pref} is a \dfn{prefix path},
or \dfn{prefix},
with respect to the vertex
which is its tail.
That is,
the path
$\Rvarseq{pref}{V} \in \Vmyfn{Upaths}{G}$ is a prefix of $\V{nd} \in \V{V}$
iff $\Velement{pref}{\Vlastix{pref}} = \V{nd}$.
\dfEnd
\end{definition}

\begin{theorem}[Prefix Uniqueness]
\label{th:prefix-uniqueness-from-tail}
In a tree,
two prefixes of the same node
with the same length
are identical if they are equally long.
That is,
\begin{equation}
\label{eq:th-prefix-uniqueness-050}
\begin{gathered}
    \forall\;\V{pref1} \in \Vmyfn{paths}{G} :
    \forall\;\V{pref2} \in \Vmyfn{paths}{G} :
\\ \Velement{pref1}{\Vlastix{pref1}} = \Velement{pref2}{\Vlastix{pref2}}
    \land \Vlastix{pref1} = \Vlastix{pref2}
\\ \implies \V{pref1} = \V{pref2}.  \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is by minimal counter-example.
\begin{equation}
\label{eq:th-prefix-uniqueness-105}
\begin{gathered}
\Vmyfn{INST}{\Vnat{ex}} \ldefined
\\ \forall\; \Rvarseq{pref1}{V} \in \Vmyfn{Upaths}{G} :
    \forall\; \Rvarseq{pref2}{V} \in \Vmyfn{Upaths}{G} :
\\ \Velement{pref1}{\Vlastix{pref1}} = \Velement{pref2}{\Vlastix{pref2}}
        \land \Vlastix{pref1} = \Vlastix{pref2} = \V{ex}
\\ \implies \V{pref1} = \V{pref2}
    \\ \myparbox{\cuz{}
        Macro for EI for adaptation of \Eref{th-prefix-uniqueness-050}
         setting $\Vnat{ex}=\Vlastix{pref2}$.}
\end{gathered}
\end{equation}
We assume there is a minimal \Vnat{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:th-prefix-uniqueness-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:th-prefix-uniqueness-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:th-prefix-uniqueness-130}{%
$\Velement{prefZ1}{\Vlastix{pref1}} = \Velement{prefZ2}{\Vlastix{pref2}}$
\linebreak $\land\; \Vlastix{prefZ1} = \Vlastix{prefZ2} = 0$
    \bcuz{} AF implication.
    New free \Rvarseq{prefZ1}{V} and \Rvarseq{prefZ2}{V}.
}
\mypareq{eq:th-prefix-uniqueness-140}{%
    $\V{prefZ1} = \V{prefZ2}$
    \cuz{} \Eref{th-prefix-uniqueness-130}.}
\mypareq{eq:th-prefix-uniqueness-150}{%
    $\myfn{INST}{0}$
    \cuz{} Implication from
        \Eref{th-prefix-uniqueness-130} to
        \Eref{th-prefix-uniqueness-140};
        Macrofication using
        \Eref{th-prefix-uniqueness-105}.}
\mypareq{eq:th-prefix-uniqueness-160}{%
    $\V{bad} > 0$
    \cuz{} \Eref{th-prefix-uniqueness-110},
        \Eref{th-prefix-uniqueness-150}.
}
\begin{equation}
\label{eq:th-prefix-uniqueness-170}
\begin{gathered}
\exists\; \Rvarseq{pref1}{V} \in \Vmyfn{Upaths}{G} :
    \exists\; \Rvarseq{pref2}{V} \in \Vmyfn{Upaths}{G} :
\\ \Velement{pref1}{\Vlastix{pref1}} = \Velement{pref2}{\Vlastix{pref2}}
\\ \land\; \Vlastix{pref1} = \Vlastix{pref2} = \V{bad}
\\ \land\; \V{pref1} \ne \V{pref2}
\\ \myparbox{\cuz{} Expansion of \Eref{th-prefix-uniqueness-110}
        using \Eref{th-prefix-uniqueness-105}.}
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:th-prefix-uniqueness-190}
\Rvarseq{pref1}{V} \in \Vmyfn{Upaths}{G} \land
    \Rvarseq{pref2}{V} \in \Vmyfn{Upaths}{G}
\\ \label{eq:th-prefix-uniqueness-200}
    \land\; \Velement{pref1}{\Vlastix{pref1}} = \Velement{pref2}{\Vlastix{pref2}}
\\ \label{eq:th-prefix-uniqueness-210}
    \land\; \Vlastix{pref1} = \Vlastix{pref2} = \Vnat{bad}
\\ \label{eq:th-prefix-uniqueness-220}
    \land\; \V{pref1} \ne \V{pref2}
\\ \nonumber \myparbox{\cuz{}
    EI of \Eref{th-prefix-uniqueness-170}
    with new free \Rvarseq{pref1}{V} for bound \Rvarseq{pref1}{V} and
    new free \Rvarseq{pref2}{V} for bound \Rvarseq{pref2}{V}.}
\end{gather}
\mypareq{eq:th-prefix-uniqueness-230}{%
    $\Rvarseq{pref1}{V} = \seq{ \V{pref1First}, \Velement{suf1}{0} \ldots \Velement{suf1}{\Vlastix{suf1}} }$
    \bcuz{} \Eref{th-prefix-uniqueness-160}, \Eref{th-prefix-uniqueness-210}.
    New \Rvarseq{suf1}{V}, new \Rvar{pref1First}{V}.}
\mypareq{eq:th-prefix-uniqueness-240}{%
    $\Rvarseq{pref2}{V} = \seq{ \V{pref2First}, \Velement{suf2}{0} \ldots \Velement{suf2}{\Vlastix{suf2}} }$
    \bcuz{} \Eref{th-prefix-uniqueness-160}, \Eref{th-prefix-uniqueness-210}.
    New \Rvarseq{suf2}{V}, new \Rvar{pref2First}{V}.}
\mypareq{eq:th-prefix-uniqueness-250}{%
    $\Vlastix{suf1} = \Vlastix{suf2} = \Vdecr{bad}$ \cuz{} \Eref{th-prefix-uniqueness-210},
        \Eref{th-prefix-uniqueness-230}, \Eref{th-prefix-uniqueness-240}.}
\mypareq{eq:th-prefix-uniqueness-260}{%
    $\Rvarseq{suf1}{V} \in \Vmyfn{Upaths}{G}$
        \bcuz{} \longThref{Subpath}{subpath},
            \Eref{th-prefix-uniqueness-190}, \Eref{th-prefix-uniqueness-230}.}
\mypareq{eq:th-prefix-uniqueness-270}{%
    $\Rvarseq{suf2}{V} \in \Vmyfn{Upaths}{G}$
        \cuz{} \Thref{subpath},
            \Eref{th-prefix-uniqueness-190}, \Eref{th-prefix-uniqueness-240}.}
\mypareq{eq:th-prefix-uniqueness-280}{%
    $\Velement{suf1}{\Vlastix{suf2}} = \Velement{suf2}{\Vlastix{suf2}}$ \cuz{} \Eref{th-prefix-uniqueness-200},
        \Eref{th-prefix-uniqueness-230}, \Eref{th-prefix-uniqueness-240}.}
\mypareq{eq:th-prefix-uniqueness-290}{%
    $\myfn{INST}{\Vdecr{bad}}$ \cuz{} \Eref{th-prefix-uniqueness-120},
        \Eref{th-prefix-uniqueness-160}.}
\mypareq{eq:th-prefix-uniqueness-300}{%
$\Velement{suf1}{\Vlastix{suf1}} = \Velement{suf2}{\Vlastix{suf2}}
        \land \Vlastix{suf1} = \Vlastix{suf2} = \Vdecr{bad}$
\linebreak $\implies\; \V{suf1} = \V{suf2}$
\bcuz{} Expansion of
        \Eref{th-prefix-uniqueness-290}
        using \Eref{th-prefix-uniqueness-105};
        \Eref{th-prefix-uniqueness-260};
        \Eref{th-prefix-uniqueness-270};
        and EI of \V{pref1} as \V{suf1} and \V{pref2} as \V{suf2}.}
\mypareq{eq:th-prefix-uniqueness-310}{%
    $\Rvarseq{suf1}{V} = \Rvarseq{suf2}{V}$
    \bcuz{} \Eref{th-prefix-uniqueness-250},
        \Eref{th-prefix-uniqueness-280},
        \Eref{th-prefix-uniqueness-300}.}
\mypareq{eq:th-prefix-uniqueness-320}{%
    $\Velement{suf1}{0} = \Velement{suf2}{0}$
        \cuz{} \Eref{th-prefix-uniqueness-310}.}
\mypareq{eq:th-prefix-uniqueness-330}{%
    $\Rvar{pref1First}{V} \in \myfn{parents}{\Velement{suf1}{0}}$
        \cuz{} \Eref{th-prefix-uniqueness-230}.}
\mypareq{eq:th-prefix-uniqueness-340}{%
    $\Rvar{pref2First}{V} \in \myfn{parents}{\Velement{suf2}{0}}$
        \cuz{} \Eref{th-prefix-uniqueness-240}.}
\mypareq{eq:th-prefix-uniqueness-350}{%
    $\Rvar{pref2First}{V} \in \myfn{parents}{\Velement{suf1}{0}}$
        \cuz{} \Eref{th-prefix-uniqueness-320},
            \Eref{th-prefix-uniqueness-340}.}
And since, by the definition of a tree, there is at most one parent for every
node,
\mypareq{eq:th-prefix-uniqueness-360}{%
    $\Rvar{pref1First}{V} = \Rvar{pref2First}{V}$ \cuz{} \Dfref{tree},
        \Eref{th-prefix-uniqueness-350}.}
\mypareq{eq:th-prefix-uniqueness-370}{%
     $\Rvarseq{pref1}{V} = \Rvarseq{pref2}{V}$ \cuz{} \Eref{th-prefix-uniqueness-230},
        \Eref{th-prefix-uniqueness-240},
        \Eref{th-prefix-uniqueness-310},
        \Eref{th-prefix-uniqueness-360}.}
Equation \Eref{th-prefix-uniqueness-370} is contrary to
\Eref{th-prefix-uniqueness-220}, which shows the reductio
from \Eref{th-prefix-uniqueness-110}--%
    \Eref{th-prefix-uniqueness-120} to
    \Eref{th-prefix-uniqueness-370}.
From the reductio we know that there is no counter-example to the
lemma, which shows us the lemma.
\myqed
\end{proof}

\begin{theorem}[Rootpath Non-duplication]
\label{th:rootpath-non-duplication}
Every node in a tree is the tail at most one root path.
That is,
if \V{root} is the root of \V{G},
\mypareq{eq:th-rootpath-non-duplication-010}{%
    $\nexists\; \V{from} \in \V{V} :
    \tuple{\V{from}, \V{root}} \in \V{E}$
    \bcuz{} \Eref{def-tree-root} in \longDfref{Tree}{tree};}
\V{nd} is an arbitrary node of \V{G},
\mypareq{eq:th-rootpath-non-duplication-020}{%
    $\V{nd} \in \V{V}$;
}
and \V{path1} and \V{path2} are two root paths to \V{nd}
in \V{G},
\begin{gather}
\label{eq:th-rootpath-non-duplication-030}
    \V{path1} \in \Vmyfn{paths}{G}
\\ \label{eq:th-rootpath-non-duplication-040}
    \land\; \V{path2} \in \Vmyfn{paths}{G}
\\ \label{eq:th-rootpath-non-duplication-050}
    \land\; \Velement{path1}{0} = \V{root} =\Velement{path2}{0}
\\ \label{eq:th-rootpath-non-duplication-060}
    \land\; \left(\begin{gathered}
        \VlastElement{path1} = \V{nd}
        \\ = \VlastElement{path2}
    \end{gathered}\right)
    \\ \nonumber
\myparbox{\cuz{} \longThref{Root Path Theorem}{root-path};}
\end{gather}
then
\V{path1} and \V{path2} are equivalent:
\mypareq{eq:th-rootpath-non-duplication-070}{%
    $\V{path1} = \V{path2}$.
    \thEnd
}
\end{theorem}

\begin{proof}
The proof is by reductio.
For the reductio,
we assume that \V{path1} and \V{path2}
are distinct:
\mypareq{eq:th-rootpath-non-duplication-100}{%
    $\V{path1} \ne \V{path2}$
    \cuz{} AF reductio.}
One of $\V{path1}$ and $\V{path2}$ must be shorter
than the other.
We arbitrarily choose \V{path1} as the shortest,
introducing two convenience variables as mnemonics.
Reversing the choice would produce a symmetric proof,
so this choice is without loss of generality.
\begin{gather}
\label{eq:th-rootpath-non-duplication-158}
\V{short} = \V{path1}
   \land\; \V{long} = \V{path2}
\\ \label{eq:th-rootpath-non-duplication-160}
   \land\; \Vsize{short} < \Vsize{long}
\\ \nonumber
\myparbox{\cuz{}
    \Eref{th-rootpath-non-duplication-100}, WLOG;
    CV $\V{short} \in \Vmyfn{paths}{G}$;
    \linebreak[1]
    CV $\V{long} \in \Vmyfn{paths}{G}$.}
\end{gather}
\mypareq{eq:th-rootpath-non-duplication-170}{%
    $\V{longSuffix} = \Velement{long}{%
            \left( \subtract{\Vsize{long}}{\Vsize{short}} \right)
            \ldots\;\Vlastix{long}
     }$
    \bcuz{} \longThref{Subpath}{subpath},
         \Eref{th-rootpath-non-duplication-160}; CV \Rvarseq{longSuffix}{V}.}
The first element of \V{longSuffix} is at
location \subtract{\Vsize{long}}{\Vsize{short}} in \V{longpath},
and the last element of \V{longSuffix} is at \Vlastix{long} in \V{longpath},
so that
\begin{equation}
\label{eq:th-rootpath-non-duplication-180}
\begin{gathered}
    \Vsize{longSuffix}
    \\ = \subtract{\Vlastix{long}}
        { \decr{ \left(
                {\subtract{\Vsize{long}}{\Vsize{short}}}
            \right) } }
    \\ = \subtract{\Vlastix{long}}
            { \left(
                {\subtract{\Vlastix{long}}{\Vsize{short}}}
            \right) }
    \\ = \Vsize{short}
    \\ \myparbox{\cuz{} \Eref{th-rootpath-non-duplication-170}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-rootpath-non-duplication-190}
\begin{gathered}
    \VlastElement{longSuffix} = \VlastElement{long}
    \\ = \VlastElement{short}
    \\ \myparbox{\cuz{} \Eref{th-rootpath-non-duplication-060},
        \Eref{th-rootpath-non-duplication-158},
        \Eref{th-rootpath-non-duplication-170}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-rootpath-non-duplication-200}{%
    $\V{longSuffix} = \V{short}$
    \bcuz{} \longThref{Prefix Uniqueness}{prefix-uniqueness-from-tail},
        \Eref{th-rootpath-non-duplication-180},
        \Eref{th-rootpath-non-duplication-190}.}
\mypareq{eq:th-rootpath-non-duplication-210}{%
    $\Velement{longSuffix}{0} =
        \Velement{long}
             {\subtract{\Vsize{long}}{\Vsize{short}}}$
    \cuz{} \Eref{th-rootpath-non-duplication-170}.}
\mypareq{eq:th-rootpath-non-duplication-220}{%
    $\Velement{longSuffix}{0} = \Velement{short}{0} = \V{root}$
    \bcuz{} \Eref{th-rootpath-non-duplication-050},
        \Eref{th-rootpath-non-duplication-158},
        \Eref{th-rootpath-non-duplication-200}.}
\mypareq{eq:th-rootpath-non-duplication-230}{%
    $\Velement{long}
             {\subtract{\Vsize{long}}{\Vsize{short}}}
             = \V{root}$
    \bcuz{} \Eref{th-rootpath-non-duplication-210},
        \Eref{th-rootpath-non-duplication-220}.}
\mypareq{eq:th-rootpath-non-duplication-240}{%
    $\subtract{\Vsize{long}}{\Vsize{short}} \ge 1$
        \cuz{} \Eref{th-rootpath-non-duplication-160}.}
\mypareq{eq:th-rootpath-non-duplication-250}{%
    $\Velement{long}{%
                 \begin{gathered}
                 \decr{
                     \left( \subtract{\Vsize{long}}{\Vsize{short}} \right)
                 },
                 \\ \left( \subtract{\Vsize{long}}{\Vsize{short}} \right)
                 \end{gathered}
             }
             \in \V{E}$
    \bcuz{} \longDfref{Path}{path},
        \Eref{th-rootpath-non-duplication-040},
        \Eref{th-rootpath-non-duplication-158},
        \Eref{th-rootpath-non-duplication-160},
        \Eref{th-rootpath-non-duplication-240}.}
\mypareq{eq:th-rootpath-non-duplication-260}{%
    $\Velement{long}{%
                 \begin{gathered}
                 \decr{
                     \left( \subtract{\Vsize{long}}{\Vsize{short}} \right)
                 },
                 \\ \V{root}
                 \end{gathered}
             }
             \in \V{E}$
    \bcuz{}
        \Eref{th-rootpath-non-duplication-230},
        \Eref{th-rootpath-non-duplication-250}.}
Equation \Eref{th-rootpath-non-duplication-260} is contrary to
\Eref{th-rootpath-non-duplication-010}, which shows the reductio
and the theorem.
\myqed
\end{proof}

\begin{theorem}[Rootpath uniqueness]
\label{th:rootpath-uniqueness}
Every node in a tree has exactly one root path.
\thEnd
\end{theorem}

\begin{proof}
Let $\V{nd} \in \V{V}$ be an arbitrary node in \V{G}.
From the definition of a tree \Dfref{tree}
we know that there is at least one root path to \V{nd}.
From the Rootpath Non-duplication Theorem
\Thref{rootpath-non-duplication}, we know there
is at most one.
The theorem follows from these two facts.
\myqed
\end{proof}

\begin{theorem}[Tree acyclicity]
\label{th:tree-acyclicity}
A tree is an acyclic directed graph.
\thEnd
\end{theorem}

\begin{proof}
That a tree is a directed graph follows immediately from
its definition \Dfref{tree}.
The proof that a tree is acyclic
is by reductio, assuming that a tree contains a cycle
to show a contradiction.

Let
$\Rvarseq{cycle}{V} = \seq{\Rvar{dup}{V}\ldots\Rvar{dup}{V}}$
be a cycle in \V{G}.
\begin{gather}
\label{eq:th-tree-acyclicity-100}
    \Velement{cycle}{0} = \V{dup}
\\ \label{eq:th-tree-acyclicity-110}
    \land\; \Velement{cycle}{\Vlastix{cycle}} = \V{dup}
\\ \label{eq:th-tree-acyclicity-120}
    \land\; \Vlastix{cycle} > 0
\\ \nonumber
    \myparbox{\cuz{} AF reductio.
        \longDfref{Cycle}{cycle}.
        \linebreak
        New free $\V{cycle} \in \V{V} \tfnMap \naturals$.
        New free $\V{dup} \in \V{V}$.}
\end{gather}
Let \V{root} be the root of \V{G}:
\mypareq{eq:th-tree-acyclicity-130}{%
    $\forall\; \V{from} \in \V{V} :
    \nexists\; \tuple{\V{from}, \V{root}} \in \V{E}$
    \bcuz{} \longDfref{Tree}{tree}.
    New $\V{root} \in \V{V}$ for convenience.
}
\mypareq{eq:th-tree-acyclicity-150}{%
    $\tuple{ \Velement{cycle}{\decr{\Vlastix{p}}}, \V{dup}
        } \in \V{E}$
        \bcuz{} \longThref{Edge of To-Node}{edge-to-node},
            \Eref{th-tree-acyclicity-110},
            \Eref{th-tree-acyclicity-120}.}
\mypareq{eq:th-tree-acyclicity-160}{%
    $\tuple{ \Velement{cycle}{\decr{\Vlastix{p}}}, \Velement{cycle}{0}
        } \in \V{E}$
        \bcuz{} \Eref{th-tree-acyclicity-100},
            \Eref{th-tree-acyclicity-150}.}
From \Eref{th-tree-acyclicity-160},
two useful facts about the values of \V{cycle} emerge.
First, all of the values of \V{cycle} are the to-node of an edge
whose from-node is in \V{cycle}.
\begin{equation}
\label{eq:th-tree-acyclicity-170}
\begin{gathered}
    \forall\; \V{nd} \in \Ran{\V{cycle}} : \exists\; \V{from} :
    \\ \tuple{ \V{from}, \VVelement{cycle}{nd} } \in \V{E}
        \land \V{from} \in \Ran{\V{cycle}}
    \\ \myparbox{\cuz{} \Eref{th-tree-acyclicity-100},
            \Eref{th-tree-acyclicity-160}.}
\end{gathered}
\end{equation}
A second useful fact about the values of \V{cycle}
is that none of them are \V{root}.
\mypareq{eq:th-tree-acyclicity-180}{%
    $\forall\; \V{nd} \in \Ran{\V{cycle}} :
        \V{nd} \ne \V{root}$
        \bcuz{} \Eref{th-tree-acyclicity-130},
            \Eref{th-tree-acyclicity-160}.}
Recalling from the definition of a tree, that every node has at
most one parent, we see that
\begin{equation}
\label{eq:th-tree-acyclicity-190}
\begin{gathered}
    \forall\; \V{nd} \in \Ran{\V{cycle}} :
    \forall\; \V{parent} \in \V{V} :
    \\ \tuple{ \V{parent}, \V{to} } \in \V{E} \implies
        \V{parent} \in \Ran{\V{cycle}}
    \\ \myparbox{\cuz{}
        \Eref{def-tree-hierarchy} in \Dfref{tree},
        \Eref{th-tree-acyclicity-170}.}
\end{gathered}
\end{equation}

We now look at the rootpath to \V{dup}.
\begin{gather}
\label{eq:th-tree-acyclicity-200}
    \Velement{rootpath}{0} = \V{root}
\\ \label{eq:th-tree-acyclicity-210}
    \land\; \VlastElement{rootpath} = \V{dup}
\\ \nonumber
    \myparbox{\cuz{} \Eref{def-tree-rootpath} in \Dfref{tree},
    \Eref{th-tree-acyclicity-100}.
    New free $\V{rootpath} \in \V{V} \tfnMap \naturals$.}
\end{gather}
Next we consider the first occurrence of a value of \V{cycle} in
\V{rootpath}.
Since $\V{dup} \in \Ran{\V{rootpath}}$
and \V{dup} is a node in \V{cycle},
we know there is
such a first occurrence.
We leave it open whether
this first occurrence is an occurrence of \V{dup} or
of some other value of \V{cycle}.
\begin{gather}
\label{eq:th-tree-acyclicity-220}
    \VVelement{rootpath}{firstCycIx} \in \Ran{\V{cycle}}
\\ \label{eq:th-tree-acyclicity-230}
    \land\; \forall\; \Vnat{i} < \V{firstCycIx} :
        \VVelement{rootpath}{i} \notin \Ran{\V{cycle}}
\\ \nonumber \myparbox{\cuz{} New free
    $\Vnat{firstCycIx} \in \Vsize{rootpath}$ for
    convenience.}
\end{gather}
Since the first occurrence is in \V{cycle} it is not
root and therefore not at the head of the rootpath:
\mypareq{eq:th-tree-acyclicity-240}{%
    $\Vnat{firstCycIx} > 0$
    \cuz{} \Eref{th-tree-acyclicity-180},
        \Eref{th-tree-acyclicity-200},
        \Eref{th-tree-acyclicity-220}.}
\mypareq{eq:th-tree-acyclicity-250}{%
    $\tuple{ \Velement{rootpath}{\Vdecr{firstCycIx}},
        \VVelement{rootpath}{firstCycIx} } \in \V{E}$
    \bcuz{} \longThref{Edge of To-Node}{edge-to-node},
        \Eref{th-tree-acyclicity-220},
        \Eref{th-tree-acyclicity-240}.}
By our definition, \Vnat{firstCycIx} is the location
of the first occurrence of a value of \V{cycle} in \V{rootpath},
so that
\mypareq{eq:th-tree-acyclicity-260}{%
    $\Velement{rootpath}{\Vdecr{firstCycIx}} \notin \Ran{\V{cycle}}$
    \bcuz{} \Eref{th-tree-acyclicity-230},
        \Eref{th-tree-acyclicity-250}.}
But we also established that every from-node in an edge to a
node of a cycle is also in the cycle, so that
\mypareq{eq:th-tree-acyclicity-270}{%
    $\Velement{rootpath}{\Vdecr{firstCycIx}} \in \Ran{\V{cycle}}$
    \bcuz{} \Eref{th-tree-acyclicity-170},
        \Eref{th-tree-acyclicity-250}.}
Equation \Eref{th-tree-acyclicity-260} is contrary to
\Eref{th-tree-acyclicity-270}, which shows the reductio
and this theorem.
\myqed
\end{proof}

\begin{theorem}[Tree path maximum]
\label{th:tree-path-maximum}
The length of a path in a tree is at most
the number of nodes in the tree.
That is,
\[
    \forall\; \V{path} \in \Vmyfn{path}{G} :
        \Vsize{path} \le \Vsize{V}. \quad \thEnd
\]
\end{theorem}

\begin{proof}
The proof is direct.
A tree is by definition a directed graph.
No path in any tree
contains a cycle \Thref{tree-acyclicity}.
Hence \V{G} is an acyclic directed graph.
By the Acyclic Path Maximum theorem
\Thref{acyclic-path-maximum},
the length of any path
in an acyclic directed graph
is at most the node count of
that acyclic directed graph.
\Vsize{V} is the node count of \V{G}
and the theorem follows.
\myqed
\end{proof}

\begin{theorem}[Tree Non-empty]
\label{th:tree-non-empty}
Every tree has at least one node.
\thEnd
\end{theorem}

\begin{proof}
By the definition of a tree \Dfref{tree},
it must have a root node.
\myqed
\end{proof}

\begin{theorem}[Tree path set finiteness]
\label{th:tree-pathset-finiteness}
The count of paths for any tree is finite.
That is,
\[
     \size{\Vmyfn{path}{G}} \in \naturals.
     \quad \thEnd
\]
\end{theorem}

\begin{proof}
A tree is an acyclic directed graph \Thref{tree-acyclicity}.
This theorem therefore follows from the
Acyclic Pathset Finiteness Theorem
\Thref{acyclic-pathset-finiteness}.
\myqed
\end{proof}

\begin{theorem}[Leaf path]
\label{th:tree-leaf-path}
Every node in a tree is the start of at
least one leaf path.
\thEnd
\end{theorem}

\begin{proof}
Let \V{leaves} be the set of leaves in \V{G}.
We select an arbitrary node of \V{G}, call it \V{nd} and
notice that the trivial path whose only element is \V{nd}
is one of the paths of \V{G}.
\mypareq{eq:th-tree-leaf-path-100}{%
     $\seq{\V{nd}} \in \Vmyfn{paths}{G}$
     \cuz{} \longDfref{Path}{path}.
     New arbitrary $\V{nd} \in \V{V}$.
}
\mypareq{eq:th-tree-leaf-path-110}{%
    $\V{pathset} \defined \set{ \V{p} \in \Vmyfn{paths}{G} : \Velement{p}{0} = \V{nd} }$
    \bcuz{} New \V{pathset} for convenience.
}
\mypareq{eq:th-tree-leaf-path-115}{%
    $\Vsize{pathset} \in \naturals$
    \bcuz{} \longThref{Tree path set finiteness}{tree-pathset-finiteness},
        \Eref{th-tree-leaf-path-110}.
}
\mypareq{eq:th-tree-leaf-path-120}{%
    $\V{pathset} \neq \emptyset$
    \cuz{} \Eref{th-tree-leaf-path-100}, \Eref{th-tree-leaf-path-110}.
}
Let $\prec$
be the strict partial order
\mypareq{eq:th-tree-leaf-path-125}{%
    $\V{a} \prec \V{b} \ldefined \set{ \tuple{\V{a},\V{b}} \in \V{pathset} \times \V{pathset} : \Vsize{a} < \Vsize{b}}$
    \bcuz{} New $\prec$ for convenience.
}
Because \V{pathset} is non-empty and finite,
there is at least one maximal element in it:
\mypareq{eq:th-tree-leaf-path-130}{%
    $\exists\; \V{max} \in \V{pathset} : \forall\; \V{a} \in \V{pathset} : \neg ( \V{max} \prec \V{a})$
    \bcuz{} \Eref{th-tree-leaf-path-115}, \Eref{th-tree-leaf-path-120}.
}
\begin{gather}
\label{eq:th-tree-leaf-path-150}
    \V{max1} \in \V{pathset}
\\ \label{eq:th-tree-leaf-path-160}
    \forall\; \V{a} \in \V{pathset} : \neg ( \V{max1} \prec \V{a})
\\ \nonumber
    \myparbox{\cuz{}
       EI of bound \V{max} in \Eref{th-tree-leaf-path-130} with
       new \V{max1}.}
\end{gather}
Using a reductio,
we now show that
\V{max1} is a leaf path.
Assume, for the reductio,
that \V{max1} is not a leaf path.
\mypareq{eq:th-tree-leaf-path-190}{%
    $\Velement{max1}{\Vlastix{max1}} \notin \V{leaves}$
    \bcuz{} AF reductio, \longDfref{Leaf path}{tree}.}
\mypareq{eq:th-tree-leaf-path-200}{%
    $\exists\; \V{oneMore} \in \V{V} : \tuple{ \Velement{max1}{\Vlastix{max1}}, \V{oneMore} } \in \V{E}$
    \bcuz{} \longDfref{Leaf}{tree}, \Eref{th-tree-leaf-path-190}.}
\mypareq{eq:th-tree-leaf-path-205}{%
    $\tuple{ \Velement{max1}{\Vlastix{max1}}, \V{oneMore} } \in \V{E}$
    \bcuz{} EI of bound \V{oneMore} in \Eref{th-tree-leaf-path-190}
    with new free \V{oneMore}.}
\mypareq{eq:th-tree-leaf-path-210}{%
    $\V{evenLonger} \ldefined \family{
            \Velement{max1}{0} \ldots \Velement{max1}{\Vlastix{max1}},
            \V{oneMore} }$
    \bcuz{} New \V{evenLonger} for convenience.}
\mypareq{eq:th-tree-leaf-path-215}{%
    $\V{evenLonger} \in \Vmyfn{paths}{G}$
    \bcuz{} \longDfref{Path}{path},
        \Eref{th-tree-leaf-path-205},
        \Eref{th-tree-leaf-path-210}.}
\mypareq{eq:th-tree-leaf-path-220}{%
    $\Velement{evenLonger}{0} = \Velement{max1}{0} = \V{nd}$
    \bcuz{} \Eref{th-tree-leaf-path-110},
        \Eref{th-tree-leaf-path-150}, \Eref{th-tree-leaf-path-210}.
}
\mypareq{eq:th-tree-leaf-path-230}{%
    $\V{evenLonger} \in \V{pathset}$
        \cuz{} \Eref{th-tree-leaf-path-215},
            \Eref{th-tree-leaf-path-220}.
}
\mypareq{eq:th-tree-leaf-path-240}{%
    $\V{max1} \prec \V{evenLonger}$
        \cuz{} \Eref{th-tree-leaf-path-125},
            \Eref{th-tree-leaf-path-210}.
}
\mypareq{eq:th-tree-leaf-path-250}{%
    $\neg\; ( \V{max1} \prec \V{evenLonger} )$
    \bcuz{} UI of bound \V{a} in \Eref{th-tree-leaf-path-160}
        with \V{evenLonger}.
}
Equation \Eref{th-tree-leaf-path-240} contradicts
\Eref{th-tree-leaf-path-250}, which shows the reductio
from \Eref{th-tree-leaf-path-190} to
\Eref{th-tree-leaf-path-250}.
From the reductio we conclude that \V{max1} is a leaf
path starting at \V{nd}.
Since our choices of \V{G} and \V{nd} were arbitrary,
we conclude that every node of a tree is the start of a leaf
path.
\myqed
\end{proof}

\begin{theorem}[Tree Leaf Existence]
\label{th:tree-leaf-existence}
Every tree has at least one leaf node.
\thEnd
\end{theorem}

\begin{proof}
It follows from \Thref{tree-non-empty}
and \Thref{tree-leaf-path},
that every tree contains at least one leaf path.
By definition, a leaf path contains a leaf node.
\myqed
\end{proof}

\chapter{Non-unary trees}
\label{chap:non-unary-trees}

For the purpose of this chapter we introduce
the opaque realm \realm{NOD}.
Intuitively, a value of realm \realm{NOD} is a ``node''.
The set of values of
\realm{NOD} must be non-empty, but is otherwise arbitrary.

% Macros for this chapter only
\newcommand{\nod}[1]{\ensuremath{#1_\realm{NOD}}}
\newcommand{\Vnod}[1]{\nod{\V{#1}}}
\newcommand{\Vnodset}[1]{\Rvarset{#1}{NOD}}
\newcommand{\Vnodseq}[1]{\Rvarseq{#1}{NOD}}

This chapter is a new context scope,
which contains
the \dfn{context tree}
\begin{equation}
\label{eq:non-unary-chap-context-tree}
\V{G} = \tuple{\Vnodset{V},\V{E}}.
\end{equation}
In this context scope,
$\Vnodset{leaves} \subseteq \V{V}$
will be the set of leaves in \V{G}.

\section{Tree ancestries}

\begin{ldef}[New Parents]
\label{ldef:new-parents}
Let the ``new parents'' function,
\[
    \deffn{NewParents}{\powerset{\Vnodset{V}}}{\powerset{\Vnodset{V}}},
\]
be the set of the direct parents of a node set that are not already in that node set,
so that
\begin{equation}
    \myfn{NewParents}{\Vnodset{nds}} = \set{
        \begin{gathered}
            \Vnod{parent} : \exists\; \Vnod{child} :
            \\ \V{child} \in \V{nds} \land \V{parent} \notin \V{nds}
            \\ \land\; \tuple{\V{parent}, \V{child}} \in \V{E}
        \end{gathered}
    }. \quad \dfEnd
\end{equation}
\end{ldef}

\begin{ldef}[Ancestry]
\label{ldef:ancestry}
Let the ``ancestry'' function,
\[
    \deffn{Anc}{%
        \left(
            \naturals \times \powerset{\Vnodset{V}}
        \right)}
        {\powerset{\V{V}}},
\]
be such that
\begin{equation}
    \myfn{Anc}{\Vnat{N},\Vnodset{base}} =
    \left(\begin{gathered}
    \V{N}=0 \condOpA \V{base} \condOpB
    \\ \myfn{Anc}{\Vdecr{N},\V{base}}
    \\ \cup\; \bigfn{NewParents}{\myfn{Anc}{\Vdecr{N},\V{base}}}
    \end{gathered}\right).
        \quad \dfEnd
\end{equation}
\end{ldef}

\begin{ldef}[Ancestry Predecessor]
\label{ldef:predecessor-anc}
An ancestry \Vnodset{pred}
is the ``predecessor''
of an ancestry \Vnodset{succ} iff \V{pred} is an ancestry with the same base
as \V{succ},
and the level of \V{pred} is the predecessor of the level of \V{succ}.
The predecessor of a level 0 ancestry is the empty set.
Symbolically,
\begin{equation}
    \label{eq:def-ancestry-predecessor}
\begin{gathered}
    \myfn{PredecessorAnc}{\Vnat{N},\Vnodset{base}} \ldefined
    \\ \cond{\V{N} = 0}{\emptyset}{\myfn{Anc}{\Vdecr{N},\V{base}}}.
    \quad \dfEnd
\end{gathered}
\end{equation}
\end{ldef}

\begin{lemma}[Predecessor Parent Disjointness]
\label{lem:predecessor-parent-disjointness}
An ancestry and its new parents are disjoint.
That is,
\[
    \myfn{Anc}{\Vnat{N},\Vnodset{base}} \cap
    \bigfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}} = \emptyset.
    \quad \thEnd
\]
\end{lemma}

\begin{proof}
The theorem follows immediately from the definition of \fname{NewParents}
\Ldref{new-parents}.
\myqed
\end{proof}

\begin{ldef}[Ancestry of Base]
\label{ldef:ancestry-of-base}
We say that \Vnodset{A} is an
\V{N}-level
\dfn[Ancestry-nodeset@Ancestry (of nodeset)!by level]{ancestry}
of base \Vnodset{b}
iff $\V{A}=\myfn{Anc}{\Vnat{N},\V{b}}$.
We say that \Vnodset{A} is an
\dfn[Ancestry-nodeset@Ancestry (of nodeset)]{ancestry}
of \Vnodset{nds}
iff \Vnodset{A} is an \V{N}-level ancestry of \Vnodset{nds} for
some \Vnat{N}.
\dfEnd
\end{ldef}

\begin{lemma}[Ancestry Monotonicity]
\label{lem:ancestry-monotonicity}
Every ancestry of \V{G} contains
all the ancestries of \V{G}
that have the same base
and are at the same or a lower level.
That is,
\mypareq{eq:lem-ancestry-monotonicity-050}{%
    $\forall\; \Vnat{N} : \forall\; \Vnat{i} \le \V{N} :
        \myfn{Anc}{\V{i},\Vnodset{base}} \subseteq \myfn{Anc}{\Vnat{N},\V{base}}$.
        \thEnd
}
\end{lemma}

\begin{proof}
The proof is by minimal counterexample.
Where
\begin{equation}
\label{eq:lem-ancestry-monotonicity-150}
\begin{gathered}
    \Vmyfn{INST}{\Vnat{ex}} \defined
    \\ \forall\; \Vnat{i} \le \V{ex} :
        \myfn{Anc}{\V{i},\Vnodset{base}} \subseteq \myfn{Anc}{\Vnat{ex},\V{base}}
    \\ \myparbox{\cuz{} UI of
        \Eref{lem-ancestry-monotonicity-050} with \Vnat{ex},}
\end{gathered}
\end{equation}
we assume there is a minimal \V{bad} such that
\Vmyfn{INST}{\Vnat{bad}} is false:
\begin{gather}
\label{eq:lem-ancestry-monotonicity-160}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-ancestry-monotonicity-165}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \myparbox{\cuz{} AF reductio. New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-ancestry-monotonicity-170}{%
        $\myfn{Anc}{0,\V{base}} \subseteq \myfn{Anc}{0,\V{base}}$
        \cuz{} Fact of set theory.}
\mypareq{eq:lem-ancestry-monotonicity-180}{%
    $\forall\; \Vnat{i} \le 0 :
        \myfn{Anc}{\V{i},\V{base}} \subseteq \myfn{Anc}{0,\V{base}}$
        \bcuz{} UG of \Eref{lem-ancestry-monotonicity-170}.}
\mypareq{eq:lem-ancestry-monotonicity-190}{%
        \myfn{INST}{0} \cuz{} \Eref{lem-ancestry-monotonicity-150},
            \Eref{lem-ancestry-monotonicity-180}.}
\mypareq{eq:lem-ancestry-monotonicity-200}{%
        $\Vnat{bad} > 0$ \cuz{}
            \Eref{lem-ancestry-monotonicity-160},
            \Eref{lem-ancestry-monotonicity-190}.}
\mypareq{eq:lem-ancestry-monotonicity-205}{%
    $\myfn{INST}{\Vdecr{bad}}$
    \cuz{} \Eref{lem-ancestry-monotonicity-165},
        \Eref{lem-ancestry-monotonicity-200}.
}
\mypareq{eq:lem-ancestry-monotonicity-210}{%
    $\forall\; \Vnat{i} \le \Vdecr{bad} :
        \myfn{Anc}{\V{i},\Vnodset{base}} \subseteq \myfn{Anc}{\Vdecr{bad},\V{base}}$
    \bcuz{} \Eref{lem-ancestry-monotonicity-150},
        \Eref{lem-ancestry-monotonicity-205}.
}
\mypareq{eq:lem-ancestry-monotonicity-220}{%
    $\myfn{Anc}{\Vdecr{bad},\Vnodset{base}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
    \bcuz{} \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-ancestry-monotonicity-200}.
}
\mypareq{eq:lem-ancestry-monotonicity-230}{%
    $\forall\; \Vnat{i} \le \Vdecr{bad} :
        \myfn{Anc}{\V{i},\Vnodset{base}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
    \bcuz{} \Eref{lem-ancestry-monotonicity-210},
        \Eref{lem-ancestry-monotonicity-220}.
}
\mypareq{eq:lem-ancestry-monotonicity-240}{%
        $\myfn{Anc}{\V{bad},\V{base}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
        \cuz{} Fact of set theory.}
\mypareq{eq:lem-ancestry-monotonicity-250}{%
    $\forall\; \Vnat{i} \le \V{bad} :
        \myfn{Anc}{\V{i},\Vnodset{base}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
    \bcuz{} \Eref{lem-ancestry-monotonicity-230},
        \Eref{lem-ancestry-monotonicity-240}.
}
\mypareq{eq:lem-ancestry-monotonicity-260}{%
    $\myfn{INST}{\V{bad}}$ \cuz{} \Eref{lem-ancestry-monotonicity-150},
        \Eref{lem-ancestry-monotonicity-250}.
}
Equation \Eref{lem-ancestry-monotonicity-160} is contrary to
\Eref{lem-ancestry-monotonicity-260}, which shows the reductio.
From the reductio, we conclude the absence of a minimal counter-example,
and the lemma.
\myqed
\end{proof}

\begin{lemma}[Ancestry Addition]
\label{lem:ancestry-addition}
Let a first ancestry be the base of a second ancestry.
The second ancestry is equal to a third ancestry that
has the same base as the first ancestry and whose
level is the sum of the levels of
the first and second ancestry.
That is,
\begin{equation}
\label{eq:lem-ancestry-addition-050}
    \myfn{Anc}{\V{x}, \myfn{Anc}{\V{y}, \V{base}}}
    = \myfn{Anc}{\V{x}+\V{y},\V{base}}.
    \quad \thEnd
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counter-example.
Let
\begin{equation}
\label{eq:lem-ancestry-addition-100}
\begin{gathered}
    \Vmyfn{INST}{ex} \defined
    \\ \myfn{Anc}{\V{ex}, \myfn{Anc}{\V{y}, \V{base}}}
        = \myfn{Anc}{\V{ex}+\V{y},\V{base}}
    \\ \text{\cuz{} \Eref{lem-ancestry-addition-050}
         with \Vnat{ex} for \Vnat{x}.
         New free \Vnodset{base};
         new free \Vnat{y}.}
\end{gathered}
\end{equation}
We assume there is a minimal counter-example to the theorem, \Vnat{bad}:
\begin{gather}
\label{eq:lem-ancestry-addition-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-ancestry-addition-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-ancestry-addition-170}{%
    $\forall\; \Vnodset{b} : \V{b} = \myfn{Anc}{0,\V{b}}$
        \cuz{} \longLdref{\fname{Anc}}{ancestry}.
}
\begin{equation}
\label{eq:lem-ancestry-addition-180}
\begin{gathered}
    \myfn{Anc}{0, \myfn{Anc}{\V{y}, \V{base}}}
        = \myfn{Anc}{\V{y},\V{base}}
        = \myfn{Anc}{0+\V{y},\V{base}}
    \\ \myparbox{\cuz{} \Eref{lem-ancestry-addition-170}.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-ancestry-addition-190}{%
    $\myfn{INST}{0}$ \cuz{}
        \Eref{lem-ancestry-addition-100},
        \Eref{lem-ancestry-addition-180}.
}
\mypareq{eq:lem-ancestry-addition-200}{%
    $\Vnat{bad} > 0$ \cuz{} \Eref{lem-ancestry-addition-110},
        \Eref{lem-ancestry-addition-190}.
}
\begin{equation}
\label{eq:lem-ancestry-addition-210}
\begin{gathered}
    \myfn{Anc}{\Vnat{bad}+\Vnat{y},\Vnodset{base}} =
    \\ \left( \begin{gathered}
        \bigfn{Anc}{(\Vdecr{bad})+\Vnat{y},\V{base}}
        \\ \cup\; \bigfn{NewParents}{\bigfn{Anc}{(\Vdecr{bad})+\Vnat{y},\V{base}}}
    \end{gathered} \right)
    \\ \because \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-ancestry-addition-200}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-ancestry-addition-220}
\begin{gathered}
    \bigfn{Anc}{\Vdecr{bad}, \myfn{Anc}{\V{y}, \V{base}}}
        = \bigfn{Anc}{(\Vdecr{bad})+\V{y},\V{base}}
    \\ \myparbox{\cuz{}
        expansion of \myfn{INST}{\Vdecr{bad}},
        \Eref{lem-ancestry-addition-100},
        \Eref{lem-ancestry-addition-120}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-ancestry-addition-230}
\begin{gathered}
    \myfn{Anc}{\Vnat{bad}+\Vnat{y},\Vnodset{base}} =
    \\ \left( \begin{gathered}
        \bigfn{Anc}{\Vdecr{bad}, \myfn{Anc}{\V{y}, \V{base}}}
        \\ \cup\; \bigfn{NewParents}{\bigfn{Anc}{\Vdecr{bad}, \myfn{Anc}{\V{y}, \V{base}}}}
    \end{gathered} \right)
    \\ \because \Eref{lem-ancestry-addition-210},
        \Eref{lem-ancestry-addition-220}.
\end{gathered}
\end{equation}
\mypareq{eq:lem-ancestry-addition-240}{%
    $\myfn{Anc}{\Vnat{bad}+\Vnat{y},\Vnodset{base}} =
    \bigfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}$
    \bcuz{} \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-ancestry-addition-230}.
}
\mypareq{eq:lem-ancestry-addition-250}{%
    \Vmyfn{INST}{\Vnat{bad}} \cuz{} \Eref{lem-ancestry-addition-100},
        \Eref{lem-ancestry-addition-240}.
}
Equation \Eref{lem-ancestry-addition-250} is contrary to
\Eref{lem-ancestry-addition-110}, which shows the reductio
beginning at \Eref{lem-ancestry-addition-110}--%
\Eref{lem-ancestry-addition-120}.
From the reductio we conclude the absence of a counter-example,
and the lemma.
\myqed
\end{proof}

\begin{lemma}[Ancestry Closure]
\label{lem:ancestry-closure}
Every ancestry is
the union of all the ancestries with the same base,
and at the same or a lower level.
That is,
let \V{base} be an arbitrary subset of \V{V}.
Then
\mypareq{eq:lem-ancestry-closure-050}{%
    $\forall\; \Vnat{N} :
        \myfn{Anc}{\Vnat{N},\V{base}} =
    \bigcup \set{ \forall\; \Vnat{i} \le \V{N} : \myfn{Anc}{\V{i},\V{base}}}$.
    \thEnd
}
\end{lemma}

\begin{proof}
The proof is by minimal counterexample.
Let
\begin{equation}
\label{eq:lem-ancestry-closure-150}
\begin{gathered}
\Vmyfn{INST}{\Vnat{ex}} \ldefined
        \myfn{Anc}{\V{ex},\V{base}} =
    \bigcup \set{ \forall\; \Vnat{i} \le \V{ex} : \myfn{Anc}{\V{i},\V{base}}}
\\ \myparbox{\cuz{}
    UI of \Eref{lem-ancestry-closure-050} with \Vnat{ex} for \Vnat{N}.
    New free $\V{base} \in \Vpowerset{V}.$}
\end{gathered}
\end{equation}
We assume there is a minimal \V{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:lem-ancestry-closure-160}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-ancestry-closure-165}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \because \text{AF reductio.  New free \V{bad}.}
\end{gather}
It follows from a basic fact of set theory that
\mypareq{eq:lem-ancestry-closure-170}{%
        $\myfn{Anc}{0,\V{base}}
	  = \bigcup \set{ \forall\; \Vnat{i} \le 0 : \myfn{Anc}{\V{i},\V{base}}}$
        \linebreak \cuz{} Trivial.  This is \myfn{INST}{0}.
}
\mypareq{eq:lem-ancestry-closure-180}{%
    $\V{bad} > 0$ \cuz{}
    \Eref{lem-ancestry-closure-160},
    \Eref{lem-ancestry-closure-170}.}
\mypareq{eq:lem-ancestry-closure-250}{%
        $\myfn{Anc}{\Vdecr{bad},\V{base}} =
    \bigcup \set{ \forall\; \Vnat{i} \le \Vdecr{bad} : \myfn{Anc}{\V{i},\V{base}}}$
        \linebreak \cuz{} \Eref{lem-ancestry-closure-150},
            \Eref{lem-ancestry-closure-165}, \Eref{lem-ancestry-closure-180}.
            This is \myfn{INST}{\Vdecr{bad}}.
}
\mypareq{eq:lem-ancestry-closure-270}{%
    $\myfn{Anc}{\Vdecr{bad},\V{base}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
        \linebreak \cuz{} \longLmref{Ancestry Monotonicity}{ancestry-monotonicity},
            \Eref{lem-ancestry-closure-180}.
}
\mypareq{eq:lem-ancestry-closure-280}{%
    $\bigcup \set{ \forall\; \Vnat{i} \le \Vdecr{bad} : \myfn{Anc}{\V{i},\V{base}}}
      \subseteq \myfn{Anc}{\V{bad},\V{base}}$
        \linebreak \cuz{} \Eref{lem-ancestry-closure-250},
        \Eref{lem-ancestry-closure-270}.
}
\mypareq{eq:lem-ancestry-closure-290}{%
        $\forall\;\V{whole} \in \powerset{\V{V}} :
        \forall\;\V{part} \in \powerset{\V{V}} :
	\linebreak
        \V{part} \subseteq \V{whole} \implies \V{whole} = \V{whole} \cup \V{part}$
        \bcuz{} Fact of set theory.
}
\mypareq{eq:lem-ancestry-closure-200}{%
      $\myfn{Anc}{\V{bad},\V{base}} =
      \myfn{Anc}{\V{bad},\V{base}} \cup
        \left(
            \bigcup \set{ \forall\; \Vnat{i} \le \Vdecr{bad} : \myfn{Anc}{\V{i},\V{base}}}
        \right)$
        \linebreak \cuz{} \Eref{lem-ancestry-closure-280},
            \Eref{lem-ancestry-closure-290}.
}
\mypareq{eq:lem-ancestry-closure-210}{%
      $\myfn{Anc}{\V{bad},\V{base}} =
    \bigcup \set{ \forall\; \Vnat{i} \le \V{bad} : \myfn{Anc}{\V{i},\V{base}}}$
        \linebreak \cuz{} \Eref{lem-ancestry-closure-200}.
        This is \Vmyfn{INST}{bad}.
}
Equation \Eref{lem-ancestry-closure-210} is contrary to
\Eref{lem-ancestry-closure-160}, which shows the reductio.
From the reductio, we conclude that there is no counter-example,
and the lemma follows.
\myqed
\end{proof}

\begin{lemma}[Ancestry from path]
\label{lem:ancestry-from-path}
The start of a path is in every ancestry
whose base contains the last value of the path
and whose level is at least the last index of the path.
That is,
\begin{equation}
\label{eq:ancestry-from-path-050}
\begin{gathered}
\forall\; \Vnat{N} :
\forall\; \V{path} \in \Vmyfn{paths}{G} :
\forall\; \Vnodset{base} :
\\ \Vlastix{path} \le \V{N}
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{\V{N}, \V{base}}.
    \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counterexample.
Let $\V{base} \in \Vpowerset{V}$ be an arbitrary nodeset.
Where
\begin{equation}
\label{eq:ancestry-from-path-150}
\begin{gathered}
\Vmyfn{INST}{ex} \defined \forall\; \V{path} \in \Vmyfn{paths}{G} :
\\ \Vlastix{path} \le \V{ex}
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{\V{ex}, \V{base}}.
\\ \myparbox{\cuz{}
        UI of \Eref{ancestry-from-path-050} with \V{ex}.}
\end{gathered}
\end{equation}
we assume there is a minimal \V{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:ancestry-from-path-160}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:ancestry-from-path-165}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \because \text{AF reductio.  New free \V{bad}.}
\end{gather}
We first examine the subcase $\V{bad} = 0$,
introducing a new variable, \V{pathZero},
for this purpose.
\begin{gather}
\label{eq:ancestry-from-path-170}
    \Vlastix{pathZero} = 0
\\ \label{eq:ancestry-from-path-175}
    \land\; \Velement{pathZero}{\Vlastix{pathZero}} \in \V{base}
\\ \nonumber
    \because \text{AF subcase.  New free \V{pathZero}.}
\end{gather}
\mypareq{eq:ancestry-from-path-180}{%
    $\V{base} = \myfn{Anc}{0,\Vnodset{base}}$
    \cuz{} \longLdref{\fname{Anc}}{ancestry}.
}
\mypareq{eq:ancestry-from-path-190}{%
    $\Velement{pathZero}{0} \in \myfn{Anc}{0,\Vnodset{base}}$
    \bcuz{} \Eref{ancestry-from-path-170},
        \Eref{ancestry-from-path-175}, \Eref{ancestry-from-path-180}.
}
\begin{equation}
\label{eq:ancestry-from-path-200}
\begin{gathered}
\Vlastix{pathZero} \le 0
    \land \Velement{pathZero}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{pathZero}{0} \in \myfn{Anc}{0, \V{base}}.
\\ \myparbox{\cuz{}
        Implication from \Eref{ancestry-from-path-170}--%
            \Eref{ancestry-from-path-175}
            to \Eref{ancestry-from-path-190}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:ancestry-from-path-210}
\begin{gathered}
\forall\; \V{path} \in \Vmyfn{paths}{G}
\\ \Vlastix{path} \le 0
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{0, \V{base}}.
\\ \myparbox{\cuz{}
    UG of \V{pathZero} in \Eref{ancestry-from-path-200}.}
\end{gathered}
\end{equation}
\mypareq{eq:ancestry-from-path-280}{%
    \Vmyfn{INST}{0}
    \cuz{} \Eref{ancestry-from-path-150},
        \Eref{ancestry-from-path-210}.
}
We can now eliminate the subcase $\V{bad} = 0$:
\mypareq{eq:ancestry-from-path-290}{%
    $\V{bad} > 0$ \cuz{}
        \Eref{ancestry-from-path-160},
        \Eref{ancestry-from-path-280}.
}
We proceed to look at the other possible values for \V{bad}.
\begin{gather}
\label{eq:ancestry-from-path-310}
    \Vlastix{path} \le \V{bad}
\\ \label{eq:ancestry-from-path-320}
    \land\; \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \label{eq:ancestry-from-path-330}
    \land\; \Velement{path}{0} \notin \myfn{Anc}{\V{bad}, \V{base}}.
\\ \nonumber
   \myparbox{\cuz{} \Eref{ancestry-from-path-150},
       \Eref{ancestry-from-path-160}.
       New free \V{path}.
       }
\end{gather}
\begin{gather}
\label{eq:ancestry-from-path-340}
    \Velement{path}{0} \notin \myfn{Anc}{\Vdecr{bad}, \V{base}}
\\ \label{eq:ancestry-from-path-345}
    \land\; \Velement{path}{0} \notin
        \bigfn{NewParents}{\myfn{Anc}{\Vdecr{bad},\V{base}}}
\\ \nonumber
    \myparbox{\cuz{} \Eref{ancestry-from-path-330},
        \longLdref{\fname{Anc}}{ancestry}.}
\end{gather}
Recall that if \V{bad} is a minimal counter-example,
then \Vdecr{bad} cannot be a counter-example:
\mypareq{eq:ancestry-from-path-350}{%
    \Vmyfn{INST}{\Vdecr{bad}}
    \cuz{} \Eref{ancestry-from-path-165},
        \Eref{ancestry-from-path-290}.
}
\begin{equation}
\label{eq:ancestry-from-path-360}
\begin{gathered}
\forall\; \V{path} \in \Vmyfn{paths}{G} :
\\ \Vlastix{path} \le \Vdecr{bad}
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{\Vdecr{bad}, \V{base}}.
\\ \myparbox{\cuz{} Expansion of
    \Eref{ancestry-from-path-350} using \Eref{ancestry-from-path-165}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:ancestry-from-path-370}
\begin{gathered}
\Vlastix{path} \le \Vdecr{bad}
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{\Vdecr{bad}, \V{base}}
\\ \myparbox{\cuz{} UI of bound \V{path} in \Eref{ancestry-from-path-360}
    with \V{path} from
    \Eref{ancestry-from-path-310}--\Eref{ancestry-from-path-330}.}
\end{gathered}
\end{equation}
\mypareq{eq:ancestry-from-path-380}{%
    $\Vlastix{path} > \Vdecr{bad}$
        \cuz{} \Eref{ancestry-from-path-320},
            \Eref{ancestry-from-path-330}, \Eref{ancestry-from-path-370}.}
We conclude that, if there is a counter-example,
its path and ancestry are such that
the last index of the path and the level of the ancestry
``fit'' exactly:
\mypareq{eq:ancestry-from-path-390}{%
    $\Vlastix{path} = \V{bad}$
        \cuz{} \Eref{ancestry-from-path-310},
            \Eref{ancestry-from-path-380}.}
Now it will be helpful to consider a suffix of \V{path}.
\begin{gather}
\label{eq:ancestry-from-path-410}
    \left( \V{suffix} \ldefined \family{ 1 \le \Vnat{i} \le \Vlastix{path} : \VVelement{path}{i} }
        \right)
\\ \label{eq:ancestry-from-path-412}
    \land\; \V{suffix} \in \Vmyfn{paths}{G}
\\ \nonumber
    \myparbox{\cuz{} \longDfref{Path}{path},
    \Eref{ancestry-from-path-290}, new \V{suffix} for convenience.}
\end{gather}
\mypareq{eq:ancestry-from-path-420}{%
    $\Vlastix{suffix} = \subtract{\Vlastix{path}}{1}$
    \cuz{} \Eref{ancestry-from-path-410}.}
\mypareq{eq:ancestry-from-path-430}{%
    $\Vlastix{suffix} = \Vdecr{bad}$
    \cuz{} \Eref{ancestry-from-path-390},
        \Eref{ancestry-from-path-420}.}
\mypareq{eq:ancestry-from-path-440}{%
    $\Velement{suffix}{\Vlastix{suffix}} = \Velement{path}{\Vlastix{path}}$
    \cuz{} \Eref{ancestry-from-path-410}.}
\mypareq{eq:ancestry-from-path-450}{%
    $\Velement{suffix}{\Vlastix{suffix}} \in \V{base}$
    \cuz{} \Eref{ancestry-from-path-320},
        \Eref{ancestry-from-path-440}.}
\mypareq{eq:ancestry-from-path-460}{%
    $\Velement{suffix}{0} \in \myfn{Anc}{\Vdecr{bad}, \V{base}}$
    \bcuz{} UI  of \V{path} in \Eref{ancestry-from-path-360}
             with \V{suffix},
        \linebreak \Eref{ancestry-from-path-412}, \Eref{ancestry-from-path-430},
        \Eref{ancestry-from-path-450}.}
\mypareq{eq:ancestry-from-path-470}{%
    $\tuple{ \Velement{path}{0}, \Velement{path}{1} } \in \V{E}$
    \cuz{} \longDfref{Path}{path},
        \Eref{ancestry-from-path-290}.
}
\mypareq{eq:ancestry-from-path-480}{%
    $\Velement{path}{1} = \Velement{suffix}{0}$
        \cuz{} \Eref{ancestry-from-path-410}.
}
\mypareq{eq:ancestry-from-path-490}{%
    $\tuple{ \Velement{path}{0}, \Velement{suffix}{0} } \in \V{E}$
    \cuz{} \Eref{ancestry-from-path-470},
        \Eref{ancestry-from-path-480}.}
\begin{equation}
\label{eq:ancestry-from-path-500}
\begin{gathered}
    \exists\; \Vnod{child} :
    \\ \V{child} \in \myfn{Anc}{\Vdecr{bad}, \V{base}}
    \\ \land\; \Velement{path}{0} \notin \myfn{Anc}{\Vdecr{bad}, \V{base}}
    \\ \land\; \tuple{\Velement{path}{0}, \V{child}} \in \V{E}
    \\ \myparbox{\cuz{} \Eref{ancestry-from-path-340},
        EG of \Velement{suffix}{0} with \V{child} in \Eref{ancestry-from-path-460} and
            \Eref{ancestry-from-path-490}.}
\end{gathered}
\end{equation}
\mypareq{eq:ancestry-from-path-510}{%
    $\Velement{path}{0} \in \myfn{NewParents}{\myfn{Anc}{\Vdecr{bad}, \V{base}}}$
    \bcuz{} \longLdref{\fname{NewParents}}{new-parents},
        \Eref{ancestry-from-path-500}.
}
Equation \Eref{ancestry-from-path-510} is contrary to
\Eref{ancestry-from-path-345}, which shows the reductio.
From the reductio, we conclude that there is no counter-example
and the lemma follows.
\myqed
\end{proof}

\section{Tree generations}

\begin{ldef}[Generation]
\label{ldef:generation}
A \dfn{generation}
is the set of nodes
added to the set of ancestors of \V{leaves} at ancestry level \V{N}.
That is,
\[
    \deffn{gen}{ \left(
            \naturals\times\Vpowerset{V}
    \right) } {\Vpowerset{V}}
\]
is the function whose value is the \V{N}'th generation of \V{G}
from \V{base},
if
\begin{equation}
\nonumber
    \Vmyfn{gen}{\V{N},\V{base}} \ldefined
    \set{
    \begin{gathered}
     \V{N}= 0 \condOpA \V{base} \condOpB
        \\ \subtract{\myfn{Anc}{\V{N},\V{base}}}
            {\myfn{Anc}{\Vdecr{N},\V{base}}}
    \end{gathered}
    }. \quad \dfEnd
\end{equation}
\end{ldef}

\begin{lemma}[Generation New Parent Equivalence]
\label{lem:generation-new-parent-equivalence}
If an ancestry has a predecessor,
the generation of that ancestry is equivalent to
the new parents of the predecessor.
That is,
\begin{equation}
\begin{gathered}
\forall\; \V{base} \in \Vpowerset{V} :
    \forall\; \Vnat{N} :
\\ \myfn{gen}{\V{N}+1,\V{base}} =
    \myfn{NewParents}{\V{N},\V{base}}.
    \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
\mypareq{eq:lem-generation-new-parent-equivalence-100}{%
    $\Vmyfn{gen}{\V{N}+1,\V{base}} =
        \subtract{\myfn{Anc}{\V{N}+1,\V{base}}}
            {\myfn{Anc}{\V{N},\V{base}}}$
            \linebreak \cuz{} \longLdref{Generation}{generation}.}
\mypareq{eq:lem-generation-new-parent-equivalence-110}{%
    $\myfn{Anc}{\Vnat{N}+1,\Vnodset{base}} =
        \myfn{Anc}{\V{N},\V{base}}
        \cup \bigfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}}$
    \linebreak \cuz{} \longLdref{Ancestry}{ancestry}.}
\begin{equation}
\label{eq:lem-generation-new-parent-equivalence-120}
\begin{gathered}
    \Vmyfn{gen}{\V{N}+1,\V{base}} =
    \\ \left( \myfn{Anc}{\V{N},\V{base}} \cup
            \myfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}} \right)
    \\ - \, \myfn{Anc}{\V{N},\V{base}}
    \\ \because \Eref{lem-generation-new-parent-equivalence-100},
        \Eref{lem-generation-new-parent-equivalence-110}.
\end{gathered}
\end{equation}
We know that the result
of subtracting one of two disjoint sets
from their union is the other set.
That is,
\begin{equation}
\label{eq:lem-generation-new-parent-equivalence-130}
\begin{gathered}
     \forall\; \V{part1} \in \Vpowerset{V} :
     \forall\; \V{part2} \in \Vpowerset{V} :
     \\ \forall\; \V{nextGen} \in \Vpowerset{V} :
     \\ \V{part1} \cap \V{part2} = \emptyset
     \\ \land\; \V{nextGen} = \subtract{(\V{part1} \cup \V{part2})}{\V{part1}}
     \\ \implies \V{nextGen} = \V{part2}
     \\ \myparbox{\cuz{} Fact of set theory.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-generation-new-parent-equivalence-140}{%
    $\myfn{Anc}{\Vnat{N},\Vnodset{base}} \cap
        \bigfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}} = \emptyset$
    \linebreak \cuz{}
        \longLmref{Predecessor--Parent Disjointness}{predecessor-parent-disjointness}.}
\mypareq{eq:lem-generation-new-parent-equivalence-150}{%
    $\Vmyfn{gen}{\V{N}+1,\V{base}} =
        \bigfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}}$
        \linebreak \cuz{} \Eref{lem-generation-new-parent-equivalence-120},
            \Eref{lem-generation-new-parent-equivalence-130},
            \Eref{lem-generation-new-parent-equivalence-140}.}
The lemma is the universal generalization of
\Eref{lem-generation-new-parent-equivalence-150}
for \Vnat{N} and \Vnodset{base}.
\myqed
\end{proof}

\begin{lemma}[Generation Disjointness]
\label{lem:generation-disjointness}
Every pair of generations with different levels
and the same base is disjoint.
That is,
\begin{equation}
\nonumber
\begin{gathered}
    \forall\;\Vnat{a} :
    \forall\;\Vnat{b} :
    \forall\;\V{base}\in\Vpowerset{V} :
    \\ \V{a} \ne \V{b} \implies
    \myfn{gen}{\V{a},\V{base}} \cap \myfn{gen}{\V{b},\V{base}} = \emptyset.
    \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is by reductio.
\begin{gather}
\label{eq:lem-generation-disjointness-100}
    \V{lo} \ne \V{hi}
\\ \label{eq:lem-generation-disjointness-110}
     \land\; \myfn{gen}{\V{lo},\V{base}} \cap \myfn{gen}{\V{hi},\V{base}} \ne \emptyset.
\\ \nonumber
    \because \text{AF reductio.
        New free \Vnat{lo}, \Vnat{hi}.
        New free $\V{base} \in \Vpowerset{V}$.}
\end{gather}
We need to choose one of the two variables, \V{lo} and \V{hi},
to be first in numeric order.
The mentions of \V{hi} and \V{lo} have been symmetric up to this point,
so that we may assume
\mypareq{eq:lem-generation-disjointness-120}{%
     $\V{lo} < \V{hi}$ \cuz{} \text{WLOG}.
}
\mypareq{eq:lem-generation-disjointness-150}{%
    $\exists\; \V{nd} : \V{nd} \in \ \myfn{gen}{\V{lo},\V{base}}
        \land \V{nd} \in \ \myfn{gen}{\V{hi},\V{base}}$
    \bcuz{} \Eref{lem-generation-disjointness-110}.
}
\begin{gather}
\label{eq:lem-generation-disjointness-160}
    \V{nd} \in \ \myfn{gen}{\V{lo},\V{base}}
\\ \label{eq:lem-generation-disjointness-170}
        \land\; \V{nd} \in \ \myfn{gen}{\V{hi},\V{base}}
\\ \nonumber
    \text{\cuz{} \Eref{lem-generation-disjointness-150}, EI of \V{nd}.}
\end{gather}
\mypareq{eq:lem-generation-disjointness-180}{%
    $\V{nd} \in \ \myfn{Anc}{\V{lo},\V{base}}$
        \bcuz{} Def of \fname{gen} \Ldref{generation},
            \Eref{lem-generation-disjointness-160}.
}
\mypareq{eq:lem-generation-disjointness-190}{%
     $\V{lo} \le \Vdecr{hi}$ \cuz{} \Eref{lem-generation-disjointness-120}.
}
\mypareq{eq:lem-generation-disjointness-200}{%
    $\V{nd} \in \myfn{Anc}{\Vdecr{hi},\V{base}}$
    \bcuz{} \longLmref{Ancestry monotonicity}{ancestry-monotonicity},
            \Eref{lem-generation-disjointness-180},
            \Eref{lem-generation-disjointness-190}.
}
\mypareq{eq:lem-generation-disjointness-210}{%
    $\myfn{Anc}{\Vdecr{hi},\Vnodset{base}} \cap
    \bigfn{NewParents}{\myfn{Anc}{\Vdecr{hi},\V{base}}} = \emptyset$
    \bcuz{} \longLmref{Predecessor--Parent Disjointness}{predecessor-parent-disjointness}.
}
\mypareq{eq:lem-generation-disjointness-220}{%
    $\V{nd} \notin \myfn{NewParents}{\myfn{Anc}{\Vdecr{hi},\V{base}}}$
       \cuz{} \Eref{lem-generation-disjointness-200},
            \Eref{lem-generation-disjointness-210}.
}
\mypareq{eq:lem-generation-disjointness-230}{%
    $\V{nd} \notin \myfn{gen}{\var{hi},\V{base}}$
        \bcuz{} \longLmref{Generation--New-Parent Equivalence}
            {generation-new-parent-equivalence},
        \Eref{lem-generation-disjointness-220}.
}
Equation \Eref{lem-generation-disjointness-230} is contrary to
\Eref{lem-generation-disjointness-170},
which shows the
reductio.
\mypareq{eq:lem-generation-disjointness-240}{%
    $\neg (\V{lo} \ne \V{hi}
     \land \myfn{gen}{\V{lo},\V{base}} \cap \myfn{gen}{\V{hi},\V{base}} \ne \emptyset)$
    \bcuz{} Reductio from \Eref{lem-generation-disjointness-100}--%
    \Eref{lem-generation-disjointness-110} to
    \Eref{lem-generation-disjointness-230}.
}
\mypareq{eq:lem-generation-disjointness-250}{%
    $\V{lo} = \V{hi}
     \lor \myfn{gen}{\V{lo},\V{base}} \cap \myfn{gen}{\V{hi},\V{base}} = \emptyset$
     \cuz{} \Eref{lem-generation-disjointness-240}.
}
\mypareq{eq:lem-generation-disjointness-260}{%
    $\V{lo} \ne \V{hi}
     \implies \myfn{gen}{\V{lo},\V{base}} \cap \myfn{gen}{\V{hi},\V{base}} = \emptyset$
     \cuz{} \Eref{lem-generation-disjointness-250}.
}
The lemma is the universal generalization of \Eref{lem-generation-disjointness-260}
for \Vnat{lo}, \Vnat{hi} and \Vnodset{base}.
\myqed
\end{proof}

\begin{lemma}[Generation Coverage]
\label{lem:generation-coverage}
The union of all generations of level \V{N} or less
is equal to the \V{N}-level ancestry of the same base.
That is,
if \V{base} is an arbitrary subset of \V{V},
then
\mypareq{eq:lem-generation-coverage-050}{%
    $\forall\; \Vnat{N} :
    \myfn{Anc}{\V{N},\V{base}} =
    \bigcup \set{ \forall\; \Vnat{i} \le \V{N} : \myfn{gen}{\V{i},\V{base}}}$.
    \thEnd
}
\end{lemma}

\begin{proof}
The proof is by minimal counterexample.
Let
\mypareq{eq:lem-generation-coverage-150}{%
    $\Vmyfn{INST}{ex} \defined
        \myfn{Anc}{\V{ex},\V{base}} =
        \bigcup \set{ \forall\; \Vnat{i} \le \V{ex} : \myfn{gen}{\V{i},\V{base}}}$
    \cuz{} UI of
        \Eref{lem-generation-coverage-050} with \V{ex}.
}
We assume there is a minimal \V{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:lem-generation-coverage-160}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-generation-coverage-165}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \because \text{AF reduction.  New free \V{bad}.}
\end{gather}
\mypareq{eq:lem-generation-coverage-170}{%
    $\myfn{Anc}{0,\V{base}} = \V{base} =
    \myfn{gen}{0,\V{base}}$
    \linebreak \cuz{}
        \longLdref{\fname{Anc}}{ancestry},
        Def of \fname{gen} \Ldref{generation}.
}
\mypareq{eq:lem-generation-coverage-180}{%
    \Vmyfn{INST}{0} \cuz{} \Eref{lem-generation-coverage-150},
        \Eref{lem-generation-coverage-170}.
}
\mypareq{eq:lem-generation-coverage-190}{%
    $\V{bad} > 0$ \cuz{}
        \Eref{lem-generation-coverage-160},
        \Eref{lem-generation-coverage-180}.
}
Expanding $\neg \Vmyfn{INST}{bad}$,
\mypareq{eq:lem-generation-coverage-200}{%
    $\myfn{Anc}{\V{bad},\V{base}} \ne
        \bigcup \set{ \forall\; \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}}}$
        \bcuz{} \Eref{lem-generation-coverage-150},
            \Eref{lem-generation-coverage-160}.
}
\mypareq{eq:lem-generation-coverage-210}{%
    $\myfn{Anc}{\V{bad},\V{base}} =
    \myfn{Anc}{\Vdecr{bad},\V{base}}
    \cup \bigfn{NewParents}{\myfn{Anc}{\Vdecr{bad},\V{base}}}$
        \bcuz{} \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-generation-coverage-180}.
}
\mypareq{eq:lem-generation-coverage-220}{%
    $\myfn{Anc}{\V{bad},\V{base}} =
    \myfn{Anc}{\Vdecr{bad},\V{base}}
    \cup \bigfn{gen}{\var{bad},\V{base}}$
        \bcuz{} \longLmref{Generation--New-Parent Equivalence}
            {generation-new-parent-equivalence},
        \Eref{lem-generation-coverage-210}.
}
\mypareq{eq:lem-generation-coverage-230}{%
    $\myfn{Anc}{\Vdecr{bad},\V{base}} =
        \bigcup \set{ \forall\; \Vnat{i} \le \Vdecr{bad} : \myfn{gen}{\V{i},\V{base}}}$
    \bcuz{} \Eref{lem-generation-coverage-150},
        \Eref{lem-generation-coverage-165}.
}
\mypareq{eq:lem-generation-coverage-240}{%
    $\myfn{Anc}{\V{bad},\V{base}} =
        \bigcup \set{ \forall\; \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}}}$
    \bcuz{} \Eref{lem-generation-coverage-220},
        \Eref{lem-generation-coverage-230}.
}
\mypareq{eq:lem-generation-coverage-250}{%
    $\myfn{INST}{\V{bad},\V{base}}$
    \cuz{} \Eref{lem-generation-coverage-150},
        \Eref{lem-generation-coverage-240}.
}
Equation \Eref{lem-generation-coverage-250} is contrary
to \Eref{lem-generation-coverage-160}, which shows the reductio
from \Eref{lem-generation-coverage-160}--%
\Eref{lem-generation-coverage-165} to
\Eref{lem-generation-coverage-250}.
This reductio proves the absence
of a counter-example,
which in turn gives us the lemma.
\myqed
\end{proof}


\begin{ldef}[Surface]
\label{ldef:surface}
The \dfn{surface} of a subset of \V{V}
is the set of its elements which have parents outside of
that subset.
This is,
let
\[
    \deffn{Surface}{\Vpowerset{V}}{\Vpowerset{V}}
\]
be such that
\begin{equation}
\label{eq:def-surface}
\begin{gathered}
\myfn{Surface}{\V{ndset}} \ldefined
\\ \set{
    \begin{gathered}
    \V{child} \in \V{ndset} :
        \exists\; \V{parent} \in \V{V} :
    \\ \tuple{\V{parent},\V{child}} \in \V{E}
        \land \V{parent} \notin \V{ndset}
    \end{gathered}
}.  \quad \dfEnd
\end{gathered}
\end{equation}
\end{ldef}

The definition of ``surface'' \Eref{def-surface}
implies that the root node
is never an element of the surface.

\begin{lemma}[Surface Disjointness]
\label{lem:surface-disjointness}
The predecessor and the surface of an ancestry are disjoint.
That is,
\begin{equation}
\label{eq:lem-surface-disjointness-050}
\begin{gathered}
    \forall\;\V{base} \in \Vpowerset{V} :
        \forall\;\Vnat{N} :
    \\ \myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}}
        \cap \myfn{PredecessorAnc}{\V{N},\V{base}} = \emptyset.
        \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counter-example.
Let
\begin{equation}
\label{eq:lem-surface-disjointness-100}
\begin{gathered}
    \Vmyfn{INST}{ex} \defined
    \\ \left( \begin{gathered}
    \myfn{Surface}{\myfn{Anc}{\V{ex},\Vnodset{base}}}
        \\ \cap\; \myfn{PredecessorAnc}{\V{ex},\V{base}}
    \end{gathered} \right) = \emptyset
    \\ \text{\cuz{} EI of \Eref{lem-surface-disjointness-050}
         with \Vnat{ex} for \Vnat{N}.
         New free \Vnodset{base}.}
\end{gathered}
\end{equation}
We assume there is a minimal counter-example to the theorem, \Vnat{bad}:
\begin{gather}
\label{eq:lem-surface-disjointness-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-surface-disjointness-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-surface-disjointness-130}{%
    $\myfn{PredecessorAnc}{0,\V{base}} = \emptyset$
    \bcuz{} Def of ancestry predecessor \Eref{def-ancestry-predecessor}.
}
\mypareq{eq:lem-surface-disjointness-140}{%
    $\myfn{Surface}{\myfn{Anc}{0,\V{base}}}
        \cap \myfn{PredecessorAnc}{0,\V{base}} = \emptyset$
    \bcuz{} \Eref{lem-surface-disjointness-130}.
}
\mypareq{eq:lem-surface-disjointness-150}{%
    $\myfn{INST}{0}$ \cuz{} \Eref{lem-surface-disjointness-100},
        \Eref{lem-surface-disjointness-140}.
}
\mypareq{eq:lem-surface-disjointness-160}{%
    $\V{bad} > 0$ \cuz{} \Eref{lem-surface-disjointness-110},
        \Eref{lem-surface-disjointness-150}.
}
\begin{gather}
\label{eq:lem-surface-disjointness-200}
    \V{nd} \in \myfn{Surface}{\myfn{Anc}{\V{bad},\V{base}}}
\\ \label{eq:lem-surface-disjointness-210}
    \land\; \V{nd} \in \myfn{Anc}{\Vdecr{bad},\V{base}}
\\ \nonumber
    \myparbox{\cuz{}
        \longLdref{\fname{PredecessorAnc}}{predecessor-anc},
        \linebreak \Eref{lem-surface-disjointness-100},
        \Eref{lem-surface-disjointness-110},
        \Eref{lem-surface-disjointness-160}.
        New free \Vnod{nd}.}
\end{gather}
\begin{gather}
\label{eq:lem-surface-disjointness-220}
    \V{nd} \in \myfn{Anc}{\V{bad},\V{base}}
\\ \label{eq:lem-surface-disjointness-230}
    \land\; \exists\; \Vnod{parent} :
    \left( \begin{gathered}
        \tuple{\V{parent},\V{nd}} \in \V{E}
        \\ \land\; \V{parent} \notin \myfn{Anc}{\V{bad},\V{base}}
    \end{gathered} \right)
    \\ \nonumber
        \myparbox{\cuz{} Def of ``surface'' \Eref{def-surface},
        \Eref{lem-surface-disjointness-200}.}
\end{gather}
\begin{gather}
\label{eq:lem-surface-disjointness-240}
    \tuple{\Vnod{parent},\V{nd}} \in \V{E}
\\ \label{eq:lem-surface-disjointness-250}
        \land\; \V{parent} \notin \myfn{Anc}{\V{bad},\V{base}}
\\ \nonumber
    \myparbox{\cuz{} EI of \Eref{lem-surface-disjointness-230}.
        New free \Vnod{parent}.}
\end{gather}
\mypareq{eq:lem-surface-disjointness-260}{%
    $\myfn{Anc}{\V{bad},\V{base}}
        = \myfn{Anc}{\Vdecr{bad},\V{base}} \cup \myfn{NewParents}{\myfn{Anc}{\Vdecr{bad},\V{base}}}$
    \bcuz{} \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-surface-disjointness-160}.
}
\mypareq{eq:lem-surface-disjointness-270}{%
     $\V{parent} \in \myfn{Anc}{\Vdecr{bad},\V{base}} \implies
     \V{parent} \in \myfn{Anc}{\V{bad},\V{base}}$
     \bcuz{} \Eref{lem-surface-disjointness-260}.
}
\mypareq{eq:lem-surface-disjointness-280}{%
     $\V{parent} \notin \myfn{Anc}{\Vdecr{bad},\V{base}}$
     \bcuz{} \Eref{lem-surface-disjointness-250},
         \Eref{lem-surface-disjointness-270}.
}
\mypareq{eq:lem-surface-disjointness-290}{%
    $\V{nd} \in \myfn{Anc}{\Vdecr{bad},\V{base}}$
    \linebreak $\land\; \V{parent} \notin \myfn{Anc}{\Vdecr{bad},\V{base}}$
    \linebreak $\land\; \tuple{\V{parent},\V{nd}} \in \V{E}$
    \bcuz{} \Eref{lem-surface-disjointness-210},
        \Eref{lem-surface-disjointness-240},
        \Eref{lem-surface-disjointness-280}.
}
\mypareq{eq:lem-surface-disjointness-300}{%
    $\exists\; \Vnod{child} : \V{child} \in \myfn{Anc}{\Vdecr{bad},\V{base}}$
    \linebreak $\land\; \V{parent} \notin \myfn{Anc}{\Vdecr{bad},\V{base}}$
    \linebreak $\land\; \tuple{\V{parent},\V{child}} \in \V{E}$
    \bcuz{} EG of \Eref{lem-surface-disjointness-290} with \Vnod{child}
        for \Vnod{nd}.
}
\mypareq{eq:lem-surface-disjointness-320}{%
    $\Vnod{parent} \in \myfn{NewParents}{\myfn{Anc}{\Vdecr{bad},\V{base}}}$
    \bcuz{} \longLdref{\fname{NewParents}}{new-parents},
        \Eref{lem-surface-disjointness-300}.
}
\mypareq{eq:lem-surface-disjointness-330}{%
    $\V{parent} \in \myfn{Anc}{\V{bad},\V{base}}$ \cuz{}
        \longLdref{\fname{Anc}}{ancestry},
        \Eref{lem-surface-disjointness-320}.
}
Equation \Eref{lem-surface-disjointness-330}
is contrary to
\Eref{lem-surface-disjointness-250}
and shows the reductio from
\Eref{lem-surface-disjointness-110}--\Eref{lem-surface-disjointness-120}
to \Eref{lem-surface-disjointness-330}.
From the reductio, we conclude the absence of
a counter-example, and the lemma.
\myqed
\end{proof}

\begin{lemma}[Generation Surface Containment]
\label{lem:generation-surface-containment}
The surface of the N-level ancestry of a base is a subset
of the N-level generation of that same base.  That is,
\begin{equation}
\label{eq:lem-generation-surface-containment-050}
\myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}} \subseteq \myfn{gen}{\V{N},\V{base}}.
\quad \thEnd
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counter-example.
Let
\begin{equation}
\label{eq:lem-generation-surface-containment-100}
\begin{gathered}
    \Vmyfn{INST}{ex} \ldefined
    \\ \myfn{Surface}{\myfn{Anc}{\V{ex},\V{base}}} \subseteq \myfn{gen}{\V{ex},\V{base}}.
    \\ \text{\cuz{} \Eref{lem-generation-surface-containment-050}
         with \Vnat{ex} for \Vnat{N}.
         New free \Vnodset{base}.}
\end{gathered}
\end{equation}
We assume there is a minimal counter-example to the lemma, \Vnat{bad}:
\begin{gather}
\label{eq:lem-generation-surface-containment-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-generation-surface-containment-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \myparbox{\cuz{} AF reductio.
    New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-generation-surface-containment-210}{%
    $\forall\; \V{ndset} \in \Vpowerset{V} :
        \myfn{Surface}{\V{ndset}} \subseteq \V{ndset}$
    \bcuz{} Def of \fname{Surface} \Eref{def-surface}.
}
\mypareq{eq:lem-generation-surface-containment-220}{%
    $\myfn{Surface}{\myfn{Anc}{0,\V{base}}} \subseteq \myfn{Anc}{0,\V{base}}$
    \bcuz{} \Eref{lem-generation-surface-containment-210}.
}
\mypareq{eq:lem-generation-surface-containment-230}{%
    $\myfn{Anc}{0,\V{base}} = \V{base} = \myfn{gen}{0,\V{base}}$
    \linebreak \cuz{}
        \longLdref{\fname{Anc}}{ancestry},
        Def of \fname{gen} \Ldref{generation}.
}
\mypareq{eq:lem-generation-surface-containment-240}{%
    $\myfn{Surface}{\myfn{Anc}{0,\V{base}}} \subseteq \myfn{gen}{0,\V{base}}$
    \bcuz{} \Eref{lem-generation-surface-containment-220},
        \Eref{lem-generation-surface-containment-230}.
        \linebreak This is the expansion of
        \Eref{lem-generation-surface-containment-100}
        as \myfn{INST}{0}.}
\mypareq{eq:lem-generation-surface-containment-250}{%
    $\Vnat{bad} > 0$
    \cuz{} \Eref{lem-generation-surface-containment-110},
        \Eref{lem-generation-surface-containment-240}.}
\mypareq{eq:lem-generation-surface-containment-260}{%
    $\myfn{Surface}{\myfn{Anc}{\V{bad},\V{base}}} \subseteq \myfn{Anc}{\V{bad},\V{base}}$
    \bcuz{} \Eref{lem-generation-surface-containment-210}.
}
\mypareq{eq:lem-generation-surface-containment-270}{%
    $\myfn{Surface}{\myfn{Anc}{\V{bad},\V{base}}}
        \cap \myfn{Anc}{\Vdecr{bad},\V{base}} = \emptyset$
        \bcuz{} \longLmref{Surface Disjointness}{surface-disjointness},
            \Eref{lem-generation-surface-containment-250}.
}
\mypareq{eq:lem-generation-surface-containment-275}{%
    $\forall\; \Vnodset{small} : \forall\; \Vnodset{big} : \forall\; \Vnodset{other} :$
    \linebreak $\var{small} \subseteq \var{big} \land \var{small} \cap \V{other} = \emptyset$
    \linebreak $\implies \var{small} \subseteq \subtract{\var{big}}{\V{other}}$
    \bcuz{} Fact of set theory.
}
\mypareq{eq:lem-generation-surface-containment-280}{%
    $\myfn{Surface}{\myfn{Anc}{\V{bad},\V{base}}} \subseteq
        \subtract{\myfn{Anc}{\V{bad},\V{base}}}
        {\myfn{Anc}{\Vdecr{bad},\V{base}}}$
    \bcuz{} \Eref{lem-generation-surface-containment-260},
        \Eref{lem-generation-surface-containment-270},
        \Eref{lem-generation-surface-containment-275}.
}
\mypareq{eq:lem-generation-surface-containment-290}{%
    $\myfn{Surface}{\myfn{Anc}{\V{bad},\V{base}}} \subseteq
        \myfn{gen}{\V{bad},\V{base}}$
    \bcuz{} Def of \fname{gen} \Ldref{generation},
        \Eref{lem-generation-surface-containment-280}.
        \linebreak This is the expansion of
        \Eref{lem-generation-surface-containment-100}
        as \Vmyfn{INST}{bad}.}
\Eref{lem-generation-surface-containment-290} is contrary to
\Eref{lem-generation-surface-containment-110}
which shows the reductio.
From the reductio, we conclude that there is no counter-example
and the lemma follows from the universal generalization of \V{base}.
\myqed
\end{proof}

\begin{lemma}[Generation Union Tree Containment]
\label{lem:generation-union-tree-containment}
The union of all generations of the leaves
whose level is less than the node count
of \V{G},
is the set of nodes in \V{G}.
That is,
\begin{equation}
\label{eq:lem-generation-union-tree-containment-050}
\V{V} = \bigcup \set{
    \begin{gathered}
        \V{g} \in \Vpowerset{V} : \forall\; \Vnat{N} < \Vsize{V} :
            \\ \V{g} = \myfn{gen}{\V{N},\V{leaves}}
    \end{gathered}}.  \quad \thEnd
\end{equation}
\end{lemma}

\begin{proof}
The proof is direct.
It follows immediately from the statement of the theorem that
every element of the union of generations is an element of \V{V}:
\begin{equation}
\label{eq:lem-generation-union-tree-containment-100}
\V{V} \subseteq \bigcup \set{
    \begin{gathered}
        \V{g} \in \Vpowerset{V} : \forall\; \Vnat{N} < \Vsize{V} :
            \\ \V{g} = \myfn{gen}{\V{N},\V{leaves}}
    \end{gathered}}
    \because \Eref{lem-generation-union-tree-containment-050}.
\end{equation}
We now proceed to
to prove the converse of
\Eref{lem-generation-union-tree-containment-100}.
\begin{equation}
\label{eq:lem-generation-union-tree-containment-130}
\begin{gathered}
\forall\; \V{nd} \in \V{V} :
\exists\; \V{lpath} \in \Vmyfn{paths}{G} :
\\ \Velement{lpath}{0} = \V{nd}
\\ \land\; \Velement{lpath}{\Vlastix{lpath}} \in \V{leaves}.
\\ \because \longThref{Tree Leaf Path Theorem}{tree-leaf-path}.
\end{gathered}
\end{equation}
Let \Vnod{nd} be an arbitrary node.
\begin{equation}
\label{eq:lem-generation-union-tree-containment-140}
\begin{gathered}
\exists\; \V{lpath} \in \Vmyfn{paths}{G} :
\\ \Velement{lpath}{0} = \V{nd}
\\ \land\; \Velement{lpath}{\Vlastix{lpath}} \in \V{leaves}
\\ \myparbox{\cuz{}
    UI of \Eref{lem-generation-union-tree-containment-130}
    with new free \V{nd}.}
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:lem-generation-union-tree-containment-150}
\V{lpath} \in \Vmyfn{paths}{G} :
\\ \label{eq:lem-generation-union-tree-containment-160}
    \land\; \Velement{lpath}{0} = \V{nd}
\\ \label{eq:lem-generation-union-tree-containment-170}
    \land\; \Velement{lpath}{\Vlastix{lpath}} \in \V{leaves}
\\ \nonumber
    \myparbox{\cuz{}
        EI of bound \V{lpath}
        in \Eref{lem-generation-union-tree-containment-140}
        with new free \V{lpath}.}
\end{gather}
By the Leaf Path Finiteness Lemma,
every path of a tree is of length at most \Vsize{V},
so that
\mypareq{eq:lem-generation-union-tree-containment-180}{%
    $\Vsize{lpath} \le \Vsize{V}$
    \cuz{} \Thref{tree-path-maximum}.
}
\begin{equation}
\label{eq:lem-generation-union-tree-containment-300}
\begin{gathered}
\forall\; \Vnat{N} :
\forall\; \V{path} \in \Vmyfn{paths}{G} :
\forall\; \Vnodset{base} :
\\ \Vlastix{path} \le \V{N}
    \land \Velement{path}{\Vlastix{path}} \in \V{base}
\\ \implies \Velement{path}{0} \in \myfn{Anc}{\V{N}, \V{base}}
\\ \because \longLmref{Ancestry from Path}{ancestry-from-path}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-union-tree-containment-310}
\begin{gathered}
\V{lpath} \in \Vmyfn{paths}{G}
\\ \land\; \Vlastix{lpath} < \Vsize{V}
\\ \land\; \Velement{lpath}{\Vlastix{lpath}} \in \Vnodset{leaves}
\\ \implies \Velement{lpath}{0} \in \bigfn{Anc}{\decr{\Vsize{V}}, \V{leaves}}
\\ \myparbox{\cuz{}
    UI in \Eref{lem-generation-union-tree-containment-300}
    of bound \Vnat{N} with \decr{\Vsize{V}},
    \linebreak of bound \V{path} with \V{lpath}, and
    \linebreak of bound \Vnodset{base} with \Vnodset{leaves}.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-generation-union-tree-containment-320}{%
    $\Velement{lpath}{0} \in \bigfn{Anc}{\decr{\Vsize{V}}, \V{leaves}}$
    \bcuz{} \Eref{lem-generation-union-tree-containment-150},
        \Eref{lem-generation-union-tree-containment-170},
        \Eref{lem-generation-union-tree-containment-180},
        \Eref{lem-generation-union-tree-containment-310}.}
\mypareq{eq:lem-generation-union-tree-containment-330}{%
    $\Vnod{nd} \in \bigfn{Anc}{\decr{\Vsize{V}}, \V{leaves}}$
    \cuz{} \Eref{lem-generation-union-tree-containment-160},
        \Eref{lem-generation-union-tree-containment-320}.}
\mypareq{eq:lem-generation-union-tree-containment-340}{%
    $\Vnod{nd} \in \bigcup \set{ \forall\; \Vnat{i} < \Vsize{V} : \myfn{gen}{\V{i},\V{leaves}}}$
    \bcuz{} \longLmref{Generation Coverage}{generation-coverage}.}
Recall that \V{nd} was an arbitrary element of \V{V}
\Eref{lem-generation-union-tree-containment-140}.
The lemma follows from \Eref{lem-generation-union-tree-containment-100}
and the universal generalization of \V{nd} in
\Eref{lem-generation-union-tree-containment-340}.
\myqed
\end{proof}

\begin{lemma}[Generation Sum Size]
\label{lem:generation-sum-size}
If a set of generations share the same base,
the sum of their sizes is equal to
the size of their union.
That is if \V{genset} is a set of generations such that
\[
    \V{genset} =
        \set{ \begin{gathered}
            \Vnodset{nds} \in \Vpowerset{V} :
            \\ \V{nds} = \myfn{gen}{\V{i},\V{base}}
                \land \Vnat{i} \in \powerset{\naturals}
        \end{gathered} }
\]
then
\begin{equation}
    \size{\bigcup \Vnodset{genset} } =
    \sum \family{ \Vnodset{gen} \in \V{genset} : \Vsize{gen} }.
    \quad \thEnd
\end{equation}
\end{lemma}

\begin{proof}
We know from set theory that the sum of the sizes of a collection
of disjoint sets is equal to the size of the union of those sets.
By \Lmref{generation-disjointness}, the generations are disjoint.
The lemma follows from these two facts.
\myqed
\end{proof}

\begin{lemma}[Generation Addition]
\label{lem:generation-addition}
Let a first generation be the base of a second generation.
The second generation is equal to a third generation that
has the same base as the first generation and whose
level is the sum of the levels of
the first and second generation.
That is,
\begin{equation}
\label{eq:lem-generation-addition-050}
    \myfn{gen}{\V{x}, \myfn{gen}{\V{y}, \V{base}}}
    = \myfn{gen}{\V{x}+\V{y},\V{base}}.
    \quad \thEnd
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counter-example.
Let
\begin{equation}
\label{eq:lem-generation-addition-100}
\begin{gathered}
    \Vmyfn{INST}{ex} \ldefined
    \\ \myfn{gen}{\V{ex}, \myfn{gen}{\V{y}, \V{base}}}
        = \myfn{gen}{\V{ex}+\V{y},\V{base}}
    \\ \text{\cuz{} \Eref{lem-generation-addition-050}
         with \Vnat{ex} for \Vnat{x}.
         New free \Vnat{y}.
         New free \Vnodset{base}.}
\end{gathered}
\end{equation}
We assume there is a minimal counter-example to the lemma, \Vnat{bad}:
\begin{gather}
\label{eq:lem-generation-addition-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-generation-addition-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \myparbox{\cuz{} AF reductio.
    New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-generation-addition-170}{%
    $\V{base} = \myfn{gen}{0,\V{base}}$
    \bcuz{} Def of \fname{gen} \Ldref{generation}.
}
\begin{equation}
\label{eq:lem-generation-addition-180}
\begin{gathered}
    \myfn{gen}{0, \myfn{gen}{\V{y}, \V{base}}}
        = \myfn{gen}{\V{y},\V{base}}
        = \myfn{gen}{0+\V{y},\V{base}}
    \\ \myparbox{\cuz{} \Eref{lem-generation-addition-170}.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-generation-addition-190}{%
    $\myfn{INST}{0}$ \cuz{}
        \Eref{lem-generation-addition-100},
        \Eref{lem-generation-addition-180}.
}
\mypareq{eq:lem-generation-addition-200}{%
    $\V{bad} > 0$ \cuz{} \Eref{lem-generation-addition-110},
        \Eref{lem-generation-addition-190}.}
\mypareq{eq:lem-generation-addition-210}{%
    \myfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    = \myfn{Anc}{\V{bad}+\V{y},\V{base}}
    \bcuz{} \longLmref{Ancestry Addition}{ancestry-addition}.}
\begin{equation}
\label{eq:lem-generation-addition-220}
\begin{gathered}
    \myfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \\ = \bigcup \set{ \forall\; \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i}, \myfn{Anc}{\V{y}, \V{base}}}}
    \\ = \left( \begin{gathered}
            \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}} \;\cup
            \\ \bigcup \set{ \forall\; \Vnat{i} \le \Vdecr{bad} :
                \myfn{gen}{\V{i}, \myfn{Anc}{\V{y}, \V{base}}}}
        \end{gathered} \right)
    \\ \because \longLmref{Generation Coverage}{generation-coverage},
        \Eref{lem-generation-addition-200}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-addition-230}
\begin{gathered}
    \myfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \\ = \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
             \cup \myfn{Anc}{\Vdecr{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \\ \because \Lmref{generation-coverage},
        \Eref{lem-generation-addition-220}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-addition-240}
\begin{gathered}
    \myfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \\ = \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
             \cup \bigfn{Anc}{ (\Vdecr{bad}) + \V{y}, \V{base}}
    \\ \because \longLmref{Ancestry Addition}{ancestry-addition},
        \Eref{lem-generation-addition-230}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-addition-245}
\begin{gathered}
    \myfn{Anc}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \\ = \left( \begin{gathered}
        \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}} \;\cup
        \\ \bigcup \set{ \forall\; \Vnat{i} \le (\Vdecr{bad})+\V{y} :
                \myfn{gen}{\V{i}, \V{base}}}
        \end{gathered} \right)
    \\ \because \longLmref{Generation Coverage}{generation-coverage},
        \Eref{lem-generation-addition-240}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-addition-250}
\begin{gathered}
    \myfn{Anc}{\V{bad}+\V{y}, \V{base}}
    \\ = \bigcup \set{ \forall\; \Vnat{i} \le \V{bad}+\V{y} : \myfn{gen}{\V{i}, \V{base}}}
    \\ = \left( \begin{gathered}
            \myfn{gen}{\V{bad}+\V{y}, \V{base}} \;\cup
            \\ \bigcup \set{ \forall\; \Vnat{i} \le (\Vdecr{bad})+\V{y} :
                \myfn{gen}{\V{i}, \V{base}}}
        \end{gathered} \right)
    \\ \because \longLmref{Generation Coverage}{generation-coverage},
        \Eref{lem-generation-addition-200}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-generation-addition-255}
\begin{gathered}
    \left( \begin{gathered}
        \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}} \;\cup
        \\ \bigcup \set{ \forall\; \Vnat{i} \le (\Vdecr{bad})+\V{y} :
                \myfn{gen}{\V{i}, \V{base}}}
        \end{gathered} \right)
    \\ =\; \left( \begin{gathered}
            \myfn{gen}{\V{bad}+\V{y}, \V{base}} \;\cup
            \\ \bigcup \set{ \forall\; \Vnat{i} \le (\Vdecr{bad})+\V{y} :
                \myfn{gen}{\V{i}, \V{base}}}
        \end{gathered} \right)
    \\ \because \Eref{lem-generation-addition-210},
        \Eref{lem-generation-addition-245},
        \Eref{lem-generation-addition-250}.
\end{gathered}
\end{equation}
It is a fact of set theory that unions of pairwise disjoint elements
can be manipulated like summations,
in the sense that
we can ``cancel'' identical elements on both sides of the identity.
Therefore,
\begin{equation}
\label{eq:lem-generation-addition-260}
\begin{gathered}
        \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    = \myfn{gen}{\V{bad}+\V{y}, \V{base}}
    \\ \because \longLmref{Generation Disjointness}{generation-disjointness},
        \Eref{lem-generation-addition-255}.
\end{gathered}
\end{equation}
But \Eref{lem-generation-addition-260}
is contrary to the expansion of $\neg \Vmyfn{INST}{bad}$,
which we assumed for reductio:
\begin{equation}
\label{eq:lem-generation-addition-270}
\begin{gathered}
        \myfn{gen}{\V{bad}, \myfn{Anc}{\V{y}, \V{base}}}
    \neq \myfn{gen}{\V{bad}+\V{y}, \V{base}}
    \\ \because \Eref{lem-generation-addition-110}.
\end{gathered}
\end{equation}
From the contradiction of \Eref{lem-generation-addition-260}
and \Eref{lem-generation-addition-270}
we conclude the reductio from
\Eref{lem-generation-addition-110}--%
\Eref{lem-generation-addition-120} to
\Eref{lem-generation-addition-270}.
From the reductio, we conclude that there is no counter-example,
which shows the lemma.
\myqed
\end{proof}

\section{Non-unary tree size}

\begin{definition}[Non-unary tree]
\label{def:non-unary-tree}
A tree is \dfn{non-unary} iff
no node of the tree has an
out-degree of 1.
In other words, in a non-unary tree, every node is either a leaf or has
two or more child nodes.
\end{definition}

\begin{lemma}[Surface New Parents Inclusion]
\label{lem:surface-new-parents-inclusion}
For all subsets of the nodes of tree,
every child of a new parent of that subset
is an element of the surface.
That is,
\begin{gather}
\label{eq:surface-new-parents-inclusion-050}
    \forall\; \Vnodset{ndset} : \forall\; \Vnod{parent} : \forall\; \Vnod{child} :
    \\ \label{eq:surface-new-parents-inclusion-060}
        \tuple{\V{parent},\V{child}} \in \V{E} \land \V{parent} \in \Vmyfn{NewParents}{ndset}
    \\ \label{eq:surface-new-parents-inclusion-070}
        \implies\; \V{child} \in \Vmyfn{Surface}{ndset}.
\end{gather}
\end{lemma}

\begin{proof}
The proof strategy is direct.
The hypothesis \Eref{surface-new-parents-inclusion-060}
of the implication is assumed,
to show the implication's consequent \Eref{surface-new-parents-inclusion-070}.
The theorem is then obtained using universal generalization.

\begin{gather}
\label{eq:surface-new-parents-inclusion-120}
\tuple{\Vnod{parent},\Vnod{child}} \in \V{E}
\\ \label{eq:surface-new-parents-inclusion-130}
    \land\; \V{parent} \in \Vmyfn{NewParents}{nds}
\\ \nonumber \myparbox{\cuz{}
    Hypothesis of lemma \Eref{surface-new-parents-inclusion-060},
        AF implication.
        New free \Vnod{parent}, new free \Vnod{child},
        new free \Vnodset{nds}.}
\end{gather}
\begin{equation}
\label{eq:surface-new-parents-inclusion-150}
\begin{gathered}
    \exists\; \Vnod{child} : \V{child} \in \V{nds} \land \V{parent} \notin \V{nds}
    \\ \land\; \tuple{\V{parent},\V{child}} \in \V{E}
    \\ \myparbox{\cuz{}
        \longLdref{\fname{NewParents}}{new-parents},
        \Eref{surface-new-parents-inclusion-130}.}
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:surface-new-parents-inclusion-170}
    \V{child} \in \V{nds}
\\ \label{eq:surface-new-parents-inclusion-180}
    \land\; \V{parent} \notin \V{nds}
\\ \label{eq:surface-new-parents-inclusion-190}
    \land\; \tuple{\V{parent},\V{child}} \in \V{E}
\\ \nonumber
    \myparbox{\cuz{}
        EI of \Vnod{child}
        bound in \Eref{surface-new-parents-inclusion-150}
        with existing \Vnod{child}.}
\end{gather}
\mypareq{eq:surface-new-parents-inclusion-200}{%
    $\exists\; \Vnod{parent} : \V{parent} \notin \V{nds}
    \land \tuple{\V{parent},\V{child}} \in \V{E}$
    \bcuz{}
        EG of \Vnod{parent}
        in \Eref{surface-new-parents-inclusion-180} and
        \Eref{surface-new-parents-inclusion-190}.}
\mypareq{eq:surface-new-parents-inclusion-210}{%
    $\V{child} \in \Vmyfn{Surface}{nds}$
    \bcuz{} Def of \fname{Surface} \Eref{def-surface},
        \Eref{surface-new-parents-inclusion-170},
        \Eref{surface-new-parents-inclusion-200}.
}
\begin{equation}
\label{eq:surface-new-parents-inclusion-220}
\begin{gathered}
        \tuple{\V{parent},\V{child}} \in \V{E} \land \V{parent} \in \Vmyfn{NewParents}{nds}
    \\ \implies\; \V{child} \in \Vmyfn{Surface}{nds}
    \\ \myparbox{\cuz{}
                Implication from \Eref{surface-new-parents-inclusion-120}--%
                    \Eref{surface-new-parents-inclusion-130}
                to \Eref{surface-new-parents-inclusion-210}.}
\end{gathered}
\end{equation}
The theorem then follows from
\begin{itemize}
\item the UG of \Vnod{child} in \Eref{surface-new-parents-inclusion-220}
    to a new, bound \Vnod{child};
\item the UG of \Vnod{parent} in \Eref{surface-new-parents-inclusion-220}
    to a new, bound \Vnod{parent}; and
\item the UG of \Vnodset{nds} in \Eref{surface-new-parents-inclusion-220}
    to a bound \Vnodset{ndset}.  \myqed
\end{itemize}
\end{proof}

\begin{lemma}[Surface--Parents Ratio]
\label{lem:surface-parents-ratio}
For any subset of the nodes of a non-unary tree,
the surface of the subset is at least twice the size of
the subset's set of direct parents.
That is, if \V{G} is non-unary,
then
\begin{equation}
\label{eq:lem-surface-parents-ratio-050}
\begin{gathered}
    \forall\; \V{ndset} \in \Vpowerset{V} :
    \\ \size{\myfn{Surface}{\V{ndset}}} \ge
        2 \times
        \size{\myfn{NewParents}{\V{ndset}}}.
        \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is direct.
By definition,
every node in \Vmyfn{NewParents}{ndset} must have at least one
child:
\mypareq{eq:lem-surface-parents-ratio-170}{%
    $\forall\; \V{parent} \in \Vmyfn{NewParents}{ndset} :
                 \size{\Vmyfn{children}{parent}} \ge 1$
    \bcuz{} \longLdref{\fname{NewParents}}{new-parents},
    New free $\V{ndset} \in \Vpowerset{V}$.
}
In fact, since \V{G} is a non-unary tree,
every direct parent node of \V{ndset} must have at least two
children:
\mypareq{eq:lem-surface-parents-ratio-180}{%
    $\forall\; \V{parent} \in \Vmyfn{NewParents}{ndset} :
                 \size{\Vmyfn{children}{parent}} \ge 2$
    \bcuz{} \longDfref{Non-unary Tree}{non-unary-tree},
            \Eref{lem-surface-parents-ratio-170}.
}
Recall from the definition of a tree that
every node has at most one parent:
\begin{equation}
\label{eq:lem-surface-parents-ratio-190}
\begin{gathered}
    \forall\; \V{parent1} \in \V{V} :
    \forall\; \V{parent2} \in \V{V} :
    \forall\; \V{child} \in \V{V} :
    \\ \tuple{\V{parent1},\V{child}} \in \V{E}
    \land \tuple{\V{parent2},\V{child}} \in \V{E}
    \\ \implies \V{parent1} = \V{parent2}
    \\ \because \text{Def of tree, graph theory.}
\end{gathered}
\end{equation}
Therefore every parent's set of children is disjoint
from every other parent's set of children:
\begin{equation}
\label{eq:lem-surface-parents-ratio-200}
\begin{gathered}
    \forall\; \V{parent1} \in \V{V} :
    \forall\; \V{parent2} \in \V{V} :
    \\ \Vmyfn{children}{parent1}
    \cap \Vmyfn{children}{parent2} = \emptyset
    \\ \because \longLdref{\myfn{children}{}}{parents},
        \Eref{lem-surface-parents-ratio-190}.
\end{gathered}
\end{equation}
It follows that, for
the sets of children of the elements of any set of nodes,
the cardinality of their union
is equal to the sum of their cardinalities:
\begin{equation}
\label{eq:lem-surface-parents-ratio-210}
\begin{gathered}
\forall\; \Vnodset{ndset1} :
\\ \size{\bigcup \set{ \Vmyfn{children}{parent} : \V{parent} \in \V{ndset1} }}
\\ =\; \sum \family{\V{parent} \in \V{ndset1} : \size{\Vmyfn{children}{parent}} }
\\ \because \Eref{lem-surface-parents-ratio-200}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-surface-parents-ratio-220}
\begin{gathered}
\size{\bigcup \set{ \Vmyfn{children}{parent} : \V{parent} \in \Vmyfn{NewParents}{ndset} }}
\\ =\; \sum \family{\V{parent} \in \Vmyfn{NewParents}{ndset} : \size{\Vmyfn{children}{parent}} }
\\ \myparbox{\cuz{}
    \Eref{lem-surface-parents-ratio-210}, UI with \Vmyfn{NewParents}{ndset}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-surface-parents-ratio-230}
\begin{gathered}
\size{\bigcup \set{ \Vmyfn{children}{parent} : \V{parent} \in \Vmyfn{NewParents}{ndset} }}
\\ \ge\; 2 \times \size{\Vmyfn{NewParents}{ndset}}
\\ \because \Eref{lem-surface-parents-ratio-180},
        \Eref{lem-surface-parents-ratio-220}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-surface-parents-ratio-240}
\begin{gathered}
\size{\set{
    \begin{gathered}
        \V{child} \in \V{V} :
        \\ \tuple{\V{parent},\V{child}} \in \V{E}
        \\ \land\; \V{parent} \in \Vmyfn{NewParents}{ndset}
    \end{gathered} }}
\\ \ge\; 2 \times \size{\Vmyfn{NewParents}{ndset}}
\\ \because \longLdref{\myfn{children}{}}{parents},
        \Eref{lem-surface-parents-ratio-230}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-surface-parents-ratio-250}
\begin{gathered}
\set{
    \begin{gathered}
        \V{child} \in \V{V} :
        \\ \tuple{\V{parent},\V{child}} \in \V{E}
        \\ \land\; \V{parent} \in \Vmyfn{NewParents}{ndset}
    \end{gathered} }
\\ \subseteq\; \Vmyfn{Surface}{ndset}
\\ \because \longLmref{Surface New Parents Inclusion}{surface-new-parents-inclusion},
        \Eref{lem-surface-parents-ratio-240}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-surface-parents-ratio-260}
\begin{gathered}
\size{\Vmyfn{Surface}{ndset}} \ge 2 \times \size{\Vmyfn{NewParents}{ndset}}
\\ \because \Eref{lem-surface-parents-ratio-240},
        \Eref{lem-surface-parents-ratio-250}.
\end{gathered}
\end{equation}
The lemma is the universal generalization of
\Eref{lem-surface-parents-ratio-260} over \V{ndset}.
\myqed
\end{proof}

\begin{lemma}[Generation Predecessor Size]
\label{lem:generation-predecessor-size}
For the nodes of a non-unary tree,
every generation of a node set
is at least twice the size
of the successor generation with the same base.
That is, if \V{G} is non-unary,
\begin{equation}
\label{eq:lem-generation-predecessor-size-050}
\begin{gathered}
     \forall\; \Vnat{N} : \forall\; \V{base} \in \Vpowerset{V} :
     \\ \size{\myfn{gen}{\V{N},\V{base}}} \ge
     2 \times \size{\myfn{gen}{\V{N}+1,\V{base}}}.
     \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is direct.
Let \V{base} be an arbitrary subset of the nodes of
an arbitrary non-unary tree,
and let \V{N} be an arbitrary natural number.
Then
\mypareq{eq:lem-generation-predecessor-size-100}{%
    $\size{\myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}}} \ge
        2 \times
        \size{\myfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}}}$
    \bcuz{} \longLmref{Surface--Parents ratio}{surface-parents-ratio}.
}
\mypareq{eq:lem-generation-predecessor-size-110}{%
    $\myfn{gen}{\V{N}+1,\V{base}} =
        \myfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}}$
    \bcuz{} \longLmref{Generation New Parent Equivalence}
        {generation-new-parent-equivalence}.
}
\mypareq{eq:lem-generation-predecessor-size-120}{%
    $\size{\myfn{NewParents}{\myfn{Anc}{\V{N},\V{base}}}} =
        \size{\myfn{gen}{\V{N}+1,\V{base}}}$
    \bcuz{} \Eref{lem-generation-predecessor-size-110}.
}
\mypareq{eq:lem-generation-predecessor-size-130}{%
    $\size{\myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}}} \ge
        2 \times
        \size{\myfn{gen}{\myfn{Anc}{\V{N}+1,\V{base}}}}$
    \bcuz{} \Eref{lem-generation-predecessor-size-100},
        \Eref{lem-generation-predecessor-size-120}.
}
\mypareq{eq:lem-generation-predecessor-size-140}{%
    $\myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}} \subseteq \myfn{gen}{\V{N},\V{base}}$
    \bcuz{} \longLmref{Generation Surface Containment}{generation-surface-containment}
}
\mypareq{eq:lem-generation-predecessor-size-150}{%
    $\size{\myfn{gen}{\V{N},\V{base}}} \ge \size{\myfn{Surface}{\myfn{Anc}{\V{N},\V{base}}}}$
    \bcuz{} \Eref{lem-generation-predecessor-size-140}.
}
\mypareq{eq:lem-generation-predecessor-size-160}{%
    $\size{\myfn{gen}{\V{N},\V{base}}} \ge
        2 \times
        \size{\myfn{gen}{\V{N}+1,\V{base}}}$
    \bcuz{}
        \Eref{lem-generation-predecessor-size-130},
        \Eref{lem-generation-predecessor-size-150}.
}
The lemma is the universal generalization of
\Eref{lem-generation-predecessor-size-160} for \Vnat{N}
and \V{base}.
\myqed
\end{proof}

\begin{lemma}[Non-unary Subtree Size]
\label{lem:non-unary-subtree-size}
For the purposes of this lemma, let \V{G} be non-unary.
Let \V{base} be a subset of \V{V},
and let $\V{N}\in\naturals$.
Then
\begin{equation}
\label{eq:lem-non-unary-subtree-size-050}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{N} : \myfn{gen}{\V{i},\V{base}} }
    }
    \le\; 2\times\Vsize{base}.
    \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
The proof is by minimal counter-example.
Where
\begin{equation}
\label{eq:lem-non-unary-subtree-size-100}
\begin{gathered}
    \Vmyfn{INST}{ex} \defined
    \\ \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{ex} : \myfn{gen}{\V{i},\V{base}} }
    }
        \le\; 2\times\Vsize{base}
    \\ \text{\cuz{} \Eref{lem-non-unary-subtree-size-050}
         with \Vnat{ex} for \Vsize{N}.}
\end{gathered}
\end{equation}
we assume there is a minimal \V{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:lem-non-unary-subtree-size-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:lem-non-unary-subtree-size-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:lem-non-unary-subtree-size-130}{%
    $\myfn{gen}{0,\V{base}} = \V{base}$
    \cuz{} Def of \fname{gen} \Ldref{generation}.
}
\mypareq{eq:lem-non-unary-subtree-size-140}{%
    $\size{\myfn{gen}{0,\V{base}}} \le 2\times\Vsize{base}$
        \cuz{} \Eref{lem-non-unary-subtree-size-130}.
}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-150}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le (0) : \myfn{gen}{\V{i},\V{base}} }
    }
        \le\; 2\times\Vsize{base}
    \\ \text{\cuz{} \Eref{lem-non-unary-subtree-size-140}.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-non-unary-subtree-size-160}{%
    \myfn{INST}{0} \cuz{} \Eref{lem-non-unary-subtree-size-100},
        \Eref{lem-non-unary-subtree-size-150}.
}
\mypareq{eq:lem-non-unary-subtree-size-200}{%
    $\V{bad} > 0$ \cuz{}
        \Eref{lem-non-unary-subtree-size-110},
        \Eref{lem-non-unary-subtree-size-160}.
}
\mypareq{eq:lem-non-unary-subtree-size-210}{%
    $\size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \sum \set{ 0 \le \Vnat{i} \le \V{bad} : \size{\myfn{gen}{\V{i},\V{base}}} }$
    \bcuz{} \longLmref{Generation Sum Size}{generation-sum-size}.
}
\mypareq{eq:lem-non-unary-subtree-size-220}{%
    $\size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \size{\myfn{gen}{0,\V{base}}} +
    \sum \set{ 1 \le \Vnat{i} \le \V{bad} : \size{\myfn{gen}{\V{i},\V{base}}} }$
    \bcuz{} \Eref{lem-non-unary-subtree-size-200},
        \Eref{lem-non-unary-subtree-size-210}.
}
\mypareq{eq:lem-non-unary-subtree-size-230}{%
    $\size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \Vsize{base} +
    \sum \set{ 1 \le \Vnat{i} \le \V{bad} : \size{\myfn{gen}{\V{i},\V{base}}} }$
    \bcuz{} Def of \fname{gen} \Ldref{generation},
        \Eref{lem-non-unary-subtree-size-220}.
}
\mypareq{eq:lem-non-unary-subtree-size-240}{%
    $\size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \Vsize{base} +
    \sum \set{ 0 \le \Vnat{i} \le \Vdecr{bad} : \size{\myfn{gen}{\V{i}+1,\V{base}}} }$
    \bcuz{} \Eref{lem-non-unary-subtree-size-230}.
}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-250}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \\ \Vsize{base} +
    \sum \set{ 0 \le \Vnat{i} \le \Vdecr{bad} :
        \size{
            \myfn{gen}{\V{i}, \myfn{gen}{1,\V{base}}} }
    }
    \\ \because \longLmref{Generation addition}{generation-addition},
        \Eref{lem-non-unary-subtree-size-240}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-260}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } =
    \\ \Vsize{base} +
    \size{ \bigcup \set{ 0 \le \Vnat{i} \le \Vdecr{bad} :
            \myfn{gen}{\V{i}, \myfn{gen}{1,\V{base}} }
    }}
    \\ \because \longLmref{Generation Sum Size}{generation-sum-size},
        \Eref{lem-non-unary-subtree-size-250}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-270}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } \le
    \\ \Vsize{base} + 2\times\size{
            \myfn{gen}{1,\V{base}} }
    \\ \because \Eref{lem-non-unary-subtree-size-100},
        \Eref{lem-non-unary-subtree-size-120},
        \Eref{lem-non-unary-subtree-size-260}.
\end{gathered}
\end{equation}
\mypareq{eq:lem-non-unary-subtree-size-280}{%
     $\size{\myfn{gen}{0,\V{base}}} \ge
     2 \times \size{\myfn{gen}{1,\V{base}}}$
     \bcuz{}
         \longLmref{Generation Predecessor Size}{generation-predecessor-size}.
}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-290}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } \le
    \\ \Vsize{base} + \size{\myfn{gen}{0,\V{base}} }
    \\ \because \Eref{lem-non-unary-subtree-size-270},
        \Eref{lem-non-unary-subtree-size-280}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:lem-non-unary-subtree-size-300}
\begin{gathered}
    \size{\bigcup
        \set{ 0 \le \Vnat{i} \le \V{bad} : \myfn{gen}{\V{i},\V{base}} }
    } \le
        2\times\Vsize{base}
    \\ \myparbox{\cuz{}
        Def of \fname{gen} \Ldref{generation},
        \Eref{lem-non-unary-subtree-size-290}.}
\end{gathered}
\end{equation}
\mypareq{eq:lem-non-unary-subtree-size-310}{%
    \myfn{INST}{\V{bad}} \cuz{} \Eref{lem-non-unary-subtree-size-100},
        \Eref{lem-non-unary-subtree-size-300}.
}
Equation \Eref{lem-non-unary-subtree-size-310} is contrary to
\Eref{lem-non-unary-subtree-size-110}, which shows the
reductio from
\Eref{lem-non-unary-subtree-size-110}--%\Eref{lem-non-unary-subtree-size-120}
to \Eref{lem-non-unary-subtree-size-310}.
From the reductio we know that there is no counter-example,
which shows the lemma.
\myqed
\end{proof}

\begin{theorem}[Non-unary Tree Size]
\label{th:non-unary-tree-size}
The size of a non-unary tree is at most twice
its leaf count.  That is, where \V{t} is a non-unary tree,
\V{V} is the set of vertices of \V{t},
and $\V{leaves} \subseteq \V{V}$ is the set of leaves in \V{t},
then
\[
     \Vsize{V} \le 2\times\Vsize{leaves}.  \quad \thEnd
\]
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-non-unary-tree-size-100}
\begin{gathered}
\V{V} = \bigcup \set{
    \begin{gathered}
        \V{g} \in \Vpowerset{V} : \forall\; \Vnat{N} < \Vsize{V} :
            \\ \V{g} = \myfn{gen}{\V{N},\V{leaves}}
    \end{gathered}}
    \\ \myparbox{\cuz{}
        \longLmref{Generation Union Tree Containment}{generation-union-tree-containment}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-non-unary-tree-size-200}
\begin{gathered}
 \size{ \bigcup \set{
    \begin{gathered}
        \V{g} \in \Vpowerset{V} : \forall\; \Vnat{N} < \Vsize{V} :
            \\ \V{g} = \myfn{gen}{\V{N},\V{leaves}}
    \end{gathered}}}
        \le 2\times\V{leaves}.
    \\ \myparbox{\cuz{}
        \longLmref{Non-unary Subtree Size}{non-unary-subtree-size}.}
\end{gathered}
\end{equation}
The theorem follows from
\Eref{th-non-unary-tree-size-100}
and \Eref{th-non-unary-tree-size-200}.
\myqed
\end{proof}

\section{Unary paths}

\begin{definition}[Unary edge]
\label{def:unary-edge}
An edge is a unary edge iff it is the only
edge in its graph with its parent.
That is,
where $\V{G} = \tuple{\V{V}, \V{E}}$ is a graph,
then the edge $\V{edge}\in\V{E}$ is unary iff
\[
    \V{edge} = \tuple{ \Rvar{parent}{V},\Rvar{child}{V}}
        \land \Vmyfn{children}{parent} = \set{\V{child}}.
        \quad \dfEnd
\]
\end{definition}

\begin{definition}[Unary Path]
\label{def:unary-path}
A path is a unary path iff all of its edges are unary edges.
That is,
in the graph $\V{G} = \tuple{\V{V}, \V{E}}$
\Rvarseq{up}{V} is a unary path iff
it is an element of \Vmyfn{Upaths}{G} where
\[
\begin{gathered}
    \myfn{Upaths}{\V{G}} \defined
    \set{
        \begin{gathered}
            \Rvarseq{up}{V} \in \Vmyfn{paths}{G} :
                \forall\; \Vnat{ix} < \Vlastix{up} :
            \\ \myfn{children}{\VVelement{up}{ix}}
                = \set{ \VVelement{up}{\V{ix}+1} }
        \end{gathered}
    }.  \quad \dfEnd
\end{gathered}
\]
\end{definition}

Note that, by \Dfref{unary-path},
every trivial path is a unary path.

\begin{theorem}[Unary Subpath]
\label{th:unary-subpath}
Every slice of a unary path in the graph
$\V{G} = \tuple{\V{V}, \V{E}}$
is a unary path in \V{G}.
That is,
\begin{equation}
\label{eq:th-unary-subpath-050}
\begin{gathered}
\forall\; \Rvarseq{seq}{V} \in \Vmyfn{Upaths}{G} :
\\ \forall\; \Vnat{lo} \; (\V{lo} \le \Vlastix{seq}) :
    \forall\; \Vnat{hi} \; (\V{lo} \le \V{hi} \le \Vlastix{seq}) :
\\ \seq{\VVelement{seq}{lo} \ldots \VVelement{seq}{hi}} \in \Vmyfn{Upaths}{G}.
    \quad \dfEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is direct.
Let $\V{G} = \tuple{\V{V}, \V{E}}$ be a graph,
let \Rvarseq{p}{V} be a unary path in \V{G},
and let \Rvarseq{subp}{V} be a slice of \V{p}:
\mypareq{eq:th-unary-subpath-100}{%
    $\Rvarseq{up}{V} \in \Vmyfn{Upaths}{G}$ \cuz{} New free \Rvarseq{up}{V}.}
\mypareq{eq:th-unary-subpath-110}{%
    $\Vnat{lo} \le \Vnat{hi} \le \Vlastix{up}$ \cuz{} New free \Vnat{lo}, new free \Vnat{hi}.}
\mypareq{eq:th-unary-subpath-120}{%
    $\V{subp} = \seq{ \VVelement{up}{lo} \ldots \VVelement{up}{hi} }$ \cuz{} New free \Rvarseq{subp}{V}.}
By the Subpath Theorem, we know that \V{subp} is a path:
\mypareq{eq:th-unary-subpath-130}{%
    $\V{subp} \in \Vmyfn{paths}{G}$ \cuz{} \Thref{subpath}.}
\begin{equation}
\label{eq:th-unary-subpath-250}
\begin{gathered}
    \forall\; \Vnat{ix} < \Vlastix{up} :
    \myfn{children}{\VVelement{up}{ix}}
                = \set{ \VVelement{up}{\V{ix}+1} }
    \\ \because \longDfref{Unary Path}{unary-path}, \Eref{th-unary-subpath-100}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-unary-subpath-260}
\begin{gathered}
    \forall\; \Vnat{ix} \; (\V{lo} < \V{ix} < \V{hi}) :
    \myfn{children}{\VVelement{up}{ix}}
                = \set{ \VVelement{up}{\V{ix}+1} }
    \\ \because \Eref{th-unary-subpath-110}, \Eref{th-unary-subpath-250}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-unary-subpath-270}
\begin{gathered}
    \forall\; \V{ix} < \Vlastix{subp} :
    \myfn{children}{\VVelement{subp}{ix}}
                = \set{ \VVelement{subp}{\V{ix}+1} }
    \\ \because \Eref{th-unary-subpath-120}, \Eref{th-unary-subpath-260}.
\end{gathered}
\end{equation}
\mypareq{eq:th-unary-subpath-280}{%
    $\V{subp} \in \Vmyfn{Upaths}{G}$
    \bcuz{} \longDfref{Unary Path}{unary-path}, \Eref{th-unary-subpath-100},
            \Eref{th-unary-subpath-270}. \myqed}
\end{proof}

\begin{definition}[Unary Suffix Path]
\label{def:unary-suffix}
A unary path \V{suff} is a \dfn{unary suffix path},
or \dfn{unary suffix},
with respect to the vertex
which is its head.
That is,
in the graph $\V{G} = \tuple{\V{V}, \V{E}}$,
the unary path
$\Rvarseq{suff}{V} \in \Vmyfn{Upaths}{G}$ is a unary suffix of $\V{nd} \in \V{V}$
iff $\Velement{suff}{0} = \V{nd}$.
\dfEnd
\end{definition}

\begin{theorem}[Unary Suffix Uniqueness]
\label{th:unary-suffix-uniqueness}
Two unary suffixes of the same node and same length
are identical.
That is,
where $\V{G} = \tuple{\V{V}, \V{E}}$ is a graph,
then
\begin{equation}
\label{eq:th-unary-suffix-uniqueness-050}
\begin{gathered}
\forall\; \Rvarseq{suf1}{V} \in \Vmyfn{Upaths}{G} :
\forall\; \Rvarseq{suf2}{V} \in \Vmyfn{Upaths}{G} :
\\ \Velement{suf1}{0} = \Velement{suf2}{0}
    \land\; \Vlastix{suf1} = \Vlastix{suf2}
\implies \V{suf1} = \V{suf2}.  \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is by minimal counter-example.
Let $\V{G} = \tuple{\V{V}, \V{E}}$ be a new free graph.
\begin{equation}
\label{eq:th-unary-suffix-uniqueness-105}
\begin{gathered}
\Vmyfn{INST}{\Vnat{ex}} \ldefined
\\ \forall\; \Rvarseq{suf1}{V} \in \Vmyfn{Upaths}{G} :
    \forall\; \Rvarseq{suf2}{V} \in \Vmyfn{Upaths}{G} :
\\ \Velement{suf1}{0} = \Velement{suf2}{0}
        \land \Vlastix{suf1} = \Vlastix{suf2} = \V{ex}
\\ \implies \V{suf1} = \V{suf2}
    \\ \myparbox{\cuz{}
        Macro for EI for adaptation of \Eref{th-unary-suffix-uniqueness-050}
         setting $\Vnat{ex}=\Vlastix{suf2}$.}
\end{gathered}
\end{equation}
We assume there is a minimal \Vnat{bad} such that
\Vmyfn{INST}{bad} is false:
\begin{gather}
\label{eq:th-unary-suffix-uniqueness-110}
    \neg \Vmyfn{INST}{bad}
\\ \label{eq:th-unary-suffix-uniqueness-120}
    \land\; \forall\; \Vnat{i} < \V{bad} : \Vmyfn{INST}{i}
\\ \nonumber
    \text{\cuz{} AF reductio.  New free \Vnat{bad}.}
\end{gather}
\mypareq{eq:th-unary-suffix-uniqueness-130}{%
$\Velement{sufZ1}{0} = \Velement{sufZ2}{0}
        \land \Vlastix{sufZ1} = \Vlastix{sufZ2} = 0$
    \bcuz{} AF implication.
    New free \Rvarseq{sufZ1}{V}.
    New free \Rvarseq{sufZ2}{V}.
}
\mypareq{eq:th-unary-suffix-uniqueness-140}{%
    $\V{sufZ1} = \V{sufZ2}$
    \cuz{} \Eref{th-unary-suffix-uniqueness-130}.}
\mypareq{eq:th-unary-suffix-uniqueness-150}{%
    $\myfn{INST}{0}$
    \cuz{} Implication from
        \Eref{th-unary-suffix-uniqueness-130}--%
        \Eref{th-unary-suffix-uniqueness-140};
        Macrofication using
        \Eref{th-unary-suffix-uniqueness-105}.}
\mypareq{eq:th-unary-suffix-uniqueness-160}{%
    $\V{bad} > 0$
    \cuz{} \Eref{th-unary-suffix-uniqueness-110},
        \Eref{th-unary-suffix-uniqueness-150}.
}
\begin{equation}
\label{eq:th-unary-suffix-uniqueness-170}
\begin{gathered}
\exists\; \Rvarseq{suf1}{V} \in \Vmyfn{Upaths}{G} :
    \exists\; \Rvarseq{suf2}{V} \in \Vmyfn{Upaths}{G} :
\\ \Velement{suf1}{0} = \Velement{suf2}{0}
        \land \Vlastix{suf1} = \Vlastix{suf2} = \V{bad}
\\ \land\; \V{suf1} \ne \V{suf2}
\\ \myparbox{\cuz{} Expansion of \Eref{th-unary-suffix-uniqueness-110}
        using \Eref{th-unary-suffix-uniqueness-105}.}
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:th-unary-suffix-uniqueness-190}
\Rvarseq{suf1}{V} \in \Vmyfn{Upaths}{G} \land
    \Rvarseq{suf2}{V} \in \Vmyfn{Upaths}{G}
\\ \label{eq:th-unary-suffix-uniqueness-200}
    \land\; \Velement{suf1}{0} = \Velement{suf2}{0}
\\ \label{eq:th-unary-suffix-uniqueness-210}
    \land\; \Vlastix{suf1} = \Vlastix{suf2} = \Vnat{bad}
\\ \label{eq:th-unary-suffix-uniqueness-220}
    \land\; \V{suf1} \ne \V{suf2}
\\ \nonumber \myparbox{\cuz{}
    EI of \Eref{th-unary-suffix-uniqueness-170}
    with new free \Rvarseq{suf1}{V} for bound \Rvarseq{suf1}{V} and
    new free \Rvarseq{suf2}{V} for bound \Rvarseq{suf2}{V}.}
\end{gather}
\mypareq{eq:th-unary-suffix-uniqueness-230}{%
    $\Rvarseq{suf1}{V} = \seq{ \Velement{pre1}{0} \ldots \Velement{pre1}{\Vlastix{pre1}}, \V{suf1Last} }$
    \bcuz{} \Eref{th-unary-suffix-uniqueness-160}, \Eref{th-unary-suffix-uniqueness-210}.
    New \Rvarseq{pre1}{V}, new \Rvar{suf1Last}{V}.}
\mypareq{eq:th-unary-suffix-uniqueness-240}{%
    $\Rvarseq{suf2}{V} = \seq{ \Velement{pre2}{0} \ldots \Velement{pre2}{\Vlastix{pre2}}, \V{suf2Last} }$
    \bcuz{} \Eref{th-unary-suffix-uniqueness-160}, \Eref{th-unary-suffix-uniqueness-210}.
    New \Rvarseq{pre2}{V}, new \Rvar{suf2Last}{V}.}
\mypareq{eq:th-unary-suffix-uniqueness-250}{%
    $\Vlastix{pre1} = \Vlastix{pre2} = \Vdecr{bad}$ \cuz{} \Eref{th-unary-suffix-uniqueness-210},
        \Eref{th-unary-suffix-uniqueness-230}, \Eref{th-unary-suffix-uniqueness-240}.}
\mypareq{eq:th-unary-suffix-uniqueness-260}{%
    $\Rvarseq{pre1}{V} \in \Vmyfn{Upaths}{G}$
        \bcuz{} \longThref{Unary Subpath}{unary-subpath},
            \Eref{th-unary-suffix-uniqueness-190}, \Eref{th-unary-suffix-uniqueness-230}.}
\mypareq{eq:th-unary-suffix-uniqueness-270}{%
    $\Rvarseq{pre2}{V} \in \Vmyfn{Upaths}{G}$
        \cuz{} \Thref{unary-subpath},
            \Eref{th-unary-suffix-uniqueness-190}, \Eref{th-unary-suffix-uniqueness-240}.}
\mypareq{eq:th-unary-suffix-uniqueness-280}{%
    $\Velement{pre1}{0} = \Velement{pre2}{0}$ \cuz{} \Eref{th-unary-suffix-uniqueness-200},
        \Eref{th-unary-suffix-uniqueness-230}, \Eref{th-unary-suffix-uniqueness-240}.}
\mypareq{eq:th-unary-suffix-uniqueness-290}{%
    $\myfn{INST}{\Vdecr{bad}}$ \cuz{} \Eref{th-unary-suffix-uniqueness-120},
        \Eref{th-unary-suffix-uniqueness-160}.}
\mypareq{eq:th-unary-suffix-uniqueness-300}{%
$\Velement{pre1}{0} = \Velement{pre2}{0}
    \land \Vlastix{pre1} = \Vlastix{pre2} = \Vdecr{bad}$
\linebreak $\implies\; \V{pre1} = \V{pre2}$
\bcuz{} Expansion of
        \Eref{th-unary-suffix-uniqueness-290}
        using \Eref{th-unary-suffix-uniqueness-105};
        \Eref{th-unary-suffix-uniqueness-260};
        \Eref{th-unary-suffix-uniqueness-270};
        and EI of \V{suf1} as \V{pre1} and \V{suf2} as \V{pre2}.}
\mypareq{eq:th-unary-suffix-uniqueness-310}{%
    $\Rvarseq{pre1}{V} = \Rvarseq{pre2}{V}$
    \bcuz{} \Eref{th-unary-suffix-uniqueness-250},
        \Eref{th-unary-suffix-uniqueness-260},
        \Eref{th-unary-suffix-uniqueness-270},
        \Eref{th-unary-suffix-uniqueness-280},
        \Eref{th-unary-suffix-uniqueness-300}.}
\mypareq{eq:th-unary-suffix-uniqueness-320}{%
    $\Velement{pre1}{\Vlastix{pre1}} = \Velement{pre2}{\Vlastix{pre2}}$
        \cuz{} \Eref{th-unary-suffix-uniqueness-310}.}
\mypareq{eq:th-unary-suffix-uniqueness-330}{%
    $\myfn{children}{\VVelement{pre1}{\Vlastix{pre1}}}
                = \set{ \V{suf1last}  }$
        \bcuz{} \longDfref{Unary Path}{unary-path},
            \Eref{th-unary-suffix-uniqueness-190},
            \Eref{th-unary-suffix-uniqueness-230}.}
\mypareq{eq:th-unary-suffix-uniqueness-340}{%
    $\myfn{children}{\VVelement{pre2}{\Vlastix{pre2}}}
                = \set{ \V{suf2last}  }$
        \bcuz{} \Dfref{unary-path},
            \Eref{th-unary-suffix-uniqueness-190},
            \Eref{th-unary-suffix-uniqueness-240}.}
\mypareq{eq:th-unary-suffix-uniqueness-350}{%
     $\V{suf1last} = \V{suf2last}$ \cuz{}
            \Eref{th-unary-suffix-uniqueness-320},
            \Eref{th-unary-suffix-uniqueness-330},
            \Eref{th-unary-suffix-uniqueness-340}.}
\mypareq{eq:th-unary-suffix-uniqueness-360}{%
     $\V{suf1} = \V{suf2}$ \cuz{} \Eref{th-unary-suffix-uniqueness-230},
        \Eref{th-unary-suffix-uniqueness-240},
        \Eref{th-unary-suffix-uniqueness-310},
        \Eref{th-unary-suffix-uniqueness-350}.}
Equation \Eref{th-unary-suffix-uniqueness-360} is contrary to
\Eref{th-unary-suffix-uniqueness-220}, which shows the reductio
from \Eref{th-unary-suffix-uniqueness-110}--%
    \Eref{th-unary-suffix-uniqueness-120} to
    \Eref{th-unary-suffix-uniqueness-360}.
From the reductio we know that there is no counter-example to the
lemma, which shows us the lemma.
\myqed
\end{proof}

\begin{definition}[Maximal Unary Suffix]
\label{def:maximal-unary-suffix}
A unary suffix is \dfn{maximal}
iff its length is not less
than the length of any other unary suffix of the
same node.
\dfEnd
\end{definition}

\begin{theorem}[Maximal Unary Suffix Uniqueness]
\label{th:Maximal Unary Suffix Uniqueness}
Every node of an acyclic directed graph is in exactly one
maximal unary suffix.
\thEnd
\end{theorem}

\begin{proof}
The proof is direct.
\mypareq{eq:th-maximal-unary-suffix-uniqueness-100}{%
    $\V{G} = \tuple{\V{V}, \V{E}}$ \cuz{} AF theorem.
    \linebreak \V{G} is a new free acyclic directed graph.}
\mypareq{eq:th-maximal-unary-suffix-uniqueness-110}{%
    $\V{nd} \in \V{V}$ \cuz{} AF theorem.
    \linebreak \V{nd} is a new free node in \V{G}.}
\mypareq{eq:th-maximal-unary-suffix-uniqueness-120}{%
    \seq{ \V{nd} } is a unary path \cuz{} \longDfref{Unary path}{unary-path},
        trivially.}
\mypareq{eq:th-maximal-unary-suffix-uniqueness-130}{%
    \seq{ \V{nd} } is a unary suffix
         \bcuz{} \longDfref{Unary suffix}{unary-suffix},
            trivially.}
Let \V{usuffs} be the set of unary suffixes of \V{nd}:
\mypareq{eq:th-maximal-unary-suffix-uniqueness-135}{%
    $\V{usuffs} = \set{
        \V{usuf} \in \powerset{\naturals \times \V{V}} :
        \text{\V{usuf} is a unary suffix}
    }$
    \bcuz{} New \V{suffs} for convenience.}
We have shown that \V{usuffs} is non-empty:
\mypareq{eq:th-maximal-unary-suffix-uniqueness-140}{%
    $\Vsize{usuffs} > 0$ \cuz{}
        \Eref{th-maximal-unary-suffix-uniqueness-130},
        \Eref{th-maximal-unary-suffix-uniqueness-135}.
    }
Every element of \V{usuffs} is a unary suffix of \V{G},
and therefore, by the definition of unary suffix,
a unary path of \V{G}:
\mypareq{eq:th-maximal-unary-suffix-uniqueness-150}{%
    $\forall\; \Rvarseq{usuf}{V} \in \V{usuffs} : \Rvarseq{usuf}{V} \in \Vmyfn{Upaths}{G}$
    \bcuz{} \Dfref{unary-suffix},
        \Eref{th-maximal-unary-suffix-uniqueness-135}.}
By the definition of a unary path
every unary path of \V{G} is a path of \V{G}, so that every element of \V{usuffs}
is a path of \V{G}:
\mypareq{eq:th-maximal-unary-suffix-uniqueness-160}{%
    $\forall\; \Rvarseq{usuf}{V} \in \V{usuffs} : \Rvarseq{usuf}{V} \in \Vmyfn{paths}{G}$
    \bcuz{} \Dfref{unary-path},
        \Eref{th-maximal-unary-suffix-uniqueness-150}.}
\todo{Finish this proof}
\myqed
\end{proof}

\section{Compressed trees}

\chapter{Context-free grammars}
\label{chap:CFGs}

\todo[prepend, caption={``Context-free grammars'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

Let \Vsymset{alphabet} be a non-empty set of symbols.
Symbols are of realm \realm{SYM}.
Let \Rvar{x}{SYM} be a symbol.
More commonly we write
\Rvar{x}{SYM} as \Vsym{x}.

let $\V{alphabet}^\ast$ be the set of all strings
(realm \realm{STR}) formed
from those symbols.
Let \Rvar{s}{STR} be a string.
More commonly we write
\Rvar{s}{STR} as \Vstr{s}.
\size{\Vstr{s}} is the length of \Vstr{s},
counted in symbols.
Let $\V{alphabet}^+$ be
\begin{equation*}
\left\{ \Vstr{x}
\; \middle| \;
\Vstr{x} \in \V{alphabet}^\ast
\; \land \;
\Vsize{\Vstr{x}} > 0
\right\}.
\end{equation*}

A string can be seen as sequence of symbols,
and we use sequence index
to represent individual symbols of a string,
and slice notation
to represent substrings.
Thus,
the \V{i}'th character of the string,
is \sym{\V{str}[\V{i}]};
and
\begin{equation*}
\str{\V{str}[\V{a} \ldots \V{z}]} \defined
\str{\lbrace
    \V{str}[\V{a}], \;\;
    \V{str}[\V{a}+1], \;\;
    \ldots \;\;
    \V{str}[\V{z}]
\rbrace}.
\end{equation*}
We sometimes describe the last character of a
string as its rightmost:
\[
\Rightmost{\str{\Velement{str}{\V{a} \ldots \V{z}}}} = \VVelement{str}{z}.
\]

\begin{definition}[Context-free grammar]
\label{def:context-free-grammar}
A \dfn{context free grammar} (CFG) is a 4-tuple,
\begin{equation}
\begin{gathered}
\text{$[\Vsymset{nt}, \Vsymset{term}, \Vruleset{rules}, \Vsym{accept}]$ such that} \\
\Vsymset{nt} \cap \Vsymset{term} = \emptyset, \\
\text{$\Vsym{accept} \in \Vsymset{nt}$, and} \\
\text{\Vruleset{rules} is a set of elements of realm \realm{RULE}.}
\end{gathered}
\end{equation}
We will define realm \realm{RULE} shortly.
Context-free grammars have the realm CFG.

For convenience,
for a CFG, \Vcfg{g},
we write
\begin{equation*}
\begin{alignedat}{3}
& \NT{\Vcfg{g}} && \defined && \quad \Vsymset{nt}, \\
& \Term{\Vcfg{g}} && \defined && \quad \Vsymset{term}, \\
& \Accept{\Vcfg{g}} && \defined && \quad \Vsym{accept}, \\
& \Rules{\Vcfg{g}} && \defined && \quad \Vruleset{rules}, \quad \text{and} \\
& \Vocab{\Vcfg{g}} && \defined && \quad \Vsymset{nt} \cup \Vsymset{term}.
\end{alignedat}
\end{equation*}
The elements of the set \NT{\V{g}}, are non-terminal symbols,
or simply \dfn{non-terminals}.
The elements of the set \Term{\V{g}}, are terminal symbols,
or simply \dfn{terminals}.
\Accept{\V{g}} is the \dfn{accept symbol},
and in the parsing literature is often called
the ``start symbol''.
\Vocab{\V{g}} is
the \dfn{vocabulary} of \Vext{g}.

A \dfn{rule}
(realm \realm{RULE})
is a duple
\begin{equation*}
\left[ \Vsym{lhs} \de \Vstr{rhs} \right]
\end{equation*}
such that
\begin{equation*}
\Vsym{lhs} \in \NT{\V{g}} \; \land \;
\Vstr{rhs} \in \Vocab{\V{g}}^\ast.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\V{r}}$ and $\RHS{\V{r}}$, respectively.
\dfEnd
\end{definition}

\begin{definition}[Context grammar]
Definitions, theorems
and other statements in this document will imply
the existence of a grammar.
If not stated otherwise,
all statements are in context of the \dfn{context grammar}.
A context grammar is always in scope.
The global-scope context grammar, until redefined,
is \Vcfg{g}.
\dfEnd
\end{definition}

\begin{definition}[Rule size]
\label{def:rule-size}
The size of \V{r}, $\size{\Vrule{r}}$,
is the number of symbols on its RHS,
so that
\begin{equation*}
\size{\Vrule{r}} = \size{\RHS{\V{r}}}.
\end{equation*}
If $\size{\Vrule{r}} = 0$, then \Vrule{r}
is an \dfn{empty rule}.
If $\size{\Vrule{r}} = 1$, then \Vrule{r}
is a \dfn{unit rule}.
\end{definition}

\begin{definition}[Terminal string convention]
\label{def:terminal-string}
When a string
is a string of terminal symbols,
we will write it in boldface and enclose it
in a box: for example,
\Vterm{terminal}.
\end{definition}

As stated earlier \Pgref{page:binding-discussion},
each new scope can require definitions and/or
other statements to set it up.
We will avoid this tedium in favor of \dfn{binding assumptions}.
The binding assumptions are a set of assumptions
that are implicitly made,
unless stated otherwise,
whenever a new variable is introduced.
The following definition
introduces the first of our binding assumptions.

\begin{definition}[SYM, STR and RULE binding assumptions]
\label{def:sym-str-rule-binding}
Let \V{cg} be the context grammar of the most recent scope
when a variable is bound.
Unless otherwise stated,
\begin{itemize}
\item a variable of \realm{SYM} is assumed to be an element of \Vocab{\V{cg}};
\item a variable of \realm{STR} is assumed to be an element of $\Vocab{\V{cg}}^\ast$; and
\item a variable of \realm{RULE} is assumed to be an element of \Rules{\V{cg}}.
\end{itemize}
\dfEnd
\end{definition}

As examples of
\Dfref{sym-str-rule-binding},
\begin{align*}
& \text{\Vsym{sym} will imply that $\Vsym{sym} \in \Vocab{\V{g}}$}, \\
& \text{$\V{sym}_\realm{SYM}$ will imply that $\Vsym{sym} \in \Vocab{\V{g}}$}, \\
& \text{\Vstr{str} will imply that $\Vstr{str} \in \Vocab{\V{g}}^\ast$}, \\
& \text{$\V{str}_\realm{STR}$ will imply that $\Vstr{str} \in \Vocab{\V{g}}^\ast$, and} \\
& \text{\Vterm{str} will imply that $\Vterm{str} \in \Term{\V{g}}^\ast$}, \\
& \text{\Vrule{r} will imply that $\Vrule{r} \in \Rules{\V{g}}$}.
\end{align*}

A grammar describes a rewriting system.
We consider
\begin{equation}
\label{eq:rewrite-step}
\Vstr{prefix}\Vsym{lhs}\Vstr{suffix} \derives \Vstr{prefix}\Vstr{rhs}\Vstr{suffix}
\end{equation}
a valid rewrite step of \Vcfg{g} iff
\begin{gather*}
\Vstr{prefix} \in \Vocab{\Vcfg{g}}, \\
\Vstr{suffix} \in \Vocab{\Vcfg{g}}, \quad \text{and} \\
[\Vsym{lhs} \de \Vstr{rhs}] \in \Rules{\Vcfg{g}}.
\end{gather*}
The valid rewrite step of
\Eref{rewrite-step}
is a \dfn{right rewrite step} iff
\Vsym{lhs} is the rightmost non-terminal.

A sequence of iterated rewrite steps of \Vcfg{g}, such that
\begin{equation}
\label{eq:derivation}
\Vstr{s1} \derives \Vstr{s2} \derives \Vstr{s3} \ldots \Vstr{secN}
\end{equation}
is called a \dfn{derivation} of \Vcfg{g},
and a rewrite step that is part of a derivation is called
a \dfn{derivation step}.
A derivation is a \dfn{right derivation} iff
all of its rewrite steps are right rewrite steps.

The sequence of strings in a derivation, its sequence of strings
is called its \dfn{derivation sequence}.
The derivation sequence of \Eref{derivation} is
\begin{equation*}
\Vstr{s1}, \Vstr{s2}, \Vstr{s3} \ldots \Vstr{secN}.
\end{equation*}

We explicitly indicate that some \Vstr{x} derives \Vstr{y} in \V{k} steps
by writing
\begin{equation*}
\Vstr{x} \xderives{\V{k}} \Vstr{y}.
\end{equation*}
Writing $\Vstr{x} \xderives{1} \Vstr{y}$ is equivalent
to writing $\Vstr{x} \derives \Vstr{y}$.

By convention,
every string derives itself in zero steps.
\begin{equation*}
\Vstr{x} = \Vstr{y} \iff \Vstr{x} \xderives{0} \Vstr{x}.
\end{equation*}
A derivation of zero steps is called a
\dfn{trivial derivation}.
A derivation of more than zero steps is called a
\dfn{non-trivial derivation}.

We write
\begin{equation*}
\Vstr{x} \deplus \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in one or more steps.
We write
\begin{equation*}
\Vstr{x} \destar \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in zero or more steps.
Where
\begin{equation*}
\V{d} = \Vstr{first} \destar \Vstr{last}
\end{equation*}
is a derivation and
\Vstrset{dseq} is the derivation sequence of \V{d},
then saying that
\begin{equation*}
\V{dseq}[\V{i}] = \Vstr{s}
\end{equation*}
is equivalent to saying that
\begin{equation*}
\V{d} = \Vstr{first} \xderives{\V{i}} \Vstr{s} \destar \Vstr{last}.
\end{equation*}
Note that, as a result of the above definitions,
$\epsilon \xderives{0} \epsilon$ and
$\epsilon \destar \epsilon$.

We write
\begin{equation*}
\Vstr{x} \xderives{R} \Vstr{y}.
\end{equation*}
to say that \Vstr{x} right derives \Vstr{y}.
The ``$R$'' superscript may be combined with any of the other superscripts
so that, for example,
\begin{equation*}
\Vstr{x} \xderives{R+} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in one or more steps;
and
\begin{equation*}
\Vstr{x} \xderives{R(\V{k})} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in \V{k} steps.

Let \Vcfg{g} be a CFG.
A \dfn{sentential form} is a string which can be derived from the start
symbol, \Accept{\Vcfg{g}}.
If the sentential form \Vterm{sf} is a string of terminals,
then \Vterm{sf} is a \dfn{sentence} of \Vcfg{g}.

Let $\Vstr{a} \in \Vocab{\V{g}}^\ast$.
The \dfn{language} of \V{g} and \Vstr{a},
is
\begin{equation}
\label{eq:language}
\myL{\Vcfg{g}, \Vstr{a}}
\defined
\set{
\quad
\begin{aligned}
& \Vterm{sentence} \quad \text{such that} \\
& \qquad \begin{aligned}
  & \Vstr{a} \destar \Vterm{sentence} \\
  & \land \Vterm{sentence} \in \Term{\Vcfg{g}}^\ast.
  \end{aligned}
\end{aligned}
\quad
}
\end{equation}
The \dfn{language} of the grammar \Vcfg{g} is its set
of sentential forms:
\[
\myL{\V{g}} \defined \myL{\V{g}, \Accept{\V{g}}}.
\]

\begin{definition}[Parse]
\label{def:parse}
A \dfn{full parse} is a duple \tuple{ \Vcfg{g}, \Vterm{w} } such that
$\Vterm{w} \in \myL{\V{g}}$.
A \dfn{partial parse} is a duple \tuple{ \Vcfg{g}, \Vterm{inp} } such that,
for some \Vterm{lookahead},
\[
\tuple{ \Vcfg{g}, \Vterm{inp}\Vterm{lookahead} }
\]
is a full parse.
A partial parse,
\tuple{ \Vcfg{g}, \Vterm{inp} }
is \dfn{consistent} with a full parse \tuple{ \Vcfg{g}, \Vterm{w} }
iff
\[
\exists \; \Vterm{lookahead} : \Vterm{inp}\Vterm{lookahead} = \Vterm{w}.
\]
A \dfn{partial parse} is often called, simply, a \dfn{parse}.
Note that, by this definition,
a full parse is a special case of a partial parse.
\dfEnd
\end{definition}

\begin{definition}[Context scopes]
\label{def:context-scope}
From this point on, the global scope
and every new
definition or theorem scope
will be a \dfn{context scope}.
Unless otherwise stated,
the \dfn{context} of these scopes
will contain the partial parse $[\Vcfg{g}, \Vstr{w}]$.
\dfEnd
\end{definition}

\begin{definition}[Context input]
From this point on, unless otherwise stated,
the \dfn{context input} is the input of the context parse.
Often, the context contains the partial parse $[\Vcfg{g}, \Vstr{w}]$,
so that the \dfn{context input} is \Vstr{w}.
\dfEnd
\end{definition}

\begin{definition}[Context grammar redefined]
From this point on, unless otherwise stated,
the \dfn{context grammar} is the grammar of the context parse.
Often, the context contains the partial parse $[\Vcfg{g}, \Vstr{w}]$,
so that the \dfn{context grammar} is \Vcfg{g}.
\dfEnd
\end{definition}

We say that symbol \Vsym{x} is \dfn{nullable} iff
$\Vsym{x} \destar \epsilon$.
\Vsym{x} is \dfn{nulling} iff it always derives the null
string, that is, for all \Vstr{y}
\begin{equation*}
\Vsym{x} \destar \Vstr{y} \implies \Vstr{y} \destar \epsilon.
\end{equation*}
\Vsym{x} is \dfn{non-nullable} iff it is not nullable.
\Vsym{x} is a \dfn{proper nullable} iff it is nullable,
but not nulling.

Locations in the input will be of realm \realm{LOC}.
By tradition, the input to the parse is written as \Cw{},
where $\Cw \in \Term{\V{g}}^\ast$.
\Vsize{w} will be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\V{n}$,
where $\V{n} = \Vsize{w}$.

Unless \Vstr{w}, the context input, is otherwise specified,
use of the realm \realm{LOC} will bind
its variable,
so that
\begin{equation*}
\myparbox{\Vloc{j} will imply that $0 \le \V{j} \le \size{\Vterm{w}}$.} \\
\end{equation*}

\begin{definition}[Token]
\label{def:token}
In discussion that are closer to the implementation,
the contents of the location \Vloc{j}
in an input \Vstr{w}
will be treated as a \dfn{token}.
A token is a duple
\[
  \left[ \Vsym{token}, \V{tokenValue} \right],
\]
where \Vsym{token} = \Velement{w}{j} is the \dfn{token symbol},
and \V{tokenValue} is the \dfn{token value}.

The token value is irrelevant to parsing --- it exists
in order to be passed on to the semantics.
In \Marpa,
the implementation of the token is a pointer to a structure
containing a pointer to the token symbol,
and a pointer to the token value,
which is a value of arbitrary realm.\footnote{
In C language,
these can be implemented as so-called ``void pointers''.}
\dfEnd
\end{definition}

Where an input location, \Vloc{j},
is the token, $\V{token} = [\Vsym{sym}, \V{value}]$,
we will write
\begin{gather*}
\Symbol{\Vloc{j}} \defined \Vsym{sym} = \VVelement{w}{j} \\
\Symbol{\V{token}} \defined \Vsym{sym} = \VVelement{w}{j} \\
\Origin{\Vloc{j}} \defined \V{j} \\
\Origin{\V{token}} \defined \V{j} \\
\Current{\Vloc{j}} \defined \V{j}+1 \\
\Current{\V{token}} \defined \V{j}+1.
\end{gather*}
The usefulness of these notations will be clearer later, when
we introduce parse items.

\chapter{LR grammars}
\label{chap:lr-grammars}

\todo[prepend, caption={``LR grammars'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Handle]
Let \Vstr{sf} be a sentential form of \Vcfg{g}.
A \dfn{handle} of \Vstr{sf} is an ordered pair $[\Vrule{r}, \V{i}]$ such
that,
for some $\Vterm{lookahead} \in \Term{\V{g}}^\ast$,
\begin{gather}
0 \le \V{i} \le \size{\Vstr{sf}}, \\
\begin{aligned}
\Accept{\V{g}} \xderives{R\ast} & \Vstr{stack}\Vsym{lhs}\Vterm{lookahead} \\
   \xderives{R} & \Vstr{stack}\Vsym{rhs}\Vterm{lookahead} \\
   = & \Vstr{sf},
\end{aligned}
\\
\Vrule{r} = \V{lhs} \de \Vstr{rhs}, \;\; \text{and} \\
\V{i} = \size{\Vstr{stack}\Vstr{rhs}}. \quad \dfEnd
\end{gather}
\end{definition}

Let $\Vtermset{phrases} = \Term{\V{g}}^\ast$.
In this book, a \dfn{partition}, call it $\pi$,
is a subset of \powerset{\V{phrases}},
where all of the elements of $\pi$ are non-empty and disjoint,
and where the elements of $\pi$ ``cover'' \V{phrases} in the sense that
their union is equal to \V{phrases}.
That is,
\begin{gather}
\pi \subseteq \powerset{\V{phrases}},
\\ \forall \; \V{x} \in \pi : \V{x} \neq \emptyset,
\\ \V{phrases} = \bigcup \set{\V{x} : \Vtermset{x} \in \pi} \text{, and}
\\ \forall \; \Vtermset{x} \in \pi : \forall \; \Vtermset{y} \in \pi : \V{x} \cap \V{y} = \emptyset \lor \V{x} = \V{y}.
\end{gather}
Note that in this book $\pi$ will usually, but not necessarily,
be of finite cardinality.

We call an element of $\pi$, a \dfn{cell}.
When two terminal strings, \Vterm{ss1} and \Vterm{ss2},
are in the same cell of $\pi$,
we say that \V{ss1} and \V{ss2} are ``equivalent modulo $\pi$'', or
\[
\V{ss1} \iff \V{ss2} \pmod\pi.
\]


\begin{definition}[LR grammar]
\label{def:LR}
Intuitively, an LR($\pi$) grammar is a grammar where two
sentential forms that differ only in their lookaheads
have the same handle,
if those lookaheads are in the same cell of $\pi$.
More precisely\footnote{
    This definition follows \cite[p. 70]{Culik1973}.
},
\Vint{g} is LR($\pi$) for a partition $\pi$ if,
given
\begin{gather*}
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history1}\Vsym{lhs1}\Vstr{lookahead1} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead1},
\end{aligned}
\\
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history2}\Vsym{lhs2}\Vstr{lookahead3} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead2}\text{, and}
\end{aligned}
\\ \Vstr{lookahead1} \iff \Vstr{lookahead2} ( \text{mod $\pi$} )
\end{gather*}
we have
\begin{gather*}
\Vstr{history2} = \Vstr{history1},
\\ \Vsym{lhs2} = \Vsym{lhs1}\text{, and}
\\ \Vstr{lookahead3} = \Vstr{lookahead2}.
    \quad \dfEnd
\end{gather*}
\end{definition}

Every LR($\pi$) grammar is unambiguous.
If a grammar is unambiguous then it is LR($\pi$)
for the partition of infinite cardinality,
\[
\pi = \set{ \V{cell} \in \powerset{\Term{\V{g}}^\ast} : \Vsize{cell} = 1 }.
\]

We say that an LR($\pi$) grammar is LR-regular (LRR)
iff $\pi$ is a finite regular partition.
We say that an LR($\pi$) grammar is LR($\V{k}$)
iff
\begin{equation*}
\pi =
\set{
    \begin{gathered}
        \V{cell} \in \powerset{\Term{\V{g}}^\ast} :
            \exists\; \Vterm{u} :
        \\ \left(\begin{gathered}
                \Vsize{u} < \V{k} \condOpA \V{cell} = \set{\V{u}} \condOpB
                \\ \V{cell} = \set{\V{u}\V{v} : \V{v} \in \Term{\V{g}}^\ast}
            \end{gathered}\right)
    \end{gathered}
}.
\end{equation*}

\chapter{External grammars}
\label{chap:EXTs}

\todo[prepend, caption={``External grammars'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\section{Cycles}

An \dfn{external grammar} (realm \realm{EXT}) is a cycle-free CFG.
A CFG is \dfn{cycle-free} if it contains no cycles.
A \dfn{cycle string}, or \dfn{cycle} is a sentential form
which non-trivially derives itself.
Intuitively, a cycle is the parsing equivalent of an infinite loop.
A \dfn{cycle derivation} is any derivation of the form
\begin{equation*}
\label{eq:cycle}
 \Vstr{cycle} \deplus \Vstr{cycle}.
\end{equation*}
Cycle strings and cycle derivations are also often called
simply ``cycles''.

Cycles are recursions but most recursions
are not cycles.
In a recursion, a symbol \Vsym{recurse} derives itself as part of a string.
That is, \Vsym{recurse} is recursive iff
\begin{equation*}
 \Vsym{recurse} \deplus \Vstr{pre} \cat \Vsym{recurse} \cat \Vstr{post}.
\end{equation*}
\Vsym{recurse} is a cycle only if
both \Vstr{pre} and \Vstr{post} are the null string:
\begin{equation*}
 \Vstr{pre} = \Vstr{post} = \epsilon.
\end{equation*}

Earlier implementations of \Marpa have supported cycles,
but the current implementation of \Marpa does not.
When the current implementation finds a cycle in a grammar,
it print a message describing the cycle and throws
a fatal error.
This seems to be the behavior users desire.

There is no indication that cycles are of practical use.
Nor does the theoretical literature suggest potential uses for
them.
Support for cycles did require complicated extra code,
and removing the support of cycles seems to be cost-free.

When \Marpa did support cycles,
it did so by cutting the cycle short at cycle depth 1.
Intuitively, the cycle depth is the number of times the cycle
string returns to itself.
More precisely, if \Vstrset{d} is the derivation sequence
of the cycle derivation
$\Vstr{cycle} \xderives{\V{n}} \Vstr{cycle}$
then the cycle depth is
\begin{equation}
\left| \;
\left\lbrace \;
\V{i}
\quad \middle| \quad
\begin{gathered}
0 \le \V{i} \le \V{n} \quad \text{and} \; \\
\V{d}[\V{i}] = \Vstr{cycle}
\end{gathered}
\; \right\rbrace
\; \right|
\end{equation}
It follows that
a cycle of cycle depth 1 begins and ends with the
cycle string, but does not derive the cycle string
in any of its intermediate steps.

\chapter{Useless symbols}
\label{chap:useless-symbols}

\todo[prepend, caption={``Useless symbols'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\section{Unproductive symbols}
The current implementation of
\Marpa supports unproductive
symbols.
By default, \Marpa treats an unproductive symbol
as a fatal error,
but the user may bypass the fatal error
by marking the unproductive symbol as a ``unicorn''.

A \dfn{productive symbol}
is one which derives a terminal string.
That is, \Vsym{prod} is productive if,
for some \Vterm{phrase},
\begin{equation*}
\Vterm{phrase} \in \Term{\V{g}}^\ast
\;  \land \;
  \Vsym{prod} \destar \Vterm{phrase}
\end{equation*}
It is vacuously true that
the empty string is a terminal string,
and therefore all nullable
symbols are productive.

A symbol is \dfn{unproductive} if it is not
productive.
In the current version of \Marpa,
unproductive symbols can be implemented by
defining
unicorn terminals.
A \dfn[Unicorn@Unicorn terminal]{unicorn terminal}
is a terminal symbol
which can never be
found in the input.

In the \Marpa implementation,
a terminal can be made into a unicorn by
requiring that
a terminal match a regular expression that does not match
any string in $\Vocab{\Vint{g}}^\ast$.
For example,
the POSIX expression
\texttt{[\char`^ \char`\\ D\char`\\ d]}
indicates that the terminal
must match a single character which
is both a digit and a non-digit.
This is, of course, not possible
and therefore any terminal required to match that POSIX expression
will never match anything in the input.

Unproductive symbols often prove useful in \Marpa
applications.
One example of the utility of unproductive symbols
is interaction with custom lexers.
Marpa allows custom lexers, and these can feed lexemes
to the parser under application control,
observe the result,
and react accordingly.
The custom lexer can hand control back to \Marpa's
own lexer at any point.

It is often convenient for \Marpa to use its internal lexer
in most situations,
switching over to custom lexer only at certain points.
To do this, unicorn terminals can
be defined and placed in the grammar at points where the
custom lexer should take over.
Marpa will never recognize a unicorn in the input,
but it does keep track of when unicorns are expected.
Marpa can be configured to trigger a run-time event
when the unicorn is expected.
The application will receive control when the event occurs,
and it can then invoke the custom lexer.

For our theoretical results in this book,
we treat all symbols as if they were productive.
This is consistent with being conservative,
since a ``unicorn'' can always be feed to the parser
by a custom lexer and,
in that sense, all \Marpa symbols are productive.

\section{Inaccessible symbols}

An \dfn{inaccessible symbol} is one which can never be derived from
the accept symbol.
That is, \Vsym{noacc} is inaccessible in \Vext{g} iff
\begin{equation*}
\neg \; \exists \; \Vstr{pre}, \Vstr{post} \; \mid \; \Accept{\V{g}} \destar \Vstr{pre} \Vsym{noacc} \Vstr{post}.
\end{equation*}

The current implementation of \Marpa supports inaccessible symbols.
The user can specify \Marpa's treatment
of inaccessible symbols:
inaccessible symbols can be silently ignored,
they can be reported as warnings,
or they can be treated as fatal errors.
Treatment of inaccessible symbols can be specified
symbol by symbol,
and the default treatment can also be specified.
The ``factory default'' is
to report inaccessible symbols as warnings.

Inaccessible symbols have been found useful in \Marpa applications
that use higher-level languages ---
languages which generate other languages.
Often the generated grammar consists of
\begin{itemize}
\item ``custom''
rules which vary, and
\item ``boilerplate'' rules,
which are the same in every generated grammar.
\end{itemize}
It can be useful to have sections of the ``boilerplate''
be optional,
so that some of the ``boilerplate'' rules are only used
if the ``custom'' rules call for them.
If the custom rules do not call for a set of optional rules
within the boilerplate,
those boilerplate rules become inaccessible.

Optional boilerplate can be implemented
by silencing inaccessibility errors for the symbols in the optional boilerplate.
Usually it is possible to locate a topmost symbol for
each section of optional boilerplate,
in which case the warning need only be silenced for those
topmost symbols.

\chapter{Rewriting the grammar}
\label{chap:rewrite}

\todo[prepend, caption={``Rewriting the grammar'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\section{The Aycock-Horspool parser}

A parser that claims to be practical must accept
grammars with nullable and nulling symbols
and empty rules.
But difficulties in handling these correctly
have bedeviled previous attempts
at Earley%
\index{recce-general}{Earley, Jay}
parsing.

Marpa follows Aycock and Horspool~\cite{AH2002} in using
grammar rewrites to handle these issues.
Marpa's recognizer works using
Marpa internal grammars, but
Marpa's grammar rewriting allows the user to specify
their parse in terms of an external grammar.
In \Marpa, parsing, evaluation and
run-time events are performed
in such a way that the internal grammar
is invisible to
the user.

We defined \Marpa's external grammars in
\Chref{EXTs}.
Recall that an external grammar may be any cycle-free grammar.
Marpa internal grammars are described by construction
in this section.
They will be defined more carefully
in \Chref{INTs}.
This section describes the rewrite
from an external grammar
to an internal grammar.

Grammar rewriting was not the focus
of \cite{AH2002} ---
instead that paper centered on revising Earley's%
\index{recce-general}{Earley, Jay}
algorithm
to use LR(0) states.
The first version of \Marpa~\cite{Kegler2019}
used the LR(0) states of \cite{AH2002},
combining them with Joop Leo's improvements~\cite{Leo1991}.

Adding the LR(0) states to the first version of \Marpa
made its theory much more complex,
and the change offered
no complexity gains in terms of Landau notation.
The hope was that benefits would be seen in practice.

Unfortunately, these benefits did not emerge.
Marpa's experience in using, developing and maintaining
LR(0) states in \Marpa's early versions showed
that LR(0) states added
greatly to the complexity of the code;
were an obstacle to the implementation of run-time features
in the parser;
and produced no real improvement in speed or in space requirements.
On the other hand,
Aycock and Horspool's idea
of using grammar rewriting
to deal with
with nullable symbols became
fundamental to the \Marpa algorithm.

Rewriting grammars is a common
technique
in the parsing literature,
but many of these rewrites are almost totally without
usefulness in practice.
In the academic literature,
a rewritten grammar may recognize the same language,
but often it will not preserve the semantics
of the grammar.

Very few users of CFG's are
content with recognizing a language.
Almost always the user of a grammar-driven parser
formulated the rules and symbols of that grammar
in terms of a semantics,
and that user wants the parser
to preserve their semantics.
If a rewrite scrambles the user's semantics,
then that rewrite cannot play an essential role
in a general-purpose practical parsing tool.

Marpa restricts itself to using rewrites that
are \dfn{semantics-safe}.
The intent behind
semantics-safe rewrites
is that they
allow the rules and symbols of the external grammar
to be reconstructed from a parse using the internal grammar,
not just at evaluation time,
but at run-time as well.
\Sref{augmentation},
\Sref{eliminating-proper_nullables},
\Sref{removing-nulling-symbols},
and
\Sref{trivial-grammars}
describe the rewrites
applied by \Marpa.
We will state \Marpa's definition
of semantics-safe more precisely
in section \Sref{semantics-safe}.

\section{Augmenting the grammar}
\label{sec:augmentation}

Let \Vint{g} be a \Marpa internal grammar
such that $\Vsym{accept} = \Accept{\Vint{g}}$.
There must be
a dedicated \dfn{accept rule} for \Vint{g},
\begin{equation}
\label{eq:def-accept-rule}
\begin{gathered}
  \Vrule{accept} = \tuple{\Vsym{accept} \de \Vsym{top}} \text{ such that} \\
  \Vsym{top} \in \NT{\Vint{g}} \\
  \land \; \forall \; \Vrule{r} \left(\Vsym{accept} \notin \RHS{\Vrule{r}} \right)
  \\
  \land \; \forall \; \Vrule{r}
      \left(
        \LHS{\Vrule{r}} = \Vsym{accept} \implies \Vrule{r} = \Vrule{accept}
      \right)
\end{gathered}
\end{equation}

The dedicated accept rule is also called
the \dfn{augment rule}.
The semantics for the augment rule are simple ---
the semantics of \Vsym{top} are passed up to
\Vsym{accept}.
This is obviously semantics-safe.

In the \Marpa implementation,
the general method
described in section \Sref{semantics-safe}
for dealing with the semantics of rewritten rules
is not applied to the accept rule.
Instead, the accept rule is
dealt with as a special case.

\section{Eliminating proper nullables}
\label{sec:eliminating-proper_nullables}

Recall that a proper nullable
is a nullable symbol which is not nulling.
Marpa eliminates proper nullables with an improved
version of the rewrite
of~\cite[Algorithm 2.10, p. 148]{AU1972}
and~\cite{AH2002}.
In \Marpa's rewrite,
rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
This is similar to a rewrite into Chomsky form.

Every proper nullable symbol is then cloned into two others:
one nulling and one non-nullable.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols.
New rules are created as necessary to ensure that all possible combinations
of nulling and non-nullable symbols are accounted for.

The rewrite in~\cite{AH2002}
was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the post-rewrite grammar of \cite{AH2002}
could be \order{e^\ell}, where $\ell$ is the maximum length of a rule in the
the pre-rewrite grammar of~\cite{AH2002},
\[
\ell = \max\left(\set{ \Vsize{r} : \Vrule{r} \in \Rules{\Vext{preRewrite}} }\right).
\]
In our version, the worst case growth in the number of rules is \order{\ell}.

In traditional complexity terms,
because $\ell$ is a constant which
depends on the grammar, and often a very reasonable one,
minimizing the rule count in terms of $\ell$ does not
appear to be a important consideration.
In practice, however, languages and language statements which have many optional clauses are important.
The SELECT statement of SQL is one example.
A straight-forward BNF representation of the syntax of a statement
with many optional clauses has many proper nullables on the RHS,
so that, in these cases,
going exponential on $\ell$ has a cost
which it is worth some trouble to avoid.

\section{Removing nulling symbols}
\label{sec:removing-nulling-symbols}

After the rewrite of
\Sref{eliminating-proper_nullables},
all the nullable symbols in the grammar are also
nulling symbols.
Note that, if \Vext{g} is an external grammar,
and
if \varprime{g}{'} is \Vext{g}
revised to eliminate
all nulling symbols,
that
the language of the two grammars is the same: $\myL{\Vext{g}} = \myL{\varprime{g}{'}}$.
Similarly,
other than
the presence and absence of the nodes for the nulling symbols themselves,
the parse trees generated from \Vext{g} are the same as those generated from \varprime{g}{'}.

Thus, \Marpa may rewrite its grammars to eliminate nulling symbols.
For the purposes of evaluation and to allow the generation of parse-time events
based on nulled symbols,
Marpa records the locations of the eliminated nulling symbols.

A corner case occurs when eliminating nulling symbols from two distinct
rules makes them identical.
In this book,
we assume a rewrite that creates
a new LHS symbol for each rule that must remain distinct;
and that adds new rules to derive the new LHS symbols from the original ones.
Less clean theoretically,
but easier in practice,
is to have the implementation treat rules as distinct
even if they differ only in their recorded nulling symbols.
This is what the \Marpa implementation does.

\section{Trivial grammars and null parses}
\label{sec:trivial-grammars}

Null parses are parses of zero length inputs.
The \Marpa implementation
handles null parses by treating them
as a special case.

\dfn{Degenerate grammars} are grammars which do not accept any string.
A degenerate grammar is treated as a fatal error.

\dfn{Trivial grammars} are grammars which only accept
zero length inputs.
No \Marpa internal grammar can be a trivial grammar.
Trivial grammars are handled as a special case.

\section{Semantics-safe rewriting}
\label{sec:semantics-safe}

This section describes the restrictions
that rewrites must follow in order to be semantics-safe.
Consider an input \Cw{} and
an external grammar, \Vext{g}.
Let \Vint{g} be the internal grammar that is
the rewritten version of \Vext{g}.

Let
\begin{equation}
\label{eq:parse-tree}
\V{pt} = \V{tree}(\V{g},\Vstr{w})
\end{equation}
be the parse tree that results from a parse of \Vstr{w}
by a grammar \V{g}.
If \V{g} is an internal grammar,
\V{pt} is an \dfn{internal parse}.
If \V{g} is an external grammar,
\V{pt} is an \dfn{external parse}.

Every node of \V{pt} is a \dfn{symbol instance}.
The symbol instances of the parse correspond one-to-one with 4-tuples
of the form
\begin{equation*}
[ \Vsym{s}, \Vloc{start}, \V{length}, \V{depth} ].
\end{equation*}
where
\begin{itemize}
\item \Vsym{s} is the node's symbol.
\item \boldRange{w}{\Vloc{start}}{(\Vloc{start}+\V{length}-1)}
is the terminal string derived from \V{s}.
\item \V{depth}
is the ``brick depth''~\Lref{loc-brick-depth}.
\end{itemize}

In an internal grammar, there are
brick and mortar symbols.
Internal \dfn{brick} symbols correspond, many-to-one, to
external symbols.
Internal \dfn{mortar} symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.
All terminal symbols are bricks.

Recall \V{pt} from \Eref{parse-tree}.
If the symbol of an instance in \V{pt} is a brick,
the instance is a \dfn{brick instance}.
If the symbol of an instance in \V{pt} is a mortar symbol,
the instance is a \dfn{mortar instance}.

The
\dfn{brick depth}~\label{loc-brick-depth}
of an instance in a external grammar is
the depth of the node of the instance measured in the traditional way --
the distance in nodes to the root.
In an internal grammar, the brick depth is more complicated.
Intuitively, it is the distance of the node
in brick nodes.
More precisely,
\begin{itemize}
\item The brick depth of the root instance of an internal grammar is undefined.
\item Since an internal grammar is augmented, the root instance must
have only one child, a brick instance whose brick depth is defined to be zero.
\item For all other brick nodes,
the brick depth is one plus the brick depth of its parent node.
\item For mortar nodes, the brick depth is the brick depth of its parent node.
\end{itemize}

For evaluation, it is useful to convert the internal parse tree to
a \dfn{brick tree}.
To create a brick tree, we traverse the internal parse tree in pre-order,
ignoring internal nodes,
and creating instances for the brick nodes.
Note that this implies that the root node (the node
whose symbol is \Accept{\Vint{g}}) is ignored.

When, while constructing a brick tree,
a brick node is encountered that is marked with memoized
nulling symbols,
new instances must be created for the nulling symbols.
The new instances are called \dfn{nulling instances}.

Let \V{marked} be a node in a parse tree.
Assume that \V{marked} has
one or more recorded nulling instances.
Recorded with each nulling instance will be whether it occurs
before or after \V{marked}.
In the brick tree, each nulling instance will take the form
\begin{equation*}
[ \Vsym{nulling}, \Vloc{bound}, 0, \V{depth} ]
\end{equation*}
where
\begin{itemize}
\item \Vsym{nulling} is the memoized nulling symbol.
\item \Vloc{bound} is the location of the nulling symbol.
If the nulling instance occurs before \V{marked},
\V{bound} is the start location of \V{marked}.
If the nulling instance occurs after \V{marked},
\V{bound}
is the location
immediately after
\V{marked}.
\item \V{depth} is the same as the depth of \V{marked}.
\end{itemize}
Nulling instances are brick instances.

In what follows, we will want to match instances from an internal
parse
to instances of an external parse of the same input.

\begin{definition}[Equality of symbol instances]
Symbol instances are \dfn{equal} if their elements
are equal.  This is, if \V{node1} and \V{node2} are
instances,
\begin{equation*}
\begin{aligned}
& \V{node1} = \V{node2} \quad \defined \\
& \quad
\left(
\begin{gathered}
\exists \; \Vsym{s}, \Vloc{start}, \V{length}, \V{depth} : \\
\V{node1} = [ \Vsym{s}, \Vloc{start}, \V{length}, \V{depth} ] \\
\land \;\; \V{node2} = [ \Vsym{s}, \Vloc{start}, \V{length}, \V{depth} ]
\end{gathered}
\right) \quad \dfEnd
\end{aligned}
\end{equation*}
\end{definition}
Duplicate symbol instances do not occur within a parse tree,
so we will usually be speaking of the equality of symbol
instances in different trees.

\begin{definition}[Semantics-safe]
Let
\begin{itemize}
\item
$\V{Etree} = \V{parse}(\Vext{g},\Vstr{w})$
be the parse from
an external grammar,
\item $\V{Itree} = \V{parse}(\Vint{g},\Vstr{w})$
be the parse from
an internal grammar, and
where \Vint{g} is a rewrite of \Vext{g},
\item \V{Btree} be the brick tree
produced from \V{Itree}.
\end{itemize}
Let \V{InstanceMatches} be the relation defined by
the equality of symbol instances
in \V{Btree} and \V{Etree}:
\begin{equation*}
\V{InstanceMatches} \; \defined \;
\left\lbrace
\begin{gathered}
[ \V{Enode}, \V{Bnode} ] : \\
\begin{aligned}
& \V{Enode} \in \V{Etree} \\
\land \; & \V{Bnode} \in \V{Btree} \\
\land \; & \V{Enode} = \V{Bnode}
\end{aligned}
\end{gathered}
\right\rbrace
\end{equation*}

A rewrite is \dfn{semantics-safe}, iff
\V{InstanceMatches}
is a one-to-one correspondence
(bijection). \dfEnd{}
\end{definition}

\chapter{\Marpa internal grammars}
\label{chap:INTs}

\todo[prepend, caption={``\Marpa internal grammars'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Internal grammar]
\label{def:internal-grammar}
A Marpa internal grammar (realm \realm{INT}) is
a Marpa external grammar which is augmented,
and which has no nullable symbols.
\dfEnd
\end{definition}

The notations for CFG's \Chref{CFGs}
carry over to internal grammars,
although with the changes imposed
by the restricted form of INT's.
Noteworthy among these changes are
that \Marpa internal grammars

\begin{itemize}
\item do not allow nulling symbols,
\item do not allow the input to a parse to be of length zero, and
\item do not allow the RHS of a rule to be empty.
\end{itemize}

In the rest of this book,
unless otherwise stated,
we will be assuming that the grammar
we are discussing
is a \Marpa internal grammar.
The restrictions imposed on INT's allow us
to state some
definitions, for example \Dfref{rr},
more simply than we could if they had to apply
to EXT's, or to CFG's,

\begin{definition}[Right recursion]
\label{def:rr}
Let \Vint{g} be an internal grammar.
Let \Vstr{sf} be a sentential form of \V{g} non-trivially derived from
\Vsym{rr}, so that
\begin{equation}
\label{eq:rr}
\Vsym{rr} \derives \Vstr{rhs} \destar \Vstr{sf}.
\end{equation}
The derivation of \Eref{rr}
is \dfn{right-recursive} iff
\begin{equation}
\Rightmost{\Vstr{sf}} = \Vsym{rr}.
\end{equation}
We also say that the rule used in the first step of \Eref{rr},
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{[\Vsym{rr} \de \Vstr{rhs}]}
\end{equation*}
and that the symbol $\Vsym{rr}$
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{\Vsym{rr}}. \quad \dfEnd
\end{equation*}
\end{definition}
\Dfref{rr} implies that, for any rule \V{r}, $\V{r} \in \Rules{\V{g}}$,
we have
\begin{equation}
\begin{gathered}
\RightRecursive{\V{r}} \implies \RightRecursive{\LHS{\V{r}}}.
\end{gathered}
\end{equation}

\begin{theorem}[Right-recursive step uses right recursive rule]
Let \Vint{g} be a \Marpa internal grammar.
If \Vrule{r} is a rule used in a step of a right recursive derivation,
then \Vrule{r} is right recursive.
\thEnd
\end{theorem}

\begin{proof}
Note that for the case of a right recursion of length 1,
the theorem is trivially true.
We now consider the case of an arbitrarily chosen step
in a right recursion in \V{g} of
length 2 or more:
\begin{align}
\label{eq:rr-proof-10a}
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{eq:rr-proof-10b}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{eq:rr-proof-10c}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\label{eq:rr-proof-10d}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}
The arbitrarily chosen step is from
\Eref{rr-proof-10b} to
\Eref{rr-proof-10c}
and uses the rule $[ \Vsym{symX} \de \Vstr{rhsX} ]$.
We may ``pump'' the derivation of
\Eref{rr-proof-10a}--\Eref{rr-proof-10d}
to produce another right recursion in \V{g}
\begin{align}
\nonumber
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{eq:rr-proof-20a}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{eq:rr-proof-20b}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{rhs1} \\
\label{eq:rr-proof-20c}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vsym{symX} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}

Isolating the derivation from \Eref{rr-proof-20a} to
\Eref{rr-proof-20c} we have
\begin{equation}
 \Vsym{symX} \derives \Vstr{rhsX} \destar
 \Vstr{leftX} \Vstr{preX} \Vsym{symX}
\end{equation}
so that
\begin{equation}
\RightRecursive{[ \Vsym{symX} \de \Vstr{rhsX} ]} \because \Dfref{rr}. \quad \myqed
\end{equation}
\end{proof}

\chapter{Earley items}
\label{chap:earley-item}

\todo[prepend, caption={``Earley items'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Context redefined]
\label{def:context-parse-int}
Recall our definition of context scope \Dfref{context-scope}.
We now redefine it.
From this point on,
and unless otherwise stated, the contents
of every context scope are
the partial parse $[\Vint{g}, \Vstr{w}]$.
The difference between this and \Dfref{context-scope}
is that the grammar of the context parse is now
a \Marpa internal grammar.
\dfEnd
\end{definition}

Since this book is intended to describe \Marpa,
many of our results take advantage of Marpa{}'s rewriting,
and simplify the presentation by
assuming \Marpa internal grammars.
Descriptions of
\Earley{} for more general CFGs
are plentiful.%
\footnote{%
    Focusing on the classic Earley-related literature,
    these include
    \cite[pp. 320-321]{AU1972},
    \cite{AH2002},
    \cite{Earley1968},
    \cite{Earley1970},
    \cite{GJ2008}, and
    \cite{Leo1991}.%
}

Let $\Vrule{r} \in \Rules{\Vint{g}}$
be a rule.
Recall that $\Vsize{r}$
is the length of the RHS of \V{r}.
A \dfn{dotted rule} (realm \realm{DR}) is a duple, $[\Vrule{r}, \V{pos}]$,
where $0 \le \V{pos} \le \size{\Vrule{r}}$.
The position, \V{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z}]
\end{equation*}
is the dotted rule with the dot at
$\V{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.
\DottedRules{\Vcfg{g}} is the set of dotted rules
in \Vcfg{g}, that is,
\begin{align*}
& \DottedRules{\Vcfg{g}} \defined \\
& \qquad \qquad \left\lbrace \;
\begin{aligned}
& \hspace{0pt} [ \Vrule{r}, \V{pos} ] \quad \text{such that} \; \\
& \begin{aligned}
         & \Vrule{r} \in \Rules{\Vint{g}} \\
\qquad \land \quad & 0 \le \V{pos} \le \size{\RHS{\Vrule{r}}} \\
\end{aligned} \\
\end{aligned}
\; \right\rbrace .
\end{align*}
Where the choice of \Vcfg{g} is clear in context,
and unless otherwise specified,
\Vdr{dr} will imply that $\V{dr} \in \DottedRules{\V{g}}$.
For convenience, where
\[
\Vdr{x} = [\Vrule{r}, \V{pos}]
\]
is a dotted rule,
\begin{equation*}
\begin{gathered}
\text{$\Rule{\Vdr{x}} \defined \Vrule{r}$ and} \\
\DotPos{\Vdr{x}} \defined \V{pos}.
\end{gathered}
\end{equation*}

\begin{definition}[Size of grammar]
\label{def:grammar-size}
The size of a grammar, \Vsize{\V{g}} is
the number of distinct dotted rules it allows.
That is,
\[
\size{\Vcfg{g}} = \size{\DottedRules{\V{g}}}.
\]
\end{definition}

\begin{definition}[Dotted rule properties]
\label{def:dotted-rule-properties}
If we let \Vdr{x} be a dotted rule, such that
\begin{equation}
\Vdr{x} = \left[ \Vrule{x}, \V{pos} \right],
\end{equation}
then
\begin{align}
& \LHS{\Vdr{x}} && \defined && \quad
\LHS{\Vrule{x}} \\
%
& \Predot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\Vdecr{pos}], \; \text{if $\V{pos} > 0$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-postdot}
& \Postdot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\V{pos}], \; \text{if $\V{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-next}
& \Next{\Vdr{x}} && \defined && \quad
\begin{cases}
[\Vrule{x}, \V{pos}+1], \; \text{if $\V{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-penult}
& \Penult{\Vdr{x}} && \defined && \quad
\begin{cases}
\Postdot{\Vdr{x}}, \; \text{if $\V{pos} = \decr{\size{\Vrule{x}}}$} \\
\Lambda, \quad \text{otherwise}
\end{cases}
%
\end{align}
\end{definition}

\begin{definition}[Penult]
\label{def:penult}
A dotted rule, \Vdr{d},
is a \dfn{penult} if it is such that $\Penult{\V{d}} \neq \Lambda$.
The \dfn{initial dotted rule} is
\begin{equation*}
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\quad \dfEnd
\end{equation*}
\end{definition}
\begin{definition}[Prediction]
\label{def:prediction}
A \dfn{predicted dotted rule},
or \dfn{prediction} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{rhs} ].
\quad \dfEnd
\end{equation*}
\end{definition}
\begin{definition}[Completion]
\label{def:completion}
A \dfn{completed dotted rule},
or \dfn{completion},
is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{rhs} \mydot ].
\end{equation*}
A dotted rule which is not a completion is called an
\dfn{incomplete rule},
or an \dfn{incompletion}.
\dfEnd{}
\end{definition}
\begin{definition}[Confirmation]
\label{def:confirmation}
A \dfn{confirmed dotted rule},
or \dfn{confirmation},
is a dotted rule
with a dot position greater than zero.
A dotted rule is \dfn{unconfirmed} iff
it is not confirmed.
\dfEnd{}
\end{definition}
\begin{definition}[Medial]
\label{def:medial}
A rule which is both an incompletion and a confirmation
is called a \dfn{medial rule}, or \dfn{medial}.
\dfEnd
\end{definition}

\begin{definition}[EIM]
\label{def:eim}
A Earley item (realm \realm{EIM}) is a triple
\begin{equation}
\label{eq:d-eim}
    [\Vdr{dotted}, \Vorig{i}, \Vcurr{j}]
\end{equation}
of dotted rule, origin and current location.
\dfEnd
\end{definition}

The origin is the location where recognition of the rule
started.
In the Earley%
\index{recce-general}{Earley, Jay}
parsing literature,
the origin is often called the ``parent''.
For convenience, the realms \realm{ORIG} and \realm{CURR} will be synonyms
for \realm{LOC}, so that
\[
\realm{LOC} = \realm{ORIG} = \realm{CURR}
\]
The realm \realm{ORIG} will
indicate that the variable designates
an origin element of an Earley item; and
the realm \realm{CURR} will
indicate that the variable designates
a current location element of an Earley item.
where
\[
\Veim{x} = [\Vdr{x}, \Vorig{x}, \Vcurr{x}]
\]
is an arbitrary \realm{EIM},
\begin{gather}
\label{eq:def-dr}
\DR{\Veim{x}} \defined \Vdr{x}, \\
\label{eq:def-origin}
\Origin{\Veim{x}} \defined \Vorig{x}, \text{ and} \\
\label{eq:def-curr}
\Current{\Veim{x}} \defined \Vcurr{x}.
\end{gather}

When a dotted rule concept is applied to an \realm{EIM},
it is the same as applying that concept to the \realm{EIM}s
dotted rule.
When a rule concept is applied to an dotted rule
it is the same as applying that concept to the rule
of the dotted rule.
When a rule concept is applied to an \realm{EIM}
it is the same as applying that concept to the rule
of the dotted rule of the \realm{EIM}.

For example, let
\begin{equation*}
\Veim{x} = [\Vdr{x}, \Vorig{x}, \Vcurr{x}] =
\left[[\Vrule{x}, \V{pos}], \Vorig{x} , \Vcurr{x} \right].
\end{equation*}
Then
\begin{equation*}
\begin{gathered}
\Penult{\Veim{x}} = \Penult{\Vdr{x}}, \\
\Postdot{\Veim{x}} = \Postdot{\Vdr{x}}, \\
\LHS{\Veim{x}} = \LHS{\Vdr{x}} = \LHS{\Vrule{x}}, \\
\RHS{\Veim{x}} = \RHS{\Vdr{x}} = \RHS{\Vrule{x}}, \\
\end{gathered}
\end{equation*}
and so forth.

Similarly, we say that
an \realm{EIM} is a \dfn{penult} if its dotted rule is a penult,
an \realm{EIM} is a \dfn{prediction} if its dotted rule is a prediction,
and so on.

\begin{definition}[PIM]
\label{def:pim}
A \dfn{parse item} is either an Earley or a Leo item.
Parse items are of realm \realm{PIM},
so that
\[
    \realm{PIM} = \realm{EIM} \cup \realm{LIM}.
\]
Leo items (realm \realm{LIM})
will be defined in \Chref{leo}.
\end{definition}

\todo{Revisit productivity, accessibility.  Require in INT?}
\begin{definition}[EIM validity]
\label{def:eim-validity}
Let
\begin{equation}
\label{eq:eim-valid-10}
\Veim{x} = \left[ \left[ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2}
   \right], \Vorig{i}, \Vcurr{j} \right]
\end{equation}
be an arbitrary \realm{EIM}.
\Veim{x} is \dfn{valid}
iff
\begin{gather}
\label{eq:eim-valid-20}
\V{i} \le \V{j} \le \Vlastix{inp}, \\
\label{eq:eim-valid-22a}
\left[ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \right] \in \Rules{\V{g}},
\\
\label{eq:eim-valid-22b}
\Accept{\V{g}} \destar \Vstr{before} \Vsym{lhs} \Vstr{after}, \\
\label{eq:eim-valid-22c}
\Vstr{before} \destar \V{inp}[0  \ldots  \Vdecr{i})], \text{ and} \\
\label{eq:eim-valid-22d}
\Vstr{rhs1} \destar \V{inp}[\V{i}  \ldots  (\Vdecr{j})].
\end{gather}
\Veim{x}
is \dfn{useful}
iff
\begin{equation*}
\label{eq:eim-valid-30}
\Vstr{rhs2}\Vstr{after} \destar \V{inp}[\V{j}  \ldots  \Vlastix{inp}].
\end{equation*}
The boolean function \Valid{\Veim{x}} is true iff \Veim{x}
is valid.
\dfEnd
\end{definition}

\begin{definition}[Symbol of an \realm{EIM}]
\begin{equation*}
\Symbol{\Veim{eim}} \defined \LHS{\V{eim}}. \quad \dfEnd
\end{equation*}
\end{definition}

\begin{theorem}[EIM direction]
\label{th:eim-direction}
If \Veim{x} is a valid \realm{EIM},
then $\Origin{\V{x}} + \size{\RHS{\V{x}}} = \Current{\V{x}}$.
Also,
the origin of \Veim{x} is at or before its current location.
\thEnd
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-10}
\begin{gathered}
\text{Let \Veim{x} be a valid \realm{EIM}, where} \\
\Veim{x} = \left[ \left[ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2}
   \right], \Vorig{i}, \Vcurr{j} \right] \\
   \because \text{ WLOG, \Eref{eim-valid-10} in \Dfref{eim-validity}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-12}
\begin{gathered}
\Vorig{i} + \size{\Vstr{rhs1}} = \Vloc{j} \\
   \because \Eref{eim-valid-22d} \text{ in } \Dfref{eim-validity},
    \Eref{th-eim-origin-not-after-current-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-14}
\begin{gathered}
\Origin{\V{x}} + \size{\Vstr{rhs1}} = \Current{\V{x}} \\
\because
\Eref[Origin]{def-origin},
\longThref{EIM direction}{eim-direction},
\Eref{th-eim-origin-not-after-current-10},
\Eref{th-eim-origin-not-after-current-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-16}
\Origin{\V{x}} \le \Current{\V{x}} \because
    \Eref{th-eim-origin-not-after-current-14}.
    \quad \myqed
\end{equation}
The theorem is
\Eref{th-eim-origin-not-after-current-14}
and
\Eref{th-eim-origin-not-after-current-16}.
\end{proof}

\begin{definition}[Matching]
\label{def:matching-1}
We say that an \V{l} of some realm \dfn{matches}
a \V{r} of some realm if
\Matches{\V{l},\V{r}},
where \V{Matches} is an overloaded boolean function.
Where \Matches{\V{l},\V{r}},
we say that \V{l} is a \dfn{left match} of \V{r},
and that \V{r} is a \dfn{right match} of \V{l}.

Three of the overloadings of \V{Matches} are as follows.
The match of an \realm{EIM} with another \realm{EIM},
\begin{equation}
\label{eq:def-matches-eim-eim}
\begin{gathered}
\Matches{\Veim{left},\Veim{right}} \defined \\
\Postdot{\V{left}} = \Symbol{\V{right}} \\
\land \; \Current{\V{left}} = \Origin{\V{right}}.
\end{gathered}
\end{equation}
The match of an \realm{EIM} with a token,
\begin{equation*}
\begin{gathered}
\Matches{\Veim{left},\V{token}} \defined \\
         \Postdot{\V{left}} = \Symbol{\V{token}} \\
\land \; \Current{\V{left}} = \Origin{\V{token}}.
\end{gathered}
\end{equation*}
The match of an \realm{EIM} with a location,
\begin{equation*}
\begin{gathered}
\Matches{\Veim{left},\Vloc{j}} \defined \\
         \Postdot{\V{left}} = \Symbol{\Vloc{j}} \\
\land \; \Current{\V{left}} = \Origin{\Vloc{j}}.
\quad \dfEnd
\end{gathered}
\end{equation*}
\end{definition}

\begin{theorem}[Matching \realm{EIM}s direction]
\label{th:matching-eims-direction}
\begin{equation}
\label{eq:th-matching-eims-direction-03}
\begin{gathered}
\Matches{\Veim{left},\Veim{right}} \\
\implies \Current{\V{left}} \le \Current{\V{right}}.
    \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
By \Eref{th-matching-eims-direction-03} and
\Eref{def-matches-eim-eim} in the definitions of
``matching'' \Dfref{matching-1}, we have
\begin{equation}
\label{eq:th-matching-eims-direction-10}
\Current{\V{left}} = \Origin{\V{right}}.
\end{equation}
The theorem follows from
\Eref{th-matching-eims-direction-10}
and \Thref{eim-direction}.
\end{proof}

\begin{lemma}[Reduction]
\label{lem:reduction}
let
\begin{equation}
\label{eq:th-reduction-lemma-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \realm{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{equation}
\label{eq:th-reduction-lemma-04}
\Vsym{postdot} \destar \boldRangeDecr{inp}{\Vloc{j}}{\Vloc{k}}.
\end{equation}
Then
\begin{equation}
\label{eq:th-reduction-lemma-06}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \realm{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{k}.
  \quad \thEnd
\end{gathered}
\end{equation}
\end{lemma}

\begin{proof}
\begin{equation}
\label{eq:th-reduction-lemma-10}
\begin{gathered}
\Rule{\Veim{pred}} \in \Rules{\V{g}} \\
\because \Eref{eim-valid-22a} \text{ in }
    \longDfref{EIM validity}{eim-validity},
    \Eref{th-reduction-lemma-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-12}
\begin{gathered}
\Rule{\Veim{pred}} =
\Rule{\Veim{reduced}}
\because
    \Eref{th-reduction-lemma-02}
    \Eref{th-reduction-lemma-06}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-14}
\begin{gathered}
\Rule{\Veim{reduced}} \in \Rules{\V{g}} \\
\because
    \Eref{th-reduction-lemma-10},
    \Eref{th-reduction-lemma-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-16}
\begin{gathered}
  \Accept{\V{g}} \destar \Vstr{before} \cat \Vsym{lhs} \cat \Vstr{after} \\
  \because \Eref{eim-valid-22b} \text{ in }
    \longDfref{EIM validity}{eim-validity},
    \Eref{th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-18}
\begin{gathered}
  \Vstr{before} \destar \boldRangeDecr{inp}{0}{\V{i}} \\
  \because \Eref{eim-valid-22c} \text{ in }
    \Dfref{eim-validity},
    \Eref{th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-20}
\begin{gathered}
  \Vstr{pt1} \destar \boldRangeDecr{inp}{\V{i}}{\V{j}} \\
  \because \Eref{eim-valid-22d} \text{ in }
    \Dfref{eim-validity},
    \Eref{th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-22}
\begin{gathered}
  \Vstr{pt1}\Vsym{postdot} \destar \boldRangeDecr{inp}{\V{i}}{\V{k}} \\
  \because
    \Eref{th-reduction-lemma-04},
    \Eref{th-reduction-lemma-20}.
  \end{gathered}
\end{equation}
The validity of
\Veim{reduced} \Eref{th-reduction-lemma-06} follows from
\Eref{th-reduction-lemma-14},
\Eref{th-reduction-lemma-16},
\Eref{th-reduction-lemma-18}, and
\Eref{th-reduction-lemma-22}.
\myqed
\end{proof}

\begin{theorem}[Reduction by completion]
\label{th:reduction-by-completion}
Let
\begin{equation}
\label{eq:th-reduction-by-completion-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \realm{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{gather}
\label{eq:th-reduction-by-completion-04}
\text{\Veim{cuz} be a valid \realm{EIM} such that} \\
\label{eq:th-reduction-by-completion-05}
\Postdot{\V{cuz}} = \Lambda, \\
\label{eq:th-reduction-by-completion-06}
\Matches{\V{pred}, \V{cuz}}, \text{ and} \\
\label{eq:th-reduction-by-completion-08}
\Current{\V{cuz}} = \Vloc{k}.
\end{gather}
Then
\begin{equation}
\label{eq:th-reduction-by-completion-10}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \realm{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{k}.
      \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-reduction-by-completion-22}
\Origin{\Veim{cuz}} = \Vloc{j} \because
    \Eref{th-reduction-by-completion-02},
    \Eref{th-reduction-by-completion-06}.
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-24}
\LHS{\Veim{cuz}} = \Vsym{postdot} \because
    \Eref{th-reduction-by-completion-02},
    \Eref{th-reduction-by-completion-06}.
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-26}
\begin{gathered}
\exists \; \Vstr{rhs} : \Veim{cuz} = \tuple{\tuple{\Vsym{postdot} \de \V{rhs} \mydot}, \Vorig{k}}@\Vloc{j} \\
\because \Eref{th-reduction-by-completion-05},
        \Eref{th-reduction-by-completion-08},
        \Eref{th-reduction-by-completion-22},
        \Eref{th-reduction-by-completion-24}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-28}
\begin{gathered}
\Vsym{postdot} \destar \boldRangeDecr{inp}{\Vloc{j}}{\Vloc{k}} \\
   \because
        \Eref{th-reduction-by-completion-04},
        \Eref{th-reduction-by-completion-26}
        \longDfref{EIM validity}{eim-validity}.
\end{gathered}
\end{equation}
The theorem follows from
\Lmref{reduction},
\Eref{th-reduction-by-completion-02}, and
\Eref{th-reduction-by-completion-28}.
\myqed
\end{proof}

\begin{theorem}[Reduction by token]
\label{th:reduction-by-token}
Let
\begin{equation}
\label{eq:th-reduction-by-token-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \realm{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{gather}
\label{eq:th-reduction-by-token-04}
\Vsym{postdot} = \boldElement{inp}{\V{j}}
\end{gather}
Then
\begin{equation}
\label{eq:th-reduction-by-token-10}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \realm{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @(\V{j}+1).
  \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The theorem follows from
\Lmref{reduction},
\Eref{th-reduction-by-token-02}, and
\Eref{th-reduction-by-token-04}.
\myqed
\end{proof}

\chapter{Earley tables}
\label{chap:earley-table}

\todo[prepend, caption={``Earley tables'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

The definitions of this section
assume
the partial parse $[\Vint{g}, \Vstr{w}]$.

\begin{definition}[Earley set {[}ES{]}]
\label{def:earley-set}
An Earley set
(realm \realm{ES})
is as a set of \dfn{parse items}.
That is,
\[
\realm{ES} = \powerset{\realm{PIM}}. \quad \dfEnd
\]
\end{definition}

\begin{definition}[\realm{ES} Adjoinment]
\label{def:adjoin}
An Earley set is a set
in the mathematical sense,
so that \realm{PIM}s are \dfn{adjoined} to
an Earley set:
\[
    \Ves{after} = \Ves{before} \cup \Vpim{pim}.
\]
This means that if \V{p} is already
an element of \V{es},
\V{es} is unchanged:
\[
    \Vpim{p} \in \Ves{before} \implies \V{before} =
    \Ves{before} \cup \Vpim{pim} =
    \V{after}.
\]
An adjoin of \V{pim} to \V{before} that renders
$\V{after} \ne \V{before}$ is also called
an \dfn{add} of \V{pim} to \V{before}.
An adjoin of \V{pim} to \V{before} that leaves
$\V{after} = \V{before}$ is also called
a \dfn{proper adjoin} of \V{pim} to \V{before}.
\dfEnd
\end{definition}

\begin{definition}[ES validity]
\label{def:es-validity}
\begin{equation}
\begin{gathered}
\Valid{\Ves{x}} \defined
    \forall \; \Vpim{pim} \in \V{x} : \Valid{\V{pim}}.
\end{gathered}
\end{equation}
If \Valid{\Ves{x}}, we say that \V{x}
is \dfn{valid}.
\dfEnd
\end{definition}

Note that in \Dfref{es-validity}, we have yet to
define \Valid{\Vpim{lim}}, where \V{lim} is a \realm{LIM}.
We will do so later \Dfref{lim-validity}.

\begin{definition}[Earley table {[}ET{]}]
\label{def:et}
An Earley table, realm \realm{ET},
is a finite sequence of Earley sets.
That is,
\[
    \realm{ET} = \set{ \V{et} \in \powerset{\naturals\times\realm{ES}} :
        \exists\; \V{length} \in \naturals :
        (\deffn{et}{\V{length}}{\realm{ES}})}. \quad \dfEnd
\]
\end{definition}

\begin{definition}[Earley table validity]
\label{def:et-validity}
An Earley table is \dfn{valid} iff
\begin{itemize}
\item the sequence contains an Earley set for every symbol in the context input
    plus an initial Earley set,
\item all of these Earley sets are valid, and
\item the current location of every \realm{EIM} in each of the Earley sets
   is the same as the index of that Earley set in the Earley table.
\end{itemize}
That is, where \V{tab} is an Earley table,
\begin{equation}
\label{eq:def-et-validity-10}
\begin{gathered}
\Valid{\Vet{tab}} \defined \Vlastix{tab} = \size{\Vstr{w}}
    \\ \land \; \forall \; \Vnat{ix} \in \Dom{\V{et}} :
    \\ \left( \begin{gathered}
            \Valid{\VVelement{tab}{ix}}
            \\ \land \; \forall \; \Veim{eim} \in
            \VVelement{tab}{ix} : \Current{\V{eim}} = \V{ix}
        \end{gathered} \right).
\end{gathered}
\end{equation}
\end{definition}

Note that, by definition \Dfref{et-validity},
an Earley table may be vacuously valid.
That is, an Earley table all of whose Earley sets are empty is valid,
as long as it is of the correct length.

\begin{definition}[Context Earley table and sets]
\label{def:context-earley-table}
From this point on,
and unless stated otherwise,
\begin{itemize}
\item every context scope contains its own \dfn{context Earley table},
or \dfn{context table},
\item the context table is valid for the context parse,
\item the context table is \Vet{S}.%
\footnote{%
   The use of \V{S} for the Earley table goes back to Earley~\cite{Earley1968}.
   The items in an Earley set at location \Vloc{j} can be seen
   as encoding the state of the parse at \V{j},
   so that ``S'' stands for ``state set''.
}
\dfEnd
\end{itemize}
\end{definition}

\begin{definition}[ES binding assumption]
\label{def:es-binding}
The binding assumption for an ES, call it \Ves{es},
is that,
where \Vet{cxt} is the context table,
\[
    \exists \; \Vloc{i} \in \Dom{\Vet{cxt}}: \VVelement{cxt}{i} = \V{es}.
\]
Typically, \V{cxt} is \V{S}.
\dfEnd
\end{definition}

\begin{definition}[ES current location]
\label{def:es-current}
Let \V{es} be a valid ES and let \V{table}
be a valid ET.
Then
\begin{equation}
\label{eq:def-es-current-10}
    \Current{\V{es}} \defined \rIota \; \Vnat{i} : \VVelement{table}{i} = \V{es}.
\end{equation}
Where the value of \Vet{table} is not specified,
it is understood to be the context table.
The validity of \V{table} guarantees the uniqueness of \V{i} in
\Eref{def-es-current-10}, so that \Current{\V{es}} exists for
any valid \Ves{es}.
We say that \Current{\Ves{es}} is the
\dfn{current location} of \V{es}.
\dfEnd
\end{definition}

\begin{theorem}[ET--EIM current location]
\label{def:et-eim-current}
Let \VVelement{tab}{j} be a valid Earley set
in a valid table.  Then
\[
    \begin{gathered}
    \forall \; \Veim{eim} \in \Current{\VVelement{tab}{j}} :
    \\ \Current{\Veim{eim}} = \Current{\VVelement{tab}{j}} = \V{j}.
    \quad \thEnd
    \end{gathered}
\]
\end{theorem}

\begin{proof}
This theorem follows from
the definition of Earley table validity
\Dfref{et-validity}
and the definition of ES current location
\Dfref{es-current}.
\end{proof}

\begin{theorem}[ES--EIM current location]
\label{def:es-eim-current}
Let \Ves{es} be an Earley set.  Then
\[
    \forall \; \Veim{eim} \in \Current{\V{es}} : \Current{\Veim{eim}} = \Current{\V{es}}.
    \quad \thEnd
\]
\end{theorem}

\begin{proof}
This theorem follows from
the ES binding assumption \Dfref{es-binding},
and the ``ET--EIM current location'' theorem
\Dfref{et-eim-current}.
\myqed
\end{proof}

An Earley parser works by building an Earley table.
Earley set 0 is the initial Earley set.
Earley set $\Vloc{i}+1$ describes the state of
the parse after processing the input at location \Vloc{i}.

\begin{definition}[ES by location]
\label{def:es-by-location}
Let \Vet{cxt} be the context table.
It is often convenient to refer to an Earley set
by its location instead of a name.
In such cases we could abuse our
notation slightly by writing
$\es{(\Vloc{j})}$ instead of $\Vet{cxt}[\Vloc{j}]$.
In fact, when our intent is clear,
we will carry the abuse a bit further,
and abbreviate $\es{(\Vloc{j})}$ to \VVelement{S}{j}.
\dfEnd
\end{definition}

We will require an idea of direction
within an Earley table.

\begin{definition}[Earley table directionality]
\label{def:et-directionality}
A sequence of locations within a Earley table is \dfn{left-to-right}
if it is non-decreasing.
A sequence of locations within a Earley table is \dfn{right-to-left}
if it is non-increasing.
\dfEnd
\end{definition}

\begin{definition}[Input acceptance]
\label{def:input-acceptance}
Recall that
there was a unique accept symbol,
\Accept{\Vint{g}}, in \Vint{g}.
The Earley table \V{table}
\dfn{accepts} the input \Vterm{inp} iff
\begin{gather*}
\tuple{\Vdr{accept}, 0} \in \Vet{table}\left[ \Vsize{\Vterm{inp}} \right]
\\ \land \; \LHS{\Vdr{accept}} = \Accept{\Vint{g}}.
    \quad \dfEnd
\end{gather*}
\end{definition}

In the proofs of the next two theorems,
we include explicit derivation steps stating the context
and binding assumptions,
and we derive basic facts from these steps.
After that, we will usually introduce facts directly
when they are obvious
from the context and binding assumptions.

\begin{theorem}[Prediction locations]
\label{th:predictions-locations}
Let \Veim{pred} be a prediction in Earley set \Ves{es}.
Then
\[
\Origin{\Veim{pred}} = \Current{\Veim{pred}} = \Current{\V{es}}.
\quad \thEnd
\]
\end{theorem}

\begin{proof}
\mypareq{eq:th-predictions-location-10-05}{%
    \Valid{\Vet{S}} \cuz{}
    \longDfref{Context Earley table and sets}{context-earley-table}.
}
\mypareq{eq:th-predictions-location-10-10}{%
    $\Ves{es} \in \V{S}$ \cuz{}
    \longDfref{Context table}{context-earley-table},
    \longDfref{ES binding assumption}{es-binding},
    \Ves{es} from statement of theorem.
}
\mypareq{eq:th-predictions-location-10-15}{%
    \Valid{\Ves{es}} \cuz{}
    \longDfref{ET validity}{et-validity},
    \Eref{th-predictions-location-10-05},
    \Eref{th-predictions-location-10-10}.
}
\begin{equation}
\label{eq:th-predictions-location-10-70}
\myparbox{\Veim{pred} is a prediction
    in \Ves{es}
    \cuz{} \text{AF theorem}.
}
\end{equation}
\mypareq{eq:th-predictions-location-10-75}{%
    $\Current{\Veim{pred}} = \Current{\Ves{es}}$ \linebreak
    \cuz{} \longDfref{ET validity}{et-validity},
        \Eref{th-predictions-location-10-05},
        \Eref{th-predictions-location-10-10},
        \Eref{th-predictions-location-10-70}.
}
\begin{equation}
\label{eq:th-predictions-location-10-80}
\Valid{\Veim{pred}}
    \because \longDfref{ES validity}{es-validity},
    \Eref{th-predictions-location-10-15},
    \Eref{th-predictions-location-10-70}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-10-90}
\begin{gathered}
\Veim{pred} = \tuple{
        \tuple{ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2} },
        \Vorig{i}, \Vloc{j} }
   \\ \myparbox{\cuz{}
       \longDfref{EIM}{eim},
        \Eref{th-predictions-location-10-70},
       new free \Vsym{lhs}, \Vstr{rhs1}, \Vstr{rhs2}, \Vorig{i},
       \Vloc{j}.
   }
\end{gathered}
\end{equation}
\mypareq{eq:th-predictions-location-12}{%
$\Vstr{rhs1} = \epsilon$ \cuz{} \longDfref{prediction}{prediction},
    \Eref{th-predictions-location-10-70},
    \Eref{th-predictions-location-10-90}.
}
\begin{equation}
\label{eq:th-predictions-location-14}
\begin{gathered}
\Vstr{rhs1} \destar \V{inp}[\V{i}  \ldots  \Vdecr{j}]
\\ \myparbox{\cuz{}
    \Eref{eim-valid-22d} in \longDfref{EIM validity}{eim-validity},
    \Eref{th-predictions-location-10-90},
    new \Vstr{inp} for convenience.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-16}
\V{inp}[\V{i}  \ldots  \Vdecr{j}] = \epsilon
\because
\Eref{th-predictions-location-12},
\Eref{th-predictions-location-14}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-17}
\V{j} \le \V{i}
\because \Eref{th-predictions-location-16}.
\end{equation}
\mypareq{eq:th-predictions-location-18}{%
    $\V{i} \le \V{j}$ \cuz{}
    \Eref{eim-valid-20} in \longDfref{EIM validity}{eim-validity},
    \Eref{th-predictions-location-10-90}.
}
\begin{equation}
\label{eq:th-predictions-location-20}
\V{i} = \V{j} \because
  \Eref{th-predictions-location-17},
  \Eref{th-predictions-location-18}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-22}
\Origin{\Veim{pred}} = \Current{\Veim{pred}}
  \because \Eref{th-predictions-location-10-90},
  \Eref{th-predictions-location-20}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-24}
\begin{gathered}
\Origin{\Veim{pred}} = \Current{\Veim{pred}} = \Current{\Ves{es}}
  \\ \because \Eref{th-predictions-location-10-75},
      \Eref{th-predictions-location-22}.
  \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[ES subset count]
\label{th:es-subset-count}
Let \Veimset{eims} be a subset of \Ves{es}:
$\V{eims} \subseteq \Ves{es}$.
Let \V{origs} be the set of origins of
the elements of \V{eims}:
\[
    \V{origs} = \set{ \Vloc{orig} :
        \exists \; \Veim{eim} \in \V{eims} : \V{orig} = \Origin{\V{eim}}
    }.
\]
Then
\[
    \size{\Veimset{eims}} = \bigorder{\size{\Veimset{origs}}}.
    \quad \thEnd
\]
\end{theorem}

\begin{proof}
The proof proceeds by noting that every \realm{EIM} is a tuple;
noting that the number of possible values for every tuple is on the order of
the product of the number of possible values for each of that tuple's elements;
and calculating this product.
More specifically,
the number of possible values for every \realm{EIM}
is the product of
\begin{itemize}
\item the number of possible dotted rules;
\item the number of possible current locations for \realm{EIM}s in \Veimset{eims}; and
\item the number of possible origin locations for \realm{EIM}s in \Veimset{eims}.
\end{itemize}
This means that
\begin{equation*}
\begin{aligned}
    \size{\Veimset{eims}} & = \bigorder{
        \begin{gathered}
        \size{\DottedRules{\Vcfg{g}}}
            \times \size{\Veimset{origs}}
        \\ \times \size{\set{\Current{\Ves{es}}}}
        \end{gathered}
    }
    \\ & = \size{\DottedRules{\Vcfg{g}}} \times \size{\set{\Current{\Ves{es}}}}
    \\ & \quad\quad \times \bigorder{\size{\Veimset{origs}}}
\end{aligned}
\end{equation*}
$\size{\DottedRules{\Vint{g}}}$ is a constant
that depends on the grammar,
and \set{\Current{\Ves{es}}} is obviously a singleton,
so that
\begin{equation}
\begin{aligned}
    \size{\Veimset{eims}} & =
        \Oc \times 1 \times
        \bigorder{\size{\Veimset{origs}}}
    \\ & = \bigorder{\size{\Veimset{origs}}}.
        \quad \myqed
\end{aligned}
\end{equation}
\end{proof}

\begin{theorem}[Prediction rule non-duplication]
\label{th:prediction-rule-non-duplication}
For every rule \Vrule{r},
there is at most one prediction \Veim{pred} in an \realm{ES}
such $\Rule{\V{pred}} = \V{r}$.
\thEnd
\end{theorem}

\begin{proof}
The proof strategy is a reductio.
We assume that two distinct predictions,
\Veim{pred1} and \Veim{pred2}, are in the same \realm{ES}
and have the same rule:
\begin{gather}
\label{eq:th-prediction-rule-non-duplication-100}
\Rule{\V{pred1}} = \Rule{\V{pred2}}
\\ \label{eq:th-prediction-rule-non-duplication-110}
\land\; \Current{\V{pred1}} = \Current{\V{pred2}} = \Current{\Ves{es}}
\\ \label{eq:th-prediction-rule-non-duplication-120}
\land\; \Veim{pred1} \ne \Veim{pred2}
\\ \nonumber
    \because \text{AF reductio,
        new arbitrary \Veim{pred1}, \Veim{pred2}, \Ves{es}.}
\end{gather}
\mypareq{eq:th-prediction-rule-non-duplication-140}{%
   $\V{pred1} = \tuple{\tuple{\Rule{\V{pred1}},0},\Current{\V{es}},\Current{\V{es}}}$
   \linebreak \cuz{} \longDfref{\realm{EIM}}{eim},
    \longDfref{prediction}{prediction},
    \longThref{Prediction Locations}{predictions-locations},
    \Eref{th-prediction-rule-non-duplication-110}.
}
\mypareq{eq:th-prediction-rule-non-duplication-150}{%
   $\V{pred2} = \tuple{\tuple{\Rule{\V{pred2}},0},\Current{\V{es}},\Current{\V{es}}}$
   \linebreak \cuz{} \longDfref{\realm{EIM}}{eim},
    \longDfref{prediction}{prediction},
    \longThref{Prediction Locations}{predictions-locations},
    \Eref{th-prediction-rule-non-duplication-110}.
}
\mypareq{eq:th-prediction-rule-non-duplication-160}{%
    $\V{pred1} = \V{pred2}$ \cuz{}
        \Eref{th-prediction-rule-non-duplication-100},
        \Eref{th-prediction-rule-non-duplication-140},
        \Eref{th-prediction-rule-non-duplication-150}.
}
Equation \Eref{th-prediction-rule-non-duplication-160}
contradicts
\Eref{th-prediction-rule-non-duplication-120},
which shows the reductio and the theorem.
\myqed
\end{proof}

\begin{theorem}[ES predictions count]
\label{th:es-prediction-count}
The count of valid predictions in an Earley set is
at most $\size{\Rules{\Vint{g}}} = \Oc{}$.
\thEnd
\end{theorem}

\begin{proof}
Recall that \Vint{g} is a constant so that
\mypareq{eq:th-es-prediction-count-100}{%
    $\size{\Rules{\Vint{g}}} = \Oc{}$.
}
This proof follows immediately from
\Eref{th-es-prediction-count-100} and
the Prediction Rule Non-duplication Theorem
\Thref{prediction-rule-non-duplication}.
\myqed
\end{proof}

\begin{theorem}[ES \realm{EIM} count]
\label{th:es-eim-count}
The count of valid \realm{EIM}s in the Earley set at \Vloc{j} is \order{\V{j}}.
\thEnd
\end{theorem}

\begin{proof}
From the context and the binding assumptions,
we know that
\VVelement{S}{j} is the Earley set at \Vloc{j},
that \VVelement{S}{j} is valid
and therefore that all \realm{EIM}s in \VVelement{S}{j} are valid.

Let \V{origs} be the set of origins of \realm{EIM}s in an arbitrary Earley set \VVelement{S}{j}, so that
\begin{equation}
\label{eq:th-es-eim-count-10}
\V{origs} = \set{
    \begin{gathered}
    \Vloc{orig} : \exists \; \Veim{eim} \in \VVelement{S}{j} : \V{orig} = \Origin{\V{eim}}
    \end{gathered}
    }.
\end{equation}
\mypareq{eq:th-es-eim-count-12}{%
    $\Origin{\V{eim}} \le \Current{\Veim{eim}}$
    \cuz{} \Eref{eim-valid-20} in \longDfref{EIM validity}{eim-validity}.
}
\mypareq{eq:th-es-eim-count-14}{%
    $\V{eim} \in \VVelement{S}{j} \implies \Origin{\V{eim}} \le \Current{\VVelement{S}{j}}$
    \cuz{} \longDfref{ES--EIM current location}{es-eim-current},
        \Eref{th-es-eim-count-12}.
}
\mypareq{eq:th-es-eim-count-16}{%
    $\V{eim} \in \VVelement{S}{j} \implies \Origin{\V{eim}} \le \V{j}$
        \cuz{} \longDfref{ES Current location}{es-current}
        \Eref{th-es-eim-count-14}.
}
The theorem follows trivially if $\Vsize{origs} = 0$,
so that for the rest of this
proof we assume that $\Vsize{origs} \ge 1$.
\mypareq{eq:th-es-eim-count-20}{%
$\V{origs} \subseteq \set{ 0 \ldots \V{j} }$
\cuz{} \Eref{th-es-eim-count-10},
    \Eref{th-es-eim-count-16}.
}
\mypareq{eq:th-es-eim-count-22}{%
    $\Vsize{origs} \le \V{j}+1$
        \cuz{} \Eref{th-es-eim-count-20}.
}
\mypareq{eq:th-es-eim-count-24}{%
    $\Vsize{origs} = \order{\V{j}}$.
        \cuz{} \Eref{th-es-eim-count-22}.
}
This theorem follows immediately
from \Eref{th-es-eim-count-24}
and
the ``ES subset count'' theorem \Thref{es-subset-count}.
\myqed
\end{proof}

\chapter{The Leo algorithm}
\label{chap:leo}

\todo[prepend, caption={``The Leo algorithm'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\section{Joop Leo's discovery}

In his 1968 Ph.D. thesis~\cite[p. 60]{Earley1968},
Jay Earley conjectured
that \Earley\! could be
modified to be \On{} for
the deterministic context-free grammars.
Recall that the
deterministic context-free grammars (DCFG's)
are
the union of the
\V{LR}(\V{k})
grammars for all \V{k}.
Earley gave no details of the method he had in mind
and left the field shortly thereafter.
By 1973 Jay Earley had earned a second Ph.D. and
was a practicing psychotherapist in California.

Earley had been correct.
In a 1991 paper, Joop Leo~\cite{Leo1991}
modified the Earley's algorithm so that
it was was linear,
not just for all DCFG's,
but for the LR-regular grammars,
a superset.
Leo did his work while unaware of the 1968 conjecture ---
Leo first encountered Earley's surmise in the course of writing up his
own discovery.%
\footnote{Leo, Joop, personal email to Jeffrey Kegler,
11 September, 2015.}

The problem both investigators noticed was that,
while \Earley{} is \On{}
for left recursion,
it is $\order{\V{n}^2}$ for right recursion.
This is because \Earley\! creates \realm{EIM}s
to represent a right recursion at every parse location
where the right recursion might end.
At those locations where the right recursion does not end,
most of these \realm{EIM}s will be useless.

\section{\Marpa and right recursion}

Leo's solution was to
memoize penults.
Leo restricted the memoization to situations where
the penult is unambiguous.
Every right recursion involves a penult,
so that Leo's method also memoizes right recursions.
Recall that we
call a dotted rule \Vdr{d},
a \dfn{penult}, if $\Penult{\V{d}} \neq \Lambda$.
In Leo's original algorithm, any penult
was treated as a potential right-recursion.

\Marpa applies the Leo memoizations in more restricted circumstances.
For \Marpa to consider a dotted rule
\Vdr{candidate} for Leo memoization,
not only must \Vdr{candidate} be a penult
but \Rule{\V{candidate}} must be right-recursive.

By restricting Leo memoization to right-recursive rules,
\Marpa incurs the cost of Leo memoization only in cases
where Leo sequences could be arbitrarily
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the maximum length of a Leo sequence is less than a constant,
often a small constant.
For limited-length sequences,
the payoff of Leo memoization is also limited.
In practice, the payoff seldom justifies
even the modest overhead of Leo memoization.

A future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
reward the trouble of including
them in Leo memoizations.
Such cases would be unusual,
but they may occur often enough to be worth
providing for.

Omission of a Leo memoization does not affect correctness,
so \Marpa's restriction of Leo memoization
preserves the correctness as shown in Leo~\cite{Leo1991}.
\todo{Refer to later proof that omission of memoization does not affect correctness}
Later in this book
we will prove this,
and we will also
show that this change also leaves
the complexity results of
Leo~\cite{Leo1991} intact.

The current \Marpa implementation does not perform
Leo memoization in Earley set 0.
Earley set 0 memoization
would have to be implemented as a special case,
while omission of $\es{(0)}$ memoization
results in at most one extra \realm{EIM} per ES.

\section{Leo directionality}

Leo's memoization method is simple from one point of view ---
it involves a sequence of memos, called Leo items.
The Leo items are matched to certain \realm{EIM}s in a straightforward way.
An \realm{EIM} matched to an Leo item is called a ``source \realm{EIM}''.

Nonetheless, many have found the Leo method
challenging to the intuition.
One reason may be that
the natural order of the sequence of \realm{EIM}s runs
in a direction opposite to the direction of
the intuitive order of the Leo items.
To understand
the building of the sequences of matched source \realm{EIM}s
and Leo items, one must think
in both directions at once.
In the hopes of orienting the reader prior to our journey
into the details of the Leo method,
this section will give a high-level overview
of Leo directionality.

Recall from
\Dfref{et-directionality}
that we defined the
``left-to-right'' direction
within an \realm{ET} as the direction
in which input location is non-decreasing.
Similarly, we defined
the ``right-to-left'' direction
as that in in which input location is non-increasing.

The natural direction of the source \realm{EIM}s is
the same as that of the Earley sets --- left-to-right.
The intuitive direction of the Leo items, however, is the one
in which the right recursion must be unfolded.
The conceptual order of the right recursion,
and therefore the order in which a memoized right recursion must be rebuilt,
is from the bottom up.

The trickiness occurs because,
as the Earley sets are built,
left-to-right, each new Leo item becomes the bottom of a new potential
right recursion.
Therefore the intuitive, bottom-up order of Leo items
is right-to-left in terms of the Earley sets.

For the implementation, this reversal works out fine --
\realm{EIM}s and their Leo memos are both created left-to-right.
But an investigator studying the implementation
must think simultaneously
in right-to-left and left-to-right terms.

\chapter{Leo items}
\label{chap:leo-items}

\todo[prepend, caption={``Leo items'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Leo items]
\label{def:lims}

In \cite{Leo1991}, penults were memoized
using what Leo called ``transitive items''.
In this book Leo's ``transitive items''
will be called \dfn{Leo items}.
Recall that every parse item (\realm{PIM})
is either
an Earley item or a Leo item.

A Leo item is of realm \realm{LIM},
where
\[
\realm{LIM} = \set{ [ \Vdr{top}, \Vsym{transition}, \Vorig{top}, \Vcurr{j} ] }.
\]
If
\[
\Vlim{x} =
\tuple{ \Vdr{top}, \Vsym{transition}, \Vorig{top}, \Vcurr{j} }
\]
is an arbitrary \realm{LIM},
we say that
\Vsym{transition} is the \dfn{transition symbol} of \Vlim{x},
and
\begin{align}
\DR{\Vlim{x}} & \defined \Vdr{top}, \\
\Transition{\Vlim{x}} & \defined \Vsym{transition}, \\
\Origin{\Vlim{x}} & \defined \Vorig{top} \text{, and} \\
    \Current{\Vlim{x}} & \defined \Vcurr{j}.
       \quad \dfEnd
\end{align}
\end{definition}

\begin{definition}[Transition of a \realm{PIM}]
\label{def:transition}
Let \Vpim{pim} be an arbitrary \realm{PIM}.
The \dfn{transition symbol},
or \dfn{transition},
of \V{pim} is
\mypareq{}{%
    \Transition{\V{pim}} if \V{pim} is a \realm{LIM}, and
    \linebreak \Postdot{\V{pim}} if \V{pim} is an \realm{EIM}.
}
In either case, the transition symbol
of \V{pim}
may be written
\Transition{\V{pim}}. \dfEnd
\end{definition}

\begin{definition}[Current location of a \realm{PIM}]
\label{def:pim-current}
Let \Vpim{pim} be an arbitrary \realm{PIM}.
The \dfn{current location},
or \dfn{location},
of \V{pim} is
\mypareq{}{%
    \Current{\V{pim}} if \V{pim} is a \realm{LIM}, and
    \linebreak \Current{\V{pim}} if \V{pim} is an \realm{EIM}.
}
In either case, the current location
of \Vpim{pim}
may be written
\Current{\V{pim}}. \dfEnd
\end{definition}

\begin{definition}[Leo unique]
\label{def:leo-unique}
\begin{equation}
\label{eq:def-leo-unique}
\begin{gathered}
\LeoUnique{\Vloc{j}} \defined \\
  \left\lbrace
    \begin{gathered}
    \Veim{x} \in \VVelement{S}{j} :
        \Penult{\V{x}} \neq \Lambda
        \\ \land \; \forall \; \Veim{y} \in \VVelement{S}{j} :
        \\ \Postdot{\V{x}} = \Postdot{\V{y}}
            \implies \V{x} = \V{y}
    \end{gathered}
  \right\rbrace .
\end{gathered}
\end{equation}
Note in
\Eref{def-leo-unique}
that,
while \Veim{x} is restricted to penults,
\Veim{y} ranges over all the
\realm{EIM}s of Earley set \VVelement{S}{j},
including those \realm{EIM}s which are not penults.
We say that
an \realm{EIM} \Veim{uniq}
is \dfn{Leo unique} in \VVelement{S}{j}
iff
$\Veim{uniq} \in \LeoUnique{\VVelement{S}{j}}$.
\dfEnd{}
\end{definition}

\begin{definition}[Leo Eligible]
\label{def:leo-eligible}
\begin{equation*}
\label{eq:def-leo-eligible}
\LeoEligible{\Vloc{i}} \defined \\
\left\lbrace \;\;
\begin{gathered}
\Veim{eligible} \in \LeoUnique{\V{i}} :
\\ \RightRecursive{\Veim{eligible}}
\end{gathered}
\;\; \right\rbrace
\end{equation*}
and
\begin{equation*}
\label{eq:def-is-leo-eligible}
\myfn{IsLeoEligible}{\Veim{x}} \defined \\
\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\V{i}}.
\end{equation*}
We say that
an \realm{EIM} \Veim{eligible}
is \dfn{Leo eligible}
iff
\myfn{IsLeoEligible}{\V{eligible}}.
\dfEnd{}
\end{definition}

\begin{theorem}[Leo eligible location]
\label{th:leo-eligible-location}
\[
\myfn{IsLeoEligible}{\Veim{x}} \iff
\Veim{x} \in \LeoEligible{\Current{\V{x}}}.
\quad \thEnd
\]
\end{theorem}

\begin{proof}
For the forward direction we assume the antecedent to
show the consequent.
\begin{equation}
\label{eq:th-leo-eligible-location-10}
\myparbox{%
    $\myfn{IsLeoEligible}{\Veim{x}}$ \cuz{}
    AF forward direction.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-12}
\myparbox{%
   $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\V{i}}$
   \linebreak
   \cuz{}
       \longDfref{\V{IsLeoEligible}}{leo-eligible},
       \Eref{th-leo-eligible-location-10}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-14}
\myparbox{$\Veim{x} \in \LeoEligible{\Vloc{j}}$
   \cuz{} \Eref{th-leo-eligible-location-12},
   existential instantiation, arbitrary \Vloc{j}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-15}
\myparbox{$\Veim{x} \in \LeoUnique{\V{j}}$ \cuz{}
       \longDfref{Leo Eligible}{leo-eligible},
        \Eref{th-leo-eligible-location-14}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-16}
\myparbox{$\Veim{x} \in \VVelement{S}{j}$ \cuz{}
       \longDfref{Leo Unique}{leo-unique},
        \Eref{th-leo-eligible-location-15}.%
}
\end{equation}
\mypareq{eq:th-leo-eligible-location-18}{%
    $\Vloc{j} = \Current{\Veim{x}}$
    \cuz{} \longDfref{ET--EIM current location}{et-eim-current},
        \Eref{th-leo-eligible-location-16}.
}
\begin{equation}
\label{eq:th-leo-eligible-location-20}
\Veim{x} \in \LeoEligible{\Current{\V{x}}} \because
    \Eref{th-leo-eligible-location-14},
    \Eref{th-leo-eligible-location-18}.
\end{equation}
Equation \Eref{th-leo-eligible-location-20} is the consequent of
the forward direction, so that the derivation
\Eref{th-leo-eligible-location-10}--%
\Eref{th-leo-eligible-location-20} shows
the forward direction.

For the reverse direction, again we assume the antecedent
to show the consequent.
\begin{equation}
\label{eq:th-leo-eligible-location-30}
\myparbox{%
    $\Veim{x} \in \LeoEligible{\Current{\V{x}}}$
    \linebreak \cuz{}
    AF reverse direction.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-32}
\myparbox{%
    $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\V{i}}$
    \linebreak \cuz{}
    \Eref{th-leo-eligible-location-30},
    existential generalization.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-34}
\myparbox{%
   \myfn{IsLeoEligible}{\Veim{x}}
   \linebreak \cuz{}
       \longDfref{\V{IsLeoEligible}}{leo-eligible},
       \Eref{th-leo-eligible-location-32}.%
}
\end{equation}
Equation \Eref{th-leo-eligible-location-34} is the consequent of
the reverse direction, so that the derivation
\Eref{th-leo-eligible-location-30}--%
\Eref{th-leo-eligible-location-34} shows
the reverse direction.
\myqed
\end{proof}

\begin{theorem}[Leo eligible penult]
Every Leo eligible \realm{EIM} is a penult.  That is,
\label{th:leo-eligible-penult}
\[
  \myfn{IsLeoEligible}{\Veim{x}} \implies \Penult{\V{x}}.
  \quad \thEnd
\]
\end{theorem}

\begin{proof}
The proof is direct.
We assume the hypothesis to
show the conclusion.
\begin{equation}
\label{eq:th-leo-eligible-penult-10}
\myparbox{%
    $\myfn{IsLeoEligible}{\Veim{x}}$ \cuz{}
    hypothesis of theorem.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-12}
\myparbox{%
   $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\V{i}}$
   \linebreak
   \cuz{}
       \longDfref{\V{IsLeoEligible}}{leo-eligible},
       \Eref{th-leo-eligible-penult-10}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-14}
\myparbox{$\Veim{x} \in \LeoEligible{\Vloc{j}}$
   \cuz{} \Eref{th-leo-eligible-penult-12},
   existential instantiation, arbitrary \Vloc{j}.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-15}
\myparbox{$\Veim{x} \in \LeoUnique{\V{j}}$ \cuz{}
       \longDfref{Leo Eligible}{leo-eligible},
        \Eref{th-leo-eligible-penult-14}.
}
\end{equation}
\mypareq{eq:th-leo-eligible-penult-16}{%
$\Penult{\Veim{x}}$ \cuz{}
       \longDfref{Leo Unique}{leo-unique},
        \Eref{th-leo-eligible-penult-15}.
}
\Eref{th-leo-eligible-penult-16} is the conclusion of
the theorem and the derivation
\Eref{th-leo-eligible-penult-10}--%
\Eref{th-leo-eligible-penult-16}
show the theorem.
\myqed
\end{proof}

\begin{theorem}[Leo eligible postdot uniqueness]
\label{th:leo-eligible-postdot-uniqueness}
If \Veim{x} is a Leo eligible \realm{EIM},
then it is the only \realm{EIM} at that location
with that postdot symbol.
That is, if
\begin{gather}
  \label{eq:th-leo-eligible-postdot-uniqueness-02}
    \myfn{IsLeoEligible}{\Veim{x}} \\
  \label{eq:th-leo-eligible-postdot-uniqueness-04}
    \land \; \Current{\V{x}} = \Current{\Veim{y}} \\
  \label{eq:th-leo-eligible-postdot-uniqueness-06}
    \land \; \Postdot{\V{x}} = \Postdot{\V{y}}
\end{gather}
then
\begin{gather}
  \label{eq:th-leo-eligible-postdot-uniqueness-08}
  \V{x} = \V{y}.
  \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof strategy is direct,
assuming the hypothesis to show the conclusion.
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-14}
\begin{gathered}
\Veim{x} \in \LeoEligible{\Current{\V{x}}} \\
\because \longThref{Leo eligible location}{leo-eligible-location},
    \Eref{th-leo-eligible-postdot-uniqueness-02}.
\end{gathered}
\end{equation}
\mypareq{eq:th-leo-eligible-postdot-uniqueness-22}{%
    $\Veim{x} \in \LeoUnique{\Current{\V{x}}}$
    \linebreak \cuz{} \longDfref{Leo Eligible}{leo-eligible},
    \Eref{th-leo-eligible-postdot-uniqueness-14}.
}
\mypareq{eq:th-leo-eligible-postdot-uniqueness-24}{%
    $\forall \; \Veim{y} \in \VVelement{S}{\Current{\V{x}}} :$
    \linebreak $\Postdot{\V{x}} = \Postdot{\V{y}} \implies \V{x} = \V{y}$
    \linebreak \cuz{} \Eref{th-leo-eligible-postdot-uniqueness-22},
        \longDfref{Leo unique}{leo-unique}.
}
%
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-25-20}
\begin{gathered}
    \myparbox{%
        $\forall \; \Veim{z} \in \Velement{S}{\Current{\V{y}}} :$ \linebreak
        $\Postdot{\V{x}} = \Postdot{\V{z}} \implies \V{x} = \V{z}$ \linebreak
        \cuz
            \Eref{th-leo-eligible-postdot-uniqueness-04},
            \Eref{th-leo-eligible-postdot-uniqueness-24}.
    }
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-26}
\begin{gathered}
    \myparbox{%
        $\Postdot{\V{x}} = \Postdot{\V{y}}
            \implies \V{x} = \V{y}$
        \linebreak \cuz{} \Eref{th-leo-eligible-postdot-uniqueness-25-20},
        instantiating \Veim{z}
        as \Veim{y}.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-28}
\begin{gathered}
    \V{x} = \V{y}
    \because
        \Eref{th-leo-eligible-postdot-uniqueness-06},
        \Eref{th-leo-eligible-postdot-uniqueness-26}.
        \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[Leo eligible \realm{EIM} count]
\label{th:leo-eligible-eim-count}
There are at most \size{\Vocab{\Vint{g}}} Leo eligible
\realm{EIM}s in any ES.
That is,
\[
\begin{gathered}
\forall \; \Vloc{j} \in \Dom{\Vet{S}} :
\\ \size{
   \set{
            \Veim{x} \in \Velement{S}{\V{j}} :
    \myfn{IsLeoEligible}{\V{x}}
   }
}
\le \size{\Vocab{\Vint{g}}}.
\quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
Let \Veim{elig} be a Leo eligible \realm{EIM} in \Velement{S}{\V{j}}.
By \Thref{leo-eligible-postdot-uniqueness},
\V{elig} is the only \realm{EIM} in \Velement{S}{\V{j}}
with \Postdot{\V{elig}} as its postdot symbol.
This means there cannot be more Leo eligible \realm{EIM}s
in \Velement{S}{\V{j}} than there are postdot symbols.
There are only
$\size{\Vocab{\V{g}}}$ symbols in \Vint{g},
so the number of postdot symbols,
and therefore the number of Leo eligible \realm{EIM}s,
cannot exceed
$\size{\Vocab{\V{g}}}$.
\myqed
\end{proof}

\begin{definition}[Next eligible \realm{EIM}]
\label{def:next-eligible-eim}
\[
\begin{gathered}
\myfn{NextEligible}{\Veim{next},\Veim{prev}} \\
\defined \; \Matches{\V{next},\V{prev}} \\
\land \; \myfn{IsLeoEligible}{\V{next}} \land \myfn{IsLeoEligible}{\V{prev}}.
\end{gathered}
\]
We say that \Veim{next} is the \dfn{next Leo eligible \realm{EIM}},
or \dfn{next eligible \realm{EIM}}, of \Veim{prev}.
We say that \Veim{prev} is the \dfn{previous Leo eligible \realm{EIM}},
or \dfn{previous eligible \realm{EIM}}, of \Veim{next}.
\dfEnd
\end{definition}

\begin{theorem}[Next eligible connection]
\label{th:next-eligible-connection}
\begin{gather*}
\myfn{NextEligible}{\Veim{next},\Veim{prev}} \\
\implies
    \left( \begin{gathered}
    \Postdot{\V{next}} = \Symbol{\V{prev}} \\
    \land \; \Current{\V{next}} = \Origin{\V{prev}}
    \end{gathered} \right).
    \quad \thEnd
\end{gather*}
\end{theorem}

\begin{proof}
The theorem follows from
\longDfref{Next eligible \realm{EIM}}{next-eligible-eim}
and \longDfref{Matching \realm{EIM}s}{matching-1}.
\myqed
\end{proof}

\begin{theorem}[Next eligible direction]
\label{th:next-eligible-direction}
The current location of eligible \realm{EIM}s in previous-to-next order
is non-increasing.  That is,
\begin{gather*}
    \myfn{NextEligible}{\Veim{next},\Veim{prev}} \\
    \implies \Current{\V{next}} \le \Current{\V{prev}}.
    \quad \thEnd
\end{gather*}
\end{theorem}

\begin{proof}
The theorem follows from
\longThref{Next eligible connection}{next-eligible-connection}
and
\longThref{EIM direction}{eim-direction}.
\myqed
\end{proof}

\begin{theorem}[Next eligible uniqueness]
\label{th:next-eligible-uniqueness}
\begin{gather}
    \label{eq:th-next-eligible-uniqueness-02}
    \myfn{NextEligible}{\Veim{next1},\Veim{prev}} \\
    \label{eq:th-next-eligible-uniqueness-04}
    \land \; \myfn{NextEligible}{\Veim{next2},\Veim{prev}} \\
    \nonumber \implies \\
    \label{eq:th-next-eligible-uniqueness-06}
    \V{next1} = \V{next2}.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
We assume the hypothesis of the theorem to show the conclusion.
\begin{equation}
\label{eq:th-next-eligible-uniqueness-10}
\begin{gathered}
    \Postdot{\V{next1}} = \Symbol{\V{prev}} \\
    \land \; \Current{\V{next1}} = \Origin{\V{prev}}
    \\ \because
    \longThref{Next eligible connection}{next-eligible-connection},
    \Eref{th-next-eligible-uniqueness-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-uniqueness-12}
\begin{gathered}
    \Postdot{\V{next2}} = \Symbol{\V{prev}} \\
    \land \; \Current{\V{next2}} = \Origin{\V{prev}}
    \\ \because
    \Thref{next-eligible-connection},
    \Eref{th-next-eligible-uniqueness-04}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-uniqueness-14}
\begin{gathered}
    \Postdot{\V{next1}} = \Postdot{\V{next2}} \\
    \land \; \Current{\V{next1}} = \Current{\V{next2}}
    \\ \because
    \Eref{th-next-eligible-uniqueness-10},
    \Eref{th-next-eligible-uniqueness-12}.
\end{gathered}
\end{equation}
\mypareq{eq:th-next-eligible-uniqueness-16}{%
    $\myfn{IsLeoEligible}{\V{next1}}$ \linebreak
        \cuz{}
        \longDfref{Next eligible \realm{EIM}}{next-eligible-eim},
        \Eref{th-next-eligible-uniqueness-02}.
}
\mypareq{eq:th-next-eligible-uniqueness-18}{%
    $\V{next1} = \V{next2}$ \linebreak
    \cuz{}
    \longThref{Leo eligible postdot uniqueness}{leo-eligible-postdot-uniqueness},
    \Eref{th-next-eligible-uniqueness-14},
    \Eref{th-next-eligible-uniqueness-16}.
    This is the conclusion of the theorem. \myqed
}
\end{proof}

\chapter{Leo eligible sequences}
\label{ch:les}

\todo[prepend, caption={``Leo eligible sequences'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Next eligible sequence]
\label{def:next-eligible-sequence}
A \dfn{next eligible sequence} (realm \realm{LES}) is
a non-empty sequence of valid, Leo eligible \realm{EIM}s
such that every \realm{EIM}
except the top \realm{EIM}
is the next eligible \realm{EIM} of the
previous \realm{EIM} in the sequence.  That is,
\[
\realm{LES} = \set{
   \begin{gathered}
       \Veimseq{les} : \forall \; \Vnat{i} \in \Dom{\V{les}} :
       \\ \Valid{\VVelement{les}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{les}{i}}
       \\ \land \; \left(
           \V{i} < \Vlastix{les} \implies \myfn{NextEligible}{\Velement{les}{\V{i}+1}, \VVelement{les}{i}}
       \right)
   \end{gathered}
   }.
\]
If \V{les} is an \realm{LES},
we say that \Velement{les}{0} is the \dfn{base} of \V{les},
and that \Velement{les}{\Vlastix{les}} is the \dfn{top} of \V{les}.
\dfEnd
\end{definition}

\begin{theorem}[Next Leo eligible rule]
\label{th:next-eligible-rule}
If
\begin{gather}
\label{eq:th-next-eligible-rule-03a}
\Current{\Veim{middle}} = \Current{\Veim{next}} \\
\label{eq:th-next-eligible-rule-03b}
\land \; \myfn{NextEligible}{\V{middle},\V{prev}} \\
\label{eq:th-next-eligible-rule-03c}
\land \; \myfn{NextEligible}{\V{next},\V{middle}}
\end{gather}
then
\begin{gather}
\label{eq:th-next-eligible-rule-06a}
\tuple{\Vsym{lhs} \de \Vsym{rhs}} \in \Rules{\Vint{g}} \\
\label{eq:th-next-eligible-rule-06b}
\land \; \Symbol{\V{middle}} = \Vsym{lhs} \\
\label{eq:th-next-eligible-rule-06c}
\land \; \Symbol{\V{prev}} = \Vsym{rhs}.
\quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct, assuming the hypothesis to
show the conclusion.
\begin{equation}
\label{eq:th-next-eligible-rule-10}
\begin{gathered}
\myparbox{\Matches{\V{middle},\V{prev}} \linebreak
\cuz{} \longDfref{Next Leo eligible}{next-eligible-eim},
    \Eref{th-next-eligible-rule-03b}.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-12}
\Postdot{\V{middle}} \neq \Lambda \because
    \Eref{th-next-eligible-rule-10}.
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-14}
\begin{gathered}
\myparbox{%
\DR{\V{middle}} =
    $\tuple{\Vsym{lhs} \de \Vstr{before} \mydot \Vsym{rhs} \Vstr{after}}$
    \linebreak \cuz{} \Eref{th-next-eligible-rule-12}, for arbitrary
    \Vsym{lhs}, \Vstr{before}, \Vsym{rhs}, \Vstr{after}.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-16}
\Vsym{lhs} = \Symbol{\V{middle}} \because \Eref{th-next-eligible-rule-14}.
\end{equation}
%
\mypareq{eq:th-next-eligible-rule-17}{%
    $\Postdot{\V{middle}} = \Symbol{\V{prev}}$ \linebreak
    \cuz{} \Eref{th-next-eligible-rule-10},
    \Eref{def-matches-eim-eim} in \longDfref{Matching}{matching-1}.%
}
\begin{equation}
\label{eq:th-next-eligible-rule-18}
\Vsym{rhs} = \Symbol{\V{prev}} \because \Eref{th-next-eligible-rule-14},
    \Eref{th-next-eligible-rule-17}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-20}{%
    \myfn{IsLeoEligible}{\V{middle}} \linebreak
    \cuz{}
    \longDfref{Next leo eligible}{next-eligible-eim},
    \Eref{th-next-eligible-rule-03b}.
}
\mypareq{eq:th-next-eligible-rule-24}{%
    $\V{middle} \in \LeoEligible{\Current{\V{middle}}}$ \linebreak
        \cuz{} \longThref{Leo eligible location}{leo-eligible-location},
            \Eref{th-next-eligible-rule-20}.
}
\mypareq{eq:th-next-eligible-rule-26}{%
    $\V{middle} \in \LeoUnique{\Current{\V{middle}}}$ \linebreak
        \cuz{} \longDfref{Leo Eligible}{leo-eligible},
        \Eref{th-next-eligible-rule-24}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-28}
\Penult{\V{middle}} \neq \Lambda
    \because \longDfref{Leo unique}{leo-unique},
    \Eref{th-next-eligible-rule-26}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-29}{\Vint{g} has no nullable symbols
    \cuz{} \longDfref{Marpa internal grammar}{internal-grammar}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-30}
    \Vstr{after} = \epsilon \because
        \Eref{th-next-eligible-rule-14},
        \Eref{th-next-eligible-rule-28},
        \Eref{th-next-eligible-rule-29}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-32}{%
    \Matches{\V{next},\V{middle}} \linebreak
    \cuz{} \longDfref{Next Leo eligible}{next-eligible-eim},
    \Eref{th-next-eligible-rule-03c}.
}
\mypareq{eq:th-next-eligible-rule-34}{%
    $\Current{\V{next}} = \Origin{\V{middle}}$ \linebreak
    \cuz{} \Eref{def-matches-eim-eim} in \longDfref{Matching}{matching-1},
    \Eref{th-next-eligible-rule-32}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-36}
    \Current{\V{middle}} = \Origin{\V{middle}} \because
    \Eref{th-next-eligible-rule-03a},
    \Eref{th-next-eligible-rule-34}.
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-38}
    \Vstr{before} = \epsilon \because
        \Eref{th-next-eligible-rule-14},
        \Eref{th-next-eligible-rule-29},
        \Eref{th-next-eligible-rule-36}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-40}{%
    $\DR{\V{middle}} = \tuple{\Vsym{lhs} \de \mydot \Vsym{rhs}}$
    \linebreak \cuz{}
    \Eref{th-next-eligible-rule-14},
    \Eref{th-next-eligible-rule-30},
    \Eref{th-next-eligible-rule-38}.%
}
\begin{equation}
\label{eq:th-next-eligible-rule-42}
    \tuple{\Vsym{lhs} \de \Vsym{rhs}} \in \Rules{\Vint{g}}
    \because \Eref{th-next-eligible-rule-40}.
\end{equation}
The theorem is
    \Eref{th-next-eligible-rule-16},
    \Eref{th-next-eligible-rule-18}, and
    \Eref{th-next-eligible-rule-42}.
\myqed
\end{proof}

\begin{theorem}[LES slice]
\label{th:les-slice}
Every slice of an \realm{LES} is an \realm{LES}.
That is,
\begin{gather}
\label{eq:th-les-slice-02}
\V{seq} \in \realm{LES}
\\ \label{eq:th-les-slice-04}
\land \; \Vnat{a} \le \Vnat{b} \le \Vlastix{seq}
\\ \label{eq:th-les-slice-06}
\land \; \V{subseq} = \family{\VVelement{seq}{a} \ldots \VVelement{seq}{b}}
\\ \nonumber
\implies
\\ \label{eq:th-les-slice-08}
\V{subseq} \in \realm{LES}.
\quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct, assuming the hypothesis of
the theorem to show the conclusion.
\mypareq{eq:th-les-slice-10}{%
   \Vles{seq} is an \realm{EIM} sequence \linebreak
   \cuz{} \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-slice-02}.%
}
\mypareq{eq:th-les-slice-12}{%
   \Vles{subseq} is an \realm{EIM} sequence \linebreak
   \cuz{} \Eref{th-les-slice-06},
    \Eref{th-les-slice-10}.
}
\begin{equation}
\label{eq:th-les-slice-14}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\V{seq}} :
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \V{i} < \Vlastix{seq} \implies \myfn{NextEligible}{\Velement{seq}{\V{i}+1}, \VVelement{seq}{i}}
   \right)
   \\ \because \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-slice-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-slice-16}
\begin{gathered}
   \forall \; \Vnat{i} \in \set{ \Vnat{ii} : \V{a} \le \V{ii} \le \V{b} }
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \V{i} < \Vlastix{seq} \implies \myfn{NextEligible}{\Velement{seq}{\V{i}+1}, \VVelement{seq}{i}}
   \right)
    \\ \because \Eref{th-les-slice-04},
    \Eref{th-les-slice-14}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-slice-18}{%
    $\forall \; \V{i} : \V{i} < \V{b} \implies \V{i} < \Vlastix{seq}$
        \cuz{} \Eref{th-les-slice-04}.
}
\begin{equation}
\label{eq:th-les-slice-20}
\begin{gathered}
   \forall \; \Vnat{i} \in \set{ \Vnat{ii} : \V{a} \le \V{ii} \le \V{b} }
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \V{i} < \V{b} \implies \myfn{NextEligible}{\Velement{seq}{\V{i}+1}, \VVelement{seq}{i}}
   \right)
    \\ \because \Eref{th-les-slice-16},
    \Eref{th-les-slice-18}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-slice-30}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\V{subseq}} :
   \\ \Valid{\VVelement{subseq}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{subseq}{i}}
   \\ \land \; \left(
       \begin{gathered}
       \V{i} < \Vlastix{subseq} \implies
       \\ \myfn{NextEligible}{\Velement{subseq}{\V{i}+1}, \VVelement{subseq}{i}}
       \end{gathered}
   \right)
    \\ \because \Eref{th-les-slice-06},
        \Eref{th-les-slice-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-slice-40}
\V{subseq} \in \realm{LES}
  \because \longDfref{LES}{next-eligible-sequence},
   \Eref{th-les-slice-12},
   \Eref{th-les-slice-30}.
\end{equation}
\Eref{th-les-slice-40} is the conclusion
of this theorem.
\myqed
\end{proof}

\begin{theorem}[LES extension]
\label{th:les-extension}
Let \V{les} be an \realm{EIM} sequence,
and let \V{next} be an \realm{EIM}.
Then
\mypareq{eq:th-les-extension-02}{%
   $\family{\Velement{\V{les}}{0}\ldots
       \Velement{les}{\Vlastix{les}}, \V{next}} \in \realm{LES}$
}
iff
\begin{gather}
\label{eq:th-les-extension-04}
    \V{les} \in \realm{LES}
\\ \label{eq:th-les-extension-05}
    \land \; \Valid{\V{next}}
\\ \label{eq:th-les-extension-06}
    \land \; \myfn{IsLeoEligible}{\V{next}}
\\ \label{eq:th-les-extension-07}
    \land \; \myfn{NextEligible}{\V{next},\Velement{les}{\Vlastix{les}}}.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
For the forward direction,
we assume
\Eref{th-les-extension-02}
to show
\Eref{th-les-extension-04}--\Eref{th-les-extension-07}.
Equation \Eref{th-les-extension-04} follows via
the ``\realm{LES} slice'' theorem
\Thref{les-slice}.
Equations \Eref{th-les-extension-05}--\Eref{th-les-extension-07}
follow immediately from the definition of \realm{LES} \Dfref{next-eligible-sequence}.

It remains to show the reverse direction.
For this we assume \Eref{th-les-extension-04}--\Eref{th-les-extension-07}
to show
\Eref{th-les-extension-02}.
\begin{equation}
\label{eq:th-les-extension-10}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\V{les}} :
   \\ \Valid{\VVelement{les}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{les}{i}}
   \\ \land \; \left(
       \V{i} < \Vlastix{les} \implies \myfn{NextEligible}{\Velement{les}{\V{i}+1}, \VVelement{les}{i}}
   \right)
   \\ \because \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-extension-04}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-extension-12}
\begin{gathered}
   \V{ext} = \left [
       \Velement{\V{les}}{0}\ldots
       \Velement{les}{\Vlastix{les}}, \V{next}
   \right ]
   \\ \because \text{new \Vles{ext} for convenience.}
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-13-10}{%
   \V{les} is an \realm{EIM} sequence
       \cuz{} \longDfref{LES}{next-eligible-sequence},
        \Eref{th-les-extension-04}.
}
\mypareq{eq:th-les-extension-13-20}{%
   \V{next} is an \realm{EIM}
       \cuz{} AF theorem.
}
\mypareq{eq:th-les-extension-13-30}{%
   \V{ext} is an \realm{EIM} sequence
   \cuz{} \Eref{th-les-extension-13-10},
        \Eref{th-les-extension-13-20}.
}
\begin{equation}
\label{eq:th-les-extension-14}
\begin{gathered}
   \forall \; \Vnat{i} \in (\Dom{\V{ext}} - \set{\Vlastix{ext}}) :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \V{i} < \decr{\Vlastix{ext}} \implies \myfn{NextEligible}{\Velement{ext}{\V{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \Eref{th-les-extension-10},
    \Eref{th-les-extension-12}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-16}{%
   $\Velement{ext}{\Vlastix{ext}} = \V{next}$ \cuz{}
        \Eref{th-les-extension-12}.
}
\mypareq{eq:th-les-extension-18}{%
    $\Valid{\Velement{ext}{\Vlastix{ext}}}$
    \cuz{} \Eref{th-les-extension-05},
        \Eref{th-les-extension-16}.
}
\mypareq{eq:th-les-extension-20}{%
    $\myfn{IsLeoEligible}{\Velement{ext}{\Vlastix{ext}}}$
    \cuz{} \Eref{th-les-extension-06}.
        \Eref{th-les-extension-16}.
}
\begin{equation}
\label{eq:th-les-extension-22}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\V{ext}} :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \V{i} < \decr{\Vlastix{ext}} \implies \myfn{NextEligible}{\Velement{ext}{\V{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-extension-14},
    \Eref{th-les-extension-18},
    \Eref{th-les-extension-20}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-24}{%
    $\myfn{NextEligible}{\Velement{ext}{\Vlastix{ext}},\Velement{les}{\Vlastix{les}}}$
    \cuz{} \Eref{th-les-extension-07}, \Eref{th-les-extension-16}.
}
\mypareq{eq:th-les-extension-26}{%
    $\myfn{NextEligible}{\Velement{ext}{\Vlastix{ext}},\Velement{ext}{\decr{\Vlastix{ext}}}}$
    \cuz{} \Eref{th-les-extension-12}, \Eref{th-les-extension-24}.
}
\begin{equation}
\label{eq:th-les-extension-28}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\V{ext}} :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \V{i} < \Vlastix{ext} \implies \myfn{NextEligible}{\Velement{ext}{\V{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \Eref{th-les-extension-22}, \Eref{th-les-extension-26}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-30}{%
    $\V{ext} \in \realm{LES}$
        \cuz{} \longDfref{LES}{next-eligible-sequence},
        \Eref{th-les-extension-13-30}, \Eref{th-les-extension-28}.
}
The equation \Eref{th-les-extension-30} is the conclusion of the
reverse direction.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES adjacent direction]
\label{th:les-adjacent-direction}
If \Veim{prev} is an element of an \realm{LES},
and \Veim{next} is the next element,
then the current location of \V{next}
is less than or equal to the current location of \V{prev}.
That is, if
\[
\V{les} \in \realm{LES}
\land \; \Vnat{i} \le \decr{\Vlastix{les}}
\]
then
\[
\Current{\Velement{les}{\V{i}+1}} \le
\Current{\VVelement{les}{i}}.
\quad \thEnd
\]
\end{theorem}

\begin{proof}
The lemma follows from
   \longDfref{LES}{next-eligible-sequence},
   and \longThref{next eligible \realm{EIM} direction}{next-eligible-direction}.
\myqed
\end{proof}

\begin{theorem}[LES direction]
\label{th:les-direction}
The current location of
every element of an \realm{LES} is at or before
the current location of every lesser-indexed element
of the \realm{LES}.
That is,
\[
\begin{gathered}
\forall \; \V{les} \in \realm{LES} :
\forall \; \Vnat{hi} \in \Dom{\V{les}} :
\forall \; \Vnat{lo} \le \V{hi} :
\\ \Current{\VVelement{les}{hi}} \le
\Current{\VVelement{les}{lo}}.
\quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
The proof is by induction
on the higher-indexed
of the two \realm{LES} elements to be compared.
We will find that every \realm{LES} is finite,
but we have yet to prove this,
so we allow the \realm{LES} to be either finite
or countably infinite.
We let the induction index, \Vnat{indx},
be the higher
of the two \realm{LES} indexes.
The induction hypothesis is
\begin{equation}
\label{eq:th-les-direction-10}
\begin{gathered}
\myfn{INDH}{\Vnat{indx}} \defined
\\ \forall \; \V{les} \in \realm{LES} : \forall \; \Vnat{lo} \le \V{indx} :
    \V{indx} \in \Dom{\V{les}}
\\  \implies \Current{\VVelement{les}{indx}} \le \Current{\VVelement{les}{lo}}.
\end{gathered}
\end{equation}

We first show the induction hypothesis with an
induction index of 0,
as the basis of the induction.
\mypareq{eq:th-les-direction-12}{%
    $\V{indx} = 0$ \cuz{} AF basis,
        \Eref{th-les-direction-10}.
}
\mypareq{eq:th-les-direction-14}{%
    $\V{lesB} \in \realm{LES}$ \cuz{} new free \V{lesB}.
}
\mypareq{eq:th-les-direction-15}{%
    $\Vsize{lesB} \ge 1$
        \cuz{} \longDfref{LES}{next-eligible-sequence}.
}
\mypareq{eq:th-les-direction-16}{%
    $\V{indx} \in \Dom{\V{lesB}}$ \cuz{}
        \Eref{th-les-direction-12},
        \Eref{th-les-direction-15}.
}
\mypareq{eq:th-les-direction-17-10}{%
    $\V{loB} \le \V{indx}$ \cuz{}
        new free \V{loB}.
}
\mypareq{eq:th-les-direction-17-20}{%
    $\V{loB} = \V{indx}$ \cuz{}
        \Eref{th-les-direction-12},
        \Eref{th-les-direction-17-10}.
}
\mypareq{eq:th-les-direction-18}{%
    $\Current{\VVelement{lesB}{indx}} = \Current{\VVelement{lesB}{loB}}$
     \cuz{} \Eref{th-les-direction-16},
     \Eref{th-les-direction-17-20}.
}
\mypareq{eq:th-les-direction-19-10}{%
    $\V{indx} \in \Dom{\V{lesB}}$
    \linebreak $\implies \Current{\VVelement{lesB}{indx}} \le \Current{\VVelement{lesB}{loB}}$
    \linebreak \cuz{} \Eref{th-les-direction-16}, \Eref{th-les-direction-18}.
}
\begin{equation}
\label{eq:th-les-direction-19-20}
\begin{gathered}
\forall \; \Vnat{lo} \le \V{indx} : \V{indx} \in \Dom{\V{lesB}}
\\ \implies \Current{\VVelement{lesB}{indx}} \le \Current{\VVelement{lesB}{lo}}.
\\ \myparbox{\cuz{}
    UG of \V{loB},
    \Eref{th-les-direction-17-10},
    \Eref{th-les-direction-19-10}.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-direction-19-30}
\begin{gathered}
\forall \; \Vles{les} \in \realm{LES} : \forall \; \Vnat{lo} \le \V{indx} : \V{indx} \in \Dom{\V{les}}
\\ \implies \Current{\VVelement{les}{indx}} \le \Current{\VVelement{les}{lo}}
\\ \myparbox{\cuz{}
    UG of \V{lesB},
    \Eref{th-les-direction-14},
    \Eref{th-les-direction-19-20}.
    This is \myfn{INDH}{0}, and the basis of the induction.
}
\end{gathered}
\end{equation}
%
For the step, we assume
\myfn{INDH}{\V{n}} to show \myfn{INDH}{\V{n}+1}.
\begin{equation}
\label{eq:th-les-direction-30}
\begin{gathered}
    \forall \; \V{les} \in \realm{LES} : \forall \; \Vnat{lo} \le \V{n} :
\\ \V{n} \in \Dom{\V{les}} \implies \Current{\VVelement{les}{n}} \le \Current{\VVelement{les}{lo}}.
\\ \because \myfn{INDH}{\V{n}} \text{ AF step}.%
\end{gathered}
\end{equation}
\mypareq{eq:th-les-direction-31}{%
   $\V{indx} = \V{n}+1$ \cuz{} AF step.
}
\mypareq{eq:th-les-direction-32}{%
    $\V{lesP} \in \realm{LES}$ \cuz{} new free \V{loP}.
}
\mypareq{eq:th-les-direction-34}{%
    $\V{loP} \in \naturals$ \cuz{} new free \V{loP}.
}
\mypareq{eq:th-les-direction-36}{%
    $\Vnat{loP} \le \V{n} \land \V{n}+1 \in \Dom{\V{lesP}}$
    \cuz{} AF derivation.
}
\mypareq{eq:th-les-direction-37}{%
    $\Vnat{loP} \le \V{n}$
        \cuz{} \Eref{th-les-direction-36}.
}
\mypareq{eq:th-les-direction-37-10}{%
    $\V{n}+1 \in \Dom{\V{lesP}}$
        \cuz{} \Eref{th-les-direction-36}.
}
\mypareq{eq:th-les-direction-38}{%
    $\V{n} \in \Dom{\V{lesP}}$ \cuz{} \V{lesP} is a sequence,
        \Eref{th-les-direction-37-10}.
}
\mypareq{eq:th-les-direction-40}{%
    $\Current{\VVelement{lesP}{n}} \le \Current{\VVelement{lesP}{loP}}$
    \linebreak \cuz{} \Eref{th-les-direction-30},
        \Eref{th-les-direction-32},
        \Eref{th-les-direction-34},
        \Eref{th-les-direction-37},
        \Eref{th-les-direction-38}.
}
\mypareq{eq:th-les-direction-42}{%
    $\Current{\VVelement{lesP}{\V{n}+1}} \le \Current{\VVelement{lesP}{n}}$
    \cuz{}
    \longThref{LES adjacent direction}{les-adjacent-direction},
        \Eref{th-les-direction-32},
        \Eref{th-les-direction-37-10},
        \Eref{th-les-direction-38}.
}
\mypareq{eq:th-les-direction-44}{%
    $\Current{\VVelement{lesP}{\V{n}+1}} \le \Current{\VVelement{lesP}{loP}}$
    \linebreak \cuz{}
        \Eref{th-les-direction-40}, \Eref{th-les-direction-42}.
}
\begin{equation}
\label{eq:th-les-direction-46}
\begin{gathered}
    \Vnat{loP} \le \V{n} \land \V{n}+1 \in \Dom{\V{lesP}}
    \\ \implies \Current{\VVelement{lesP}{\V{n}+1}} \le \Current{\VVelement{lesP}{loP}}
    \\ \because \text{derivation
        \Eref{th-les-direction-36}--\Eref{th-les-direction-44}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-les-direction-50}{%
    $\Current{\VVelement{lesP}{\V{n}+1}} \le \Current{\VVelement{lesP}{\V{n}+1}}$
    \linebreak \cuz{} reflexivity, \Eref{th-les-direction-32},
        \Eref{th-les-direction-37-10}.
}
\begin{equation}
\label{eq:th-les-direction-52}
\begin{gathered}
    \Vnat{loP} = \V{n}+1 \land \V{n}+1 \in \Dom{\V{lesP}}
    \\ \implies \Current{\VVelement{lesP}{\V{n}+1}} \le \Current{\VVelement{lesP}{loP}}
    \\ \because \Eref{th-les-direction-46},
        \Eref{th-les-direction-50}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-direction-54}
\begin{gathered}
    \forall \; \V{les} \in \realm{LES} : \forall \; \V{lo} \le \V{n} + 1: \V{n}+1 \in \Dom{\V{les}}
    \\ \implies \Current{\VVelement{les}{\V{n}+1}} \le \Current{\VVelement{les}{lo}}
    \\ \myparbox{\cuz{}
        universal generalization of \V{loP} as \V{lo} and
        of \V{lesP} as \V{les},
        \Eref{th-les-direction-32}, \Eref{th-les-direction-34},
        \Eref{th-les-direction-52}.
        This is \myfn{INDH}{\V{n}+1}.
    }
\end{gathered}
\end{equation}
Equations \Eref{th-les-direction-30}--\Eref{th-les-direction-54}
derive \myfn{INDH}{\V{n}+1} from \myfn{INDH}{\V{n}},
which shows the step of the induction.
With the step and its basis
\Eref{th-les-direction-19-30},
we have the induction and theorem.
\myqed
\end{proof}

\begin{theorem}[LES current location run]
\label{th:les-current-location-run}
If two elements of an \realm{LES} are at the same
current location, they bound a run of \realm{LES} elements
at that current location.
That is, if
\begin{gather}
\label{eq:th-current-location-run-02}
\Vles{les} \in \realm{LES}
\\ \label{eq:th-current-location-run-04}
\land \; 0 \le \Vnat{start} \le \Vnat{end} \le \Vlastix{les}
\\ \label{eq:th-current-location-run-07}
\land \; \Vloc{loc} = \Current{\VVelement{les}{start}} = \Current{\VVelement{les}{end}}
\end{gather}
then
\begin{equation}
\label{eq:th-current-location-run-08}
\forall \; \V{i} \in \set{ \V{ii} : \V{start} \le \V{ii} \le \V{end}} :
    \Current{\VVelement{les}{i}} = \V{loc}.
    \quad \thEnd
\end{equation}
\end{theorem}

\begin{proof}
The proof strategy is to assume the hypothesis to show a reductio.
\mypareq{eq:th-current-location-run-10-10}{%
    $\Vsize{les} \ge 1$
        \cuz{} \longDfref{LES}{next-eligible-sequence},
        \Eref{th-current-location-run-02}.
}
\begin{equation}
\label{eq:th-current-location-run-10-20}
\begin{gathered}
\exists \; \Vnat{i} \in \set{ \Vnat{ii} : \V{start} \le \V{ii} \le \V{end}} :
    \Current{\VVelement{les}{i}} \neq \V{loc}
    \\ \myparbox{\cuz{} Contrary of
        \Eref{th-current-location-run-08},
        \Eref{th-current-location-run-10-10},
        AF reductio.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-current-location-run-12}
\begin{gathered}
\V{start} \le \V{bad} \le \V{end} \land
    \Current{\VVelement{les}{bad}} \neq \V{loc}
    \\ \myparbox{\cuz{} \Eref{th-current-location-run-10-20},
        instantiating \V{i} as \V{bad}.%
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-current-location-run-16}{%
    $\Current{\VVelement{les}{end}} \le \Current{\VVelement{les}{bad}} \le \Current{\VVelement{les}{start}}$
    \linebreak
    \cuz{} \longThref{LES direction}{les-direction},
    \Eref{th-current-location-run-12}.%
}
\mypareq{eq:th-current-location-run-18}{%
    $\Current{\VVelement{les}{bad}} \neq \V{loc}$
    \cuz{} \Eref{th-current-location-run-12}.%
}
\mypareq{eq:th-current-location-run-20}{%
    $\Current{\VVelement{les}{end}} \le \Current{\VVelement{les}{bad}} < \Current{\VVelement{les}{start}}$
    \linebreak
    \cuz{} \Eref{th-current-location-run-07},
    \Eref{th-current-location-run-16},
    \Eref{th-current-location-run-18}.%
}
\mypareq{eq:th-current-location-run-22}{%
    $\Current{\VVelement{les}{end}} < \Current{\VVelement{les}{start}}$
    \cuz{} \Eref{th-current-location-run-20}.%
}
\mypareq{eq:th-current-location-run-24}{%
    $\V{loc} < \V{loc}$
    \cuz{} \Eref{th-current-location-run-07},
    \Eref{th-current-location-run-22}.%
}
\Eref{th-current-location-run-24} is a contradiction and shows the
reductio and the theorem.
\myqed
\end{proof}

\begin{theorem}[LES unit derivation]
\label{th:les-unit-derivation}
If an \realm{LES} is of finite length at least 3,
and begins and ends at the same current location,
then there is a non-trivial unit derivation from
the symbol of its penultimate element to the
symbol of its first element. That is, if
\begin{gather}
    \label{eq:th-les-unit-derivation-02}
    \V{les} \in \realm{LES}
    \\ \label{eq:th-les-unit-derivation-04}
    \land \;\; 3 \le \Vsize{les} \; \land \; \Vsize{les} \in \naturals
    \\ \label{eq:th-les-unit-derivation-06}
    \land \; \Current{\Velement{les}{0}} =
    \Current{\Velement{les}{\Vlastix{les}}}
\end{gather}
then
\begin{equation}
    \label{eq:th-les-unit-derivation-08}
    \Symbol{\Velement{les}{\decr{\Vlastix{les}}}} \deplus
    \Symbol{\Velement{les}{0}}.
    \quad \thEnd
\end{equation}
\end{theorem}

\begin{proof}
The proof is by induction.
The induction variable is the length of the \realm{LES},
offset by 3.
The induction hypothesis is
\begin{equation}
\label{eq:th-les-unit-derivation-10}
\begin{gathered}
    \myfn{INDH}{\Vnat{offX}} \defined
    \\ \forall \; \V{les} \in \set{
        \begin{gathered}
        \Vles{ines} :
            \Vsize{ines} = \V{offX} + 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\decr{\Vlastix{ines}}}} \deplus
    \Symbol{\Velement{ines}{0}}.
\end{gathered}
\end{equation}

We take \myfn{INDH}{0} as the basis,
showing it directly.
\mypareq{eq:th-les-unit-derivation-11}{%
    $\size{\Vles{lesB}} = 3 \land
    \Current{\Velement{lesB}{\Vlastix{lesB}}}
        = \Current{\Velement{lesB}{0}}$
    AF derivation,
    new \Vles{lesB}.
}
\mypareq{eq:th-les-unit-derivation-12}{%
    $\Vles{lesB} = \family{\Veim{prev}, \Veim{middle}, \Veim{next}}$
    \cuz{} \Eref{th-les-unit-derivation-11},
    new \Veim{prev}, \Veim{middle}, \Veim{next}
    for convenience.
}
\mypareq{eq:th-les-unit-derivation-14}{%
    \Current{\V{prev}} = \Current{\V{next}}
    \cuz{} \Eref{th-les-unit-derivation-11},
        \Eref{th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-16}{%
    \Current{\V{prev}} =
    \Current{\V{middle}} =
    \Current{\V{next}}
    \cuz{} \longThref{LES current location run}{les-current-location-run},
        \Eref{th-les-unit-derivation-14}.
}
\mypareq{eq:th-les-unit-derivation-18}{%
    \myfn{NextEligible}{\V{next},\V{middle}}
    \cuz{} \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-20}{%
    \myfn{NextEligible}{\V{middle},\V{prev}}
    \cuz{} \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-22}{%
    $\tuple{\Symbol{\V{middle}} \de \Symbol{\V{prev}}}
     \in \Rules{\Vint{g}}$
    \linebreak \cuz{} \Eref{th-les-unit-derivation-16},
        \Eref{th-les-unit-derivation-18},
        \Eref{th-les-unit-derivation-20},
        \longThref{LES rule}{next-eligible-rule}.
}
\mypareq{eq:th-les-unit-derivation-24}{%
    $\Symbol{\Velement{lesB}{\decr{\Vlastix{lesB}}}} \deplus
        \Symbol{\Velement{lesB}{0}}$
    \linebreak \cuz{} \Eref{th-les-unit-derivation-12},
        \Eref{th-les-unit-derivation-22}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-25-10}
\begin{gathered}
    \size{\Vles{lesB}} = 3 \land
    \Current{\Velement{lesB}{\Vlastix{lesB}}}
        = \Current{\Velement{lesB}{0}}
    \\ \implies \Symbol{\Velement{lesB}{\decr{\Vlastix{lesB}}}} \deplus
        \Symbol{\Velement{lesB}{0}}
    \\ \myparbox{\cuz{}
        derivation \Eref{th-les-unit-derivation-11}--%
        \Eref{th-les-unit-derivation-24}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-unit-derivation-26}
\begin{gathered}
    \forall \; \V{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{2}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{1}} \deplus
    \Symbol{\Velement{ines}{0}}.
    \\ \myparbox{%
    \cuz{} universal generalization of arbitrary \V{lesB},
        \Eref{th-les-unit-derivation-25-10}.
        This is \myfn{INDH}{0}, the basis of the induction.
    }
\end{gathered}
\end{equation}

For the step we assume
\myfn{INDH}{\V{n}}
to show \myfn{INDH}{\V{n}+1}.
\begin{equation}
\label{eq:th-les-unit-derivation-30}
\begin{gathered}
    \forall \; \V{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = \V{n} + 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\decr{\Vlastix{ines}}}} \deplus
    \Symbol{\Velement{ines}{0}}
    \\ \myparbox{\cuz{} \myfn{INDH}{\V{n}}, AF step.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-unit-derivation-32}
\begin{gathered}
    \Vsize{\Vles{lesP}} = \V{n} + 3 + 1
    \\ \land \; \Current{\Velement{lesP}{\Vlastix{lesP}}} =
        \Current{\Velement{lesP}{0}}
    \\ \myparbox{\cuz{}
        AF derivation; new \V{lesP}.
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-36}{%
    $\forall \; \Vnat{i} < \Vlastix{lesP} : \Current{\VVelement{lesP}{i}} = \V{locP}$
    \linebreak \cuz{}
    \longThref{LES current location run}{les-current-location-run},
    \Eref{th-les-unit-derivation-32},
     new \V{locP} for convenience.
}
\mypareq{eq:th-les-unit-derivation-38}{%
    $\Vles{pre} = \family{\Velement{lesP}{0} \ldots \Velement{lesP}{\V{n}+2}}$
    \linebreak \cuz{} \longThref{LES slice}{les-slice},
    \Eref{th-les-unit-derivation-32};
    new \Vles{pre} for convenience.
}
\mypareq{eq:th-les-unit-derivation-39}{%
    $\Vsize{pre} = \V{n} + 3$
    \cuz{} \Eref{th-les-unit-derivation-38}.%
}
\mypareq{eq:th-les-unit-derivation-40}{%
    $\forall \; \Vnat{i} \le \V{n}+2 : \VVelement{pre}{i} = \VVelement{lesP}{i}$
    \cuz{} \Eref{th-les-unit-derivation-38}.
}
\mypareq{eq:th-les-unit-derivation-42}{%
    $\Current{\Velement{pre}{0}} = \Current{\Velement{pre}{\Vlastix{pre}}}$
    \linebreak \cuz{} \Eref{th-les-unit-derivation-36},
        \Eref{th-les-unit-derivation-39},
        \Eref{th-les-unit-derivation-40}.
}
\mypareq{eq:th-les-unit-derivation-46}{%
    $\Symbol{\Velement{pre}{\decr{\Vlastix{pre}}}} \deplus
    \Symbol{\Velement{pre}{0}}$
    \linebreak \cuz{} \Eref{th-les-unit-derivation-30},
    \Eref{th-les-unit-derivation-39},
    \Eref{th-les-unit-derivation-42}.
}
\mypareq{eq:th-les-unit-derivation-47-10}{%
    $\Vlastix{pre}-1 = \Vsize{pre}-2
        = \V{n}+1$
    \cuz{} \Eref{th-les-unit-derivation-39}.
}
\mypareq{eq:th-les-unit-derivation-47-20}{%
    $\V{n}+1
    = \Vsize{lesP}-3 =
    \Vlastix{lesP}-2$
    \cuz{} \Eref{th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-47-30}{%
    $\Vlastix{pre}-1 = \Vlastix{lesP}-2$
    \cuz{} \Eref{th-les-unit-derivation-47-10},
    \Eref{th-les-unit-derivation-47-20}.
}
\mypareq{eq:th-les-unit-derivation-48}{%
    $\Symbol{\Velement{lesP}{\Vlastix{lesP}-2}} \deplus
    \Symbol{\Velement{lesP}{0}}$
    \cuz{} \Eref{th-les-unit-derivation-40},
    \Eref{th-les-unit-derivation-46},
    \Eref{th-les-unit-derivation-47-30}.
}
\mypareq{eq:th-les-unit-derivation-50}{%
    \myfn{NextEligible}{%
        \Velement{lesP}{\Vlastix{lesP}},
        \Velement{lesP}{\Vlastix{lesP}-1}}
    \linebreak \cuz{} \longDfref{LES}{next-eligible-sequence},
    \Eref{th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-52}{%
    \myfn{NextEligible}{%
        \Velement{lesP}{\Vlastix{lesP}-1},
        \Velement{lesP}{\Vlastix{lesP}-2}}
    \linebreak \cuz{} \Dfref{next-eligible-sequence},
    \Eref{th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-54}{%
    $\Current{\Velement{lesP}{\Vlastix{lesP}-1}} = \Current{\Velement{lesP}{\Vlastix{lesP}-2}}$
    \linebreak \cuz{} \Eref{th-les-unit-derivation-32},
        \Eref{th-les-unit-derivation-36}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-56}
\begin{gathered}
    \tuple{
    \begin{gathered}
    \Symbol{\Velement{lesP}{\Vlastix{lesP}-1}}
    \\ \de \Symbol{\Velement{lesP}{\Vlastix{lesP}-2}}
    \end{gathered}
    }
     \in \Rules{\Vint{g}}
    \\ \because \longThref{LES rule}{next-eligible-rule},
    \Eref{th-les-unit-derivation-50},
    \Eref{th-les-unit-derivation-52},
    \Eref{th-les-unit-derivation-54}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-58}{%
    $\Symbol{\Velement{lesP}{\Vlastix{lesP}-1}} \deplus
    \Symbol{\Velement{lesP}{0}}$
    \cuz{} \Eref{th-les-unit-derivation-48},
    \Eref{th-les-unit-derivation-56}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-59-10}
\begin{gathered}
    \Vsize{\Vles{lesP}} = \V{n} + 3 + 1
    \\ \land \; \Current{\Velement{lesP}{\Vlastix{lesP}}} =
        \Current{\Velement{lesP}{0}}
    \\ \implies \Symbol{\Velement{lesP}{\Vlastix{lesP}-1}} \deplus
        \Symbol{\Velement{lesP}{0}}
    \\ \myparbox{\cuz{}
        derivation \Eref{th-les-unit-derivation-32}--%
        \Eref{th-les-unit-derivation-58}.
    }
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-les-unit-derivation-60}
\begin{gathered}
    \forall \; \V{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = \V{n} + 3 + 1
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\Vlastix{ines}-1}} \deplus
    \Symbol{\Velement{ines}{0}}
    \\ \myparbox{\cuz{}
        universal generalization of arbitrary \Vles{lesP},
        \Eref{th-les-unit-derivation-59-10}.
        This is \myfn{INDH}{\V{n} + 1}.
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-62}{%
    $\myfn{INDH}{\V{n}} \implies \myfn{INDH}{\V{n}+1}$
    \cuz{} derivation
        \Eref{th-les-unit-derivation-30}--\Eref{th-les-unit-derivation-60}.
        This is the step of the induction.
}

We have the basis of the induction
\Eref{th-les-unit-derivation-26}
and its step \Eref{th-les-unit-derivation-62}.
With the induction, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES finiteness]
\label{th:les-finiteness}
Let \V{les} be an arbitrary \realm{LES}.
Then
\begin{gather}
\label{eq:th-les-finiteness-02}
\Vsize{les} \le \V{max} \times (\Current{\Velement{les}{0}}+1),
\\ \label{eq:th-les-finiteness-04}
    \text{where } \Vnat{max} = \size{\Vocab{\Vint{g}}}+1.
\end{gather}
Also, every \realm{LES} is finite:
\begin{equation}
\label{eq:th-les-finiteness-06}
\forall \; \V{les} \in \realm{LES} : \V{les} \in \naturals.
\quad \thEnd
\end{equation}
\end{theorem}

\begin{proof}

The proof proceeds by reductio.
Before embarking on the reductio proper,
we note two facts.
First,
\mypareq{eq:th-les-finiteness-10}{%
    $\size{\Vles{les}} \ge 1$
        \cuz{} \longDfref{LES}{next-eligible-sequence}.
}
Equation \Eref{th-les-finiteness-10} will allow us to assume
that the element \Velement{les}{0} exists.
Second,
\begin{equation}
\label{eq:th-les-finiteness-12}
\begin{gathered}
\left( \begin{gathered}
    \forall \; \Vloc{i} \le \Current{\Velement{les}{0}} :
    \\ \size{\set{\Veim{eim} \in \V{les}: \Current{\var{eim}} = \var{i}}}
    \le \var{max}
\end{gathered} \right)
\\ \implies \Vsize{les} \le \var{max} \times (\Current{\Velement{les}{0}}+1).
\\ \because \Eref{th-les-finiteness-02}, \Eref{th-les-finiteness-04}.
\end{gathered}
\end{equation}
We now begin the reductio itself.
\mypareq{eq:th-les-finiteness-14}{%
    $\Vsize{les} > \var{max} \times (\Current{\Velement{les}{0}}+1)$
    \cuz{} negation of \Eref{th-les-finiteness-02}, AF reductio.%
}
\mypareq{eq:th-les-finiteness-16}{%
    $\exists \; \Vloc{i} \le \Current{\Velement{les}{0}} : \size{\set{\Veim{eim} \in \var{les}: \Current{\var{eim}} = \var{i}}}
        > \var{max}$
        \cuz{}
        contrapositive of \Eref{th-les-finiteness-12},
        \Eref{th-les-finiteness-14}.
}
\mypareq{eq:th-les-finiteness-18}{%
    $\size{\set{\var{eim} \in \var{les}: \Current{\var{eim}} = \var{i}}} > \var{max}$
    \cuz{} \Eref{th-les-finiteness-16}, existential instantiation of \var{i}.
}

The sequence \var{les} is countable,
and from \Eref{th-les-finiteness-18} we know that there are at
least $\var{max}+1$ elements
of \var{les} whose current location is \var{i}.
Therefore we can count out in \var{les} until we hit the
$\var{max}+1$'th element whose current location is \Vloc{i}.
Call this element \VVelement{les}{z}, so that
\begin{equation}
\label{eq:th-les-finiteness-20}
\begin{gathered}
   \Current{\VVelement{les}{z}} = \var{i}
   \\ \land \; \size{\set{\Vnat{ix} : \var{ix} \le \var{z} \land \Current{\VVelement{les}{ix}} = \var{i}}}
       = \var{max}+1.
   \\ \myparbox{\cuz{} \Eref{th-les-finiteness-18}, new \var{z}.}
\end{gathered}
\end{equation}

Let \Vles{les1} be the prefix of \var{les} such that
\begin{equation}
\label{eq:th-les-finiteness-22}
\Vles{les1} = \Velement{les}{0\ldots \var{z}}
    \because \longThref{LES slice}{les-slice},
        \Eref{th-les-finiteness-20}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-22-50}
    \forall \; \Vnat{ix} \le \var{z} : \VVelement{les1}{ix} = \VVelement{les}{ix}
    \because \Eref{th-les-finiteness-22}.
\end{equation}
There are \var{max} + 1 elements in \var{les1} whose
current location is \Vloc{i}:
\begin{equation}
\label{eq:th-les-finiteness-23b}
\begin{gathered}
    \size{\set{\Vnat{ix} \in \Dom{\var{les1}} : \Current{\VVelement{les1}{ix}} = \var{i}}}
       = \var{max}+1
    \\ \because \Eref{th-les-finiteness-20},
            \Eref{th-les-finiteness-22},
            \Eref{th-les-finiteness-22-50}.
\end{gathered}
\end{equation}
Since \var{les1} is a sequence
and contains at least one element whose current location is \Vloc{i}
\Eref{th-les-finiteness-23b},
there clearly is a first element of \var{les1}
whose current location is \Vloc{i}.
Call that first element \VVelement{les1}{a},
so that
\begin{gather}
\label{eq:th-les-finiteness-24a}
   \Vnat{a} \le \Vlastix{les1},
\\ \label{eq:th-les-finiteness-24b}
   \Current{\VVelement{les1}{a}} = \var{i}\text{, and}
\\ \label{eq:th-les-finiteness-26}
   \forall \; \var{ii} < \var{a} : \Current{\VVelement{les1}{ii}} \neq \var{i}.
\end{gather}

We let \var{run} be a new \realm{LES}:
\begin{equation}
\label{eq:th-les-finiteness-28}
\Vles{run} = \Velement{les1}{\var{a}\ldots \var{z}}
    \because \Thref{les-slice},
        \Eref{th-les-finiteness-22},
        \Eref{th-les-finiteness-24a}.
\end{equation}
\Vles{run} is so called because it is
a run of \realm{EIM}s with the property that
they share the same current location,
though we have yet to prove this.
We do so next.
\begin{equation}
\label{eq:th-les-finiteness-28-10}
\begin{gathered}
   \Current{\VVelement{les1}{a}} = \var{i} = \Current{\VVelement{les1}{z}}
       \\ \because \Eref{th-les-finiteness-20}, \Eref{th-les-finiteness-22-50},
       \Eref{th-les-finiteness-24b}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-28-20}
\begin{gathered}
    \forall \; \Vnat{ix} \in \set{ \Vnat{ix1} : \var{a} \le \var{ix1} \le \var{z}} :
    \\ \Current{\VVelement{les1}{ix}} = \Vloc{i}
    \\ \because \longThref{LES current location run}{les-current-location-run},
    \Eref{th-les-finiteness-28-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-28-30}
   \forall \; \Vnat{ix} \in \Dom{\var{run}} :
       \Current{\VVelement{run}{ix}} = \var{i}
   \because
    \Eref{th-les-finiteness-28},
    \Eref{th-les-finiteness-28-20}.
\end{equation}
We next characterize the cardinality of \var{run}.
\begin{equation}
\label{eq:th-les-finiteness-30}
\begin{gathered}
   \forall \; \var{ii} \in \Dom{\var{les1}} :
   \\ \Current{\VVelement{les1}{ii}} = \var{i}
   \iff \var{a} \le \var{ii} \le \var{z}
   \\ \because \Eref{th-les-finiteness-22},
    \Eref{th-les-finiteness-26},
    \Eref{th-les-finiteness-28-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-33}
   \Vsize{run} = \var{max}+1
   \because
    \Eref{th-les-finiteness-23b},
    \Eref{th-les-finiteness-28},
    \Eref{th-les-finiteness-30}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-34}
   \Vsize{run} =
     \size{\Vocab{\Vint{g}}}+2
\\ \because \Eref{th-les-finiteness-04},
    \Eref{th-les-finiteness-33}.
\end{equation}

We now look at a prefix of \var{run}.
\begin{equation}
\label{eq:th-les-finiteness-36}
\Veimseq{prerun} = \var{run}\left[ 0\ldots \size{\Vocab{\Vint{g}}} \right].
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-37}
\forall \; \var{i} \in \Dom{\var{prerun}} : \VVelement{prerun}{i} = \VVelement{run}{i}
\because \Eref{th-les-finiteness-36}
\end{equation}
Every \realm{EIM} in \var{prerun} has a symbol,
and since
\mypareq{}{%
there are only \size{\Vocab{\Vint{g}}} symbols in \Vint{g}, and
}
\mypareq{}{%
\var{prerun} contains \size{\Vocab{\Vint{g}}+1} \realm{EIM}s
    \Eref{th-les-finiteness-36},
}
at least one symbol must be used twice as the symbol of an
\realm{EIM} in \var{prerun}.
Let this symbol be \Vsym{cycleLHS},
and let the first and second \realm{EIM}s with \Vsym{cycleLHS} as their symbol be
\VVelement{prerun}{aa} and
\VVelement{prerun}{zz}, respectively:
\begin{equation}
\label{eq:th-les-finiteness-38}
\begin{gathered}
    \Symbol{\VVelement{prerun}{aa}} = \Symbol{\VVelement{prerun}{zz}}
    \land \; \var{aa} < \var{zz}
    \\ \text{\cuz{} arbitrary \Vnat{aa}, \Vnat{zz}.}%
\end{gathered}
\end{equation}

Let
\begin{equation}
\label{eq:th-les-finiteness-40}
\begin{gathered}
\Vles{cycle} = \Velement{run}{\var{aa}\ldots \var{zz}+1}
    \\ \myparbox{\cuz{}
        \longThref{LES slice}{les-slice},
        \Eref{th-les-finiteness-28},
        \Eref{th-les-finiteness-37},
        \Eref{th-les-finiteness-38}.
    }
\end{gathered}
\end{equation}
In \Eref{th-les-finiteness-40},
we know that \Velement{run}{\var{zz}+1} exists because of
\Eref{th-les-finiteness-34} and \Eref{th-les-finiteness-36}.
\begin{equation}
\label{eq:th-les-finiteness-42}
    \Symbol{\VVelement{run}{aa}} = \Symbol{\VVelement{run}{zz}}
    \because \Eref{th-les-finiteness-37},
        \Eref{th-les-finiteness-38}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-44}
\begin{gathered}
    \Symbol{\Velement{cycle}{0}} = \Symbol{\Velement{cycle}{\decr{\Vlastix{cycle}}}}
    \\ \because \Eref{th-les-finiteness-40}, \Eref{th-les-finiteness-42}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-46}
    \var{aa} < \var{zz} \because \Eref{th-les-finiteness-38}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-48}
    \Vsize{cycle} \ge 3 \because \Eref{th-les-finiteness-40}, \Eref{th-les-finiteness-46}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-52}
\begin{gathered}
        \forall \; \Vnat{ix} \in \Dom{\var{cycle}} : \Current{\VVelement{cycle}{ix}} = \Vloc{i}
        \\ \because \Eref{th-les-finiteness-28-30},
            \Eref{th-les-finiteness-40}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-54}
\begin{gathered}
    \Symbol{\Velement{cycle}{\decr{\Vlastix{cycle}}}} \deplus
    \Symbol{\Velement{cycle}{0}}
    \\ \myparbox{\longThref{LES unit derivation}{les-unit-derivation},
        \Eref{th-les-finiteness-40},
        \Eref{th-les-finiteness-48},
        \Eref{th-les-finiteness-52}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-56}
\begin{gathered}
    \Symbol{\Velement{cycle}{0}} \deplus
    \Symbol{\Velement{cycle}{0}}
    \\ \because
        \Eref{th-les-finiteness-44},
        \Eref{th-les-finiteness-54}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-finiteness-58}{\Vint{g} does not contain a cycle
    \cuz{} \longDfref{Marpa internal grammar}{internal-grammar}.
    This contradicts \Eref{th-les-finiteness-56} and concludes
    the reductio beginning at \Eref{th-les-finiteness-14}.%
}
\begin{equation}
\label{eq:th-les-finiteness-60}
\begin{gathered}
    \Vsize{les} \le \var{max} \times (\Current{\Velement{les}{0}}+1)
    \\ \myparbox{\cuz{} negation of \Eref{th-les-finiteness-14}
        by reductio \Eref{th-les-finiteness-14}--\Eref{th-les-finiteness-58}.%
    }
\end{gathered}
\end{equation}

The equation \Eref{th-les-finiteness-60}
is \Eref{th-les-finiteness-02} of the theorem.
We conclude \Eref{th-les-finiteness-06} of
the theorem immediately
from \Eref{th-les-finiteness-02},
completing the proof.
\myqed
\end{proof}

\begin{theorem}[LES uniqueness]
\label{th:les-uniqueness}
Two \realm{LES}'s of the same length have the same base element
iff they are identical.
That is,
\begin{gather}
\label{eq:th-les-uniqueness-02}
   \forall \; \var{les1} \in \realm{LES} :
       \forall \; \var{les2} \in \realm{LES} :
\\ \label{eq:th-les-uniqueness-04}
   \Vsize{les1} = \Vsize{les2}
\\ \label{eq:th-les-uniqueness-06}
   \land \; \Velement{les1}{0} = \Velement{les2}{0}
\\ \nonumber \iff
\\ \label{eq:th-les-uniqueness-08}
   \var{les1} = \var{les2}.
   \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
If two \realm{LES}'s are identical, they obviously have the same
base element and length.
This shows the reverse direction of the theorem trivially.

For the forward direction, we divide the proof into two subcases,
based on the value of \Vsize{les1}.
We show the first subcase directly.
\mypareq{eq:th-les-uniqueness-10-12}{%
    $\Vsize{les1} = 1$ \cuz{} AF first subcase.
}
\mypareq{eq:th-les-uniqueness-10-14}{%
    $\Vsize{les2} = 1$ \cuz{} \Eref{th-les-uniqueness-04},
        \Eref{th-les-uniqueness-10-12}.
}
\mypareq{eq:th-les-uniqueness-10-16}{%
   $\var{les1} = \family{\Velement{les1}{0}}
       = \family{\Velement{les2}{0}} = \var{les2}$
   \linebreak \cuz{} \Eref{th-les-uniqueness-06},
        \Eref{th-les-uniqueness-10-12},
        \Eref{th-les-uniqueness-10-14}.
}
\mypareq{eq:th-les-uniqueness-10-18}{%
   $\var{les1} = \var{les2}$ \cuz{} \Eref{th-les-uniqueness-10-16}.
   This is the consequent \Eref{th-les-uniqueness-08}
      and shows the first subcase of the forward direction.
}

For the second subcase, we proceed via a reductio.
\mypareq{eq:th-les-uniqueness-10-20}{%
    $\Vsize{les1} > 1$ \cuz{} AF second subcase.
}
\mypareq{eq:th-les-uniqueness-10-22}{%
    $\Vsize{les2} > 1$ \cuz{} \Eref{th-les-uniqueness-04},
        \Eref{th-les-uniqueness-10-20}.
}
For our reductio,
we assume that the two \realm{LES}'s differ.
\mypareq{eq:th-les-uniqueness-10-24}{%
   $\var{les1} \ne \var{les2}$ \cuz{} AF reductio.
}
If \var{les1} and \var{les2} differ, they must differ
in one of their elements.
Therefore, there will be a first index at which the elements differ.
\begin{gather}
\label{eq:th-les-uniqueness-11-10}
        \VVelement{les1}{\Vnat{first}} \neq \VVelement{les2}{\var{first}}
\\ \label{eq:th-les-uniqueness-11-20}
        \land \; \forall \; \Vnat{ix} < \var{first} : \VVelement{les1}{ix} = \VVelement{les2}{ix}
\\ \nonumber
    \because \Eref{th-les-uniqueness-10-24}, \text{new \Vnat{first}.}
\end{gather}
%
Since, by hypothesis for the theorem,
\var{les1} and \var{les2} share the same base
element, we know that
\begin{equation}
\label{eq:th-les-uniqueness-20}
\var{first} > 0 \because \Eref{th-les-uniqueness-06},
    \Eref{th-les-uniqueness-11-10}.
\end{equation}
%
\begin{equation}
\label{eq:th-les-uniqueness-22}
\begin{gathered}
    \myfn{NextEligible}{%
        \VVelement{les1}{first},
        \Velement{les1}{\Vdecr{first}}}
    \\ \land \; \myfn{NextEligible}{%
        \VVelement{les2}{first},
        \Velement{les2}{\Vdecr{first}}}
\\ \because
        \longDfref{LES}{next-eligible-sequence},
        \Eref{th-les-uniqueness-10-20},
        \Eref{th-les-uniqueness-10-22},
        \Eref{th-les-uniqueness-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-uniqueness-24}
\begin{gathered}
        \Velement{les1}{\Vdecr{first}}
        \neq \Velement{les2}{\Vdecr{first}}
    \\ \because \longThref{Next eligible uniqueness}{next-eligible-uniqueness},
    \Eref{th-les-uniqueness-22}.
\end{gathered}
\end{equation}
Equation \Eref{th-les-uniqueness-24}
contradicts \Eref{th-les-uniqueness-10-18},
which shows the reductio, the second subcase
and the forward direction.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES prefix]
\label{th:les-prefix}
Consider two \realm{LES}'s.
They have different lengths but the same base element
iff the shorter one is a proper prefix of the longer one.
That is,
\begin{gather}
\label{eq:th-les-prefix-02}
   \forall \; \var{les1} \in \realm{LES} :
       \forall \; \var{les2} \in \realm{LES} :
\\ \label{eq:th-les-prefix-04}
   \Vsize{les1} < \Vsize{les2}
\\ \label{eq:th-les-prefix-06}
   \land \; \Velement{les1}{0} = \Velement{les2}{0}
\\ \nonumber \iff
\\ \label{eq:th-les-prefix-08}
   \PrPfx{\var{les1},\var{les2}}.
   \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
For the reverse direction,
\Eref{th-les-prefix-04} and
\Eref{th-les-prefix-06}
follow immediately from
\Eref{th-les-prefix-08},
via the definition of proper prefix \Dfref{prefix}.

It remains to show the forward direction.
Let
\mypareq{eq:th-les-prefix-20}{%
    $\Vles{subLes2} = \Velement{les2}{0\ldots \Vlastix{les1}}$
    \linebreak \cuz{} \longThref{LES slice}{les-slice},
       antecedent of forward direction
           \Eref{th-les-prefix-02}--\Eref{th-les-prefix-06},
           new \Vles{subLes2} for convenience.
}
\mypareq{eq:th-les-prefix-21}{%
    \PrPfx{\var{subLes2},\var{les2}} \cuz{}
        \longDfref{Proper prefix}{prefix},
        \Eref{th-les-prefix-04}, \Eref{th-les-prefix-20}.
}
\begin{equation}
\label{eq:th-les-prefix-22}
\Vsize{subLes2} = \Vsize{les1} \because \Eref{th-les-prefix-20}
\end{equation}
\mypareq{eq:th-les-prefix-24}{%
    $\Velement{subLes2}{0} = \Velement{les2}{0}$ \cuz{} \Eref{th-les-prefix-20}.
}
\mypareq{eq:th-les-prefix-26}{%
    $\Velement{subLes2}{0} = \Velement{les1}{0}$
    \linebreak \cuz{}
       antecedent of forward direction \Eref{th-les-prefix-06},
       \Eref{th-les-prefix-24}.
}
\mypareq{eq:th-les-prefix-27}{%
    $\var{subLes2} = \var{les1}$
    \linebreak \cuz{} \longThref{LES uniqueness}{les-prefix},
        \Eref{th-les-prefix-22},
        \Eref{th-les-prefix-26}.
}
\mypareq{eq:th-les-prefix-28}{%
    $\PrPfx{\Vles{les1}, \Vles{les2}}$
    \cuz{} \Eref{th-les-prefix-21},
    \Eref{th-les-prefix-27}.
}
Equation \Eref{th-les-prefix-28}
is the consequent of the forward direction,
which shows the forward direction.
With both directions, we have the proof.
\myqed
\end{proof}

\begin{theorem}[LES trichotomy]
\label{th:les-trichotomy}
If two \realm{LES}'s have the same base element,
then exactly one of the following three statements is true:
\begin{itemize}
\item The two \realm{LES}'s are identical.
\item The first \realm{LES} is a proper prefix of the second.
\item The second \realm{LES} is a proper prefix of the first.
\end{itemize}
That is, if
\begin{gather}
\label{eq:th-les-trichotomy-02}
\var{les1} \in \realm{LES}
\\ \label{eq:th-les-trichotomy-04}
\; \land \; \var{les2} \in \realm{LES}
\\ \label{eq:th-les-trichotomy-06}
\land \; \Velement{les1}{0} = \Velement{les2}{0}
\end{gather}
then
\mypareq{eq:th-les-trichotomy-08}{%
   \size{\var{fn} \in \set{\var{Eq},\var{Lt},\var{Gt}} :
       \myfn{fn}{\var{les1},\var{les2}}} = 1
}
where \var{Eq}, \var{Lt} and \var{Gt} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-les-trichotomy-09a}
    \myfn{Eq}{\Vles{x},\Vles{y}} \iff \var{x} = \var{y},
    \\ \label{eq:th-les-trichotomy-09b}
    \myfn{Lt}{\Vles{x},\Vles{y}} \iff \PrPfx{\var{les1},\var{les2}}\text{, and}
    \\ \label{eq:th-les-trichotomy-09c}
    \myfn{Gt}{\Vles{x},\Vles{y}} \iff \PrPfx{\var{les2},\var{les1}}.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct.
We first note that,
by the Trichotomy Law for the ordering of the natural numbers,
\mypareq{eq:th-les-trichotomy-10}{%
   \size{\var{fn} \in \set{\var{EqSz},\var{LtSz},\var{GtSz}} :
       \myfn{fn}{\var{les1},\var{les2}}} = 1
}
where \var{EqSz}, \var{LtSz} and \var{GtSz} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-les-trichotomy-12}
   \myfn{EqSz}{\Vles{x},\Vles{y}} \iff \Vsize{x} = \Vsize{y},
\\ \label{eq:th-les-trichotomy-14}
    \myfn{LtSz}{\Vles{x},\Vles{y}} \iff \Vsize{les1} < \Vsize{les2}\text{, and}
\\ \label{eq:th-les-trichotomy-16}
    \myfn{GtSz}{\Vles{x},\Vles{y}} \iff \Vsize{les1} > \Vsize{les2}.
\end{gather}

We now show three derivations, one for each of the functions
\var{EqSz}, \var{LtSz} and \var{GtSz}.
\mypareq{eq:th-les-trichotomy-17-18}{%
   \myfn{EqSz}{\Vles{x},\Vles{y}} \cuz{} AF derivation;
   new arbitrary \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-20}{%
   $\Vsize{x} = \Vsize{y}$ \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-18}
   using \Eref{th-les-trichotomy-12}.
}
\mypareq{eq:th-les-trichotomy-17-22}{%
   $\Vles{x} = \Vles{y}$ \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-20}
   using \longThref{LES uniqueness}{les-uniqueness},
    \Eref{th-les-trichotomy-06}.
}
\mypareq{eq:th-les-trichotomy-17-24}{%
   \myfn{Eq}{\Vles{x},\Vles{y}} \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-22}
   using \Eref{th-les-trichotomy-09a}.
}
\mypareq{eq:th-les-trichotomy-17-26}{%
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{EqSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Eq}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} equivalence \Eref{th-les-trichotomy-17-18}--\Eref{th-les-trichotomy-17-24},
   \linebreak universal generalization of \Vles{x}, \Vles{y}.
}

\mypareq{eq:th-les-trichotomy-17-28}{%
   \myfn{LtSz}{\Vles{x},\Vles{y}} \cuz{} AF derivation;
   new arbitrary \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-30}{%
   $\Vsize{x} < \Vsize{y}$ \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-28}
   using \Eref{th-les-trichotomy-14}.
}
\mypareq{eq:th-les-trichotomy-17-32}{%
   $\Vles{x} = \Vles{y}$ \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-30}
   using \longThref{LES prefix}{les-prefix},
    \Eref{th-les-trichotomy-06}.
}
\mypareq{eq:th-les-trichotomy-17-34}{%
   \myfn{Lt}{\Vles{x},\Vles{y}} \cuz{}
   equivalence with \Eref{th-les-trichotomy-17-32}
   using \Eref{th-les-trichotomy-09b}.
}
\mypareq{eq:th-les-trichotomy-17-36}{%
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{LtSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Lt}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} equivalence \Eref{th-les-trichotomy-17-28}--\Eref{th-les-trichotomy-17-34},
   \linebreak universal generalization of \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-40}{%
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{GtSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Gt}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} symmetry with
   \Eref{th-les-trichotomy-17-28}--\Eref{th-les-trichotomy-17-36}.
}

The theorem follows from
\Eref{th-les-trichotomy-10}--\Eref{th-les-trichotomy-14},
\Eref{th-les-trichotomy-17-26},
\Eref{th-les-trichotomy-17-36} and \Eref{th-les-trichotomy-17-40}.
\myqed
\end{proof}

\begin{definition}[LIM validity]
\label{def:lim-validity}
The valid \realm{LIM}s of an \realm{EIM} are defined recursively.
Let \Veim{src} be an arbitrary \realm{EIM}.
Then the set of \realm{LIM}s of \var{src} is
\begin{equation}
\nonumber
\begin{gathered}
\myfn{lims}{\var{src}} \defined
\\ \left\lbrace
    \begin{gathered}
    \tuple{ \Vdr{dr}, \Postdot{\var{src}}, \Vorig{orig}, \Current{\var{src}}} :
    \\ \Valid{\var{src}} \land \myfn{IsLeoEligible}{\var{src}}
    \\ \land \;
        \left( \begin{gathered}
            \size{\myfn{nextLIMs}{\var{src}}} = 0
            \\ \implies \var{dr} = \DR{\var{src}} \land \var{orig} = \Origin{\var{src}}
        \end{gathered} \right)
    \\ \land \;
        \left( \begin{gathered}
            \size{\myfn{nextLIMs}{\var{src}}} > 0
            \\ \implies \mylim{\ell} \in \myfn{nextLIMs}{\var{src}}
            \\ \land \; \var{dr} = \DR{\ell} \land \var{orig} = \Origin{\ell}
        \end{gathered} \right)
    \end{gathered}
\right\rbrace,
\end{gathered}
\end{equation}
where
\begin{equation}
\nonumber
\begin{gathered}
\myfn{nextLIMs}{\var{src}} \defined
\\ \set{
    \begin{gathered}
    \ell \in \realm{LIM} : \exists\; \Veim{next} :
    \\ \myfn{NextEligible}{\var{next},\Veim{src}}
        \land\; \ell \in \myfn{lims}{\var{next}}
    \end{gathered}
}.
\end{gathered}
\end{equation}
We overload \myfn{Valid}{} so that
\begin{equation}
\nonumber
\begin{gathered}
    \Valid{\mylim{\ell}} \defined \exists \; \Veim{src} : \ell \in \myfn{lims}{\var{src}}
\end{gathered}
\end{equation}
and we say that \mylim{\ell} is \dfn{valid} if \Valid{\ell}.
If $\mylim{\ell} \in \myfn{lims}{\Veim{src}}$,
we say that \realm{EIM} \var{src} is the \dfn{source} of $\ell$,
and that the \realm{LIM} $\ell$ is the \dfn{LIM} of \Veim{src}.
\dfEnd
\end{definition}

\begin{definition}[Leo source \realm{EIM}]
\label{def:leo-source-eim}
\[
    \myfn{isSource}{\Veim{src}} \defined \size{\myfn{lims}{\var{src}}} > 0.
\]
We say that \var{src} is
a \dfn{Leo source \realm{EIM}}, or a \dfn{source \realm{EIM}},
if \myfn{isSource}{\Veim{src}}.
\dfEnd
\end{definition}

\begin{theorem}[Leo source function]
\label{th:leo-source-fn}
The function
\begin{gather}
\label{eq:th-leo-source-fn-05}
\deffn{Source}{%
    \set{\mylim{\ell} : \Valid{\ell}}
}{%
    \set{\Veim{src} : \myfn{isSource}{\var{src}}}
}
\\ \label{eq:th-leo-source-fn-07}
    \text{such that } \Source{\mylim{\ell}} \defined \rIota \Veim{src} : \ell \in \myfn{lims}{\var{src}}
\end{gather}
exists and is a surjection.
\thEnd
\end{theorem}

\begin{proof}
The proof strategy is to
define a relation \var{Source-Rel},
to show that \var{Source-Rel} is the equivalent of \fname{Source},
and to show that the required properties hold for \var{Source-Rel}.
\mypareq{eq:th-leo-source-fn-10-01}{%
   $\var{Source-Rel} = \set{ \tuple{ \ell, \var{src} } \in \realm{LIM} \times \realm{EIM} :
         \ell \in \myfn{lims}{\var{src}}
   }$ exists \cuz{} obvious.
}
By the definition of \realm{LIM} validity,
\mypareq{eq:th-leo-source-fn-10-02}{%
    $\forall \; \mylim{\ell} : \Valid{\ell} \iff \exists \; \Veim{src} : \ell \in \myfn{lims}{\var{src}}$
    \cuz{} \Dfref{lim-validity}.
}
\mypareq{eq:th-leo-source-fn-10-03}{%
    $\Dom{\var{Source-Rel}} = \Dom{\var{Source}}$
    \cuz{} \Eref{th-leo-source-fn-05},
        \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-02}.
}
By the definition of Leo source \realm{EIM},
\mypareq{eq:th-leo-source-fn-10-04}{%
    $\forall \; \Veim{src} : \myfn{isSource}{\var{src}}
    \iff \exists \; \ell  : \ell \in \myfn{lims}{\var{src}}$
    \cuz{} \Dfref{leo-source-eim}.
}
\mypareq{eq:th-leo-source-fn-10-05}{%
    $\Ran{\var{Source-Rel}} = \Cod{\var{Source}}$
    \cuz{} \Eref{th-leo-source-fn-05},
        \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-04}.
}
\mypareq{eq:th-leo-source-fn-10-09-10}{%
    If \var{Source-Rel} is a function,
    then it is
    equivalent to the function \fname{Source}
    \cuz{}
        \Eref{th-leo-source-fn-05},
        \Eref{th-leo-source-fn-07},
        \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-03},
        \Eref{th-leo-source-fn-10-05}.
}
\mypareq{eq:th-leo-source-fn-10-09-20}{%
    If \var{Source-Rel} is a function,
    then it is
    a total function
    \linebreak \cuz{} \Eref{th-leo-source-fn-10-03},
        \Eref{th-leo-source-fn-10-09-10}.
}
\mypareq{eq:th-leo-source-fn-10-09-30}{%
    If \var{Source-Rel} is a function,
    then it is a surjection
    \linebreak \cuz{} \Eref{th-leo-source-fn-10-05},
        \Eref{th-leo-source-fn-10-09-10}.
}
It remains to show that \var{Source-Rel} is a function.
To this we assume, for a derivation, that two tuples of
\var{Source-Rel} have the same argument.
\begin{gather}
\label{eq:th-leo-source-fn-10-09-32}
\tuple{ \mylim{\ell}, \Veim{src1} } \in \var{Source-Rel}
\\ \label{eq:th-leo-source-fn-10-09-34}
\land \; \tuple{ \ell, \Veim{src2} } \in \var{Source-Rel}
\\ \nonumber
\text{\cuz{} AF derivation,
    new arbitrary \mylim{\ell}, \Veim{src1}, \Veim{src2}.}
\end{gather}
\mypareq{eq:th-leo-source-fn-10-09-36}{%
    $\ell \in \myfn{lims}{\var{src1}}$
    \cuz{} \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-09-32}.
}
\mypareq{eq:th-leo-source-fn-10-09-38}{%
    $\ell \in \myfn{lims}{\var{src2}}$
    \cuz{} \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-09-34}.
}
\mypareq{eq:th-leo-source-fn-10-09-40}{%
    \myfn{IsLeoEligible}{\var{src1}}
    \cuz{} \longDfref{LIM validity}{lim-validity},
        \Eref{th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-10}{%
    $\Transition{\ell} = \Postdot{\var{src1}}$
    \cuz{} \Dfref{lim-validity},
        \Eref{th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-12}{%
    $\Current{\ell} = \Current{\var{src1}}$
    \cuz{} \Dfref{lim-validity},
        \Eref{th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-22}{%
    $\Transition{\ell} = \Postdot{\var{src2}}$
    \cuz{} \Dfref{lim-validity},
        \Eref{th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-24}{%
    $\Current{\ell} = \Current{\var{src2}}$
    \cuz{} \Dfref{lim-validity},
        \Eref{th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-28}{%
    $\Postdot{\var{src1}} = \Postdot{\var{src2}}$
    \cuz{} \Eref{th-leo-source-fn-10},
        \Eref{th-leo-source-fn-22}.
}
\mypareq{eq:th-leo-source-fn-30}{%
    $\Current{\var{src1}} = \Current{\var{src2}}$
    \cuz{} \Eref{th-leo-source-fn-12},
        \Eref{th-leo-source-fn-24}.
}
\mypareq{eq:th-leo-source-fn-34}{%
    $\var{src1} = \var{src2}$
    \cuz{} \longThref{Leo eligible postdot uniqueness}{leo-eligible-postdot-uniqueness},
        \Eref{th-leo-source-fn-10-09-40},
        \Eref{th-leo-source-fn-28},
        \Eref{th-leo-source-fn-30}.
}
\begin{equation}
\begin{gathered}
\label{eq:th-leo-source-fn-36}
    \forall \; \mylim{\ell} : \forall \; \Veim{src1} : \forall \; \Veim{src2} :
    \\ \tuple{ \mylim{\ell}, \Veim{src1} } \in \var{Source-Rel}
        \land \; \tuple{ \ell, \Veim{src2} } \in \var{Source-Rel}
    \\ \implies \var{src1} = \var{src2}
    \\ \cuzbox{derivation from
        \Eref{th-leo-source-fn-10-09-32}--\Eref{th-leo-source-fn-10-09-32}
        to \Eref{th-leo-source-fn-34};
        universal generalization of \mylim{\ell}, \Veim{src1} and \Veim{src2}.
    }
\end{gathered}
\end{equation}
By the definition of a functional relation,
\mypareq{eq:th-leo-source-fn-38}{%
    \var{Source-Rel} is a function
    \cuz{} \Eref{th-leo-source-fn-36}.
}
\mypareq{eq:th-leo-source-fn-40}{%
    The total function \fname{Source} exists, and is a surjection
    \cuz{} \Eref{th-leo-source-fn-10-01},
        \Eref{th-leo-source-fn-10-09-10},
        \Eref{th-leo-source-fn-10-09-20},
        \Eref{th-leo-source-fn-10-09-30},
        \Eref{th-leo-source-fn-38}.
        \myqed
}
\end{proof}

\begin{theorem}[LIM direction]
\label{th:lim-direction}
The current location of a \realm{LIM} is
the current location of its source.
That is,
\[
\forall \; \var{src} \in \realm{EIM} :
\forall \; \ell \in \myfn{lims}{\var{src}} :
    \Current{\ell} = \Current{\var{src}}.
    \quad \thEnd
\]
\end{theorem}

\begin{proof}
The theorem follows immediately from the
definition of \realm{LIM} validity \Dfref{lim-validity}.
\myqed
\end{proof}

\chapter{Leo source sequences}
\label{chap:lss}

\todo[prepend, caption={``Leo source sequences'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{definition}[Leo source sequence {[}LSS{]}]
\label{def:lss}
A \dfn{Leo source \realm{EIM} sequence},
or \dfn{Leo source sequence} is an \realm{LES},
every element of which is a Leo source \realm{EIM}.
That is,
\begin{equation}
\label{eq:def-leo-source-sequence-10}
\realm{LSS} = \set{
    \begin{gathered}
        \var{les} \in \realm{LES} :
        \\ \forall \; \Vnat{i} \in \Dom{\var{les}} : \myfn{isSource}{\VVelement{les}{i}}
    \end{gathered}
}.  \quad \dfEnd
\end{equation}
\end{definition}

\begin{theorem}[LSS--LES identity]
\label{th:lss-les-identity}
Every \realm{LSS} is identical to exactly one \realm{LES}.
That is,
\[
\begin{gathered}
    \forall \; \Vlss{lss} : \existUniqQOp \; \Vles{les} : \var{lss} = \var{les}.
    \quad \thEnd
\end{gathered}
\]
\end{theorem}

\begin{proof}
By the definition of \realm{LSS} \Dfref{lss},
the realm \realm{LSS} is a subset of \realm{LES}.
This means that every member \realm{LSS} is a member of
the set \realm{LES}, in other words, identical to
a member of \realm{LES}, so that
\mypareq{eq:th-lss-les-identity-10}{%
    $\forall \; \Vlss{lss} : \exists \; \Vles{les} : \var{lss} = \var{les}$.
}

And obviously,
\mypareq{eq:th-lss-les-identity-20}{%
    $\forall \; \Vlss{lss} :
    \forall \; \Vles{les1} :
    \forall \; \Vles{les2} :$
    \linebreak $\var{lss} = \var{les1} \land
    \var{lss} = \var{les2} \implies
    \var{les1} = \var{les2}$.
}
The theorem follows from
\Eref{th-lss-les-identity-10} and
\Eref{th-lss-les-identity-20}.
\myqed
\end{proof}

\begin{definition}[Underlying LES]
\label{def:underlying-les}
If $\Vles{les} = \Vlss{lss}$ we say that
\var{les} is the \dfn{underlying LES} of \Vlss{lss}.
\dfEnd
\end{definition}

\begin{theorem}[Source \realm{EIM} containment]
\label{th:source-eim-containment}
Every source \realm{EIM} is the base element of at least one \realm{LSS}.
That is,
\[
\forall \; \Veim{eim} : \myfn{isSource}{\var{eim}} \implies
    \exists \; \Vlss{lss} : \var{eim} = \Velement{lss}{0}.
    \quad \thEnd
\]
\end{theorem}

\begin{proof}
Our proof strategy is first,
given an arbitrary source \realm{EIM},
call it \Veim{eim},
to construct an example
of an \realm{LSS} which has \var{eim}
as its base element.
The theorem will follow immediately by existential generalization.

\mypareq{eq:th-source-eim-containment-10-10}{%
    $\myfn{isSource}{\var{eim}}$
    \cuz{} \longDfref{Leo source \realm{EIM}}{leo-source-eim},
    AF derivation.
}
\mypareq{eq:th-source-eim-containment-10-20}{%
    $\Valid{\var{eim}}$
    \cuz{} \Dfref{leo-source-eim},
        \longDfref{LIM validity}{lim-validity},
        \Eref{th-source-eim-containment-10-10}.
}
\mypareq{eq:th-source-eim-containment-10-30}{%
    $\myfn{IsLeoEligible}{\var{eim}}$
    \cuz{} \Dfref{leo-source-eim},
        \Dfref{lim-validity},
        \Eref{th-source-eim-containment-10-10}.
}
Our example of an \realm{LSS} containing \var{eim} as its base element is
\mypareq{eq:th-source-eim-containment-12}{%
   $\var{example} = \family{\var{eim}}$ \cuz{} AF construction.
}
\mypareq{eq:th-source-eim-containment-13}{%
    $\Velement{example}{0} = \var{eim}$ \cuz{}
        \Eref{th-source-eim-containment-12}.
}
\mypareq{eq:th-source-eim-containment-14}{%
   $\Dom{\var{example}} = \set{0}$ \cuz{} \Eref{th-source-eim-containment-12}.
}
\begin{equation}
\label{eq:th-source-eim-containment-16}
\begin{gathered}
       \forall \; \Vnat{i} < \Vlastix{example} :
   \\ \myfn{NextEligible}{\Velement{example}{\var{i}+1}, \VVelement{example}{i}}.
   \\ \because \text{vacuously}, \Eref{th-source-eim-containment-13}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-source-eim-containment-17}
   \begin{gathered}
        \forall \; \Vnat{i} \in \Dom{\var{example}} :
       \\ \Valid{\VVelement{example}{i}} \land \ \myfn{IsLeoEligible}{\VVelement{example}{i}}
       \\ \land \; \left(
           \begin{gathered}
           \var{i} < \Vlastix{example}
           \\ \implies \myfn{NextEligible}{\Velement{example}{\var{i}+1}, \VVelement{example}{i}}
           \end{gathered}
       \right)
       \\ \because
            \Eref{th-source-eim-containment-10-20},
            \Eref{th-source-eim-containment-10-30},
            \Eref{th-source-eim-containment-13},
            \Eref{th-source-eim-containment-14},
            \Eref{th-source-eim-containment-16}.
   \end{gathered}
\end{equation}
\mypareq{eq:th-source-eim-containment-18}{%
   $\var{example} \in \realm{LES}$ \cuz{}
        \longDfref{LES}{next-eligible-sequence},
       \Eref{th-source-eim-containment-17}.
}
\mypareq{eq:th-source-eim-containment-20}{%
   $\var{example} \in \realm{LSS}$ \cuz{}
       \longDfref{LSS}{lss},
       \Eref{th-source-eim-containment-10-10},
        \Eref{th-source-eim-containment-13},
        \Eref{th-source-eim-containment-14},
       \Eref{th-source-eim-containment-18}.
}
\mypareq{eq:th-source-eim-containment-22}{%
   $\var{example} \in \realm{LSS} \land \Velement{example}{0} = \Veim{eim}$
  \cuz{} \Eref{th-source-eim-containment-13},
    \Eref{th-source-eim-containment-20}.
}
\mypareq{eq:th-source-eim-containment-24}{%
   $\exists \; \Vlss{lss} : \Velement{lss}{0} = \Veim{eim}$
  \cuz{} \Eref{th-source-eim-containment-22},
     existential generalization of \Vlss{example}.
}
\mypareq{eq:th-source-eim-containment-30}{%
    $\myfn{isSource}{\var{eim}} \implies
   \exists \; \Vlss{lss} : \Velement{lss}{0} = \Veim{eim}$
    \cuz{}
   derivation \Eref{th-source-eim-containment-10-10}--%
    \Eref{th-source-eim-containment-24}.
}
The theorem is the existential generalization of \Veim{eim}
in \Eref{th-source-eim-containment-30}.
\myqed
\end{proof}

\begin{definition}[Maximal LSS]
\label{def:maximal-lss}
A \dfn{maximal LSS} is an \realm{LSS} which is not the proper prefix
of another \realm{LSS}.
We use the notation \Maximal{\Vlss{lss}}
to say that \var{lss} is maximal.
\dfEnd
\end{definition}

\begin{theorem}[LSS uniqueness]
\label{th:lss-uniqueness}
If two \realm{LSS}'s of the same length have the same base element,
then they are identical.
\thEnd
\end{theorem}

\begin{proof}
The proof is direct,
assuming the hypothesis of the theorem to
show its conclusion.
Let \Vlss{lss1} and \Vlss{lss2} be two \realm{LSS}'s,
and let their underlying \realm{LES}'s be \Vles{les1}
and \Vles{les2} so that
\begin{gather}
\label{eq:th-lss-uniqueness-10}
    \var{les1} \in \realm{LES} \land \var{les2} \in \realm{LES}
\\ \label{eq:th-lss-uniqueness-12}
    \land \; \var{les1} = \var{lss1}
\\ \label{eq:th-lss-uniqueness-14}
    \land \; \var{les2} = \var{lss2}
\\ \label{eq:th-lss-uniqueness-16}
    \land \; \Vsize{lss1} = \Vsize{lss2}
\\ \label{eq:th-lss-uniqueness-18}
    \land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\\ \nonumber \because \text{hypothesis of theorem.}
\end{gather}
\mypareq{eq:th-lss-uniqueness-26}{%
    $\Vsize{les1} = \Vsize{les2}$ \cuz{}
        \Eref{th-lss-uniqueness-12},
        \Eref{th-lss-uniqueness-14},
        \Eref{th-lss-uniqueness-16}.
}
\mypareq{eq:th-lss-uniqueness-28}{%
    $\Velement{les1}{0} = \Velement{les2}{0}$ \cuz{}
        \Eref{th-lss-uniqueness-12},
        \Eref{th-lss-uniqueness-14},
        \Eref{th-lss-uniqueness-18}.
}
\mypareq{eq:th-lss-uniqueness-30}{%
     $\var{les1} = \var{les2}$ \cuz{} \longThref{LES uniqueness}{les-uniqueness},
        \Eref{th-lss-uniqueness-26}, \Eref{th-lss-uniqueness-28}.
}
\mypareq{eq:th-lss-uniqueness-32}{%
     $\var{lss1} = \var{lss2}$ \cuz{}
        \Eref{th-lss-uniqueness-10}, \Eref{th-lss-uniqueness-30}.
}
Equation \Eref{th-lss-uniqueness-32} is the conclusion
of the the theorem.
\myqed
\end{proof}

\begin{theorem}[LSS slice]
\label{th:lss-slice}
Every slice of an \realm{LSS} is an \realm{LSS}.
That is, if
\begin{gather}
\label{eq:th-lss-slice-02}
\var{seq} \in \realm{LSS}
\\ \label{eq:th-lss-slice-04}
\land \; \Vnat{a} \le \Vnat{b} \le \Vlastix{seq}
\\ \label{eq:th-lss-slice-06}
\land \; \var{subseq} = \family{\VVelement{seq}{a} \ldots \VVelement{seq}{b}}
\end{gather}
then
\begin{gather}
\label{eq:th-lss-slice-08}
\var{subseq} \in \realm{LSS}. \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
\mypareq{eq:th-lss-slice-10}{%
   $\var{seq} \in \realm{LES}$
        \cuz{} \longDfref{LSS}{lss},
        \Eref{th-lss-slice-02}.%
}
\mypareq{eq:th-lss-slice-12}{%
   $\var{subseq} \in \realm{LES}$
        \cuz{} \longThref{LES slice}{les-slice},
        \Eref{th-lss-slice-04},
        \Eref{th-lss-slice-06},
        \Eref{th-lss-slice-10}.%
}
\begin{equation}
\label{eq:th-lss-slice-20}
\begin{gathered}
    \forall \; \Vnat{i} \in \Dom{\var{seq}} :
        \size{\myfn{lims}{\VVelement{seq}{i}}} > 0
    \\ \because \longDfref{LSS}{lss},
        \Eref{th-lss-slice-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-slice-22}
\begin{gathered}
    \forall \; \Vnat{i} \in \Dom{\var{subseq}} :
        \size{\myfn{lims}{\VVelement{subseq}{i}}} > 0
    \\ \because \Eref{th-lss-slice-06},
        \Eref{th-lss-slice-20}.
\end{gathered}
\end{equation}
\mypareq{eq:th-lss-slice-24}{%
   $\var{subseq} \in \realm{LSS}$
        \cuz{} \Dfref{lss},
        \Eref{th-lss-slice-12},
        \Eref{th-lss-slice-22}. \quad \myqed
}
\end{proof}

\begin{theorem}[LSS extension]
\label{th:lss-extension}
Let \var{lss} be an \realm{LSS},
and let \var{next} be an \realm{EIM}.
Then
\mypareq{eq:th-lss-extension-02}{%
   $\family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \realm{LSS}$
}
iff
\mypareq{eq:th-lss-extension-07}{%
        $\myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
        \thEnd
}
\end{theorem}

\begin{proof}
The forward direction follows immediately from the definition
of an \realm{LSS} \Dfref{lss},
and definition of an \realm{LES} \Dfref{next-eligible-sequence}.
We prove the reverse direction, by assuming
the antecedent to show the consequent.
\mypareq{eq:th-lss-extension-10}{%
        $\myfn{isSource}{\var{next}}$
        \cuz{} antecedent \Eref{th-lss-extension-07} of
        reverse direction.
}
\mypareq{eq:th-lss-extension-12}{%
        $\myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
        \cuz{} antecedent \Eref{th-lss-extension-07} of
        reverse direction.
}
\mypareq{eq:th-lss-extension-14}{%
    $\var{lss} \in \realm{LSS}$ \cuz{} AF theorem.
}
\mypareq{eq:th-lss-extension-16}{%
    $\forall \; \Vnat{i} \in \Dom{\var{lss}} : \myfn{isSource}{\VVelement{lss}{i}}$
        \cuz{} \longDfref{LSS}{lss},
        \Eref{th-lss-extension-14}.
}
\mypareq{eq:th-lss-extension-18}{%
   $\var{extended} = \family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}}$,
       new \var{extended} for convenience.
}
\mypareq{eq:th-lss-extension-20}{%
   $\var{extended} \in \realm{LES}$
   \cuz{} \longThref{LES extension}{les-extension},
        \Eref{th-lss-extension-12},
        \Eref{th-lss-extension-18}.
}
\mypareq{eq:th-lss-extension-22}{%
    $\forall \; \Vnat{i} \in \Dom{\var{extended}} : \myfn{isSource}{\VVelement{extended}{i}}$
    \cuz{} \Eref{th-lss-extension-10},
    \Eref{th-lss-extension-16},
    \Eref{th-lss-extension-18}.
}
\mypareq{eq:th-lss-extension-24}{%
   $\var{extended} \in \realm{LSS}$
    \cuz{} \longDfref{LSS}{lss},
        \Eref{th-lss-extension-20},
        \Eref{th-lss-extension-22}.
}
The equation \Eref{th-lss-extension-24}
is the consequent \Eref{th-lss-extension-02}.
With the consequent we have the reverse direction,
and with both directions we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LSS generalized extension]
\label{th:lss-generalized-extension}
Let \var{lss} be an \realm{LSS}.
Then
\mypareq{eq:th-lss-generalized-extension-02}{%
   $\exists \; \Veim{next} : \family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \realm{LSS}$
}
iff
\mypareq{eq:th-lss-generalized-extension-07}{%
        $\exists \; \Veim{next} : \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
        \thEnd
}
\end{theorem}

\begin{proof}
We first prove the forward direction, assuming the antecedent
to show the consequent.
\begin{equation}
\label{eq:th-lss-generalized-extension-10}
\begin{gathered}
   \family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \Veim{next}} \in \realm{LSS}
   \\ \myparbox{\cuz{}
       antecedent \Eref{th-lss-generalized-extension-02} of forward
       direction, existential instantiation of new free \Veim{next}.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-generalized-extension-12}
\begin{gathered}
    \myfn{isSource}{\var{next}}
    \land
    \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
    \\ \because \longThref{LSS extension}{lss-generalized-extension},
    \Eref{th-lss-generalized-extension-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-generalized-extension-14}
\begin{gathered}
    \exists \; \Veim{next} :
    \\ \myfn{isSource}{\var{next}}
        \land \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
    \\ \because \Eref{th-lss-generalized-extension-12},
    \text{existential generalization of \Veim{next}.}
\end{gathered}
\end{equation}

The equation \Eref{th-lss-generalized-extension-14}
is the consequent \Eref{th-lss-generalized-extension-07},
and with it we have the forward direction.
The reverse direction follows from the same line of reasoning:
first, existential instantiation;
second, invocation of the \realm{LSS} extension theorem \Thref{lss-generalized-extension};
third, existential generalization.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS termination]
\label{th:maximal-lss-termination}
Let \var{lss} be an \realm{LSS}.
Then
\mypareq{eq:th-maximal-lss-termination-02}{%
   \Maximal{\var{lss}}
}
iff
\mypareq{eq:th-maximal-lss-termination-07}{%
        $\nexists \; \Veim{next} : \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
        \thEnd
}
\end{theorem}

\begin{proof}
We prove the mutual implication directly.
\begin{equation}
\label{eq:th-maximal-lss-termination-10}
\begin{gathered}
   \Maximal{\var{lss}} \iff
   \\
   \nexists \; \Veim{next} : \family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \realm{LSS}
       \\ \because \longDfref{Maximal LSS}{maximal-lss}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-termination-12}
\begin{gathered}
   \left( \nexists \; \Veim{next} : \family{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \realm{LSS} \right)
        \\ \iff \nexists \; \Veim{next} :
        \\ \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
       \\ \because
            \longThref{LSS generalized extension}{lss-generalized-extension}.
\end{gathered}
\end{equation}
The theorem follows immediately from
\Eref{th-maximal-lss-termination-10}
and \Eref{th-maximal-lss-termination-12}.
\myqed
\end{proof}

\begin{theorem}[LSS prefix]
\label{th:lss-prefix}
Consider two \realm{LSS}'s.
They have different lengths but the same base element
iff the shorter one is a proper prefix of the longer one.
That is,
\begin{gather}
\label{eq:th-lss-prefix-02}
   \forall \; \var{lss1} \in \realm{LSS} :
       \forall \; \var{lss2} \in \realm{LSS} :
\\ \label{eq:th-lss-prefix-04}
   \Vsize{lss1} < \Vsize{lss2}
\\ \label{eq:th-lss-prefix-06}
   \land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\\ \nonumber \iff
\\ \label{eq:th-lss-prefix-08}
   \PrPfx{\var{lss1},\var{lss2}}.
   \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
This proof is similar to the one for ``\realm{LES} prefix''
\Thref{les-prefix}.
For the reverse direction,
\Eref{th-lss-prefix-04} and
\Eref{th-lss-prefix-06}
follow immediately from
\Eref{th-lss-prefix-08},
via the definition of proper prefix \Dfref{prefix}.

It remains to show the forward direction.
Let
\mypareq{eq:th-lss-prefix-20}{%
    $\Vlss{subLss2} = \Velement{lss2}{0\ldots \Vlastix{lss1}}$
    \linebreak \cuz{} \longThref{LSS slice}{lss-slice},
       antecedent of forward direction
           \Eref{th-lss-prefix-02}--\Eref{th-lss-prefix-06},
           new \Vlss{subLss2} for convenience.
}
\mypareq{eq:th-lss-prefix-21}{%
    \PrPfx{\var{subLss2},\var{lss2}} \cuz{}
        \longDfref{Proper prefix}{prefix},
        \Eref{th-lss-prefix-04}, \Eref{th-lss-prefix-20}.
}
\begin{equation}
\label{eq:th-lss-prefix-22}
\Vsize{subLss2} = \Vsize{lss1} \because \Eref{th-lss-prefix-20}
\end{equation}
\mypareq{eq:th-lss-prefix-24}{%
    $\Velement{subLss2}{0} = \Velement{lss2}{0}$ \cuz{} \Eref{th-lss-prefix-20}.
}
\mypareq{eq:th-lss-prefix-26}{%
    $\Velement{subLss2}{0} = \Velement{lss1}{0}$
    \linebreak \cuz{}
       antecedent of forward direction \Eref{th-lss-prefix-06},
       \Eref{th-lss-prefix-24}.
}
\mypareq{eq:th-lss-prefix-27}{%
    $\var{subLss2} = \var{lss1}$
    \linebreak \cuz{} \longThref{LSS uniqueness}{lss-prefix},
        \Eref{th-lss-prefix-22},
        \Eref{th-lss-prefix-26}.
}
\mypareq{eq:th-lss-prefix-28}{%
    $\PrPfx{\Vlss{lss1}, \Vlss{lss2}}$
    \cuz{} \Eref{th-lss-prefix-21},
    \Eref{th-lss-prefix-27}.
}
Equation \Eref{th-lss-prefix-28}
is the consequent of the forward direction,
which shows the forward direction.
With both directions, we have the proof.
\myqed
\end{proof}

\begin{theorem}[LSS trichotomy]
\label{th:lss-trichotomy}
If two \realm{LSS}'s of the same length have the same base element,
then there are three mutually exclusive possibilities:
\begin{itemize}
\item They are identical.
\item The first \realm{LSS} is a proper prefix of the second.
\item The second \realm{LSS} is a proper prefix of the first.
\end{itemize}
That is, if
\begin{gather}
\label{eq:th-lss-trichotomy-02}
\var{lss1} \in \realm{LSS}
\\ \label{eq:th-lss-trichotomy-04}
\; \land \; \var{lss2} \in \realm{LSS}
\\ \label{eq:th-lss-trichotomy-06}
\land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\end{gather}
then
\mypareq{eq:th-lss-trichotomy-08}{%
   \size{\var{fn} \in \set{\var{Eq},\var{Lt},\var{Gt}} :
       \myfn{fn}{\var{lss1},\var{lss2}}} = 1
}
where \var{Eq}, \var{Lt} and \var{Gt} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-lss-trichotomy-09a}
    \myfn{Eq}{\Vlss{x},\Vlss{y}} \iff \var{x} = \var{y},
    \\ \label{eq:th-lss-trichotomy-09b}
    \myfn{Lt}{\Vlss{x},\Vlss{y}} \iff \PrPfx{\var{lss1},\var{lss2}}\text{, and}
    \\ \label{eq:th-lss-trichotomy-09c}
    \myfn{Gt}{\Vlss{x},\Vlss{y}} \iff \PrPfx{\var{lss2},\var{lss1}}.
    \quad \thEnd
\end{gather}
\end{theorem}

\begin{proof}
This proof is similar to that for
the ``\realm{LES} trichotomy'' theorem \Thref{les-trichotomy}.
The proof strategy is direct, assuming
the hypothesis of the theorem to show
its conclusion.

We first note that,
by the Trichotomy Law for the ordering of the natural numbers,
\mypareq{eq:th-lss-trichotomy-10}{%
   \size{\var{fn} \in \set{\var{EqSz},\var{LtSz},\var{GtSz}} :
       \myfn{fn}{\var{lss1},\var{lss2}}} = 1
}
where \var{EqSz}, \var{LtSz} and \var{GtSz} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-lss-trichotomy-12}
   \myfn{EqSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{x} = \Vsize{y},
\\ \label{eq:th-lss-trichotomy-14}
    \myfn{LtSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{lss1} < \Vsize{lss2}\text{, and}
\\ \label{eq:th-lss-trichotomy-16}
    \myfn{GtSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{lss1} > \Vsize{lss2}.
\end{gather}

We now show three derivations, one for each of the functions
\var{EqSz}, \var{LtSz} and \var{GtSz}.
\mypareq{eq:th-lss-trichotomy-17-18}{%
   \myfn{EqSz}{\Vlss{x},\Vlss{y}} \cuz{} AF derivation;
   new arbitrary \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-20}{%
   $\Vsize{x} = \Vsize{y}$ \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-18}
   using \Eref{th-lss-trichotomy-12}.
}
\mypareq{eq:th-lss-trichotomy-17-22}{%
   $\Vlss{x} = \Vlss{y}$ \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-20}
   using \longThref{LSS uniqueness}{lss-uniqueness},
    \Eref{th-lss-trichotomy-06}.
}
\mypareq{eq:th-lss-trichotomy-17-24}{%
   \myfn{Eq}{\Vlss{x},\Vlss{y}} \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-22}
   using \Eref{th-lss-trichotomy-09a}.
}
\mypareq{eq:th-lss-trichotomy-17-26}{%
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{EqSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Eq}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} equivalence \Eref{th-lss-trichotomy-17-18}--\Eref{th-lss-trichotomy-17-24},
   \linebreak universal generalization of \Vlss{x}, \Vlss{y}.
}

\mypareq{eq:th-lss-trichotomy-17-28}{%
   \myfn{LtSz}{\Vlss{x},\Vlss{y}} \cuz{} AF derivation;
   new arbitrary \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-30}{%
   $\Vsize{x} < \Vsize{y}$ \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-28}
   using \Eref{th-lss-trichotomy-14}.
}
\mypareq{eq:th-lss-trichotomy-17-32}{%
   $\Vlss{x} = \Vlss{y}$ \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-30}
   using \longThref{LSS prefix}{lss-prefix},
    \Eref{th-lss-trichotomy-06}.
}
\mypareq{eq:th-lss-trichotomy-17-34}{%
   \myfn{Lt}{\Vlss{x},\Vlss{y}} \cuz{}
   equivalence with \Eref{th-lss-trichotomy-17-32}
   using \Eref{th-lss-trichotomy-09b}.
}
\mypareq{eq:th-lss-trichotomy-17-36}{%
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{LtSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Lt}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} equivalence \Eref{th-lss-trichotomy-17-28}--\Eref{th-lss-trichotomy-17-34},
   \linebreak universal generalization of \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-40}{%
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{GtSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Gt}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} symmetry with
   \Eref{th-lss-trichotomy-17-28}--\Eref{th-lss-trichotomy-17-36}.
}

The theorem follows from
\Eref{th-lss-trichotomy-10}--\Eref{th-lss-trichotomy-14},
\Eref{th-lss-trichotomy-17-26},
\Eref{th-lss-trichotomy-17-36} and \Eref{th-lss-trichotomy-17-40}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS non-duplication]
\label{th:maximal-lss-non-duplication}
An \realm{EIM} is the base of at most one maximal \realm{LSS}.
\thEnd
\end{theorem}

\begin{proof}
Call the base \realm{EIM}, \Veim{base}.
If there is not a maximal \realm{LSS} with base \Veim{base}, we
have the theorem trivially.
(This will be the case if \var{base} was not a source \realm{EIM}.)

For the rest of this theorem, then,
we may assume the existence of these
two arbitrary, but not necessarily distinct, maximal \realm{LSS}'s:
\mypareq{eq:th-maximal-lss-non-duplication-10}{%
    $\Velement{max1}{0} = \Veim{base}$ \cuz{} AF proof,
       arbitrary \var{max1}.
}
\mypareq{eq:th-maximal-lss-non-duplication-12}{%
    $\Velement{max2}{0} = \Veim{base}$ \cuz{} AF proof,
       arbitrary \var{max2}.
}
\mypareq{eq:th-maximal-lss-non-duplication-14}{%
    $\Maximal{\var{max1}} \land \Maximal{\var{max2}}$
    \cuz{} AF proof.
}
We now proceed by reductio.
\mypareq{eq:th-maximal-lss-non-duplication-18}{%
    $\var{max1} \neq \var{max2}$ \cuz{} AF reductio.
}
\mypareq{eq:th-maximal-lss-non-duplication-20}{%
    $\PrPfx{\var{max1},\var{max2}}
    \lor \PrPfx{\var{max2},\var{max1}}$
    \cuz{} \longThref{LSS trichotomy}{lss-trichotomy},
    \Eref{th-maximal-lss-non-duplication-10},
    \Eref{th-maximal-lss-non-duplication-12}, and
    \Eref{th-maximal-lss-non-duplication-18}.
}
\mypareq{eq:th-maximal-lss-non-duplication-22}{%
    $\neg \Maximal{\var{max1}} \lor \neg \Maximal{\var{max2}}$
    \cuz{} \longDfref{Maximal LSS}{maximal-lss}.
    This contradicts \Eref{th-maximal-lss-non-duplication-14}.
}
\mypareq{eq:th-maximal-lss-non-duplication-24}{%
    $\var{max1} = \var{max2}$ \cuz{} reductio
    \Eref{th-maximal-lss-non-duplication-18}--%
    \Eref{th-maximal-lss-non-duplication-22}.
}
Since the choice of \var{max1} and \var{max2} was arbitrary,
\Eref{th-maximal-lss-non-duplication-24} shows the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS existence]
\label{th:maximal-lss-existence}
Every Leo source \realm{EIM} is the base of at least one
maximal \realm{LSS}.  That is,
\mypareq{eq:th-maximal-lss-existence-02}{%
    $\myfn{isSource}{\var{eim}} \implies
    \exists \; \var{max} : \Velement{max}{0} = \var{eim} \land \Maximal{\var{max}}$.
    \thEnd
}
\end{theorem}

\begin{proof}
The proof strategy is direct.
We assume the antecedent of the theorem to show its
consequent.
Consider \var{based}, the set of all \realm{LSS}'s
with \var{eim} as their base:
\mypareq{eq:th-maximal-lss-existence-10-10}{%
$\Vlssset{based} = \set{
   \Vlss{lss} : \Velement{lss}{0} = \var{eim}}$
    \linebreak \cuz{}
        \longDfref{LSS}{lss},
        antecedent of theorem \Eref{th-maximal-lss-existence-02},
       new \var{based} for convenience.
}
\begin{equation}
\label{eq:th-maximal-lss-existence-10-20}
\forall \; \Vlss{lss} : \var{lss} \in \var{based} \iff \Velement{lss}{0} = \var{eim}
   \because \Eref{th-maximal-lss-existence-10-10}.
\end{equation}
\var{based} is never empty, because
any Leo source \realm{EIM} is the base of
at least one \realm{LSS}
of length~1:
\mypareq{eq:th-maximal-lss-existence-14}{%
    $\Vsize{based} \ge 1$
       \cuz{} \longThref{Leo source containment}{source-eim-containment},
            \Eref{th-maximal-lss-existence-02}.
}
For any length,
\var{based} contains at most one \realm{LSS} of that length:
\begin{equation}
\label{eq:th-maximal-lss-existence-16}
\begin{gathered}
\forall \; \Vlss{lss1} \in \var{based} :
\forall \; \Vlss{lss2} \in \var{based} :
\\ \Vsize{lss1} = \Vsize{lss2} \implies
\var{lss1} = \var{lss2}
\\ \because \longThref{LSS uniqueness}{lss-uniqueness},
    \Eref{th-maximal-lss-existence-10-20}.
\end{gathered}
\end{equation}
Among the \realm{LES}'s with \var{eim} as their base,
there is a maximum length,
call it \Vnat{possible}:
\begin{equation}
\label{eq:th-maximal-lss-existence-18}
\begin{gathered}
\forall \; \Vles{les} : \Velement{les}{0} = \var{eim}
    \implies \Vsize{les} \le \Vnat{possible}
     \\ \text{where } \var{possible} = \size{\Vocab{\Vint{g}}}+1 \times (\Current{\Velement{les}{0}}+1)
    \\ \because \longThref{LES finiteness}{les-finiteness}.
\end{gathered}
\end{equation}
Therefore, there is a maximum possible \realm{LSS} length for
\realm{LSS}'s with \var{eim} as their base.
\mypareq{eq:th-maximal-lss-existence-20}{%
    The length of an \realm{LSS} is the same as that of its underlying \realm{LES}
    \cuz{} \longThref{LSS--LES identity}{lss-les-identity}.
}
\begin{equation}
\label{eq:th-maximal-lss-existence-22}
\begin{gathered}
\forall \; \Vles{lss} \in \var{based} : \Vsize{lss} \le \var{possible}
    \\ \because \Eref{th-maximal-lss-existence-10-20},
    \Eref{th-maximal-lss-existence-18},
        \Eref{th-maximal-lss-existence-20}.
\end{gathered}
\end{equation}
Since there is a maximum \realm{LSS} size for the elements of \var{based},
and since, for each \realm{LSS} size, there is at most one \realm{LSS} in \var{based},
we know that \var{based} is a finite set:
\mypareq{eq:th-maximal-lss-existence-24}{%
    $\Vsize{based} \in \naturals$
    \cuz{}
        \Eref{th-maximal-lss-existence-16},
        \Eref{th-maximal-lss-existence-22}.
}
Since \var{based} is finite and contains at least one element,
there is a maximum size of \realm{LSS}'s actually in \var{based},
and at least one \realm{LSS} is of that size:
\begin{equation}
\label{eq:th-maximal-lss-existence-26}
\begin{gathered}
\exists \; \Vlss{max} \in \var{based} :
\forall \; \Vlss{lss} \in \var{based} :
    \Vsize{max} \ge \Vsize{lss}
\\ \because \Eref{th-maximal-lss-existence-14},
    \Eref{th-maximal-lss-existence-24}.%
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-existence-28}
\begin{gathered}
    \Vlss{max} \in \var{based} \land
\forall \; \Vlss{lss} \in \var{based} :
    \Vsize{\var{max}} \ge \Vsize{lss}
\\ \because \Eref{th-maximal-lss-existence-26},
   \text{existential instantiation of \var{max}}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-existence-29-30}{%
    $\Vlss{max} \in \var{based}$
    \cuz{} \Eref{th-maximal-lss-existence-28}.
}
\mypareq{eq:th-maximal-lss-existence-29-40}{%
    $\forall \; \Vlss{lss} \in \var{based} : \Vsize{\var{max}} \ge \Vsize{lss}$
    \cuz{} \Eref{th-maximal-lss-existence-28}.
}
\mypareq{eq:th-maximal-lss-existence-29-50}{%
    $\Velement{max}{0} = \var{eim}$
        \cuz{} \Eref{th-maximal-lss-existence-10-20},
            \Eref{th-maximal-lss-existence-29-30}.
}
We now consider, for a reductio,
the possibility that \Vlss{max} is the proper
prefix of another \realm{LSS}.
\mypareq{eq:th-maximal-lss-existence-30}{%
   $\exists \; \Vlss{maxier} : \PrPfx{\var{max}, \var{maxier}}$
   \cuz{} AF reductio.%
}
\mypareq{eq:th-maximal-lss-existence-32}{%
    $\var{maxier} \in \realm{LSS} \land \PrPfx{\var{max}, \var{maxier}}$
    \cuz{} \Eref{th-maximal-lss-existence-30},
    existential instantiation.%
}
\mypareq{eq:th-maximal-lss-existence-33}{%
    $\PrPfx{\var{max}, \var{maxier}}$
    \cuz{} \Eref{th-maximal-lss-existence-32}.
}
\mypareq{eq:th-maximal-lss-existence-34}{%
    $\Vsize{maxier} > \Vsize{max}$
    \cuz{} \longDfref{Proper prefix}{prefix},
    \Eref{th-maximal-lss-existence-30}.%
}
\mypareq{eq:th-maximal-lss-existence-36}{%
    $\Velement{maxier}{0} = \Velement{max}{0}$
    \cuz{} \longDfref{Proper prefix}{prefix},
    \Eref{th-maximal-lss-existence-30}.%
}
\mypareq{eq:th-maximal-lss-existence-40}{%
    $\var{maxier} \in \Vlssset{based}$
        \cuz{} \Eref{th-maximal-lss-existence-10-20},
        \Eref{th-maximal-lss-existence-29-50},
        \Eref{th-maximal-lss-existence-36}.
}
\mypareq{eq:th-maximal-lss-existence-42}{%
    $\var{maxier} \notin \Vlssset{based}$
        \cuz{} \Eref{th-maximal-lss-existence-29-40},
        \Eref{th-maximal-lss-existence-34}.%
}
The equation \Eref{th-maximal-lss-existence-42}
directly contradicts \Eref{th-maximal-lss-existence-40}
and gives us the reductio:
\mypareq{eq:th-maximal-lss-existence-44}{%
   $\nexists \; \Vlss{maxier} : \PrPfx{\var{max}, \var{maxier}}$
   \linebreak \cuz{} reductio
    \Eref{th-maximal-lss-existence-30}--%
    \Eref{th-maximal-lss-existence-42}.%
}
\mypareq{eq:th-maximal-lss-existence-46}{%
   \Maximal{\Vlss{max}} \cuz{}
    \longDfref{Maximal LSS}{maximal-lss},
    \Eref{th-maximal-lss-existence-44}.%
}
\begin{equation}
\label{eq:th-maximal-lss-existence-48}
\begin{gathered}
    \exists \; \var{max} : \Velement{max}{0} = \var{eim} \land \Maximal{\var{max}}
\\  \because \Eref{th-maximal-lss-existence-29-50},
    \Eref{th-maximal-lss-existence-46},
    \text{existential generalization}.
\end{gathered}
\end{equation}
Equation \Eref{th-maximal-lss-existence-48}
is the consequent of
\Eref{th-maximal-lss-existence-02},
which is what we needed for the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS uniqueness]
\label{th:maximal-lss-uniqueness}
For every Leo source \realm{EIM}, there is exactly one maximal \realm{LSS}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows immediately from the \realm{LSS} Non-duplication Theorem
\Thref{maximal-lss-non-duplication},
and the \realm{LSS} Existence Theorem \Thref{maximal-lss-existence}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS dichotomy]
\label{th:maximal-lss-dichotomy}
Every \realm{LSS} is either
a maximal \realm{LSS}, or a proper prefix of a maximal \realm{LSS}.
Also that maximal \realm{LSS} shares the same base \realm{EIM}.
That is,
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-02}
\begin{gathered}
\forall \; \Vlss{lss} : \exists \; \Vlss{max} :
\\ \Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \;
    (\var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}}).
    \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is direct.
\mypareq{eq:th-maximal-lss-dichotomy-10}{%
    $\var{lss} \in \realm{LSS}$
    \cuz{} AF theorem, new free \Vlss{lss}.
}
\mypareq{eq:th-maximal-lss-dichotomy-14}{%
    $\exists \; \var{max} : \Velement{max}{0} = \Velement{lss}{0} \land \Maximal{\var{max}}$.
        \cuz{} \longThref{Maximal LSS existence}{maximal-lss-existence},
        \Eref{th-maximal-lss-dichotomy-10}.%
}
\mypareq{eq:th-maximal-lss-dichotomy-16}{%
    $\Velement{max}{0} = \Velement{lss}{0} \land \Maximal{\var{max}}$ \cuz{}
        \Eref{th-maximal-lss-dichotomy-16}, existential instantiation.
}
\mypareq{eq:th-maximal-lss-dichotomy-17-20}{%
    $\Velement{max}{0} = \Velement{lss}{0}$ \cuz{}
        \Eref{th-maximal-lss-dichotomy-16}.
}
\mypareq{eq:th-maximal-lss-dichotomy-17-40}{%
    \Maximal{\var{max}} \cuz{} \Eref{th-maximal-lss-dichotomy-16}.
}
%
We can restate the ``\realm{LSS} trichotomy'' in the following form:
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-18}
\begin{gathered}
\forall \; \Vlss{lss1} : \forall \; \Vlss{lss2} :
\\ \Velement{lss1}{0} = \Velement{lss2}{0} \land \var{lss1} \neq \var{lss2}
\\ \implies
        \PrPfx{\var{lss1}, \var{lss2}} \lor \PrPfx{\var{lss2}, \var{lss1}}
\\ \because \Thref{lss-trichotomy}.
\end{gathered}
\end{equation}
If we instantiate \Vlss{lss1} as \var{lss}
and \Vlss{lss2} as \var{max}, we have
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-20}
\begin{gathered}
\Velement{lss}{0} = \Velement{max}{0} \land \var{lss} \neq \var{max}
\\ \implies \PrPfx{\var{lss}, \var{max}} \lor \PrPfx{\var{max}, \var{lss}}
\\ \because \Eref{th-maximal-lss-dichotomy-18},
    \text{universal instantiation}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-24}
\begin{gathered}
\var{lss} \neq \var{max} \implies \PrPfx{\var{lss}, \var{max}} \lor \PrPfx{\var{max}, \var{lss}}
\\ \because \Eref{th-maximal-lss-dichotomy-17-20},
    \Eref{th-maximal-lss-dichotomy-20}.
\end{gathered}
\end{equation}
We know that a maximal \realm{LSS} cannot be a proper prefix of any other \realm{LSS}:
\mypareq{eq:th-maximal-lss-dichotomy-26}{%
    $\neg \PrPfx{\var{max}, \var{lss}}$ \cuz{}
        \longDfref{Maximal LSS}{maximal-lss},
        \Eref{th-maximal-lss-dichotomy-17-40}.
}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-28}
\begin{gathered}
    \var{lss} \neq \var{max} \implies \PrPfx{\var{lss}, \var{max}}
        \because \Eref{th-maximal-lss-dichotomy-24},
            \Eref{th-maximal-lss-dichotomy-26}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-30}
\begin{gathered}
    \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}}
        \because \Eref{th-maximal-lss-dichotomy-28}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-32}
\begin{gathered}
\Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \; ( \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}} ).
\\ \because \Eref{th-maximal-lss-dichotomy-16},
    \Eref{th-maximal-lss-dichotomy-30}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-34}
\begin{gathered}
\forall \; \Vlss{lss} : \exists \; \Vlss{max} :
\\ \Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \; ( \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}} ).
\\ \myparbox{\cuz{}
    \Eref{th-maximal-lss-dichotomy-32},
    universal generalization of \Vlss{lss},
    existential generalization of \Vlss{max}.%
    }
\end{gathered}
\end{equation}
Equation \Eref{th-maximal-lss-dichotomy-34} is the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS top]
\label{th:maximal-lss-top}
Every \realm{LSS} with the same top as a maximal \realm{LSS} is also maximal.
That is, if
\mypareq{eq:th-maximal-lss-top-02}{%
    \Maximal{\Vlss{max}}
}
then
\mypareq{eq:th-maximal-lss-top-07}{%
   $\forall \; \Vlss{lss} :
       \Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}.$
       \thEnd
}
\end{theorem}

\begin{proof}
The proof is direct assuming the antecedent
\Eref{th-maximal-lss-top-02}
to show the consequent
\Eref{th-maximal-lss-top-07}.
\mypareq{eq:th-maximal-lss-top-10}{%
    $\nexists \; \Veim{next} :
        \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{max}{\Vlastix{max}}}$
        \linebreak \cuz{}
        \longThref{Maximal LSS termination}{maximal-lss-termination},
        antecedent \Eref{th-maximal-lss-top-02}
        of theorem.
}
To show the consequent \Eref{th-maximal-lss-top-07},
we first instantiate the universal generalization,
then we assume the antecedent of its predicate to show
its consequent.
\mypareq{eq:th-maximal-lss-top-12}{%
    $\var{lss} \in \realm{LSS}$ \cuz{} new free \Vlss{lss}.
}
\mypareq{eq:th-maximal-lss-top-14}{%
   $\Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}$
     AF derivation.
}
\mypareq{eq:th-maximal-lss-top-16}{%
    $\nexists \; \Veim{next} :
        \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
        \linebreak \cuz{}
        \Eref{th-maximal-lss-top-10},
        \Eref{th-maximal-lss-top-14}.
}
\mypareq{eq:th-maximal-lss-top-18}{%
    \Maximal{\var{lss}}
    \cuz{} \longThref{Maximal LSS termination}{maximal-lss-termination},
    \Eref{th-maximal-lss-top-16}.
}
\mypareq{eq:th-maximal-lss-top-20}{%
   $\Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}$
       \cuz{} derivation \Eref{th-maximal-lss-top-14}--%
           \Eref{th-maximal-lss-top-18}.
}
\mypareq{eq:th-maximal-lss-top-22}{%
   $\forall \; \var{lss} : \Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}$
       \linebreak \cuz{} \Eref{th-maximal-lss-top-20}, universal instantiation.
}
Equation \Eref{th-maximal-lss-top-22} is the consequent of theorem,
and with it, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS \realm{LIM}s termination]
\label{th:maximal-lss-lims-termination}
\mypareq{eq:th-maximal-lss-lims-termination-02}{%
    $\Maximal{\Vlss{max}} \iff \size{ \myfn{nextLIMs}{\Vlastix{max}} } = 0$.
    \thEnd
}
\end{theorem}

\begin{proof}
The proof is by mutual implication.
\mypareq{eq:th-maximal-lss-lims-termination-10}{%
    $\Maximal{\Vlss{max}}$
    \cuz{} left term of \Eref{th-maximal-lss-lims-termination-02},
    AF proof.
}
\mypareq{eq:th-maximal-lss-lims-termination-12}{%
        $\nexists \; \Veim{next} : \myfn{isSource}{\var{next}}
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
    \linebreak \cuz{}
    equivalence with
    \Eref{th-maximal-lss-lims-termination-10},
    using \longThref{Maximal LSS termination}{maximal-lss-termination}.
}
\mypareq{eq:th-maximal-lss-lims-termination-14}{%
        $\nexists \; \Veim{next} :
    \linebreak \size{\myfn{lims}{\var{next}}} \neq 0
        \land
        \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
    \linebreak \cuz{}
    equivalence with
    \Eref{th-maximal-lss-lims-termination-12},
    using \longDfref{Leo source \realm{EIM}}{leo-source-eim}.
}
\mypareq{eq:th-maximal-lss-lims-termination-16}{%
        $\forall \; \Veim{next} :
    \linebreak \myfn{NextEligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
        \implies \size{\myfn{lims}{\var{next}}} = 0$
    \linebreak \cuz{}
        equivalence with
        \Eref{th-maximal-lss-lims-termination-14}.
}
\begin{equation}
\label{eq:th-maximal-lss-lims-termination-80}
\begin{gathered}
\size{ \set{
    \begin{gathered}
       \ell \in \realm{LIM} :
        \\ \myfn{NextEligible}{\Veim{next},\Veim{src}}
        \land \; \ell \in \myfn{lims}{\var{next}}
    \end{gathered}
    } } = 0
    \\ \because \text{
        equivalence with
        \Eref{th-maximal-lss-lims-termination-16}}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lims-termination-82}{%
    $\size{ \myfn{nextLIMs}{\Vlastix{max}} } = 0$
    \cuz{} equivalence with
    \Eref{th-maximal-lss-lims-termination-80},
    using \longDfref{LIM validity}{lim-validity}.
}
The theorem
\Eref{th-maximal-lss-lims-termination-02}
follows from the chain of equivalences
\Eref{th-maximal-lss-lims-termination-10}--\Eref{th-maximal-lss-lims-termination-82}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS \realm{LIM} invariant]
\label{th:eq-maximal-lss-lim-invariant}
The dotted rule and origin of
every \realm{LIM} of a maximal \realm{LSS} is the same
as the
dotted rule and the origin of the top \realm{EIM}
of that maximal \realm{LSS}.
That is,
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-02}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
   \quad \thEnd
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is by induction on the last index of the \realm{LSS},
so that the induction hypothesis is
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-12}
\begin{gathered}
\myfn{INDH}{\Vnat{indx}} \defined
\\ \forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{indx} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\end{gathered}
\end{equation}

For the basis, we seek
to show \myfn{INDH}{0}.
\begin{gather}
\label{eq:th-maximal-lss-lim-invariant-14}
    \Maximal{\Vlss{max}} = 0
\\ \label{eq:th-maximal-lss-lim-invariant-16}
    \land \; \Vlastix{max} = 0
\\ \nonumber
    \myparbox{\cuz{} AF derivation, new free \Vlss{max}.}
\end{gather}
\mypareq{eq:th-maximal-lss-lim-invariant-18}{%
    $\size{ \myfn{nextLIMs}{\Velement{max}{\Vlastix{max}}} } = 0$
    \cuz{} \longThref{Maximal LSS \realm{LIM}s termination}{maximal-lss-lims-termination},
    \Eref{th-maximal-lss-lim-invariant-14}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-20}{%
    $\Dom{\var{max}} = \set{0}$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-16}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-22}{%
     $\exists \; \Vnat{i} \in \Dom{\var{max}} :
    \mylim{\ell} \in \myfn{lims}{\VVelement{max}{i}}$
    \cuz{} AF derivation, new free \mylim{\ell}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-24}{%
    $\mylim{\ell} \in \myfn{lims}{\VVelement{max}{0}}$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-20},
        \Eref{th-maximal-lss-lim-invariant-22},
        existential instantiation of \Vnat{i} with 0.
}
\mypareq{eq:th-maximal-lss-lim-invariant-26}{%
    $\size{ \myfn{nextLIMs}{\Velement{max}{0}} } = 0$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-16},
        \Eref{th-maximal-lss-lim-invariant-18}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-28}{%
    $\DR{\ell} = \DR{\Velement{max}{0}}
    \land \Origin{\ell} = \Origin{\Velement{max}{0}}$
    \cuz{} \longDfref{LIM validity}{lim-validity},
        \Eref{th-maximal-lss-lim-invariant-24},
        \Eref{th-maximal-lss-lim-invariant-26}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-30}{%
    $\DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{th-maximal-lss-lim-invariant-16},
        \Eref{th-maximal-lss-lim-invariant-28}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-32}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \myparbox{\cuz{}
    derivation \Eref{th-maximal-lss-lim-invariant-22}--%
    \Eref{th-maximal-lss-lim-invariant-30},
    universal generalization of \mylim{\ell}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-34}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = 0 } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    derivation \Eref{th-maximal-lss-lim-invariant-14}--%
    \Eref{th-maximal-lss-lim-invariant-32},
    universal generalization of \Vlss{max}.
    This is \myfn{INDH}{0}, the basis of the induction.
}
\end{gathered}
\end{equation}

For the step we assume \myfn{INDH}{\var{n}}
to show \myfn{INDH}{\var{n}+1}.
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-40}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{n} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because \myfn{INDH}{\var{n}}, \text{AF step}.
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:th-maximal-lss-lim-invariant-42}
    \var{max} \in \realm{LSS}
\\ \label{eq:th-maximal-lss-lim-invariant-43}
    \land \; \Maximal{\var{max}}
\\ \label{eq:th-maximal-lss-lim-invariant-44}
    \land \; \Vlastix{max} = \var{n}+1
\\ \nonumber
    \myparbox{\cuz{} AF derivation, new free \Vlss{max}.}
\end{gather}
\mypareq{eq:th-maximal-lss-lim-invariant-46}{%
    $1 \in \Dom{\var{max}}$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-44}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-48}{%
    $\Vlss{suffix} = \Velement{max}{1\ldots \Vlastix{max}}$
    \cuz{} \longThref{LSS slice}{lss-slice},
        \Eref{th-maximal-lss-lim-invariant-42},
        \Eref{th-maximal-lss-lim-invariant-46},
        new \Vlss{suffix} for convenience.
}
\mypareq{eq:th-maximal-lss-lim-invariant-49}{%
    $\var{suffix} \in \realm{LSS}$ \cuz{}
        \Eref{th-maximal-lss-lim-invariant-48}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-50}{%
    $\Velement{suffix}{\Vlastix{suffix}} = \Velement{max}{\Vlastix{max}}$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-48}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-52}{%
    $\Maximal{\var{suffix}}$
    \cuz{} \longThref{Maximal LSS top}{maximal-lss-top},
        \Eref{th-maximal-lss-lim-invariant-43},
        \Eref{th-maximal-lss-lim-invariant-49},
        \Eref{th-maximal-lss-lim-invariant-50}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-54}{%
    $\Vlastix{suffix} = \var{n}$
    \cuz{} \Eref{th-maximal-lss-lim-invariant-44},
        \Eref{th-maximal-lss-lim-invariant-48}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-56}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{suffix}} :
    \ell1 \in \myfn{lims}{\VVelement{suffix}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{suffix}{\Vlastix{suffix}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{suffix}{\Vlastix{suffix}}}.
\\ \myparbox{\cuz{}
    instantiation of \Vlss{max} with \Vlss{suffix},
    \Eref{th-maximal-lss-lim-invariant-40},
    \Eref{th-maximal-lss-lim-invariant-52},
    \Eref{th-maximal-lss-lim-invariant-54}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-58}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{suffix}} :
    \ell1 \in \myfn{lims}{\VVelement{suffix}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{th-maximal-lss-lim-invariant-50},
    \Eref{th-maximal-lss-lim-invariant-56}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-60}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \set{1\ldots \Vlastix{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{th-maximal-lss-lim-invariant-48},
    \Eref{th-maximal-lss-lim-invariant-58}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lim-invariant-62}{%
   \myfn{isSource}{\Velement{max}{1}}
   \cuz{} \longDfref{LSS}{lss},
        \Eref{th-maximal-lss-lim-invariant-42},
        \Eref{th-maximal-lss-lim-invariant-46}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-64}{%
    $\size{\myfn{lims}{\Velement{max}{1}}} > 0$
    \cuz{} \longDfref{Leo source \realm{EIM}}{leo-source-eim},
    \Eref{th-maximal-lss-lim-invariant-62}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-66}{%
    $\var{max} \in \realm{LES}$
    \cuz{} \longDfref{LSS}{lss},
        \Eref{th-maximal-lss-lim-invariant-42}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-68}{%
    $\myfn{NextEligible}{\Velement{max}{1},\Velement{max}{0}}$
        \cuz{} \longDfref{Leo eligible sequence}{next-eligible-sequence},
            \Eref{th-maximal-lss-lim-invariant-46},
            \Eref{th-maximal-lss-lim-invariant-66}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-70}{%
    $\size{\myfn{nextLIMs}{\Velement{max}{0}}} > 0$
    \cuz{} \longDfref{LIM validity}{lim-validity},
        \Eref{th-maximal-lss-lim-invariant-64},
        \Eref{th-maximal-lss-lim-invariant-68}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-72}
\begin{gathered}
    \myfn{lims}{\Velement{max}{0}} =
    \\ \left\lbrace \begin{gathered}
        \family{ \Vdr{dr}, \Postdot{\Velement{max}{0}}, \Vorig{orig}, \Current{\Velement{max}{0}}} :
        \\ \Valid{\Velement{max}{0}} \land \myfn{IsLeoEligible}{\Velement{max}{0}}
        \\ \land \; \mylim{\ell} \in \myfn{nextLIMs}{\Velement{max}{0}}
        \\ \land \; \var{dr} = \DR{\ell} \land \var{orig} = \Origin{\ell}
    \end{gathered} \right\rbrace
    \\ \because \Dfref{lim-validity},
        \Eref{th-maximal-lss-lim-invariant-70}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lim-invariant-74}{%
    $\Valid{\Velement{max}{0}} \land \myfn{IsLeoEligible}{\Velement{max}{0}}$
    \cuz{} \longDfref{LES}{next-eligible-sequence},
        \Eref{th-maximal-lss-lim-invariant-66}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-75}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{lims}{\Velement{max}{1}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \myparbox{\cuz{}
       \Eref{th-maximal-lss-lim-invariant-60},
       instantiation of \Vnat{i} with 1.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-76}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{nextLIMs}{\Velement{max}{0}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because \longDfref{LIM validity}{lim-validity},
    \Eref{th-maximal-lss-lim-invariant-68},
    \Eref{th-maximal-lss-lim-invariant-75}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-78}
\begin{gathered}
    \myfn{lims}{\Velement{max}{0}} =
    \\ \left\lbrace \begin{gathered}
        \tuple{ \Vdr{dr}, \Postdot{\Velement{max}{0}}, \Vorig{orig}, \Current{\Velement{max}{0}}} :
        \\ \var{dr} = \DR{\Velement{max}{\Vlastix{max}}} \land \var{orig} = \Origin{\Velement{max}{\Vlastix{max}}}
    \end{gathered} \right\rbrace
    \\ \because \Eref{th-maximal-lss-lim-invariant-72},
        \Eref{th-maximal-lss-lim-invariant-74},
        \Eref{th-maximal-lss-lim-invariant-76}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-80}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \set{0\ldots \Vlastix{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{th-maximal-lss-lim-invariant-60},
    \Eref{th-maximal-lss-lim-invariant-78}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-84}
\begin{gathered}
\var{max} \in \realm{LSS} \land \Maximal{\var{max}} \land \Vlastix{max} = \var{n}+1
\\ \implies \forall \; \mylim{\ell} \in
    \set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
        \ell1 \in \myfn{lims}{\VVelement{max}{i}}
    } :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    derivation from \Eref{th-maximal-lss-lim-invariant-42}--%
    \Eref{th-maximal-lss-lim-invariant-44} to
    \Eref{th-maximal-lss-lim-invariant-80}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-86}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{n}+1 } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    \Eref{th-maximal-lss-lim-invariant-84},
    universal generalization of free \Vlss{max}.
    }
\end{gathered}
\end{equation}
\Eref{th-maximal-lss-lim-invariant-86} is \myfn{INDH}{\var{n}+1}, which is what we
sought to show for the step of the induction.
With the step
and the basis \Eref{th-maximal-lss-lim-invariant-34}
of the induction,
we have the induction and the theorem.
\myqed
\end{proof}

\begin{theorem}[LIM Non-duplication]
\label{th:lim-non-duplication}
Every \realm{EIM} has at most one valid \realm{LIM}.
That is,
\[
\forall \; \Veim{eim} : \Valid{\var{eim}} \implies \size{\myfn{lims}{\var{eim}}} \le 1.
   \quad \thEnd
\]
\end{theorem}

\begin{proof}
Recall that a \realm{LIM} is valid iff it is an element of
\myfn{lims}{\var{x}} for some \realm{EIM} \var{x}.
The proof is by reductio.
\begin{gather}
\label{eq:th-lim-non-duplication-10}
\var{src} \in \realm{EIM}
\\ \label{eq:th-lim-non-duplication-12}
\land \; \size{\myfn{lims}{\var{src}}} \ge 2.
\\ \nonumber
\myparbox{\cuz{} new arbitrary \Veim{src}, AF reductio.}
\end{gather}
\begin{gather}
\label{eq:th-lim-non-duplication-13-10}
\ell1 \in \myfn{lims}{\var{src}}
\\ \label{eq:th-lim-non-duplication-13-20}
\land \; \ell2 \in \myfn{lims}{\var{src}}
\\ \label{eq:th-lim-non-duplication-13-30}
\land \; \ell1 \ne \ell2.
\\ \nonumber
\cuzbox{
    \Eref{th-lim-non-duplication-12},
    new \mylim{\ell1}, new \mylim{\ell2}.}
\end{gather}
\mypareq{eq:th-lim-non-duplication-14}{%
        $\mylim{\ell1} = \tuple{
            \Vdr{dr1}, \Postdot{\var{src}}, \Vorig{orig1}, \Current{\var{src}} }$
        \linebreak \cuz{}
            \longDfref{LIM validity}{lim-validity},
            \Eref{th-lim-non-duplication-13-10}, WLOG,
            new \Vdr{dr1}, new \Vdr{orig1}.
}
\mypareq{eq:th-lim-non-duplication-16}{%
        $\mylim{\ell2} = \tuple{
            \Vdr{dr2}, \Postdot{\var{src}}, \Vorig{orig2}, \Current{\var{src}} }$
        \linebreak \cuz{}
            \Dfref{lim-validity},
            \Eref{th-lim-non-duplication-13-20}, WLOG,
            new \Vdr{dr2}, new \Vdr{orig2}.
}
\mypareq{eq:th-lim-non-duplication-18}{%
    $\exists \; \var{max} : \Velement{max}{0} = \var{src} \land \Maximal{\var{max}}$
    \cuz{} \longThref{Maximal LSS existence}{maximal-lss-existence},
        \Eref{th-lim-non-duplication-10}.
}
\begin{gather}
\label{eq:th-lim-non-duplication-19}
    \var{max} \in \realm{LSS}
\\ \label{eq:th-lim-non-duplication-20}
    \Velement{max}{0} = \var{src}
\\ \label{eq:th-lim-non-duplication-22}
    \land \; \Maximal{\var{max}}
\\ \nonumber
    \cuzbox{\Eref{th-lim-non-duplication-18}, existential instantiation.}
\end{gather}
\begin{equation}
\label{eq:th-lim-non-duplication-24}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell\ell : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell\ell \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \cuzbox{%
    \longThref{Maximal-LSS--LIM invariant}{eq-maximal-lss-lim-invariant},
    \Eref{th-lim-non-duplication-22},
    universal instantiation with \Vlss{max}.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lim-non-duplication-25}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{lims}{\VVelement{max}{0}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \cuzbox{%
       \Eref{th-lim-non-duplication-19},
       \Eref{th-lim-non-duplication-20},
       \Eref{th-lim-non-duplication-24}.
   }
\end{gathered}
\end{equation}
\mypareq{eq:th-lim-non-duplication-26}{%
    $\ell1 \in \myfn{lims}{\VVelement{max}{0}}$
    \linebreak \cuz{} \Eref{th-lim-non-duplication-13-10},
        \Eref{th-lim-non-duplication-19},
        \Eref{th-lim-non-duplication-20}.
}
\mypareq{eq:th-lim-non-duplication-28}{%
    $\ell2 \in \myfn{lims}{\VVelement{max}{0}}$
    \linebreak \cuz{} \Eref{th-lim-non-duplication-13-20},
        \Eref{th-lim-non-duplication-19},
        \Eref{th-lim-non-duplication-20}.
}
\mypareq{eq:th-lim-non-duplication-30}{%
    $\DR{\ell1} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell1} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{th-lim-non-duplication-25},
    universal instantiation of $\ell$ as \mylim{\ell1},
    \Eref{th-lim-non-duplication-26}.
}
\mypareq{eq:th-lim-non-duplication-32}{%
    $\DR{\ell2} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell2} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{th-lim-non-duplication-25},
    universal instantiation of $\ell$ as \mylim{\ell2},
    \Eref{th-lim-non-duplication-28}.
}
\begin{equation}
\label{eq:th-lim-non-duplication-34}
\begin{gathered}
    \mylim{\ell1} = \tuple{
         \begin{gathered}
         \DR{\Velement{max}{\Vlastix{max}}}, \Postdot{\var{src}},
         \\ \Current{\var{src}}, \Origin{\Velement{max}{\Vlastix{max}}}
         \end{gathered}
     }
    \\ \because \Eref{th-lim-non-duplication-14},
            \Eref{th-lim-non-duplication-30}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lim-non-duplication-36}
\begin{gathered}
    \mylim{\ell2} = \tuple{
         \begin{gathered}
         \DR{\Velement{max}{\Vlastix{max}}}, \Postdot{\var{src}},
         \\ \Current{\var{src}}, \Origin{\Velement{max}{\Vlastix{max}}}
         \end{gathered}
     }
    \\ \because \Eref{th-lim-non-duplication-16},
            \Eref{th-lim-non-duplication-32}.
\end{gathered}
\end{equation}
\mypareq{eq:th-lim-non-duplication-38}{%
    $\mylim{\ell1} = \mylim{\ell2}$
    \cuz{} \Eref{th-lim-non-duplication-34},
        \Eref{th-lim-non-duplication-36}.
}
Equation \Eref{th-lim-non-duplication-38}
contradicts \Eref{th-lim-non-duplication-13-30},
which shows the reductio
and the theorem.
\myqed
\end{proof}

\begin{theorem}[Leo source bijection]
\label{th:source-fn-injection}
The \fname{Source} function exists,
and is a total bijection.
\thEnd
\end{theorem}

\begin{proof}
The strategy is to show that
\fname{Source} is an injection.
From this the theorem will follow immediately via
the \fname{Source} function theorem \Thref{leo-source-fn}.

To prove that \fname{Source}
has the injection property,
we treat \fname{Source} as a relation
and consider two tuples with distinct first elements (arguments),
but otherwise arbitrary.
We seek to show that this pair of tuples have different second elements (values).

Initially, we consider only the subcase where the size
of the domain of \fname{Source} is greater than 1:
\mypareq{eq:th-source-fn-injection-98}{%
    $\Dom{\fname{Source}} > 1$ \cuz{} AF subcase
}
\begin{gather}
\label{eq:th-source-fn-injection-100}
\land\; \tuple{\mylim{\ell1}, \Veim{src1}} \in \fname{Source}
\\ \label{eq:th-source-fn-injection-120}
\land\; \tuple{\mylim{\ell2}, \Veim{src2}} \in \fname{Source}
\\ \label{eq:th-source-fn-injection-130}
\land\; \ell1 \ne \ell2
\\ \nonumber \cuzbox{AF derivation;
        new arbitrary \mylim{\ell1}, \mylim{\ell2}, \Veim{src1}, \Veim{src2}.
        }
\end{gather}
\mypareq{eq:th-source-fn-injection-140}{%
    $\var{src1} = \var{src2}$ \cuz{} AF reductio.
}
\mypareq{eq:th-source-fn-injection-150}{%
    \var{src1} has two distinct valid \realm{LIM}s,
    $\ell1$ and $\ell2$ \cuz{} \Eref{th-source-fn-injection-100},
        \Eref{th-source-fn-injection-120},
        \Eref{th-source-fn-injection-140}.
}
\mypareq{eq:th-source-fn-injection-160}{%
    \var{src1} has at most one distinct valid \realm{LIM}
    \cuz{} \longThref{LIM Non-duplication}{lim-non-duplication}.
    This contradicts \Eref{th-source-fn-injection-150}.
}
\mypareq{eq:th-source-fn-injection-170}{%
    $\var{src1} \ne \var{src2}$ \cuz{} reductio
        \Eref{th-source-fn-injection-140}--%
        \Eref{th-source-fn-injection-160}.
}
\begin{gather}
\label{eq:th-source-fn-injection-190}
        \Dom{\fname{Source}} > 2
\\ \label{eq:th-source-fn-injection-200}
        \implies \forall\; \mylim{\ell1} : \forall\; \mylim{\ell2} : \forall\; \Veim{src1} : \forall\; \Veim{src2} :
\\ \label{eq:th-source-fn-injection-210}
        \tuple{\mylim{\ell1}, \Veim{src1}} \in \fname{Source}
            \land \; \tuple{\mylim{\ell2}, \Veim{src2}} \in \fname{Source}
\\ \label{eq:th-source-fn-injection-220}
            \land \; \ell1 \ne \ell2
\\ \label{eq:th-source-fn-injection-230}
        \implies \var{src1} \ne \var{src2}
\\ \nonumber
        \cuzbox{derivation \Eref{th-source-fn-injection-98}--%
            \Eref{th-source-fn-injection-170};
            universal generalization of \mylim{\ell1}, \mylim{\ell2}, \Veim{src1}, \Veim{src2}.
        }
\end{gather}
The subequation \Eref{th-source-fn-injection-200}--\Eref{th-source-fn-injection-230}
instantiates the definition
of an injection,
and every function with an empty or singleton domain is an injection,
so that
\mypareq{eq:th-source-fn-injection-250}{%
    \fname{Source} is an injection
        \cuz{} \Eref{th-source-fn-injection-190}--\Eref{th-source-fn-injection-230};
        vacuously; trivially.
}
\mypareq{eq:th-source-fn-injection-260}{%
    The total function \fname{Source} exists and is a bijection
    \cuz{} \longThref{\fname{Source} function theorem}{leo-source-fn}.
    \myqed
}
\end{proof}

\begin{theorem}[LIM function]
\label{th:lim-fn}
The function
\mypareq{}{%
    $\deffn{Lim}{%
        \set{\Veim{src} : \myfn{isSource}{\var{src}}}
    }{%
        \set{\mylim{\ell} : \Valid{\ell}}
    }$,
}
\mypareq{}{%
    such that $\Lim{\Veim{src}} \defined \rIota \mylim{\ell} : \ell \in \myfn{lims}{\var{src}}$,
}
exists and is a bijection.
\thEnd
\end{theorem}

\begin{proof}
First, we observe that \realm{LIM} is the inverse of \fname{Source} \Thref{leo-source-fn}.
Second, with the \fname{Source} bijection theorem
\Thref{source-fn-injection}
we has shown
that \fname{Source} exists and is total and a bijection.
Third, if a total bijective function exists,
then we know that its inverse exists and is also a
total bijective function.
From these three facts, the theorem follows immediately.
\myqed
\end{proof}

\begin{definition}[LIM matching]
To deal with \realm{LIM}s,
we overload \var{Matches} as follows:
\begin{equation*}
\begin{gathered}
\Matches{\mylim{\ell},\Veim{cuz}} \defined \\
         \Postdot{\var{cuz}} = \Lambda \\
\land \; \Transition{\ell} = \Symbol{\var{cuz}} \\
\land \; \Current{\ell} = \Origin{\var{cuz}}.
  \quad \dfEnd
\end{gathered}
\end{equation*}
\end{definition}

\begin{theorem}[ES \realm{LIM} Count]
\label{th:es-lim-count}
Every ES has at most
one \realm{LIM} for every symbol in the grammar.
That is,
\[
\forall \; \Vloc{j} \in \Dom{\Vet{S}} :
\size{\set{\mylim{\ell} : \Current{\ell} = \var{j}}} \le
\size{\Vocab{\var{g}}}.
\quad \thEnd
\]
\end{theorem}

\begin{proof}
\mypareq{eq:th-es-lim-count-10}{%
$\var{j} \in \naturals$
    \cuz{} new free \Vnat{j}
}
\mypareq{eq:th-es-lim-count-12}{%
$\var{j} \in \Dom{\Vet{S}}$
    \cuz{} AF derivation.
}
\mypareq{eq:th-es-lim-count-14}{%
$\size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{IsLeoEligible}{\var{x}}
       }
    }
    \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{th-es-lim-count-12},
        \longThref{Leo Eligible \realm{EIM} count}{leo-eligible-eim-count}.
}
\mypareq{eq:th-es-lim-count-16}{%
$\forall \; \Veim{x} : \myfn{isSource}{\var{x}}
    \implies \myfn{IsLeoEligible}{\var{x}}$
\\ \cuz{} \longDfref{LIM validity}{lim-validity},
            \longDfref{Leo source}{leo-source-eim}.
}
\mypareq{eq:th-es-lim-count-18}{%
$\size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{isSource}{\var{x}}
       }
    }
    \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{th-es-lim-count-14},
        \Eref{th-es-lim-count-16}.
}
By the \realm{LIM} Non-duplication Theorem,
there is, for each source \realm{EIM},
at most one \realm{LIM} with that \realm{EIM} as its source,
so that
\mypareq{eq:th-es-lim-count-20}{%
$\size{
       \set{ \mylim{\ell} :
                \exists\; \Veim{x} \in \Velement{S}{\var{j}} :
        \ell = \Lim{\var{x}} \land \myfn{isSource}{\var{x}}
       }
    }$
    \linebreak
    $\le \size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{isSource}{\var{x}}
       }
    }$
    \linebreak \cuz{} \Thref{lim-non-duplication},
        \Eref{th-es-lim-count-18}.
}
\mypareq{eq:th-es-lim-count-24}{%
$\size{
       \set{ \mylim{\ell} :
                \exists\; \Veim{x} \in \Velement{S}{\var{j}} :
        \ell = \Lim{\var{x}} \land \myfn{isSource}{\var{x}}
       }
    }$
    $\le \size{\Vocab{\Vint{g}}}$
        \cuz{\Eref{th-es-lim-count-18},
            \Eref{th-es-lim-count-20}}.
}
\mypareq{eq:th-es-lim-count-26}{%
    $\forall\; \Veim{x} : \left( \exists\; \mylim{\ell} : \ell = \Lim{\var{x}} \right)
        \implies \myfn{isSource}{\var{x}}$
    \cuz{} \longThref{\fname{Lim} function}{lim-fn},
            \longDfref{Leo source \realm{EIM}}{leo-source-eim}.
}
\mypareq{eq:th-es-lim-count-28}{%
    $\size{
       \set{ \mylim{\ell} :
            \exists\; \Veim{x} \in \Velement{S}{\var{j}} : \ell = \Lim{\var{x}}
       }
    } \le \size{\Vocab{\Vint{g}}}$
        \linebreak \cuz{\Eref{th-es-lim-count-24},
            \Eref{th-es-lim-count-26}}.
}
\mypareq{eq:th-es-lim-count-30}{%
    $\size{
       \set{ \mylim{\ell} :
            \exists\; \Veim{x} : \Current{\var{x}} = \var{j} \land \ell = \Lim{\var{x}}
       }
    }$
    \linebreak $\le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \longDfref{ET validity}{et-validity},
        \Eref{th-es-lim-count-30}.
}
\mypareq{eq:th-es-lim-count-32}{%
    $\forall\; \Veim{x} : \Current{\var{x}} = \Current{\Lim{\var{x}}}$
    \cuz{} \longDfref{LIM validity}{lim-validity},
        \longThref{\fname{Lim} function}{lim-fn}.
}
\mypareq{eq:th-es-lim-count-34}{%
    $\size{
       \set{ \mylim{\ell} :
            \Current{\ell} = \var{j} \land \ell = \Lim{\var{x}}
       }
    } \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{th-es-lim-count-30}, \Eref{th-es-lim-count-32}.
}
\begin{equation}
\label{eq:th-es-lim-count-36}
\begin{gathered}
    \forall \; \Vloc{j} \in \Dom{\Vet{S}} :
    \size{\set{\mylim{\ell} : \Current{\ell} = \var{j}}} \le
    \size{\Vocab{\var{g}}}
    \\ \cuzbox{derivation
        \Eref{th-es-lim-count-12}--\Eref{th-es-lim-count-34};
        universal generalization of \Vloc{j}.
        \myqed
    }
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[\myfn{nextLIMs}{} size]
\label{th:next-lims-size}
For every \realm{EIM} \var{eim},
\myfn{nextLIMs}{\var{eim}} is either empty or
a singleton.
That is,
\[
    \size{\myfn{nextLIMs}{\var{eim}}} \le 1.
    \quad \thEnd
\]
\end{theorem}

\begin{proof}
\mypareq{eq:th-next-lims-cardinality-100}{%
    $\myfn{nextLIMs}{\var{eim}} =
    \set{
        \begin{gathered}
        \ell \in \realm{LIM} :
        \myfn{NextEligible}{\Veim{next},\var{eim}}
        \\ \land \; \ell \in \myfn{lims}{\var{next}}
        \end{gathered}
    }$
    \bcuz{} \longDfref{LIM validity}{lim-validity}.
}
\mypareq{eq:th-next-lims-cardinality-110}{%
    $\size{\set{ \Veim{next} : \myfn{NextEligible}{\Veim{next},\Veim{eim}}}} \le 1$
    \bcuz{} Next Eligible \realm{EIM} Theorem \Thref{next-eligible-uniqueness}.
}
\mypareq{eq:th-next-lims-cardinality-120}{%
    $\forall\; \Veim{x} : \ell \in \myfn{lims}{\var{x}} \implies \myfn{LIM}{\var{x}} = \ell$
    \bcuz{} \myfn{LIM}{} Theorem \Thref{lim-fn}.
}
\mypareq{eq:th-next-lims-cardinality-130}{%
    $\forall\; \Veim{x} : \size{ \myfn{lims}{\var{x}} } \le 1$
    \cuz{} \Eref{th-next-lims-cardinality-120}.
}
The theorem follows from
\Eref{th-next-lims-cardinality-100},
\Eref{th-next-lims-cardinality-110}, and
\Eref{th-next-lims-cardinality-130}.
\myqed
\end{proof}

\begin{theorem}[LIM predecessor function]
\label{th:lim-predecessor-fn}
Let
\[
    \deffn{LIMPredecessor}{\realm{EIM}}{\realm{LIM}}
\]
be a function such that
\vspace{1ex}
\begin{equation}
\label{eq:th-lim-predecessor-fn-050}
\begin{aligned}
& \LIMPredecessor{\Veim{src}} \defined
\\ & \qquad \begin{gathered}
    \begin{cases}
    \mylim{\ell} : \ell \in \var{nxtlims},
        \text{ if } \Vsize{nxtlims} = 1
    \\ \Lambda, \text{ if } \Vsize{nxtlims} = 0,
    \end{cases}
\end{gathered}
\\ & \text{where } \var{nxtlims} = \myfn{nextLIMs}{\Veim{src}}.
\end{aligned}
\end{equation}
Then \fname{LIMPredecessor} exists.
\thEnd
\end{theorem}

\begin{proof}
From \Eref{th-lim-predecessor-fn-050} it is clear that
\myfn{nextLIMs}{\Veim{src}} has a unique value for every \Veim{src},
if it has a value.
By the \myfn{nextLIMs}{} Size Theorem \Thref{next-lims-size},
we know that
\myfn{nextLIMs}{\Veim{src}} is either empty or a singleton,
so that the case statement in
\Eref{th-lim-predecessor-fn-050}
covers all possibilities,
and
\myfn{nextLIMs}{\Veim{src}} always has a value.
\myqed
\end{proof}

\begin{theorem}[LIM predecessor connection]
\label{th:lim-predecessor-connection}
\[
    \begin{gathered}
    \LIMPredecessor{\Veim{eim}} = \ell \land \ell \in \realm{LIM}
    \\ \iff \Transition{\ell} = \Symbol{\var{eim}} \land\; \Current{\ell} = \Origin{\var{eim}}.
        \quad \thEnd
    \end{gathered}
\]
\end{theorem}

\begin{proof}
\todo{Finish proof}
\mypareq{eq:th-lim-predecessor-connection-100}{%
    $\begin{gathered}
    \LIMPredecessor{\var{eim}} = \ell \land \ell \in \realm{LIM}
    \\ \iff \ell \in \myfn{nextLIMs}{\var{eim}}
    \end{gathered}$
    \bcuz{} \longThref{\LIMPredecessor{} Theorem}{lim-predecessor-fn}.
}
\mypareq{eq:th-lim-predecessor-connection-110}{%
    $\begin{gathered}
    \LIMPredecessor{\var{eim}} = \ell \land \ell \in \realm{LIM}
    \\ \iff
        \exists\; \Veim{next} :
        \left(\begin{gathered}
        \ell \in
            \myfn{lims}{\var{next}}
            \\ \land\; \myfn{NextEligible}{\Veim{next},\Veim{eim}}
        \end{gathered}\right)
    \end{gathered}$
    \bcuz{} \longDfref{LIM validity}{lim-validity},
        \Eref{th-lim-predecessor-connection-100}.
}
\mypareq{eq:th-lim-predecessor-connection-120}{%
    $\begin{gathered}
    \forall\; \Veim{x} : \ell \in \myfn{lims}{\var{x}}
        \\ \iff
           \left(\begin{gathered}
               \Transition{\ell} = \Postdot{\var{x}}
               \\ \land\; \Current{\ell} = \Current{\var{x}}
           \end{gathered}\right)
    \end{gathered}$
    \bcuz{} \longDfref{LIM validity}{lim-validity}.
}
\mypareq{eq:th-lim-predecessor-connection-130}{%
    $\begin{gathered}
    \forall\; \Veim{x} :
    \myfn{NextEligible}{\var{x},\var{eim}}
    \\ \iff
       \left(\begin{gathered}
           \Postdot{\var{x}} = \Symbol{\var{eim}}
           \\ \land\; \Current{\var{x}} = \Origin{\var{eim}}
       \end{gathered}\right)
    \end{gathered}$
    \bcuz{} \longThref{Next eligible connection}{next-eligible-connection}.
}
\mypareq{eq:th-lim-predecessor-connection-140}{%
    $\begin{gathered}
    \forall\; \Veim{x} : \ell \in \myfn{lims}{\var{x}} \land \myfn{NextEligible}{\var{x},\var{eim}}
    \\ \iff
       \left(\begin{gathered}
           \Transition{\ell} = \Symbol{\var{eim}}
           \\ \land\; \Current{\ell} = \Origin{\var{eim}}
       \end{gathered}\right)
    \end{gathered}$
    \bcuz{} \Eref{th-lim-predecessor-connection-120},
        \Eref{th-lim-predecessor-connection-130}.
}
\mypareq{eq:th-lim-predecessor-connection-150}{%
    $\begin{gathered}
    \LIMPredecessor{\Veim{eim}} = \ell \land \ell \in \realm{LIM}
    \\ \iff
        \exists\; \Veim{next} :
       \left(\begin{gathered}
           \Transition{\ell} = \Symbol{\var{eim}}
           \\ \land\; \Current{\ell} = \Origin{\var{eim}}
       \end{gathered}\right)
    \end{gathered}$
    \bcuz{} \Eref{th-lim-predecessor-connection-110},
        \Eref{th-lim-predecessor-connection-140}.
}
\Veim{next} is not used in
\Eref{th-lim-predecessor-connection-150}.
If we drop the useless existential quantification
in \Eref{th-lim-predecessor-connection-150},
we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LIM predecessor direction]
\label{th:lim-predecessor-direction}
\begin{gather*}
    \LIMPredecessor{\Veim{prev}} \ne \Lambda
    \\ \implies \Current{\LIMPredecessor{\var{prev}}} \le
        \Current{\var{prev}}. \quad \thEnd
\end{gather*}
\end{theorem}

\begin{proof}
This theorem follows from
the Next Eligible \realm{EIM} Direction Theorem \Thref{next-eligible-direction},
the definition of \realm{LIM} validity \Dfref{lim-validity},
the \realm{LIM} Direction Theorem \Thref{lim-direction},
and the \longThref{\LIMPredecessor{} Theorem}{lim-predecessor-fn}.
\myqed
\end{proof}

\chapter{Ancestry}
\label{chap:ancestry}

\todo[prepend, caption={``Ancestry'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\Marpa, like other variants of \Earley{}, performs
``operations'' on the Earley sets to produce new
Earley items and, in \Marpa's case,
new Leo items.
A operation may require that one or two other
\realm{PIM}s already exist in the Earley sets before
it becomes applicable.
If an operation requires that \Vpim{anc}
exist in the Earley sets
before adjoining \Vpim{desc} with an Earley set \VVelement{S}{j},
then \Vpim{anc} is said to be the \dfn{direct ancestor}
of \Vpim{desc},
and \Vpim{desc} is said to be the \dfn{direct descendant}
of \Vpim{anc}.
If \Vpim{desc} is already a member of \VVelement{S}{j},
\Vpim{anc}
becomes a direct ancestor of \Vpim{desc},
and \Vpim{desc} becomes a direct descendant of \Vpim{anc}.

An \dfn{ancestor} of \Vpim{p} is recursively defined as
\begin{itemize}
\item \var{p} itself, as its own \dfn{trivial ancestor};
\item any direct ancestor of \var{p}; and
\item any direct ancestor of an ancestor of \var{p}.
\end{itemize}
Similarly, a \dfn{descendant} of \Vpim{p} is defined to be
\var{p} itself, as its own \dfn{trivial descendant};
any direct descendant of \var{p}; and
any direct descendant of a descendant of \var{p}.

For each operation, there are at most two direct ancestors.
We follow \cite{AH2002} in calling one the ``predecessor''
and the other the ``cause''.
This terminology makes sense for some operations and,
for consistency, we stretch it to cover all cases.
If the predecessor or the cause is not applicable
to a particular operation, we represent it as a
$\Lambda$ value.

The \Marpa implementation tracks the ancestry of
\realm{PIM}s.
The reasons are tracked in \dfn{link pairs},%
\footnote{The concept of link pairs was present
in
\cite{Earley1968} and
\cite{Earley1970},
but \Marpa owes much to the
careful elaboration of the idea in
\cite{AH2002}.
}
which are essential for efficient evaluation.

As we will see below,
some \realm{PIM}s do not track link pairs.
For those \realm{PIM}s which do track link pairs,
there is at exactly one link pair for each adjunction of a
\realm{PIM} with an Earley set.

A link pair
is a duple of predecessor and cause:
\[
[ \var{predecessor}, \var{cause} ]
\]
In the \Marpa implementation,
the elements of link pairs are pointers
which are used as ``links'' to other data.
The \Marpa implementation
represents
$\Lambda$ elements
as C language null pointers.

Every link pair is either an \dfn{initial link pair}
or an \dfn{ambiguity link pair}.
An \dfn{initial link pair} is the link pair added when a new \realm{PIM} is
added to an Earley set.
An \dfn{ambiguous link pair} is a link pair
that is added to a \realm{PIM} that
is already an element of an Earley set.

A link pair has realm \realm{LP}.
The set of link pairs of a \realm{PIM}
can be written
\LinkPairs{\Vpim{pim}},
or as
\LinkPairs{\Veim{eim}}, or
\LinkPairs{\Vlim{lim}},
for the \realm{PIM} sub-realms.

\chapter{Complexity preliminaries}
\label{chap:complexity-preliminaries}

\todo[prepend, caption={``Complexity preliminaries'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Rewrite ``Complexity preliminaries'' section.}

We follow the tradition in the \Earley{} literature by dividing
the complexity analysis of \Marpa into a low-level,
and a high-level, analysis.
In the complexity context,
\dfn{resource} means time or space.

In the first phase of the low-level analysis,
all resource is charged to one of
the following:
\begin{enumerate}
\item
\label{enum:parse-charging}
The parse itself.
\item
\label{enum:es-charging}
The current \realm{ES}.
\item
\label{enum:eim-charging}
An \realm{EIM}.
\item
\label{enum:proper-adjoin-charging}
A proper adjoin of some \realm{EIM}.
\end{enumerate}
Because logic of this first phase is closely intertwined with
the code itself,
its details are presented alongside the pseudocode
\Chref{algorithm}.

A later chapter
\Chref{low-level-complexity-summary}
presents
the second and last phase of the low-level analysis.
In the second phase the charges of the first phase
are totaled.
Then charges to the parse (\ref{enum:parse-charging})
and to the \realm{ES}s
(\ref{enum:es-charging})
are \dfn{settled} as
charges to \realm{EIM}s
(\ref{enum:eim-charging});
or to proper adjoins
(\ref{enum:proper-adjoin-charging})
of \realm{EIM}s.
After this ``charge settlement'',
only charges of types
\ref{enum:eim-charging} and
\ref{enum:proper-adjoin-charging} remain.

In the statement of theorems for low-level complexity,
note that the manner in which a resource is charged
is usually a matter of tactical choice.
However,
while charging decisions are choices,
they are choices with consequences
--- they will have consequences for later theorems.

The high level complexity analyses of later chapters look
at specific grammar classes,
counting the \realm{EIM}s and proper \realm{EIM} adjoins.
These counts, multiplied by the appropriate result
of the low-level complexity analysis, show the complexity
claims for the grammar classes.

\begin{definition}[Space resource conventions]
Reads and writes do not consume space -- only allocations of space
consume space.
We will assume that the implementation allows \Oc{} space globally.
We do not call any of our pseudocode procedures recursively,
so that the \Oc{} global space can include a ``stack''
which allows \Oc{} space for every call of a pseudocode procedure.
\dfEnd
\end{definition}

\begin{definition}[Caller and callee]
In a context where pseudocode procedure \var{P1} invokes (or ``calls'')
pseudocode procedure \var{P2},
\var{P1} is the \dfn{caller} and
\var{P2} is the \dfn{callee}.
\dfEnd
\end{definition}

\begin{definition}[Housekeeping and body execution resource]
\label{obs:housekeeping-vs-body-execution}
For the purposes of complexity analysis,
we find it helpful to divide the resource consumption
of a called pseudocode procedure into \dfn{housekeeping}
and \dfn{body execution}.
Resource not consumed by housekeeping
is considered to be consumed by body execution.

Call an pseudocode procedure, \var{P}.
The housekeeping of \var{P} \Oc{} time and zero space
which, except in the case of the top-level pseudocode procedure,
is incurred by the caller of \var{P}.
Body execution of \var{P} may consume arbitrary resource,
which is incurred by \var{P}.
\dfEnd
\end{definition}

\todo{Account for top-level housekeeping}

In a common implementation, the housekeeping will be execution of
one or more prologues, which set up registers and stack for
the procedure,
and the execution of one more more epilogues,
which restore the registers and stack.

The following theorem allows some useful simplifications.
It states a condition under which we
may safely ignore the time incurred by a pseudocode procedure.

\begin{theorem}[Procedure overhead discard]
\label{th:procedure-overhead-discard}
\Oc{} time incurred by a
pseudocode procedure may be ignored.
\thEnd
\end{theorem}

\begin{proof}
We consider two cases.
The first case is the ``main'' or top-level pseudocode procedure;
and the second case is all other pseudocode procedures.
In the case of the ``main'' pseudocode procedure,
\Oc{} time may be charged to the parse.
\todo{Justify adding ignoring \Oc{} time added to the parse.}

In the case all other pseudocode procedures,
every pseudocode procedures which is not the top-level
has a caller.
By \Obref{housekeeping-vs-body-execution},
the caller always incurs \Oc{} ``housekeeping''
time when it calls callee.
If we add \Oc{} of time from callee
to the housekeeping time
of the callee, the total time is
\[
    \Oc + \Oc = \Oc,
\]
so that, in Landau notation terms,
the housekeeping time of the callee is unchanged.
\myqed
\end{proof}

\section{Counting invocations}

We first define our conventions for counting line
invocations,
and will then follow with a toy example.

\begin{definition}[Line invocation]
\label{def:line-invocation}
The number of \dfn{invocations} of a line
is the number of executions of
that line's logic.
\dfEnd
\end{definition}

\begin{definition}[Loop invocation]
\label{def:loop-invocation}
The number of \dfn{invocations} of a loop is the number of times
the loop is initialized.
Every line in a loop is either its \dfn{first line},
or an \dfn{interior} line.
The interior lines of loop are invoked zero or more times,
once for every \dfn{pass} of the loop.

The first line of a loop is invoked
\begin{itemize}
\item once for every successful test of the test condition of the loop; and
\item exactly once for an unsuccessful, and therefore final, test
of the test condition of the loop.
\dfEnd
\end{itemize}
\end{definition}

The need for a final, unsuccessful, test implies
that the first line of the loop is always invoked at last once.
In addition,
for every pass of the loop
there is exactly
one successful test of the loop condition.

The definitions of Line Invocation \Dfref{line-invocation} and
of Loop Invocation \Dfref{loop-invocation}
also imply that the number of
invocations of the first line of a loop
is only the same as the number of invocations
of the loop in a trivial case.
The two invocation counts will be identical if and only if the loop has zero passes ---
in other words, if the loop test fails the first time.

In the comments in the following fragment
``inv'' abbreviates invocation(s) and
``init'' abbreviates loop initializations.

\begin{algorithmic}[1]
\State do something
  \Comment inv=1
\label{line:toy-10}
\For{$\var{j} \in \set{1,2,3}$}
  \Comment init=1; inv=4; passes=3
\label{line:toy-20}
\State do an outer loop thing
  \Comment inv=3
\label{line:toy-30}
\For{$\var{i} \in \set{1,2}$}
  \Comment init=3; inv=9; passes=6
\label{line:toy-40}
\State do an inner loop thing
  \Comment inv=6
\label{line:toy-50}
\EndFor
\EndFor
\end{algorithmic}

According to our conventions, the following are true:
\begin{itemize}
\item Line \ref{line:toy-10} is invoked once.
\item Loop \enRefs{line:toy-20}{line:toy-50},
the outer loop,
is invoked once and therefore initialized once.
It has 3 passes.
\item Line \ref{line:toy-20},
the first line of the outer loop,
is invoked 4 times,
once for each pass of the outer loop,
and once to execute the test which terminates the outer loop.
\item Line \ref{line:toy-30} is the top-level interior line
of the outer loop.
It is invoked once for every pass of the outer loop,
or 3 times.
\item Loop \enRefs{line:toy-40}{line:toy-50},
the inner loop,
is initialized 3 times, once for every pass of the outer loop.
Loop \enRefs{line:toy-40}{line:toy-50}
is therefore said to have been
invoked 3 times.
The inner loop has 2 passes per initialization,
for a total of 6 passes.
\item Line \ref{line:toy-40},
the first line of the inner loop,
is invoked 3 times
for every initialization of the inner loop.
The 3 invocations per initialization consist of one
invocation for each of the 2 passes of the inner loop,
plus one invocation for the final test.
The total number of invocations of
line \ref{line:toy-40}
is therefore $3\times3=9$ times:
3 initializations multiplied by
3 invocations per initialization.
\item Line \ref{line:toy-50},
the interior line of the inner loop,
is a 2nd level line of the outer loop,
and a top level line of the inner loop.
Line \ref{line:toy-50}
is invoked once per pass of the inner loop, or 6 times.
\end{itemize}

The pseudocode uses a few basic control structures,
with which the reader is expected to be familiar,
and our justifications of the invocation counts
refer only to a few generalized behaviors.

\begin{definition}[Sequential and conditional execution]
\todo{Define Sequential and conditional execution}
If an execution of line \var{L2} always follows
an execution of line \var{L1} in an execution sequence,
then clearly line \var{L2} is executed
the same number of times as line \var{L1}.
In justification for line \var{L2},
we will point this out with
the notation ``line L1 Seq''.
Just as clearly in the above case,
line \var{L1} is executed as many times
as line \var{L2},
which observation our justifications notate as
``line L2 RSeq''.

If an execution of line \var{L2} sometimes follows
line an execution of \var{L1} in an execution sequence,
then line \var{L2} is executed
at most the same number of times as line \var{L1}.
We notate this observation in the form
``line L1 Cond''.
\dfEnd
\end{definition}

\section{Complexity table conventions}
\label{sec:complexity-tables}

If a table cell has a numeric value of zero,
it may be left blank to avoid clutter.
In tables with a ``Justification'' column, that column
is left empty when we consider the justification immediately
obvious from the code.

\chapter{Data structures}
\label{chap:data-structures}

\todo[prepend, caption={``Data structures'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

As preliminary to the pseudocode and
low-level complexity analysis,
this section describes some of the
data structures that are used
by more one of the pseudocode procedures.

\section{\realm{PIM}s}
\label{sec:data-eims}

A \realm{PIM} is a data structure of constant
size, excluding the non-initial link pairs,
and therefore requires \Oc{} space.
This section describes some of the
less obvious aspects of the data structures for each \realm{PIM}.

\begin{theorem}[PIM link pairs element]
\label{th:pim-link-pairs-element}
\todo{Account for linked list of direct ancestors}
\thEnd
\end{theorem}

\begin{proof}
\todo{Prove this}
\end{proof}

\begin{theorem}[PIM \var{transitions} list]
\label{th:pim-transitions-element}
The space incurred by linked lists
of the \var{transitions} array
may be ignored.
\thEnd
\end{theorem}

\begin{proof}
In every \realm{PIM}, we include an element that contains
a link reserved for use
by linked list in one entry of the \var{transitions} array
for one \realm{ES}.

Let \var{pim} be an arbitrary \realm{PIM}.
\realm{PIM}s without transition symbols,
that is, complete \realm{EIM}s do not occur
in
any linked list of a \var{transitions} array.
So if \var{pim} is in
the linked list of a \var{transitions} array
it is in the linked list of \Transition{\var{pim}}
for the \var{transitions} array of \Velement{S}{\Current{\var{pim}}}.
For any \realm{ES}, each symbol has at most one entry
in its \var{transitions} array,
so that a single link element per-\realm{PIM} suffices
for tracking \var{transitions} array links.

The link is either a pointer to another \realm{PIM}
or $\Lambda$, so that
the space incurred by the link is \Oc{}.
The \Oc{} space required for the link can be accounted
as part of the \Oc{} space consumed by the \realm{PIM}.

The \var{transitions} array will be described further
in \Sref{bookkeeper}.
\myqed
\end{proof}

\begin{theorem}[PIM Link Pairs Element]
\label{th:pim-link-pair-write}
Given access to a \realm{PIM} \var{pim},
a new link pair may be written in \Oc{}
time,
and its space requirement may be ignored.
\thEnd
\end{theorem}

\begin{proof}
The data structure for
every PIM \Vpim{pim}
includes an element which records its ancestry as a linked list
of link pairs.
A new link pair may be added at the head of this list.
The time complexity claims follow immediately from this fact.
The space incurred by the link list can be accounted
as part of the \Oc{} space consumed by the \realm{PIM}s.
\myqed
\end{proof}

\section{IDs}
\label{sec:data-ids}

\begin{theorem}[Symbol ID Complexity]
\label{th:symbol-id-complexity}
The function \ID{\Vsym{x}} maps each symbol to an
unique integer ID in \Oc{} time and zero space.
A symbol may be looked up by ID
in \Oc{} time and zero space.
Also, the entire list of symbols may be traversed
in \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
The data structure for \Vint{g}
contains a ``symbol table'',
an array for looking up symbols by ID,
so that symbol lookup by ID is \Oc{}.
The symbol structure stores its ID,
so that the time incurred by a call of
\ID{\var{x}} is \Oc{}.
In fact,
since \Vocab{\var{g}} is a constant depending on \var{g},
the symbol list is static,
so traversing the entire symbol list requires
\Oc{} time.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\begin{theorem}[Rule ID Complexity]
\label{th:rule-id-complexity}
The function \ID{\Vrule{r}} maps each rule to an
unique integer ID in \Oc{} time and zero space.
A rule may be looked up by ID
in \Oc{} time and zero space.
Also, the entire list of rules may be traversed
in \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
The data structure for \Vrule{r}
contains its ID, so that the time to
find \ID{\var{r}} is \Oc{}.
The data structure for \Vint{g}
contains an array for looking up rules by ID,
so that rule lookup by ID is \Oc{}.
In fact,
since \Rules{\var{g}} is a constant depending on \var{g},
the rule list is static,
so that traversing the entire rule list requires
\Oc{} time.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\begin{theorem}[Dotted Rule ID Complexity]
\label{th:dotted-rule-id-complexity}
The function \ID{\Vdr{x}} maps each dotted rule to
an ID in \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
One way to assign the
dotted rule ID's is to iterate through the array
of rules by rule ID,
and within each rule iterate through its possible dot
positions,
assigning an integer to each dot position.
The data structure for \Vrule{r},
allows, given a dot position \var{pos},
the ID of $\tuple{\var{r}, \var{pos}}$ to be found in \Oc{}
time.
Also, the data structure for \Vint{g}
contains an array for looking up a dotted rule in \Oc{} time,
given its ID.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\section{Earley sets}
\label{sec:data-es}

\subsection{Earley set}
\label{sec:es-complexity}

When \Marpa reaches a new current location,
it creates an empty Earley set.
For complexity analysis purposes,
an Earley set can be regarded as a linked list
of \realm{PIM}s.

Link pairs are kept in a finite set of linked lists.
The heads of the lists stored with the \realm{EIM} and
occupy trimmed space.
The rest of the lists of links pairs occupies
unassigned space.

\begin{theorem}[Earley set allocation]
\label{th:es-allocation}
The resource incurred
by allocating an empty Earley set is
charged as \Oc{} time,
\Oc{} trimmed space and zero link space.
\thEnd
\end{theorem}

\begin{proof}
Allocation of an empty linked list
incurs \Oc{} time and \Oc{} space.
\myqed
\end{proof}

\begin{theorem}[\realm{ES} add]
\label{th:es-add}
The resource incurred by adding a new
\realm{PIM} to an Earley set incurs
\Oc{} time and \Oc{} space.
\thEnd
\end{theorem}

\begin{proof}
An \realm{ES} is a linked list,
and a \realm{PIM} is added to the \realm{ES}
by putting at the head of list.
Including addition of the initial link pair,
this clearly can be done in \Oc{} time and \Oc{} space.
\myqed
\end{proof}

\begin{theorem}[\realm{ES} Link Pair Add]
\label{th:es-link-pair-add}
Adding a link pair
to an existing \realm{EIM} (a dup)
incurs \Oc{} time and \Oc{} space.
\thEnd
\end{theorem}

\begin{proof}
When an existing \realm{PIM} is added to an \realm{ES}
a link pair is added to the ancestry of
by putting at the head of a list of link pairs.
This clearly can be done in \Oc{} time and \Oc{} space.
\myqed
\end{proof}

\begin{theorem}[Earley set traversal]
\label{th:es-traversal}
Let \Ves{eset} be an Earley set.
Traversal of \var{eset}
incurs \Oc{} time and zero space for each \realm{PIM}
in \var{eset},
plus \Oc{} time and zero space for the final test.
\thEnd
\end{theorem}

\begin{proof}
The proof is by example of an implementation.
An \realm{ES} could be implemented as a dynamic array,
which would have the stated complexity for reads.
\myqed
\end{proof}

\begin{theorem}[\realm{LIM}--\realm{EIM} recharge]
\label{th:lim-eim-recharge}
Let \var{es} be an \realm{ES} with
\mypareq{eq:th-lim-eim-recharge-040}{%
    at most \var{x} resource
    charged to its \realm{EIM}s, and
}
\mypareq{eq:th-lim-eim-recharge-050}{%
    at most \var{y} resource
    charged to its \realm{LIM}s.
}
Then that resource for \var{es}
may also be accounted for as $\var{x}+\var{y}$
resource charged to each \realm{EIM} of \var{es}.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-lim-eim-recharge-210}{%
    Every \realm{LIM} in \var{es}
    has exactly one \realm{EIM} source
    \bcuz{} \longThref{\myfn{Source}{} Bijection Theorem}{source-fn-injection}.
}
\mypareq{eq:th-lim-eim-recharge-220}{%
    The source \realm{EIM} of a \realm{LIM}
    in \var{es} is also in \var{es}
    \bcuz{} \longThref{\realm{LIM} Direction Theorem}{lim-direction}.
}
\mypareq{eq:th-lim-eim-recharge-230}{%
    The resource of every \realm{LIM} in \var{es} may be recharged to exactly
    one \realm{EIM} in \var{es}
    \bcuz{} \Eref{th-lim-eim-recharge-210},
        \Eref{th-lim-eim-recharge-220}.
}
\mypareq{eq:th-lim-eim-recharge-240}{%
    The resource of every \realm{LIM} in \var{es} may be recharged in such
    a way that the resource charged to any \realm{EIM} is
    at most $\var{x}+\var{y}$
    \bcuz{} \Eref{th-lim-eim-recharge-040},
        \Eref{th-lim-eim-recharge-050},
        \Eref{th-lim-eim-recharge-230}.
}
\mypareq{eq:th-lim-eim-recharge-250}{%
    $\realm{PIM} = \realm{EIM} \cup \realm{LIM}$
        \cuz{} \longDfref{PIM}{pim}.
}
\mypareq{eq:th-lim-eim-recharge-260}{%
    All of the resource of \var{es} may be charged to
    the \realm{EIM}s in \var{es}
    in such a way that the resource charged to any \realm{EIM} is
    at most $\var{x}+\var{y}$
    \bcuz{} \Eref{th-lim-eim-recharge-240},
        \Eref{th-lim-eim-recharge-250}. \myqed
}
\end{proof}

\begin{theorem}[\realm{PIM} to \realm{EIM} consolidation]
\label{th:pim-eim-consolidation}
Let \var{es} be an \realm{ES} with
at most \var{x} resource
charged to any of its \realm{PIM}s.
Then that resource for \var{es}
may also be accounted for as $2\times\var{x}$
resource charged to each \realm{EIM} in \var{es}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \realm{LIM}--\realm{EIM} Recharge Theorem
\Thref{lim-eim-recharge}.
\myqed
\end{proof}

\begin{theorem}[Earley set \realm{EIM} traversal]
\label{th:es-eim-traversal}
Let \var{es} be an Earley set.
Traversal of \var{es}
may be charged as \Oc{} time and zero space for each \realm{EIM}
in \var{es},
plus \Oc{} time and zero space for the final test.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-es-eim-traversal-100}{%
    Traversal of \var{es}
    incurs \Oc{} time and zero space for each \realm{PIM}
    in \var{es},
    plus \Oc{} time and zero space for the final test
    \bcuz{} \longThref{\realm{ES} Traversal}{es-traversal}.
}
\mypareq{eq:th-es-eim-traversal-110}{%
    Traversal of \var{es}
    incurs $2\times\Oc{}=\Oc{}$ time and
    $2\times0=0$ space for each \realm{EIM}
    in \var{es},
    plus \Oc{} time and zero space for the final test
    \bcuz{} \longThref{\realm{PIM}--\realm{EIM} Consolidation}
        {pim-eim-consolidation}.
}
The theorem follows immediately from \Eref{th-es-eim-traversal-110}.
\myqed
\end{proof}

\section{Earley set indexing}
\label{sec:es-indexing}

\begin{theorem}[Location access time complexity]
Calling the functions \Current{\Vpim{pim}} and
\Current{\Ves{es}},
and accessing an \realm{ES}
as an element of an Earley table
(that is, in the form \Velement{S}{\Vloc{j}})
incurs \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
Our computation model, conceptually, is a random access machine,
but it will be more useful to justify
the theorem in terms of the \Marpa implementation.
The implementation description is equivalent to,
or straightforwardly translates to, a description in
terms of a RAM.
The accesses obviously require no new space.
The constant times are accomplished by
storing the integer location of that ES
in each ES;
storing ES's in a dynamic array
indexed by location;
and storing a pointer to its ES in
every \realm{PIM}.
\myqed
\end{proof}

In practice, for efficiency,
\Velement{S}{\Vloc{j}} accesses are not actually used
by the \Marpa implementation during parsing.
During parsing
\Rtwo~\cite{Marpa-R2}
builds Earley sets as a linked list of structures,
and a dynamic array indexing them is only created
when needed.
The dynamic array is not needed when
running the recognizer in normal production,
because links to Earley sets are kept in the \realm{PIM}s,
a link to the current Earley set is kept,
and the event mechanism works using links.

On the other hand,
accesses of the form
\Velement{S}{\Vloc{j}}
are made use of
for tracing and debugging in \Rtwo{}.
The trace and
debug facilities must allow the developer to
specify arbitrary Earley sets,
and the developer will often want to identify a location by
its numerical ID.

An array is also needed for evaluation,
but at evaluation time the dynamic array implementing
\Velement{S}{\Vloc{j}} accesses
usually is dynamic in theory only.
For most applications, evaluation only
occurs after the parse is complete,
at which point
the array of Earley sets is static,
so that the overhead of using a dynamic array
is minimal.

\section{\var{dotZeroUsed}}
\label{sec:dotzeroused-complexity}

\begin{definition}[\var{dotZeroUsed} Bitmap]
\label{def:dotZeroUsed-bitmap}
\var{dotZeroUsed} is a bitmap with
one bit for every rule in the grammar.
\end{definition}

\begin{theorem}[\var{dotZeroUsed} Initialization]
\label{th:dotzeroused-initialization}
\var{dotZeroUsed} initialization
takes place as part of the initial overhead of
parsing.
It incurs \Oc{} time and \Oc{} space,
which may be charged to the parse.
\thEnd
\end{theorem}

\begin{proof}
Except for the complexity claims,
all elements of the theorem
may be asserted as matters of definition,
either of the algorithm or of the complexity analysis strategy.
We show the complexity claims by
reference to a conceptual implementation.

\var{dotZeroUsed} is initialized to a cleared bitmap
of length \size{\Rules{\Vint{g}}}.
Since \var{g} is constant,
this may be done in \Oc{} time and \Oc{} space.
\myqed
\end{proof}

\begin{definition}[\var{dotZeroUsed} Correctness]
\label{def:dotZeroUsed-correctness}
We say that \var{dotZeroUsed} is \dfn{correct} for \Vet{table}
if
\[
\begin{gathered}
    \forall\; \Vrule{r} \in \Rules{\var{g}}:
    \\ \VVelement{dotZeroUsed}{\ID{\var{r}}} \iff
    \tuple{\tuple{\var{r},0},\Vlastix{table},\Vlastix{table}}
       \in \var{es}.
\end{gathered}
\]
Where \Vet{table} is understood in context
(usually because it is the context Earley table),
we simply say that \var{dotZeroUsed} is \dfn{correct}.
\dfEnd
\end{definition}

\begin{theorem}[\var{dotZeroUsed} Access]
\label{th:dotzeroused-access}
Clearing \var{dotZeroUsed} requires \Oc{} time
and zero space.
Bit accesses and writes take \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
The proof is by reference to the conceptual implementation.
The size of the bitmap is \size{\Rules{\var{g}}},
and never changes after initialization,
so that zero space is incurred.

\Rules{\var{g}} is a constant
that depends on the grammar \Dfref{context-free-grammar},
which justifies the claim of constant time for bit reads,
bit writes, and clearing of the bitmap.
\myqed
\end{proof}


\begin{definition}[Context \var{dotZeroUsed}]
\label{def:context-dotzeroused}
From this point on,
and unless stated otherwise,
every context scope contains its own \var{dotZeroUsed} bitmap.
\dfEnd
\end{definition}

\begin{definition}[\var{dotZeroUsed} Binding Assumption]
\label{def:dotzeroused-binding}
The binding assumption for \var{dotZeroUsed}
is that \var{dotZeroUsed} is correct for the context
Earley table.
(Frequently, the context Earley table is \var{S}.)
\dfEnd
\end{definition}

\section{\var{seen} PSL}
\label{sec:seen-psl-complexity}

Earley sets must not contain duplicated \realm{EIM}s and
\var{seen} is used to efficiently enforce this.

\begin{definition}[\var{seen} PSL]
\label{def:seen-psl}
For each Earley set,
\var{seen} is a PSL with
one entry for every dotted rule in the grammar.
For details on PSL's, see \Chref{per-set-lists}.
\myfn{Seen}{\Veim{x}} is a function which returns
the EIM equal to \var{x} if it already exists,
and $\Lambda$ otherwise.
\myfn{SetSeen}{\Veim{x}} is a function which adds
\var{x} to the PSL.
\end{definition}

\begin{theorem}[\var{seen} PSL]
\label{th:seen-psl-initialization}
In each Earley set,
initialization of \var{seen} consumes \Oc{} time.
\thEnd
\end{theorem}

\begin{proof}
The claims of this theorem are justified
in \Chref{per-set-lists}.
\myqed
\end{proof}

\begin{theorem}[\var{seen} PSL]
\label{th:seen-psl-access}
and \Oc{} space.
Each call of \Seen{} and \SetSeen{} consumes
consumes \Oc{} time
and zero space.
\thEnd
\end{theorem}

\begin{proof}
The claims of this theorem are justified
in \Chref{per-set-lists}.
\myqed
\end{proof}

\begin{definition}[\myfn{Seen}{} Correctness]
\label{def:seen-validity}
We say that \myfn{Seen}{} is \dfn{correct for} \Vet{table}
if
\[
    \forall\; \Veim{eim} :
    \left(\begin{gathered}
        \Seen{\var{eim}} \ne \Lambda
            \iff \Veim{eim} \in \Velement{table}{\Vlastix{table}}
    \end{gathered}\right)
\]
Where \Vet{table} is understood in context
(usually because it is the context Earley table),
we simply say that \myfn{Seen}{} is \dfn{valid}.
\dfEnd
\end{definition}

\begin{definition}[Context \var{seen} Function]
\label{def:context-seen-fn}
From this point on,
and unless stated otherwise,
every context scope contains its own \myfn{Seen}{} function.
\dfEnd
\end{definition}

\begin{definition}[\myfn{Seen}{} Binding Assumption]
\label{def:seen-binding}
The binding assumption for \myfn{Seen}{}
is that \myfn{Seen}{} is correct for the context
Earley table.
(Frequently, the context Earley table is \var{S}.)
\dfEnd
\end{definition}

\section{The \var{transitions} array of arrays}
\label{sec:transition-array}

The \var{transitions} array of arrays
is for efficient lookup
of \realm{EIM} and \realm{LIM} matches.

\begin{definition}[\var{transitions} Array]
\label{def:transitions-array}
Conceptually,
\var{transitions} is an array of arrays of sequences.
The top-level array is
indexed by Earley set location,
and each of its elements is an array of sequences.
Each second-level array, conceptually, is
indexed by transition symbol ID.
Each element of a second level array is a sequence
of \realm{PIM}s.

\transitions{\Vloc{j}}{\Vsym{t}}
is a sequence of \realm{PIM}s
whose location is \var{j} and
whose transition symbol is \Vsym{t}.
Writes to each element of the \var{transitions} array
occur only when that element is initialized.
In other words,
after the first read of \VVelement{transitions}{j},
the contents of \VVelement{transitions}{j} will not change.
\dfEnd
\end{definition}

\begin{theorem}[\realm{LIM} Transition First]
\label{th:lim-transition-first}
The sequence of \realm{PIM}s in an element of
of the \var{transitions} array of arrays
has its \realm{LIM}s first.
\thEnd
\end{theorem}

\begin{proof}
The proof is by description of a conceptual implementation.
If the sequence of \realm{PIM}s is kept as a doubly linked list,
it can be guaranteed that
\realm{LIM}s are first in the sequence.
For an actual implementation in \Rtwo{}, see
page \pageref{page:transition-lim-first}.
\myqed
\end{proof}

\begin{theorem}[\var{transitions} \realm{PIM} Sequence]
\label{th:transitions-pim-sequence}
The sequence
\transitions{\Vloc{j}}{\Vsym{t}}
is of \order{\Vloc{j}} size.
That is,
\begin{gather*}
\forall\; \Vsym{t} : \forall\; \var{j} \in \Dom{\var{S}} :
\\ \size{\transitions{\Vloc{j}}{\Vsym{t}}} = \order{\Vloc{j}}.
\quad \thEnd
\end{gather*}
\end{theorem}

\begin{proof}
Let \Vloc{j} and \Vsym{t} be arbitrary.
By the definition of the \var{transitions} array
\Dfref{transitions-array},
\transitions{\Vloc{j}}{\Vsym{t}}
is a sequence of \realm{PIM}s
whose location is \var{j} and
whose transition symbol is \Vsym{t}.
The theorem then follows from
the ES \realm{EIM} Count Theorem \Thref{es-eim-count},
and the ES \realm{LIM} Count Theorem \Thref{es-lim-count}.
\myqed
\end{proof}

\begin{theorem}[\var{transitions} Read]
\label{th:transitions-read}
A call of the form
\transitions{\Vloc{j}}{\Vsym{t}}
consumes \Oc{} time, and zero space.
\thEnd
\end{theorem}

\begin{proof}
The proof is by description of the \Rtwo{} implementation.
\Rtwo{} stores the elements of
\VVelement{transition}{j}
in sorted order, so that it can do a
a binary search of
\VVelement{transition}{j}.
By the \var{transitions} \realm{PIM} Sequence Theorem
\Thref{transitions-pim-sequence},
$\size{\VVelement{transition}{j}} = \Oc{}$,
and the search and pointer return
can be done in \Oc{} time.
No space need be incurred in the course of this read.
\myqed
\end{proof}

\begin{definition}[\var{transitions} Validity]
\label{def:transition-validity}
We say that \var{transitions} is \dfn{valid for} \Ves{es}
if
\[
\begin{gathered}
    \forall\; \Vsym{t} :
        \forall\; \var{pim} \in \transitions{\Current{\var{es}}}{\Vsym{t}} :
    \\ \var{pim} \in \var{es} \land \Transition{\var{pim}} = \Vsym{t}.
\end{gathered}
\]
If \var{transitions}{} is \dfn{valid for} \Ves{es},
then we also say that
\var{transitions}{} is \dfn{valid for} \Current{\var{es}}.

If \var{transitions} is valid for all \Vloc{j}
such that $0 \le \var{j} \le \Vlastix{(\Vet{table})}$,
we say that \var{transitions} is \dfn{valid for} \var{table}.
Where \Vet{table} is understood in context
(usually because it is the context Earley table),
we say that \var{transitions} is \dfn{valid}.
\dfEnd
\end{definition}

\begin{definition}[\var{transitions} completeness]
\label{def:transition-completeness}
We say that \var{transitions} is \dfn{complete for} \Ves{es}
if
\[
\begin{gathered}
    \forall\; \Vsym{t} : \forall\; \var{pim} \in \var{es} :
    \\ \Transition{\var{pim}} = \Vsym{t} \implies
        \var{pim} \in \transitions{\Current{\var{es}}}{\Vsym{t}}.
\end{gathered}
\]
If we say that \var{transitions} is \dfn{complete for} \Ves{es},
then we also say that
\var{transitions} is \dfn{complete for} \Current{\var{es}}.

If \var{transitions} is complete for all \Vloc{j}
such that $0 \le \var{j} \le \Vlastix{(\Vet{table})}$,
If \var{transitions} is \dfn{complete for} \Vlastix{(\Vet{table})},
we say that \var{transitions} is \dfn{complete for} \Vet{table}.
Where \Vet{table} is understood in context
(usually because it is the context Earley table),
we simply say that \var{transitions} is \dfn{complete}.
\dfEnd
\end{definition}

\begin{definition}[\var{transitions} correctness]
\label{def:transition-correctness}
Let \var{X} be an \realm{ES},
the current location of some \realm{ES},
or an \realm{ET}.
We say that \var{transitions} is \dfn{correct for} \var{X}
if \var{transitions} is valid and complete for \var{X}.
We say that \var{transitions} is \dfn{correct}
if \var{transitions} is valid and complete.
\dfEnd
\end{definition}

\begin{definition}[Context \var{transitions}]
\label{def:context-transitions}
From this point on,
and unless stated otherwise,
every context scope contains its own
\var{transitions} function.
\dfEnd
\end{definition}

\begin{definition}[\var{transitions} binding assumption]
\label{def:transitions-binding}
The binding assumption for \var{transitions}
is that \var{transitions} is valid for the context
Earley table.
(Frequently, the context Earley table is \var{S}.)
\dfEnd
\end{definition}

\section{\realm{LSS} traversal}
\label{sec:lss-traversal}

\begin{theorem}[\myfn{LIM}{} complexity]
\label{th:lim-complexity}
\todo{Do I need this?}
A call of the \myfn{LIM}{} function incurs \Oc{}
time and zero space.
\thEnd
\end{theorem}

\begin{proof}
Per the \myfn{LIM}{} Theorem \Thref{lim-fn}
and the definition of \realm{LIM} Validity \Dfref{lim-validity},
the \realm{LIM} and its source \realm{EIM}
have the same transition symbol and the same current \realm{ES}.
Per the \var{transitions} Read Theorem \Thref{transitions-read},
the linked list of \realm{PIM}s
for a given pair transition symbol and current \realm{ES}
can be found in \Oc{} time and zero space.

Since \myfn{LIM}{} is a function,
there is only one \realm{LIM} for its source \realm{EIM}
argument.
Per the \realm{LIM} Transition First Theorem \Thref{lim-transition-first},
this \realm{LIM} is first in the linked list returned by the
\myfn{Transitions}{} function.
The \realm{LIM} in that linked list can therefore be found
in \Oc{} time and zero space.
Totaling the two resource requirements,
the time incurred is $\Oc{} + \Oc{} = \Oc{}$
and the space incurred is $0+0=0$.
\myqed
\end{proof}

\begin{theorem}[\myfn{LIMPredecessor}{} complexity]
\label{th:lim-predecessor-complexity}
A call of the \LIMPredecessor{} function
incurs \Oc{} time and zero space.
\thEnd
\end{theorem}

\begin{proof}
The proof strategy is to state a conceptual algorithm
for executing \LIMPredecessor{\Veim{eim}},
and to analyze its resource requirements.
We are given \var{eim}.
The algorithm consists of the following steps, in order:
\begin{itemize}
\item Find \Symbol{\var{eim}}.
Clearly this can be done with \Oc{} time and zero space.
\item Find \Origin{\var{eim}}.
Again, this clearly can be done with \Oc{} time and zero space.
\item Execute $\var{ll} \gets \transitions{\Origin{\var{eim}}}{\Symbol{\var{eim}}}$.
Per the \var{transitions} Read Theorem \Thref{transitions-read},
this can be done in \Oc{} time and zero space.
\item
\var{ll} as returned by the previous step is a linked list of the \realm{PIM}s
for \Symbol{\var{eim}} and \Origin{\var{eim}}.
By the \LIMPredecessor{} Theorem \Thref{lim-predecessor-connection},
\var{ll} contains \LIMPredecessor{\Veim{eim}}.
By the \realm{LIM} Transition First Theorem \Thref{lim-transition-first},
the \realm{LIM} is first in \var{ll},
so the \realm{LIM} obviously may be found in \Oc{} time and zero space.
\end{itemize}

The algorithm above
Totaling the two resource requirements,
the time incurred is
$\Oc{} + \Oc{} + \Oc{} + \Oc{} = \Oc{}$
and the space incurred is $0+0+0+0=0$.
\myqed
\end{proof}

\chapter{The algorithm}
\label{chap:algorithm}

\todo[prepend, caption={``The algorithm'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\section{ES-setup op}
\label{sec:es-setup}

\todo{Write ES-setup op section}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{ES-Setup}}
\label{alg:es-setup-op}
\begin{algorithmic}[1]
\Procedure{ES-Setup}{}
\State $\forall \; \Vrule{r} \in \Rules{\Vint{g}} : \VVelement{dotZeroUsed}{\ID{\var{r}}} \gets \var{false}$
\label{line:es-setup-2}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier


\section{Bookkeeper op}
\label{sec:bookkeeper}

The \op{Bookkeeper} op is run for each \realm{ES}
after that \realm{ES} is fully populated with \realm{PIM}s.
The \op{Bookkeeper} op and creates
the \var{transition} array for that \realm{ES}.

\begin{algorithm}[H]
\caption{
    \label{alg:bookkeeper-op}
    \var{Bookkeeper}}
\begin{algorithmic}[1]
\Procedure{Bookkeeper}{\VVelement{S}{j}}
\label{line:bookkeeper-op-1}
\State Create \var{work}, an temporary working array of empty linked lists,
indexed by symbol ID, such that
$\Vsize{work} = \size{\Vocab{\var{g}}}$.
\label{line:bookkeeper-op-2}
\State $\Vnat{cnt} \gets 0$
\label{line:bookkeeper-op-3}
\For{$\Vpim{x} \in \VVelement{S}{j}$}
\label{line:bookkeeper-op-4}
\If { $\Transition{\Vpim{x}} \ne \Lambda$ }
\label{line:bookkeeper-op-5}
\State $\Vnat{symID} \gets \ID{\Transition{\Vpim{x}}}$
\label{line:bookkeeper-op-6}
\If { \Velement{work}{\var{symID}} is an empty list }
\label{line:bookkeeper-op-7}
\State $\var{cnt} \gets \var{cnt}+1$
\label{line:bookkeeper-op-8}
\EndIf
\State Add \Vpim{x} to the list at \Velement{work}{\var{symID}},
   so that \realm{LIM}s are first in the list.
\label{line:bookkeeper-op-9}
\EndIf
\EndFor
\State Create \var{transition} for \VVelement{S}{j}, $\Vsize{transition} = \var{cnt}.$
\label{line:bookkeeper-op-10}
\State Copy the non-empty entries of \var{work} to \var{transition},
   in order.
\label{line:bookkeeper-op-11}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{theorem}[\var{cnt} size]
\label{th:cnt-size}
In \Aref{alg:bookkeeper-op}, $\var{cnt} = \Oc{}$.
\thEnd
\end{theorem}

\begin{proof}
\var{cnt} is incremented at line
\ref{line:bookkeeper-op-8} only if
the list \Velement{work}{\var{symID}} is empty
at line \ref{line:bookkeeper-op-7}.
But whenever \Velement{work}{\var{symID}} is empty
at line \ref{line:bookkeeper-op-7},
an entry is added to it
at line \ref{line:bookkeeper-op-9}.
Therefore,
\mypareq{eq:th-bookkeeper-100}{%
    for every value of \var{symID},
    line \ref{line:bookkeeper-op-8} is executed at most once
    \bcuz{} \op{Bookkeeper} Algorithm \Aref{alg:bookkeeper-op}.
}
\mypareq{eq:th-bookkeeper-110}{%
   \var{symID}
   at line \ref{line:bookkeeper-op-6}
   is the ID of a symbol in \Vint{g}
   \cuz{} \Aref{alg:bookkeeper-op}.
}
\mypareq{eq:th-bookkeeper-120}{%
    Line \ref{line:bookkeeper-op-8} is executed at most
    \size{\Vocab{\var{g}}} times
    \bcuz{} \Eref{th-bookkeeper-100},
        \Eref{th-bookkeeper-110}.
}
\mypareq{eq:th-bookkeeper-130}{%
    In \Aref{alg:bookkeeper-op}, $\var{cnt} \le
    \size{\Vocab{\var{g}}}$
    \cuz{} lines \ref{line:bookkeeper-op-3},
        \ref{line:bookkeeper-op-8};
        \Eref{th-bookkeeper-120}.
}
The theorem follows from
\Eref{th-bookkeeper-130}
and that fact that
\Vocab{\var{g}} is a constant which depends on \Vint{g}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.25in}
\begin{table}[H]
\caption{
    \label{tab:bookkeeper-invocations}
    Line invocations per call of \op{Bookkeeper}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:bookkeeper-op-2}&1&\cellboxr{%
        Line \ref{line:bookkeeper-op-1} Seq.}
\\ \hline
    \ref{line:bookkeeper-op-3}&1&\cellboxr{%
       Line \ref{line:bookkeeper-op-2} Seq.}
\\ \hline
    \ref{line:bookkeeper-op-4}%
    &$1+\var{pim}$%
    &\cellboxr{%
        Line \ref{line:bookkeeper-op-3} Seq;
        \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:bookkeeper-op-5}&\var{pim}&\cellboxr{%
       Line \ref{line:bookkeeper-op-4} Seq; \Dfref{loop-invocation}.}
\\ \hline
    \ref{line:bookkeeper-op-6}&\order{\var{pim}}&\cellboxr{%
       Line \ref{line:bookkeeper-op-5} Cond.}
\\ \hline
    \ref{line:bookkeeper-op-7}&\order{\var{pim}}&\cellboxr{%
       Line \ref{line:bookkeeper-op-6} Seq.}
\\ \hline
    \ref{line:bookkeeper-op-8}&\order{\var{pim}}&\cellboxr{%
       Line \ref{line:bookkeeper-op-7} Cond.}
\\ \hline
    \ref{line:bookkeeper-op-9}&\order{\var{pim}}&\cellboxr{%
       Line \ref{line:bookkeeper-op-7} Seq.}
\\ \hline
    \ref{line:bookkeeper-op-10}&1&\cellboxr{%
        Line \ref{line:bookkeeper-op-4} Seq.}
\\ \hline
    \ref{line:bookkeeper-op-11}&1&\cellboxr{%
        Line \ref{line:bookkeeper-op-10} Seq.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:bookkeeper-invocations}
are listed in order of deduction
and refer to the \op{Bookkeeper} Algorithm \Aref{alg:bookkeeper-op}.
\var{pim} is the count of \realm{PIM}s in
\VVelement{S}{j}, that is,
$\var{pim} = \size{\VVelement{S}{j}}$.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:bookkeeper-time}
    Time per line invocation of \op{Bookkeeper}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:bookkeeper-op-2}&\cellRule{\Oc{}}&\cellboxr{%
        \Vocab{\var{g}} is a constant which depends on \Vint{g}.}
\\ \hline
    \ref{line:bookkeeper-op-3}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-4}&\cellRule{\Oc{}}&\cellboxr{%
        \realm{ES} Traversal Theorem \Thref{es-traversal}.}
\\ \hline
    \ref{line:bookkeeper-op-5}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-6}&\cellRule{\Oc{}}&\cellboxr{%
        Symbol ID Complexity Theorem \Thref{symbol-id-complexity}.}
\\ \hline
    \ref{line:bookkeeper-op-7}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-8}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-9}&\cellRule{\Oc{}}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
    \ref{line:bookkeeper-op-10}&\cellRule{\Oc{}}&\cellboxr{%
        $\Vsize{transition} = \Oc{}$ \cuz{}
        \var{cnt} Size Theorem \Thref{cnt-size}.}
\\ \hline
    \ref{line:bookkeeper-op-11}&\cellRule{\Oc{}}&\cellboxr{%
        $\Vsize{work} = \Oc{}$ \cuz{}
       Line \ref{line:bookkeeper-op-2} of \AFref{alg:bookkeeper-op}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:bookkeeper-time}
refer to the \op{Bookkeeper} Algorithm \Aref{alg:bookkeeper-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:bookkeeper-time-worksheet}
    \op{Bookkeeper} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellRule{Per call}}
&\multicolumn{1}{|l|}{\cellboxc[1.25in]{%
    Per $\realm{PIM} \in \VVelement{S}{j}$
    \\ per call}}
\\ \hline
    \ref{line:bookkeeper-op-2}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-3}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-4}&\cellRule{\Oc{}}&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-5}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-6}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-7}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-8}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-9}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:bookkeeper-op-10}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:bookkeeper-op-11}&\cellRule{\Oc{}}&
\\ \hline
    Totals&\cellRule{\Oc{}}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:bookkeeper-time-worksheet}
refer to the \op{Bookkeeper} Algorithm \Aref{alg:bookkeeper-op}.
Cell values are calculated from the
\op{Bookkeeper} Line Invocations Table \Tbref{tab:bookkeeper-invocations},
and \op{Bookkeeper} Time Table \Tbref{tab:bookkeeper-time}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Bookkeeper} Time]
\label{th:bookkeeper-time}
The time that each call of \op{Bookkeeper} incurs
can be accounted for by charging
\Oc{} time
to each \realm{EIM}
in \VVelement{S}{j} of \Aref{alg:bookkeeper-op}.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-bookkeeper-time-100}{%
    The time that each call of the \op{Bookkeeper} op incurs
    can be accounted for as \Oc{} time plus \\
    \Oc{} time per $\realm{PIM} \in \VVelement{S}{j}$
    \bcuz{} \op{Bookkeeper} Time Worksheet
        \Tbref{tab:bookkeeper-time-worksheet}.
}
\mypareq{eq:th-bookkeeper-time-110}{%
    The time that each call of the \op{Bookkeeper} op incurs
    can be accounted for as
    \Oc{} time per $\realm{PIM} \in \VVelement{S}{j}$
    \bcuz{} Procedure Overhead Time Discard Theorem
        \Thref{procedure-overhead-discard}.
}
\mypareq{eq:th-bookkeeper-time-250}{%
    The time that each call of the \op{Bookkeeper} op incurs
    can be accounted for as
    $2\times\Oc{}=\Oc{}$ time per $\realm{EIM} \in \VVelement{S}{j}$
    \bcuz{} \longThref{\realm{PIM}--\realm{EIM} Consolidation}
        {pim-eim-consolidation}.
}
The theorem follows immediately from
\Eref{th-bookkeeper-time-250}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.3in}
\begin{table}[H]
\caption{
    \label{tab:bookkeeper-space}
    Space per line invocation of \op{Bookkeeper}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:bookkeeper-op-2}&0&
        \cellboxr{%
            \Vocab{\var{g}} is a constant which depends on \Vint{g},
            so that $\Vsize{work} = \Oc$.
            \Vsize{work} resides on the stack.}
\\ \hline
    \ref{line:bookkeeper-op-3}&0&\cellRule{}
\\ \hline
    \ref{line:bookkeeper-op-4}&0&%
        \cellboxr{%
            \realm{ES} Traversal Theorem \Thref{es-traversal}.}
\\ \hline
    \ref{line:bookkeeper-op-5}&0&\cellRule{}
\\ \hline
    \ref{line:bookkeeper-op-6}&0&%
        \cellboxr{%
            Symbol ID Complexity Theorem \Thref{symbol-id-complexity}.}
\\ \hline
    \ref{line:bookkeeper-op-7}&0&\cellRule{}
\\ \hline
    \ref{line:bookkeeper-op-8}&0&\cellRule{}
\\ \hline
    \ref{line:bookkeeper-op-9}&0&%
        \cellboxr{%
            \realm{PIM} Transitions List Theorem
            \Thref{pim-transitions-element}.}
\\ \hline
    \ref{line:bookkeeper-op-10}&\Oc{}&%
        \cellboxr{%
            $\Vsize{transition} = \Oc{}$ \cuz{}
            \var{cnt} Size Theorem \Thref{cnt-size}.}
\\ \hline
    \ref{line:bookkeeper-op-11}&0&\cellRule{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:bookkeeper-space}
refer to the \op{Bookkeeper} Algorithm \Aref{alg:bookkeeper-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Bookkeeper} Space]
\label{th:bookkeeper-space}
The space that each \op{Bookkeeper} op incurs
may be accounted for by charging
\Oc{} space to \VVelement{S}{j}
of \AFref{alg:bookkeeper-op}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from the
\op{Bookkeeper} Line Invocations Table
\Tbref{tab:bookkeeper-invocations}
and the \op{Bookkeeper} Space Table
\Tbref{tab:bookkeeper-space}.
\myqed
\end{proof}

\section{Initializer op}
\label{sec:initializer}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{Initializer}}
\label{alg:initializer-op}
\begin{algorithmic}[1]
\Procedure{Initializer}{}
\State $\Veim{start} \gets \tuple{\dr{\tuple{\Vrule{accept}, 0}}, 0}$
\label{line:initializer-op-10}
\Statex \Comment \Vrule{accept} is the accept rule of \Vint{g} \Eref{def-accept-rule}.
\State $\LinkPairs{\Veim{start}} = \set{}$
\label{line:initializer-op-20}
\State Add \Veim{start} to \es{(0)}
\label{line:initializer-op-30}
\State $\VVelement{dotZeroUsed}{\ID{\Vrule{accept}}} \gets \var{true}$
\label{line:initializer-op-60}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{theorem}[\op{Initializer} Complexity]
\label{th:initializer-complexity}
Time incurred by
the \op{Initializer} op may be ignored.
Space incurred by the \op{Initializer} op may be
accounted for as \Oc{} space charged
to the start \realm{EIM}.
\thEnd
\end{theorem}

\begin{proof}
Execution in the \op{Initializer} op depends only
on the grammar \Vint{g}.
\Vint{g} is constant,
so that time and space
incurred by \op{Initializer} is \Oc{}.
By the Procedure Overhead Time Discard Theorem
\Thref{procedure-overhead-discard},
the \Oc{} time may be ignored.
\myqed
\end{proof}

\begin{observation}[Ancestry]
\label{obs:initializer-ancestry}
The \Marpa implementation does not keep
any link pairs for any \realm{PIM} in Earley set 0.
\Veim{start} does not have any non-trivial ancestor,
so that a link pair does not exist for it,
even conceptually.
And \Marpa does not track link pairs for
predictions
\Obref{prediction-ancestry}.
\obEnd{}
\end{observation}

\begin{observation}[\Earley{}'s Earley set 0]
\label{obs:earley-set-0}
\Marpa's % \linebreak
\op{Ini\-tial\-iza\-tion} op has the same effect as the construction
of Earley set 0 by \Earley{}.
\obEnd{}
\end{observation}

\section{Predicter op}
\label{sec:predicter-op}

\FloatBarrier

\begin{algorithm}[H]
\caption{\var{Predict1}}
\label{alg:predict1}
\begin{algorithmic}[1]
\Procedure{Predict1}{\Veim{prediction}}
\If{$\VVelement{dotZeroUsed}{\ID{\Rule{\var{prediction}}}} = \var{true}$}
\label{line:predict1-2}
  \State return
\label{line:predict1-3}
\EndIf
\State $\LinkPairs{\var{prediction}} = \set{}$
\label{line:predict1-4}
\State Add \var{prediction} to \Velement{S}{\Current{\var{prediction}}}
\label{line:predict1-5}
\State $\VVelement{dotZeroUsed}{\ID{\var{\Rule{\var{prediction}}}}} \gets \var{true}$
\label{line:predict1-6}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.4in}
\begin{table}[H]
\caption{
    \label{tab:predict1-time}
    Line invocations per call of \op{Predict1}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:predict1-2}&\cellRule{\Oc{}}&\cellboxr{%
        \var{dotZeroUsed} Complexity Theorem \Thref{dotzeroused-access}.}
\\ \hline
    \ref{line:predict1-3}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:predict1-4}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:predict1-5}&\cellRule{\Oc{}}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
    \ref{line:predict1-6}&\cellRule{\Oc{}}&\cellboxr{%
        \var{dotZeroUsed} Complexity Theorem \Thref{dotzeroused-access}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predict1-time}
refer to the \var{Predict1} Algorithm \AFref{alg:predict1}.}
\end{table}

\FloatBarrier

\begin{theorem}[\proc{Predict1} Time]
\label{th:predict1-time}
The time incurred by \proc{Predict1} may be ignored.
\thEnd
\end{theorem}

\begin{proof}
From \AFref{alg:predict1} it is evident that each of its 6 lines
is invoked at most once.
From the Time per Line Invocation Table \Tbref{tab:predict1-time},
we see that each line incurs at most \Oc{} time.
The time incurred by each call of \myfn{Predict1}{} is therefore
$6\times\Oc{} = \Oc{}$.
By the Procedure Overhead Time Discard Theorem
\Thref{procedure-overhead-discard},
\Oc{} time per procedure call may be ignored.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:predict1-space}
    Space per line invocation of \op{Predict1}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:predict1-2}&&\cellboxr{%
        \var{dotZeroUsed} Complexity Theorem \Thref{dotzeroused-access}.}
\\ \hline
    \ref{line:predict1-3}&&\cellRule{}
\\ \hline
    \ref{line:predict1-4}&&\cellRule{}
\\ \hline
    \ref{line:predict1-5}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
    \ref{line:predict1-6}&&\cellboxr{%
        \var{dotZeroUsed} Complexity Theorem \Thref{dotzeroused-access}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predict1-space}
refer to the \var{Predict1} Algorithm \AFref{alg:predict1}.}
\end{table}

\FloatBarrier
\begin{theorem}[\var{Predict1} Space]
\label{th:predict1-space}
If \var{dotZeroUsed} is consistent for
\VVelement{S}{\Current{\var{prediction}}},
then the space incurred by \myfn{Predict1}{\var{prediction}}
may be charged as \Oc{} space to the \realm{ES}
\VVelement{S}{\Current{\var{prediction}}}.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-predict1-space-100}{%
    The space incurred by
    \myfn{Predict1}{\Veim{prediction}}
    may be accounted for as
    \Oc{} space incurred by each invocation of line
    \ref{line:predict1-5}
    \cuz{} \Tbref{tab:predict1-space}.
}
\mypareq{eq:th-predict1-space-110}{%
    During the course of any run of \Marpa,
    line \ref{line:predict1-5} of \AFref{alg:predict1},
    is invoked at most once per rule-\realm{ES} pair
    \bcuz{} \AFref{alg:predict1},
    \longThref{Prediction Rule Non-Duplication}{prediction-rule-non-duplication}.
}
\mypareq{eq:th-predict1-space-120}{%
    During the course of any run of \Marpa,
    line \ref{line:predict1-5} of \AFref{alg:predict1}
    is invoked at most \size{\Rules{\Vint{g}}} times for each
    \realm{ES}
    \bcuz{} \Eref{th-predict1-space-110}.
}
\mypareq{eq:th-predict1-space-130}{%
    During the course of any run of \Marpa,
    calls of \myfn{Predict1}{} incur at most
    $\size{\Rules{\Vint{g}}}\times\Oc{}=\Oc{}$ space for each \realm{ES}
    \cuz{} \Eref{th-predict1-space-100},
        \Eref{th-predict1-space-120}.
}
The theorem follows immediately from \Eref{th-predict1-space-130}.
\myqed
\end{proof}

\FloatBarrier

\begin{algorithm}[H]
\caption{\var{Predicter}}
\label{alg:predicter-op}
\begin{algorithmic}[1]
\Procedure{Predicter}{\Ves{es}}
\label{line:predicter-op-1}
\Statex \hspace\algorithmicindent
    $\triangleright$ \rawparcomment[4.3in]{Loop over every \Veim{x} in \Ves{es},
    including those that \var{Predict1} adds while the loop
    is running.
    }
\For{$\Veim{x} \in \Ves{es}$}
\label{line:predicter-op-2}
  \For{$\Vrule{r} \in \set{\Vrule{r} : \Postdot{\var{x}} = \LHS{\var{r}}}$}
  \label{line:predicter-op-3}
    \State $\Veim{pred} \gets \tuple{ \dr{\tuple{\Vrule{r}, 0}}, \Current{\var{es}} , \Current{\var{es}}}$
      \label{line:predicter-op-4}
    \State \Call{Predict1}{\var{pred}}
    \label{line:predicter-op-5}
  \EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\renewcommand{\cellboxrWidth}{2in}
\renewcommand{\cellboxcWidth}{1.5in}
\begin{table}[H]
\caption{
    \label{tab:predicter-invocations}
    Line invocations per call of \op{Predicter}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:predicter-op-2}&$1+\var{eim}$&\cellboxr{%
           Line \ref{line:predicter-op-1} Seq;
           \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:predicter-op-3}
    &
        \cellboxc{%
        % Add space above and below the equation
        \rule[-.25in]{0em}{.6in}$
        1+\left(
            \begin{gathered}
            \var{eim} \; \times
            \\ \size{\Rules{\Vint{g}}}
            \end{gathered}
        \right)
        $}
    &\cellboxr{%
           Line \ref{line:predicter-op-2} Seq;
           \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:predicter-op-4}
    &\cellboxc{%
        $\var{eim}\times\fixedsize{\Rules{\var{g}}}$}
    &\cellboxr{%
           Line \ref{line:predicter-op-3} Seq;
           \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:predicter-op-5}
    &\cellboxc{%
        $\var{eim}\times\fixedsize{\Rules{\var{g}}}$%
        }
    &\cellboxr{Line \ref{line:predicter-op-4} Seq.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predicter-invocations}
are listed in order of deduction
and refer to the \op{Predicter} Algorithm \Aref{alg:predicter-op}.
\var{eim} is the number of \realm{EIM}s in
\VVelement{S}{j}, that is,
$\var{eim} = \size{\set{\var{x}\in\realm{EIM}:\var{x}\in \VVelement{S}{j}}}$.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:predicter-time}
    Time per line invocation of \op{Predicter}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:predicter-op-2}&\Oc{}&\cellboxr{%
        \realm{ES} \realm{EIM} Traversal Theorem
        \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:predicter-op-3}&\Oc{}&\cellboxr{%
        \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:predicter-op-4}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:predicter-op-5}&\Oc{}&\cellboxr{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predicter-time}
refer to the \op{Predicter} Algorithm \Aref{alg:predicter-op}.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxcWidth}{1.25in}
\begin{table}[H]
\caption{
    \label{tab:predicter-time-worksheet}
    \op{Predicter} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|r|r|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|c|}{\cellRule{Per call}}
&\multicolumn{1}{|c|}{Per $\realm{EIM} \in \VVelement{S}{j}$
    per call}
\\ \hline
    \ref{line:predicter-op-2}&$1\times\Oc{}=\Oc{}$&\cellRule{%
        $1\times\Oc{}=\Oc{}$}
\\ \hline
    \ref{line:predicter-op-3}&$1\times\Oc{}=\Oc{}$&\cellRule{%
        $\fixedsize{\Rules{\Vint{g}}}\times\Oc{}=\Oc{}$}
\\ \hline
    \ref{line:predicter-op-4}&&\cellRule{%
        $\fixedsize{\Rules{\var{g}}}\times\Oc{}=\Oc{}$}
\\ \hline
    \ref{line:predicter-op-5}&&\cellRule{%
        $\fixedsize{\Rules{\var{g}}}\times\Oc{}=\Oc{}$}
\\ \hline
    Totals&\Oc{}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predicter-time-worksheet}
    refer to the \op{Predicter} Algorithm \Aref{alg:predicter-op}.
    Cell values are calculated from the
    \op{Predicter} Line Invocations Table \Tbref{tab:predicter-invocations},
    and \op{Predicter} Time Table \Tbref{tab:predicter-time}.
    Recall that \Vint{g} is a constant so that
    $\size{\Rules{\Vint{g}}} = \Oc{}$.}
\end{table}

\FloatBarrier

\begin{theorem}[\var{Predicter} time complexity]
\label{th:predicter-time}
The incurred by each call of \op{Predicter}
can be accounted for by charging
\Oc{} time to each \realm{EIM} of \VVelement{S}{j},
where \VVelement{S}{j} is the argument of the
\op{Predicter} call.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \op{Predictor} Time Worksheet \Tbref{tab:predicter-time-worksheet}
and the Procedure Overhead Time Discard Theorem
\Thref{procedure-overhead-discard}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:predicter-space}
    Space per line invocation of \op{Predicter}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:predicter-op-2}&0&\cellboxr{%
        \realm{ES} \realm{EIM} Traversal Theorem
        \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:predicter-op-3}&0&\cellRule{}
\\ \hline
    \ref{line:predicter-op-4}&0&\cellRule{}
\\ \hline
    \ref{line:predicter-op-5}&0&\cellRule{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:predicter-space}
    refer to the \op{Predicter} Algorithm \Aref{alg:predicter-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\var{Predicter} Space Complexity]
\label{th:predicter-space}
Calls of \op{Predicter} incur no space.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows immediately from
the \op{Predicter} Space per Line Table
\Tbref{tab:predicter-space}.
\myqed
\end{proof}

If precomputation is used
\Aref{alg:predicter-op} can be sped up using Warshall's algorithm.
A version of \proc{Predicter} using precomputation
will clearly run no worse than
\proc{Predicter}.

\begin{observation}[Prediction ancestry]
\label{obs:prediction-ancestry}
The \Marpa implementation does not track
link pairs
for predictions.
\obEnd{}
\end{observation}

Tracking link pairs for a prediction would be cumbersome,
because, even in an unambiguous grammar,
each prediction can have many reasons to be in an Earley
set.

Conceptually,
predictions have predecessors, but no causes,
and the link pair generated for
\Veim{prediction} in
line \ref{line:predict1-4} of \proc{Predict1}~\Aref{alg:predict1}
would be
$\tuple{\Veim{x}, \Lambda}$,
where \Veim{x}
is the variable of line \ref{line:predicter-op-2}
of \proc{Predicter}~\Aref{alg:predicter-op},
which is not actually in scope at
line \ref{line:predict1-4} of \proc{Predict1}.

If link pairs were generated,
for the arbitrary prediction
\[
  \tuple{\left[\Vsym{lhs} \de \mydot \Vstr{rhs}\right],
      \Vnat{j}, \var{j}}
\]
the ancestry would be
\begin{equation}
\label{eq:common-predict-ancestry-10}
\left\lbrace
  \begin{gathered}
  \tuple{\Veim{pred}, \Lambda}_\realm{LP} : \\
  \Current{\var{pred}} = \Vloc{j}
  \; \land \; \Postdot{\var{pred}} = \Vsym{lhs}
  \end{gathered}
\right\rbrace .
\end{equation}
The computation of
\Eref{common-predict-ancestry-10}
could be sped up using
the transition array
\Sref{bookkeeper} for \VVelement{S}{j}.

\begin{observation}[Original Earley algorithm: Predicter Operation]
\label{obs:earley-predicter-op}
\Marpa's predicter op has the same effect as the ``Predicter''
operation of \Earley{}~\cite{Earley1970}.
\end{observation}

\section{Scanner op}
\label{def:scanner}

\FloatBarrier

\begin{algorithm}[H]
\caption{\var{Scanner}}
\label{alg:scanner-op}
\begin{algorithmic}[1]
\Procedure{Scanner}{\VVelement{S}{j}}
\Statex \lcomment{$\Vtoken{token} = \tuple{\Velement{inp}{\Vdecr{j}}, \var{val}}$.
  See \Dfref{token}.
}
\State $\var{matches} \gets \transitions{\Vdecr{j}}{\Symbol{\var{token}}}$
\label{line:scanner-op-2}
\For{every $\Veim{match} \in \Veimset{matches}$}
\label{line:scanner-op-3}
\State $\Veim{trial} \gets \tuple{
     \begin{gathered}
        \Next{\DR{\var{match}}},
        \\ \Origin{\var{match}}, \Current{\VVelement{S}{j}}
     \end{gathered}
        }
        $
\label{line:scanner-op-4}
\State $\var{lp} \gets \tuple{\Veim{match},\var{token}}_\realm{LP}$
\label{line:scanner-op-5}
  \State $\var{found} \gets \Seen{\var{trial}}$
\label{line:scanner-op-6}
  \If{$\var{found} \ne \Lambda$}
\label{line:scanner-op-7}
    \State Add \var{lp} to \LinkPairs{\var{trial}}
\label{line:scanner-op-8}
  \Else
\label{line:scanner-op-9}
    \State $\SetSeen{\var{trial}}$
\label{line:scanner-op-10}
    \State $\LinkPairs{\var{trial}} \gets \set{\var{lp}}$
\label{line:scanner-op-11}
    \State Add \var{trial} to \VVelement{S}{j}
\label{line:scanner-op-12}
  \EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.3in}
\renewcommand{\cellboxcWidth}{.8in}
\begin{table}[H]
\caption{
    \label{tab:scanner-invocations}
    Line invocations per call of \op{Scanner}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
    \multicolumn{1}{|c|}{Line}
    &\cellboxc{Invocations per call}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-8}&\var{dup}&%
        \cellboxr{This line duplicates an \realm{EIM}
            that already exists in \VVelement{S}{j}.}
\\ \hline
    \ref{line:scanner-op-12}&\var{new}&\cellboxr{%
            This line adds an \realm{EIM} to \VVelement{S}{j}.}
\\ \hline
    \ref{line:scanner-op-11}&\var{new}&\cellboxr{%
           Line \ref{line:scanner-op-12} RSeq.}
\\ \hline
    \ref{line:scanner-op-10}&\var{new}&\cellboxr{%
           Line \ref{line:scanner-op-11} RSeq.}
\\ \hline
    \ref{line:scanner-op-9}&\var{new}&\cellboxr{%
           Line \ref{line:scanner-op-10} RSeq.}
\\ \hline
    \ref{line:scanner-op-7}&
       \var{new}+\var{dup}&\cellboxr{%
           Line \ref{line:scanner-op-8} RSeq or
           line \ref{line:scanner-op-9} RSeq.}
\\ \hline
    \ref{line:scanner-op-6}&
       \var{new}+\var{dup}&\cellboxr{%
           Line \ref{line:scanner-op-7} RSeq.}
\\ \hline
    \ref{line:scanner-op-5}&
       \var{new}+\var{dup}&\cellboxr{%
           Line \ref{line:scanner-op-6} RSeq.}
\\ \hline
    \ref{line:scanner-op-4}&
       \var{new}+\var{dup}&\cellboxr{%
           Line \ref{line:scanner-op-5} RSeq.}
\\ \hline
    \ref{line:scanner-op-3}&1+\var{new}+\var{dup}&%
        \cellboxr{%
           Line \ref{line:scanner-op-4} RSeq,
           \longDfref{Loop invocation}{loop-invocation},
           \realm{ES} \realm{EIM} Traversal Theorem
           \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:scanner-op-2}&1&\cellboxr{%
       Line \ref{line:scanner-op-3} RSeq,
           \Dfref{loop-invocation}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:scanner-invocations}
are listed in order of deduction,
and refer to the \op{Scanner} Algorithm \AFref{alg:scanner-op}.
\var{dup} is the count of passes that adjoin a duplicate \realm{EIM}
to \VVelement{S}{j}.
\var{new} is the count of passes that add a new \realm{EIM}
to \VVelement{S}{j}.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:scanner-time}
    Time per line invocation of \op{Scanner}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-2}&\Oc{}&\cellboxr{%
	  \Veimset{matches} is a pointer to the head of a linked list,
	  as returned by \myfn{Transition}{}
	  \Thref{transitions-read}.}
\\ \hline
    \ref{line:scanner-op-3}&\Oc{}&\cellboxr{%
           \realm{ES} \realm{EIM} Traversal Theorem
           \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:scanner-op-4}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:scanner-op-5}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:scanner-op-6}&\Oc{}&\cellboxr{%
	  \var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:scanner-op-7}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:scanner-op-8}&\Oc{}&\cellboxr{%
	  \realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
    \ref{line:scanner-op-9}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:scanner-op-10}&\Oc{}&\cellboxr{%
	  \var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:scanner-op-11}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:scanner-op-12}&\Oc{}&\cellboxr{%
	  \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:scanner-time}
refer to the \op{Scanner} Algorithm \Aref{alg:scanner-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:scanner-time-worksheet}
    \op{Scanner} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellRule{For every call}}
&\multicolumn{1}{|l|}{\cellboxc[1.25in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[1.75in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:scanner-op-3}&\cellRule{\Oc{}}&&
\\ \hline
    \ref{line:scanner-op-3}&\cellRule{\Oc{}}&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-4}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-5}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-6}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-7}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-8}&&&\cellRule{\Oc{}}
\\ \cline{1-4}
    \ref{line:scanner-op-9}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:scanner-op-10}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    \ref{line:scanner-op-11}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    \ref{line:scanner-op-12}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    Totals&\cellRule{\Oc{}}&\Oc{}&\Oc{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:scanner-time-worksheet}
refer to the \op{Scanner} Algorithm \Aref{alg:scanner-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Scanner} time]
\label{th:scanner-time}
The time that each call of \op{Scanner} incurs may
be accounted for as follows:
\begin{itemize}
\item
By charging \Oc{} time to \Veim{trial}, when \var{trial} is added to the current ES.
\item
By charging \Oc{} time to
\Veim{trial}, when \var{trial} is a duplicate of an EIM
already in the current ES. \thEnd
\end{itemize}
\end{theorem}

\begin{proof}
By the Procedure Overhead Theorem
\Thref{procedure-overhead-discard},
we may ignore \var{proc} time.
If \var{proc} time is ignored,
this proof then follows immediately from
the \op{Scanner} Time Worksheet
\Tbref{tab:scanner-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:scanner-space}
    Space per line invocation of \op{Scanner}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-2}&0&\cellboxr{%
           \myfn{Transitions}{} Complexity Theorem
           \Thref{transitions-read}.}
\\ \hline
    \ref{line:scanner-op-3}&0&\cellboxr{%
           \realm{ES} \realm{EIM} Traversal Theorem
           \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:scanner-op-4}&0&\cellRule{}
\\ \hline
    \ref{line:scanner-op-5}&0&\cellRule{}
\\ \hline
    \ref{line:scanner-op-6}&0&\cellRule{}
\\ \hline
    \ref{line:scanner-op-7}&0&\cellboxr{%
	  \var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:scanner-op-8}&\Oc{}&\cellboxr{%
	  \realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
    \ref{line:scanner-op-9}&0&\cellRule{}
\\ \hline
    \ref{line:scanner-op-10}&0&\cellboxr{%
	  \var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:scanner-op-11}&0&\cellRule{}
\\ \hline
    \ref{line:scanner-op-12}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:scanner-space}
refer to the \op{Scanner} Algorithm \Aref{alg:scanner-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:scanner-space-worksheet}
    \op{Scanner} space worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellboxc[1.75in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[2.25in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:scanner-op-8}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:scanner-op-12}&\Oc{}&\cellRule{}
\\ \hline
    Totals&\Oc{}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:scanner-space-worksheet}
    refer to the \op{Scanner} Algorithm \Aref{alg:scanner-op}.
    Omitted lines have zero values.
}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Scanner} space]
\label{th:scanner-space}
The space that each call of \op{Scanner} incurs may
be accounted for
by
\begin{itemize}
\item charging \Oc{} space to \Veim{trial}
for calls where \Veim{trial} is a new EIM; and
\item
charging \Oc{} space to \Veim{trial}
for calls where \var{trial} is a duplicate
of an EIM already in the current ES. \thEnd
\end{itemize}
\end{theorem}

\begin{proof}
This theorem follows from
the \op{Scanner} Space Worksheet
\Tbref{tab:scanner-space-worksheet}.
\myqed
\end{proof}

\begin{observation}[Original Earley algorithm: Scanner Operation]
\label{obs:earley-scanner-op}
\Marpa's scanner op has the same effect as the ``Scanner''
operation of \Earley{}.
\obEnd
\end{observation}

\section{Reducer op}
\label{def:reducer}

\FloatBarrier

\begin{algorithm}[H]
\caption{
    \label{alg:earley-reducer-op}
    \var{Earley-reducer}}
\begin{algorithmic}[1]
\Procedure{Earley-reducer}{\VVelement{S}{j},\Veimset{matches},\Veim{cuz}}
\For{every $\Veim{match} \in \Veimset{matches}$}
      \label{line:earley-reducer-op-2}
\State $\Veim{trial} \gets \tuple{
     \begin{gathered}
        \Next{\DR{\var{match}}},
        \\ \Origin{\var{match}}, \Current{\VVelement{S}{j}}
     \end{gathered}
        }
        $
      \label{line:earley-reducer-op-3}
\State $\Vlp{lp} \gets \tuple{\Veim{match},\Veim{cuz}}$
      \label{line:earley-reducer-op-4}
\State $\var{found} \gets \Seen{\Veim{trial}}$
      \label{line:earley-reducer-op-5}
\If{$\var{found} \ne \Lambda$}
      \label{line:earley-reducer-op-6}
    \State Add \var{lp} to \LinkPairs{\Veim{found}}
      \label{line:earley-reducer-op-7}
\Else
      \label{line:earley-reducer-op-8}
    \State $\SetSeen{\Veim{trial}}$
      \label{line:earley-reducer-op-9}
    \State $\LinkPairs{\var{trial}} \gets \set{\var{lp}}$
      \label{line:earley-reducer-op-10}
    \State Add \var{trial} to \VVelement{S}{j}
      \label{line:earley-reducer-op-11}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.3in}
\renewcommand{\cellboxcWidth}{.9in}
\begin{table}[H]
\caption{
    \label{tab:earley-reducer-invocations}
    Line invocations per call of \op{Earley-reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
    \multicolumn{1}{|c|}{Line}
    &\cellboxc{Invocations per call}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:earley-reducer-op-7}&\var{dup}&%
        \cellboxr{This line duplicates an \realm{EIM}
            that already exists in \VVelement{S}{j}.}
\\ \hline
    \ref{line:earley-reducer-op-11}&\var{new}&\cellboxr{%
            This line adds an \realm{EIM} to \VVelement{S}{j}.}
\\ \hline
    \ref{line:earley-reducer-op-10}&\var{new}&\cellboxr{%
       Line \ref{line:earley-reducer-op-9} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-9}&\var{new}&\cellboxr{%
       Line \ref{line:earley-reducer-op-8} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-8}&\var{new}&\cellboxr{%
       Line \ref{line:earley-reducer-op-11} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-6}&
       \var{new}+\var{dup}&\cellboxr{%
           Line \ref{line:earley-reducer-op-7} RSeq or
           line \ref{line:earley-reducer-op-8} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-5}&
       \var{new}+\var{dup}&\cellboxr{%
       Line \ref{line:earley-reducer-op-6} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-4}&
       \var{new}+\var{dup}&\cellboxr{%
       Line \ref{line:earley-reducer-op-5} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-3}&%
       \var{new}+\var{dup}&%
       \cellboxr{Line \ref{line:earley-reducer-op-4} RSeq.}
\\ \hline
    \ref{line:earley-reducer-op-2}&%
        1+\var{new}+\var{dup}&%
        \cellboxr{%
           Line \ref{line:earley-reducer-op-3} RSeq,
           \longDfref{Loop invocation}{loop-invocation},
           \realm{ES} \realm{EIM} Traversal Theorem
           \Thref{es-eim-traversal}.
        }
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:earley-reducer-invocations}
are listed in order of deduction,
and refer to the \op{Earley-Reducer} Algorithm \AFref{alg:earley-reducer-op}.
\var{dup} is the count of passes that adjoin a duplicate \realm{EIM}
to \VVelement{S}{j}.
\var{new} is the count of passes that add a new \realm{EIM}
to \VVelement{S}{j}.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:earley-reducer-time}
    Time per line invocation of \op{Earley-reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:earley-reducer-op-2}&\Oc{}&\cellboxr{%
        \Veimset{match} is a pointer to the head of a linked list,
        as returned by \myfn{Transition}{}
        \Thref{transitions-read}.}
\\ \hline
    \ref{line:earley-reducer-op-3}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:earley-reducer-op-4}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:earley-reducer-op-5}&\Oc{}&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:earley-reducer-op-6}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:earley-reducer-op-7}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
    \ref{line:earley-reducer-op-8}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:earley-reducer-op-9}&\Oc{}&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:earley-reducer-op-10}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:earley-reducer-op-11}&\Oc{}&\cellboxr{%
	\realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:earley-reducer-time}
refer to the \op{Earley-Reducer} Algorithm \Aref{alg:earley-reducer-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-time-worksheet}
    \op{Earley-reducer} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellRule{For every call}}
&\multicolumn{1}{|l|}{\cellboxc[1.25in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[1.75in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:earley-reducer-op-2}&\cellRule{\Oc{}}&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-3}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-4}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-5}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-6}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-7}&&&\cellRule{\Oc{}}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-8}&&\cellRule{\Oc{}}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-9}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    \ref{line:earley-reducer-op-10}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    \ref{line:earley-reducer-op-11}&&\cellRule{\Oc{}}&
\\ \cline{1-4}
    Totals&\cellRule{\Oc{}}&\Oc{}&\Oc{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:earley-reducer-time-worksheet}
refer to the \op{Earley-reducer} Algorithm \Aref{alg:earley-reducer-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Earley-reducer} time]
\label{th:earley-reducer-time}
The time that \op{Earley-reducer} incurs may
be accounted for as follows:
\begin{itemize}
\item \label{item:th-earley-reducer-time-1}
By charging \Oc{} time to
\Veim{trial}, when \var{trial} is added to the current ES.
\item \label{item:th-earley-reducer-time-2}
By charging \Oc{} time to
\Veim{trial}, when \var{trial} is a duplicate of an EIM
already in the current ES. \thEnd
\end{itemize}
\end{theorem}

\begin{proof}
By the Procedure Overhead Theorem
\Thref{procedure-overhead-discard},
we may ignore \var{proc} time.
If \var{proc} time is ignored,
this proof then follows immediately from
the \op{Earley-reducer} Time Worksheet
\Tbref{tab:earley-reducer-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:earley-reducer-space}
    Space per line invocation of \op{Earley-reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:earley-reducer-op-2}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-3}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-4}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-5}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-6}&0&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:earley-reducer-op-7}&\Oc{}&\cellboxr{%
	  \realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
    \ref{line:earley-reducer-op-8}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-9}&0&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:earley-reducer-op-10}&0&\cellRule{}
\\ \hline
    \ref{line:earley-reducer-op-11}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:earley-reducer-space}
refer to the \op{Earley-reducer} Algorithm \Aref{alg:earley-reducer-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-space-worksheet}
    \op{Earley-reducer} space worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellboxc[1.75in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[2.25in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:earley-reducer-op-7}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:earley-reducer-op-11}&\Oc{}&\cellRule{}
\\ \hline
    Totals&\Oc{}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:earley-reducer-space-worksheet}
    refer to the \op{Earley-reducer} Algorithm \Aref{alg:earley-reducer-op}.
    Omitted lines have zero values.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Earley-reducer} space]
\label{th:earley-reducer-space}
The space that \op{Earley-reducer} incurs may
be accounted for
by
charging \Oc{} space to
\Veim{trial}
for every call where \Veim{trial} is a new EIM;
and \Oc{} space to
\Veim{trial}
for every calls where \var{trial} is a duplicate
of an EIM already in the current ES.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \op{Earley-reducer} Space Worksheet
\Tbref{tab:earley-reducer-space-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{LeoReducer}}
\label{alg:leo-reducer-op}
\begin{algorithmic}[1]
\Procedure{LeoReducer}{\VVelement{S}{j},\Vlim{blocker},\Veim{cuz}}
\State $\Veim{trial} \gets \tuple{
     \begin{gathered}
        \Next{\DR{\var{blocker}}},
        \\ \Origin{\var{blocker}}, \Current{\VVelement{S}{j}}
     \end{gathered}
        }$
  \label{line:leo-reducer-op-2}
\State $\Vlp{lp} \gets \tuple{\Vlim{blocker},\Veim{cuz}}$
  \label{line:leo-reducer-op-3}
\State $\var{found} \gets \Seen{\Veim{trial}}$
  \label{line:leo-reducer-op-4}
\If{$\var{found} \ne \Lambda$}
  \label{line:leo-reducer-op-5}
\State Add \var{lp} to \LinkPairs{\Veim{found}}
  \label{line:leo-reducer-op-6}
\Else
  \label{line:leo-reducer-op-7}
    \State $\SetSeen{\Veim{trial}}$
  \label{line:leo-reducer-op-8}
    \State $\LinkPairs{\var{trial}} \gets \set{\var{lp}}$
  \label{line:leo-reducer-op-9}
    \State Add \var{trial} to \VVelement{S}{j}
  \label{line:leo-reducer-op-10}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{definition}[\op{LeoReducer} passes]
\label{def:op-leo-reducer-passes}
A call of \op{LeoReducer} is a \dfn{new pass}
if it adds a new EIM to an ES.
A call of \op{LeoReducer} is a \dfn{dup pass}
if it adjoins a duplicate EIM to an ES.
\dfEnd
\end{definition}

\begin{theorem}[\op{LeoReducer} passes]
\label{th:op-leo-reducer-passes}
Every call of\ifhack{\\}{}
\op{LeoReducer} is either
a new or a dup pass, but never both.
\thEnd
\end{theorem}

\begin{proof}
From examination of \Aref{alg:leo-reducer-op},
we see that
\begin{itemize}
\item Line \ref{line:leo-reducer-op-6}
is the only line at which a duplicate EIM is adjoined to an
ES.
\item Line \ref{line:leo-reducer-op-10}
is the only line at which an EIM is added to an ES.
\item One of lines \ref{line:leo-reducer-op-6}
or \ref{line:leo-reducer-op-10} is always executed,
but never both.  \myqed
\end{itemize}
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{2.75in}
\begin{table}[t]
\caption{
    \label{tab:leo-reducer-invocations}
    Line invocations per call of \op{LeoReducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
    \multicolumn{1}{|c|}{Line}
    &\cellboxc{Invocations per call}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:leo-reducer-op-6}&\var{dup}&%
        \cellboxr{This line duplicates an \realm{EIM}
            that already exists in \VVelement{S}{j}.}
\\ \hline
    \ref{line:leo-reducer-op-10}&\var{new}&\cellboxr{%
            This line adds an \realm{EIM} to \VVelement{S}{j}.}
\\ \hline
    \ref{line:leo-reducer-op-9}&\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-10}, RSeq.}
\\ \hline
    \ref{line:leo-reducer-op-8}&\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-9}, RSeq.}
\\ \hline
    \ref{line:leo-reducer-op-7}&\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-8}, RSeq.}
\\ \hline
    \ref{line:leo-reducer-op-5}&\var{dup}+\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-6} RSeq
        or line \ref{line:leo-reducer-op-7} RSeq.
        }
\\ \hline
    \ref{line:leo-reducer-op-4}&\var{dup}+\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-5}, RSeq.}
\\ \hline
    \ref{line:leo-reducer-op-3}&\var{dup}+\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-4}, RSeq.}
\\ \hline
    \ref{line:leo-reducer-op-2}&\var{dup}+\var{new}&\cellboxr{%
        Line \ref{line:leo-reducer-op-3}, RSeq.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:leo-reducer-invocations}
are listed in order of deduction,
and refer to the \op{Leo-Reducer} Algorithm \AFref{alg:leo-reducer-op}.
\var{dup} is the count of invocations that adjoin a duplicate \realm{EIM}
to \VVelement{S}{j}.
\var{new} is the count of invocations that add a new \realm{EIM}
to \VVelement{S}{j}.
Note that $\var{dup}+\var{new}=1$.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:leo-reducer-time}
    Time per line invocation of \op{LeoReducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:leo-reducer-op-2}&\Oc{}&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-3}&\Oc{}&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-4}&\Oc{}&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:leo-reducer-op-5}&\Oc{}&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-6}&\Oc{}&\cellboxr{%
	  \realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
    \ref{line:leo-reducer-op-7}&\Oc{}&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-8}&\Oc{}&\cellboxr{%
	\var{seen} PSL Complexity Theorem \Thref{seen-psl-access}.}
\\ \hline
    \ref{line:leo-reducer-op-9}&\Oc{}&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-10}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:leo-reducer-time}
refer to the \op{Leo-Reducer} Algorithm \Aref{alg:leo-reducer-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:leo-reducer-time-worksheet}
    \op{LeoReducer} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellboxc[1.5in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[2.5in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:leo-reducer-op-2}&\cellRule{\Oc{}}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-3}&\cellRule{\Oc{}}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-4}&\cellRule{\Oc{}}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-5}&\cellRule{\Oc{}}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-6}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:leo-reducer-op-7}&\cellRule{\Oc{}}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-8}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:leo-reducer-op-9}&\cellRule{\Oc{}}&
\\ \hline
    \ref{line:leo-reducer-op-10}&\cellRule{\Oc{}}&
\\ \hline
    Totals&\cellRule{\Oc{}}&\Oc{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:leo-reducer-time-worksheet}
refer to the \op{LeoReducer} Algorithm \Aref{alg:leo-reducer-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{LeoReducer} time]
\label{th:leo-reducer-time}
\ifhack{\raggedright}{}
The time that \op{LeoReducer} incurs may
be accounted for as follows:
\begin{itemize}
\item \label{item:th-leo-reducer-time-1}
By charging \Oc{} time to
\Veim{trial}, when \var{trial} is added to the current ES.
\item \label{item:th-leo-reducer-time-2}
By charging \Oc{} time to
\Veim{trial}, when \var{trial} is a duplicate of an EIM
already in the current ES.
\thEnd
\end{itemize}
\end{theorem}

\begin{proof}
This theorem then follows from
the definition
\Dfref{op-leo-reducer-passes}
and theorem
\Thref{op-leo-reducer-passes}
for \op{LeoReducer} passes,
and the \op{LeoReducer} time worksheet
\Tbref{tab:leo-reducer-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:leo-reducer-space}
    Space per line invocation of \op{LeoReducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:leo-reducer-op-2}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-3}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-4}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-5}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-6}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-7}&\Oc{}&\cellboxr{%
	  \realm{ES} Link Pair Add Theorem \Thref{es-link-pair-add}.}
\\ \hline
    \ref{line:leo-reducer-op-8}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-9}&&\cellRule{}
\\ \hline
    \ref{line:leo-reducer-op-10}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:leo-reducer-space}
refer to the \op{LeoReducer} Algorithm \Aref{alg:leo-reducer-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:leo-reducer-space-worksheet}
    \op{LeoReducer} space worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|l|}{\cellboxc[1.75in]{%
    For every call where
    \Veim{trial} is a new \realm{EIM}}}
&\multicolumn{1}{|l|}{\cellboxc[2.25in]{%
    For every call that
    where \Veim{trial}
    duplicates an existing \realm{EIM}}}
\\ \hline
    \ref{line:leo-reducer-op-6}&&\cellRule{\Oc{}}
\\ \hline
    \ref{line:leo-reducer-op-10}&\Oc{}&\cellRule{}
\\ \hline
    Totals&\Oc{}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:leo-reducer-space-worksheet}
    refer to the \op{LeoReducer} Algorithm \Aref{alg:leo-reducer-op}.
    Omitted lines have zero values.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{LeoReducer} space]
\label{th:leo-reducer-space}
The space that \op{LeoReducer} incurs may
be accounted for
by
charging \Oc{} space to \Veim{trial}
for calls where \Veim{trial} is a new EIM; and
charging \Oc{} space to \Veim{trial}
for calls where \var{trial} is a duplicate
of an EIM already in the current ES.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \op{LeoReducer} Space Worksheet
\Tbref{tab:leo-reducer-space-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{Reducer}}
\label{alg:reducer-op}
\begin{algorithmic}[1]
\Procedure{Reducer}{\VVelement{S}{j}}
\For{$\Veim{cuz} \in \VVelement{S}{j}$ where \var{cuz} is a completion}
  \label{line:reducer-op-2}
\State $\Veimset{matches} \gets \transitions{\Origin{\var{x}}}{\LHS{\Veim{cuz}}}$
  \label{line:reducer-op-3}
\State $\var{match0} \gets \Velement{matches}{0}$
  \label{line:reducer-op-4}
\If{$\var{match0} = \Lambda$}
  \label{line:reducer-op-5}
  \State Do nothing
      \label{line:reducer-op-6}
\ElsIf{\var{match0} is a \realm{LIM}}
  \label{line:reducer-op-7}
    \State \Call{LeoReducer}{\VVelement{S}{j},\Vlim{match0},\Veim{cuz}}
      \label{line:reducer-op-8}
\Else
  \label{line:reducer-op-9}
    \State \Call{Earley-reducer}{\VVelement{S}{j},\Veimset{matches},\Veim{cuz}}
      \label{line:reducer-op-10}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\todo{Show that match0 is a LIM iff there is one in transitions}

\FloatBarrier

\renewcommand{\cellboxrWidth}{2.75in}
\begin{table}[H]
\caption{
    \label{tab:reducer-invocations}
    Line invocations per call of \op{Reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:reducer-op-2}&\order{1+\var{eim}}&\cellboxr{%
        Line \ref{line:reducer-op-2} Seq.;
        \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:reducer-op-3}&\order{\var{eim}}&\cellboxr{%
       Line \ref{line:reducer-op-2} Seq.;
        \Dfref{loop-invocation}.}
\\ \hline
    \ref{line:reducer-op-4}&\order{\var{eim}}&\cellboxr{%
       Line \ref{line:reducer-op-3} Seq.}
\\ \hline
    \ref{line:reducer-op-5}&\order{\var{eim}}&\cellboxr{%
       Line \ref{line:reducer-op-4} Seq.}
\\ \hline
    \ref{line:reducer-op-6}&\order{\var{eim}}&\cellboxr{%
        Line \ref{line:reducer-op-5} Cond.}
\\ \hline
    \ref{line:reducer-op-7}&\order{\var{eim}}&\cellboxr{%
       Line \ref{line:reducer-op-5} Seq.}
\\ \hline
    \ref{line:reducer-op-8}&\order{\var{eim}}&\cellboxr{%
        Line \ref{line:reducer-op-7} Cond.}
\\ \hline
    \ref{line:reducer-op-9}&\order{\var{eim}}&\cellboxr{%
       Line \ref{line:reducer-op-7} Seq.}
\\ \hline
    \ref{line:reducer-op-10}&\order{\var{eim}}&\cellboxr{%
        Line \ref{line:reducer-op-9} Cond.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:reducer-invocations}
are listed in order of deduction,
and refer to the \op{Reducer} Algorithm \AFref{alg:reducer-op}.
\var{eim} is the number of \realm{EIM}s in
\VVelement{S}{j}, that is,
$\var{eim} = \size{\set{\var{x}\in\realm{EIM}:\var{x}\in \VVelement{S}{j}}}$.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:reducer-time}
    Time per line invocation of \op{Reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:reducer-op-2}&\Oc{}&\cellboxr{%
        \realm{ES} \realm{EIM} Traversal Theorem
        \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:reducer-op-3}&\Oc{}&\cellboxr{%
        \myfn{Transition}{} Read Theorem
        \Thref{transitions-read}.}
\\ \hline
    \ref{line:reducer-op-4}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-5}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-6}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-7}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-8}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-9}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:reducer-op-10}&\Oc{}&\cellboxr{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:reducer-time}
refer to the \op{Reducer} Algorithm \Aref{alg:reducer-op}.}
\end{table}

\begin{theorem}[\op{Reducer} Time]
\label{th:reducer-time}
The time that each \op{Reducer} op incurs
may be accounted for by charging
\Oc{} time
to each \realm{EIM}
in \VVelement{S}{j} of \Aref{alg:reducer-op}.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-reducer-time-100}{%
    The time that each \op{Reducer} op incurs
    can be charged as \Oc{} time to the \var{proc} account
    of \op{Reducer} op;
    and \Oc{} time to
    each \realm{PIM} in in \VVelement{S}{j}
    \bcuz{} \Tbref{tab:reducer-time},
        \Tbref{tab:reducer-invocations}.
}
The theorem follows from
    the Procedure Overhead Discard Theorem \Thref{procedure-overhead-discard}.
and \Eref{th-reducer-time-100}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:reducer-space}
    Space per line invocation of \op{Reducer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:reducer-op-2}&0&\cellboxr{%
        \realm{ES} \realm{EIM} Traversal Theorem
        \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:reducer-op-3}&0&\cellboxr{%
        \myfn{Transition}{} Read Theorem
        \Thref{transitions-read}.}
\\ \hline
    \ref{line:reducer-op-4}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-5}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-6}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-7}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-8}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-9}&0&\cellRule{}
\\ \hline
    \ref{line:reducer-op-10}&0&\cellRule{}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:reducer-space}
refer to the \op{Reducer} Algorithm \Aref{alg:reducer-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Reducer} Space]
\label{th:reducer-space}
The \op{Reducer} incurs no space.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows immediately from
\Tbref{tab:reducer-space}.
\myqed
\end{proof}

\begin{theorem}[Reducer Ancestry Tracking]
\label{th:reducer-ancestry-tracking}
    The \Marpa implementation records a link pair for every
    adjoin of an \realm{EIM} generated by \op{Reducer}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
lines \ref{line:earley-reducer-op-4},
line \ref{line:earley-reducer-op-8}, and
line \ref{line:earley-reducer-op-11} of
\Aref{alg:earley-reducer-op};
and from lines \ref{line:leo-reducer-op-3},
line \ref{line:leo-reducer-op-7}, and
line \ref{line:leo-reducer-op-10} of
\Aref{alg:leo-reducer-op}.
\myqed
\end{proof}

\begin{theorem}[Reducer Ancestry Cause]
\label{th:reducer-ancestry-cause}
The cause of the link pair for an
adjoin of an \realm{EIM} generated by \op{Reducer}
will always be a completed \realm{EIM}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from lines \ref{line:reducer-op-2}, \ref{line:reducer-op-8} and
\ref{line:reducer-op-10} of \Aref{alg:reducer-op};
line \ref{line:leo-reducer-op-3} of
\Aref{alg:leo-reducer-op};
and line \ref{line:earley-reducer-op-4} of
\Aref{alg:earley-reducer-op}.
\myqed
\end{proof}

\begin{theorem}[Reducer Ancestry EIM Predecessor]
\label{th:reducer-ancestry-eim-predecessor}
The predecessor of the link pair for an
adjoin of an \realm{EIM} generated by \op{Reducer}
will always be an incomplete \realm{EIM}.
\thEnd
\end{theorem}

\begin{proof}
\Veimset{matches} in
line \ref{line:reducer-op-3} of \Aref{alg:reducer-op}
is a list of either \realm{LIM}s or incomplete \realm{EIM}s
\Thref{transitions-pim-sequence}.
At line \ref{line:reducer-op-10} of \Aref{alg:reducer-op},
\var{matches} is a non-empty list of incomplete \realm{EIM}s,
because of lines
\ref{line:reducer-op-5} and \ref{line:reducer-op-7}
of \Aref{alg:reducer-op}
and \Sref{transition-array}.
This theorem then follows from
\Aref{alg:earley-reducer-op}, especially
lines \ref{line:earley-reducer-op-2},
\ref{line:earley-reducer-op-4},
\ref{line:earley-reducer-op-8},
and \ref{line:earley-reducer-op-11}.
\myqed
\end{proof}

\begin{theorem}[Reducer Ancestry LIM Predecessor]
\label{th:reducer-ancestry-lim-predecessor}
The predecessor of the link pair for an
adjoin of an \realm{LIM} generated by \op{Reducer}
will be either a \realm{LIM},
or $\Lambda$.
\thEnd
\end{theorem}

\begin{proof}
\Veimset{matches} in
line \ref{line:reducer-op-3} of \Aref{alg:reducer-op}
is a list of either \realm{LIM}s or incomplete \realm{EIM}s
\Thref{transitions-pim-sequence}.
At line \ref{line:reducer-op-8} of \Aref{alg:reducer-op},
\var{match0} is a LIM
because of line \ref{line:reducer-op-7}
of \Aref{alg:reducer-op}.
This theorem then follows from
\Aref{alg:leo-reducer-op}, especially
lines \ref{line:leo-reducer-op-2},
\ref{line:leo-reducer-op-3},
\ref{line:leo-reducer-op-7},
and \ref{line:leo-reducer-op-10}.
\myqed
\end{proof}

\begin{observation}[Original Earley algorithm: Completer Operation]
\label{obs:earley-completer-op}
When there are no \realm{LIM}s in \Ves{predecessor}
\Marpa's \var{Reducer} op has the same effect as the ``Completer''
operation of \Earley{}~\cite{Earley1970}.\footnote{
For all other \Marpa ops we have used the name of its
equivalent in \Earley{}.
It is convenient to use a different name
for \Marpa's \var{Reducer} op,
however,
because it differs in effect from the \Earley{}
``Completer'' operation,
and because the cognates of ``complete'' are
overloaded in confusing ways.
For example, the result of an \Earley{} ``Completer'' operation
is not necessarily an \realm{EIM} completion.
}
Put another way, \Marpa's \var{Reducer} op
is the same as the Completer operation of \Earley{},
if there never is a \realm{LIM} in \Vpimset{matches} on
line \ref{line:reducer-op-3} of \Aref{alg:reducer-op}.
In this case,
we never execute \op{LeoReducer}.
\obEnd
\end{observation}

\begin{observation}[Summary of \Earley{} Completer Operation]
\label{obs:earley-completer-summary}
In what follows,
we will find the following summary of the original, \Earley{} completer
operation useful.
If
\begin{equation}
\label{eq:obs-earley-completer-summary-1}
\myparbox{$\Veim{cause}@\Vloc{current}$ is a completion,} \\
\end{equation}
\begin{equation}
\label{eq:obs-earley-completer-summary-2}
\myparbox{$\var{pred} @ (\Origin{\var{cause}})$ is an incompletion, and} \\
\end{equation}
\begin{equation}
\label{eq:obs-earley-completer-summary-3}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{cause}}$,}
\end{equation}
then we adjoin
\begin{equation}
\label{eq:obs-earley-completer-summary-4}
\myparbox{%
$\left[ \Next{\DR{\var{pred}}}, \Origin{\var{pred}} \right]@\Vloc{current}$
}
\end{equation}
into the Earley table with the link pair
\begin{equation}
\label{eq:obs-earley-completer-summary-5}
[ \var{pred}, \var{cause} ]. \quad \obEnd
\end{equation}
\end{observation}

\FloatBarrier

\section{Memoizer op}
\label{def:memoizer}

\begin{algorithm}[H]
\caption{
    \label{alg:memoizer-op}
    \op{Memoizer}}
\begin{algorithmic}[1]
\Procedure{Memoizer}{\VVelement{S}{j}}
  \label{line:memoizer-op-1}
\For{$\Veim{source} \in \LeoEligible{\VVelement{S}{j}}$}
  \label{line:memoizer-op-2}
\State $\Veim{pred} = \LIMPredecessor{\var{source}}$
  \label{line:memoizer-op-3}
\State $\Vlp{lp} = \tuple{\var{pred},\var{source}}$
  \label{line:memoizer-op-4}
\If{$\var{pred} \neq \Lambda$}
  \label{line:memoizer-op-5}
  \State $\Vlim{lim} = [\DR{\var{pred}}, \Postdot{\var{source}},$
  \Statex \hspace\algorithmicindent \hspace\algorithmicindent \hspace\algorithmicindent
          $\qquad \qquad \qquad \Origin{\var{pred}}, \Current{\var{source}} ]$
      \label{line:memoizer-op-6}
\Else
  \label{line:memoizer-op-7}
  \State $ \Vlim{lim} = [\DR{\var{source}}, \Postdot{\var{source}},$
  \Statex \hspace\algorithmicindent \hspace\algorithmicindent \hspace\algorithmicindent
          $\qquad \qquad \qquad \Origin{\var{source}}, \Current{\var{source}} ]$
      \label{line:memoizer-op-8}
\EndIf
    \State $\LinkPairs{\var{lim}} = \set{\var{lp}}$
  \label{line:memoizer-op-9}
    \State Add \Vlim{lim} to \VVelement{S}{j}
      \label{line:memoizer-op-10}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{theorem}[Leo Eligible Traversal]
\label{th:leo-eligible-traversal}
Traversal of the Leo eligible items in \var{eset}
can be charged as \Oc{} time and zero space for each \realm{EIM}
in \var{eset},
plus \Oc{} time and zero space for the final test.
\thEnd
\end{theorem}

\begin{proof}
The proof strategy to state a conceptual algorithm
which works in the required bounds.
Our conceptual algorithm has two passes.

In the first pass, we traverse \Ves{eset},
marking two bitmaps indexed by symbol ID.
The first bitmap is a one-bit counter.
The second bitmap tracks overflow of the one-bit counter.
Both bitmaps are indexed by symbol ID,
and are cleared initially.
During the traversal,
for each \realm{EIM} \var{eim},
we execute the following steps, in order:
\begin{itemize}
\item Let $\Vsym{trans} = \Transition{\var{eim}}$.
\item If \Vsym{trans} is not right recursive, we ignore \var{eim}.
\item If the bit for \Vsym{trans} is unset in the counter bitmap,
we set the bit for \Vsym{trans} in the counter bitmap.
\item If the bit for \Vsym{trans} is set in the counter bitmap,
we set the bit for \Vsym{trans} in the overflow bitmap.
\end{itemize}
Execution of the steps just described
incurs \Oc{} time per \realm{EIM}.

In the second pass, we again traverse \Ves{eset}.
For each \realm{EIM} \var{eim},
we execute the following steps, in order:
\begin{itemize}
\item Let $\Vsym{trans} = \Transition{\var{eim}}$.
\item If the bit for \Vsym{trans} is unset in the counter bitmap,
    we ignore \var{eim}.
\item If the bit for \Vsym{trans} is set in the overflow bitmap,
    we ignore that \var{eim}.
\item Any \var{eim} which remains is Leo eligible,
   and is processed accordingly.
\end{itemize}
Execution of the steps just described
incurs \Oc{} time per \realm{EIM}.

By the \realm{ES} \realm{EIM} Traversal Theorem
each traversal of \var{eset} incurs \Oc{} time
per \realm{EIM}
\Thref{es-eim-traversal}.
Adding the per-\realm{EIM} time for the first and second pass traversals
themselves
to the time for the steps itemized above in this proof for the first and second pass,
we see that
\mypareq{eq:th-leo-eligible-traversal-300}{%
    traversal of the Leo eligible items in \var{eset}
    incurs $\Oc{}+ \Oc{}+ \Oc{}+ \Oc{} = \Oc{}$ time per \realm{EIM}.
}

By the \realm{ES} Traversal Theorem
the two traversals each require zero space.
The operations use per-symbol two bitmaps which,
since the number of symbols in \Vint{g} is a constant,
may be kept on the stack, so that they incur zero additional
space.
Totaling the space requirements, we see that
\mypareq{eq:th-leo-eligible-traversal-350}{%
    traversal of the Leo eligible items in \var{eset}
    incurs zero space per \realm{EIM}.
}
This theorem is
\Eref{th-leo-eligible-traversal-300} and
\Eref{th-leo-eligible-traversal-350}.
\myqed
\end{proof}

As an aside, an implementation
using the technique described
in the proof of \Thref{leo-eligible-traversal}
might find it more
convenient to store the bitmaps with the data structure for
the parse, clearing it every time an Earley set is created.
The same resource bounds are achieved,
although the complexity analysis is slightly more complicated.

\Rtwo{} does not traverse the Leo eligible \realm{PIM}s
using the technique in the proof of \Thref{leo-eligible-traversal}.
\Rtwo{} uses an technique which interweaves the
\op{Bookkeeper} and
\op{Memoizer} ops.
This technique is faster but,
again, slightly harder to analyze for complexity.

\Rtwo{} first creates the \var{transition} array for \realm{EIM}s
only.
\Rtwo{} then traverses this partially populated
\var{transition} array, looking for entries whose index
symbol is right recursive and which have only
one \realm{EIM} in the linked list.
These \realm{EIM}s will be all and only the Leo Eligible \realm{EIM}s.

For each Leo eligible \realm{EIM} \var{src} that \Rtwo{} finds,
it creates a \realm{LIM}, using \var{src} as the source \realm{EIM}.
\Rtwo{} puts the \realm{LIM} at the head of the linked list containing
\var{src}.
Since there is only one \realm{LIM} per source \realm{EIM},
and since \Rtwo{} has already fully populated the \realm{EIM}s
in \var{transition}'s linked lists,
\label{page:transition-lim-first}
this \realm{LIM} will always be at the head of its linked list.

\FloatBarrier

\renewcommand{\cellboxrWidth}{3in}
\begin{table}[H]
\caption{
    \label{tab:memoizer-invocations}
    Line invocations per call of \op{Memoizer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
    \multicolumn{1}{|c|}{Line}
    &\cellboxc{Invocations per call}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:memoizer-op-2}&$1+\var{eim}$&\cellboxr{%
            Line \ref{line:memoizer-op-1} Seq;
            Leo Eligible Traversal Theorem \Thref{leo-eligible-traversal};
            \longDfref{Loop invocation}{loop-invocation}.}
\\ \hline
    \ref{line:memoizer-op-3}&\Oc{}&\cellboxr{%
           Line \ref{line:memoizer-op-2} Seq.;
           and because, by the Leo Eligible \realm{EIM}
           Count Theorem
           \Thref{leo-eligible-eim-count},
           there are \Oc{} Leo eligible \realm{EIM}s
           in \VVelement{S}{j}.}
\\ \hline
    \ref{line:memoizer-op-4}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-3} Seq.}
\\ \hline
    \ref{line:memoizer-op-5}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-4} Seq.}
\\ \hline
    \ref{line:memoizer-op-6}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-5} Cond.}
\\ \hline
    \ref{line:memoizer-op-7}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-5} Seq.}
\\ \hline
    \ref{line:memoizer-op-8}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-7} Cond.}
\\ \hline
    \ref{line:memoizer-op-9}&\Oc{}&\cellboxr{%
       Line \ref{line:memoizer-op-7} Seq.}
\\ \hline
    \ref{line:memoizer-op-10}&\Oc{}&\cellboxr{%
        Line \ref{line:memoizer-op-9} Seq.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:memoizer-invocations}
are listed in order of deduction
and refer to the \op{Memoizer} Algorithm \Aref{alg:memoizer-op}.
\var{eim} is the count of \realm{EIM}s in
\VVelement{S}{j}, that is,
$\var{eim} = \size{\set{\var{x}\in\realm{EIM}:\var{x}\in \VVelement{S}{j}}}$.}
\end{table}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:memoizer-time}
    Time per line invocation of \op{Memoizer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Time}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:memoizer-op-2}&\Oc{}&\cellboxr{%
            \realm{ES} Leo Eligible Traversal Theorem
            \Thref{leo-eligible-traversal},
            \realm{ES} \realm{EIM} Traversal Theorem
            \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:memoizer-op-3}&\Oc{}&\cellboxr{%
            \LIMPredecessor{} Complexity Theorem
            \Thref{lim-predecessor-complexity}.}
\\ \hline
    \ref{line:memoizer-op-4}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:memoizer-op-5}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:memoizer-op-6}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:memoizer-op-7}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:memoizer-op-8}&\Oc{}&\cellboxr{}
\\ \hline
    \ref{line:memoizer-op-9}&\Oc{}&\cellboxr{%
        \realm{PIM} Link Pair Write Complexity Theorem \Thref{pim-link-pair-write}.}
\\ \hline
    \ref{line:memoizer-op-10}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:memoizer-time}
refer to the \op{Memoizer} Algorithm \Aref{alg:memoizer-op}.}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:memoizer-time-worksheet}
    \op{Memoizer} time worksheet}
\vspace{1ex}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Line}
&\multicolumn{1}{|c|}{\cellRule{For every call}}
&\multicolumn{1}{|c|}{
    {Per $\realm{EIM} \in \VVelement{S}{j}$ per call}}
\\ \hline
    \ref{line:memoizer-op-2}&$1\times\Oc{}$&%
        \cellRule{$1\times\Oc{}$}
\\ \hline
    \ref{line:memoizer-op-3}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-4}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-5}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-6}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-7}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-8}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-9}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-10}&$\Oc{}\times\Oc{}$&\cellRule{}
\\ \hline
    Totals&\Oc{}&\cellRule{\Oc{}}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:memoizer-time-worksheet}
    refer to
    the \op{Memoizer} Algorithm \Aref{alg:memoizer-op}.
    Cell entries are calculated as the number of invocations,
    from the
    \op{Memoizer} Line Invocations Table \Tbref{tab:memoizer-invocations},
    multiplied by time incurred for each invocation,
    from the
    \op{Memoizer} Time per Line Invocation Table \Tbref{tab:memoizer-time}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Memoizer} Time Complexity]
\label{th:memoizer-time}
The time that each \op{Memoizer} op incurs
can be accounted for by charging
\Oc{} time
to each \realm{EIM}
in \VVelement{S}{j} of \Aref{alg:bookkeeper-op}.
\thEnd
\end{theorem}

\begin{proof}
\mypareq{eq:th-memoizer-time-100}{%
    The time that each \op{Memoizer} op incurs
    can be charged as \Oc{} time to the
    call of \op{Memoizer} op;
    and \Oc{} time to
    each \realm{EIM} in in \VVelement{S}{j}
    \bcuz{} \Tbref{tab:bookkeeper-time-worksheet}.
}
The theorem follows immediately from
\Eref{th-memoizer-time-100}
and the Procedure Overhead Discard Theorem \Thref{procedure-overhead-discard}.
\myqed
\end{proof}

\FloatBarrier

\renewcommand{\cellboxrWidth}{3.5in}
\begin{table}[H]
\caption{
    \label{tab:memoizer-space}
    Space per line invocation of \op{Memoizer}}
\vspace{1ex}
\begin{tabular}{|r|c|l|}
\hline
\multicolumn{1}{|c|}{\cellRule{Line}}
    &\multicolumn{1}{c|}{Space}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:memoizer-op-2}&0&\cellboxr{%
            \realm{ES} Leo Eligible Traversal Theorem
            \Thref{leo-eligible-traversal},
            \realm{ES} \realm{EIM} Traversal Theorem
            \Thref{es-eim-traversal}.}
\\ \hline
    \ref{line:memoizer-op-3}&0&\cellboxr{%
            \LIMPredecessor{} Complexity Theorem
            \Thref{lim-predecessor-complexity}.}
\\ \hline
    \ref{line:memoizer-op-4}&0&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-5}&0&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-6}&0&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-7}&0&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-8}&0&\cellRule{}
\\ \hline
    \ref{line:memoizer-op-9}&0&\cellboxr{%
            \realm{PIM} Link Pair Write Complexity Theorem \Thref{pim-link-pair-write}.}
\\ \hline
    \ref{line:memoizer-op-10}&\Oc{}&\cellboxr{%
        \realm{ES} Add Theorem \Thref{es-add}.}
\\ \hline
\end{tabular}
\mylegend{Lines in \Tbref{tab:memoizer-space}
    refer to the \op{Memoizer} Algorithm \Aref{alg:memoizer-op}.}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Memoizer} space]
\label{th:memoizer-space}
The space that each \op{Memoizer} op incurs may
be accounted for by
charging \Oc{} space to an arbitrary \realm{EIM}.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \op{Memoizer} Invocations Table \Tbref{tab:memoizer-invocations}
and the \op{Memoizer} Space per Invocation Table \Tbref{tab:memoizer-space}.
\myqed
\end{proof}

\begin{theorem}[\PreviousLink{} function]
\label{th:previous-link}
The total function
\label{def-previous-link}
\[
   \PreviousLink{\ell} \defined
      \LIMPredecessor{ \Source{\mylim{\ell}}}.
\]
exists.
We say that
\PreviousLink{\ell} is
the \dfn{previous link} of $\ell$.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \LIMPredecessor{} Theorem
\Thref{lim-predecessor-fn}
and
the \Source{} Theorem \Thref{leo-source-fn}.
\myqed
\end{proof}

\begin{observation}[\realm{LIM} Ancestry]
\label{obs:lim-ancestry}
The \Marpa implementation tracks the ancestry
of \realm{LIM}s.
The ancestry of a \realm{LIM} is a single link pair.
In the link pair for a \realm{LIM} $\ell$,
the ``cause'' is \Source{\ell}
and the ``predecessor'' is \PreviousLink{\ell}.
\obEnd
\end{observation}

\begin{theorem}[\realm{LIM} Link Pair Uniqueness]
The ancestry of
every \realm{LIM} contains exactly one
link pair.
\thEnd
\end{theorem}

\begin{proof}
This theorem follows from
the \Source{} Theorem \Thref{leo-source-fn}
the \PreviousLink{} Theorem \Thref{previous-link}
and the observation on \realm{LIM} ancestry \Obref{lim-ancestry}.
\myqed
\end{proof}

\begin{observation}[Original Earley algorithm: Memorizer Operation]
\label{obs:earley-memoizer-op}
\Marpa's \var{Memoizer} operation does not exist in \Earley{}~\cite{Earley1970}.
\obEnd{}
\end{observation}

\section{Top level of \Marpa}

\todo{Rewrite \Marpa top level section}

The top-level view of \Marpa is
shown in \Aref{alg:marpa}.

\FloatBarrier

\begin{algorithm}[H]
\caption{The \Marpa Algorithm}
\label{alg:marpa}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State $\Call{ES-Setup}{0}$
\State $\Call{Initializer}{{}}$
\State \Call{Predicter}{\Ves{(0)}}
\State $\Call{Bookkeeper}{\Velement{S}{0}}$
\For{ \Vnat{j} from 1 to \Vsize{w}, inclusive }
\State $\Call{ES-Setup}{\var{j}}$
\State $\Call{Scanner}{\VVelement{S}{j}}$
\If{$\size{\VVelement{S}{j}} = 0$}
\State return \Comment Reject \Cw{}
\EndIf
\State $\Call{Reducer}{\VVelement{S}{j}}$
\State $\Call{Predicter}{\VVelement{S}{j}}$
\State $\Call{Memoizer}{\VVelement{S}{j}}$
\label{line:marpa-line-memoizer}
\State $\Call{Bookkeeper}{\VVelement{S}{j}}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{observation}[Original Earley Algorithm: Top level]
If line \ref{line:marpa-line-memoizer},
calling the \op{Memoizer} op,
is removed,
\Aref{alg:marpa} has the same effect as \Earley{}~\cite{Earley1970}.
This can be seen from
\Aref{alg:marpa},
\Obref{earley-set-0},
\Obref{earley-scanner-op},
\Obref{earley-completer-op},
\Obref{earley-predicter-op},
and
\Obref{earley-memoizer-op}.
\obEnd
\end{observation}

\chapter{Low-level complexity summary}
\label{chap:low-level-complexity-summary}

\todo[prepend, caption={``Low-level complexity summary'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

We divide space into two kinds:
\dfn{trimmed space} and \dfn{link space}.

All space, except for links, is charged to trimmed space.
Link space may be charged either
to trimmed space or to link space.
This is a free choice
and therefore is not justified in proofs.
However, while free, the choice between link and trimmed
space has consequences
for later theorems,
and cannot be made injudiciously.

Sometimes it will be convenient to defer the decision
to charge a link pair to trimmed or link space.
Space which is yet to be assigned to trimmed or link space
is called \dfn{unassigned} space.

We use the distinction between link
and trimmed space to allow us to report
two kinds of space complexity results.
We want to report space complexity results
which apply to practical applications,
but we also want to report results which
allow ``apples to apples'' comparisons
with the space complexity claims
in prior literature.

It is a rare practical parsing application
that does not require an evaluation phase of
some kind, and
efficient evaluation is not possible without link
pairs.
Therefore, to be of practical significance, we
need to report space complexity results which
include all link space.
On the other hand, most of the Earley literature
reports results in terms of trimmed space,
and to allow accurate comparisons,
we need to be able to ignore at least some
of the space consumed by link pairs.
This is why we track some of the space consumed
by link pairs separately.

\chapter{FRAGMENTS}
\label{chap:FRAGMENTS}

\todo[prepend, caption={``FRAGMENTS'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Eliminate this chapter, by moving or deleting its contents}

This chapter consists of ``fragments'' intended to be moved elsewhere,
possibly after rewriting, or deleted.

\section{Memoizer fragments}

\begin{theorem}[Memoizer op consistency]
\label{th:memoizer-op-consistency}
If
\[
\forall\; 0 \le \Vnat{i} \le \Vnat{j} : \Valid{\VVelement{S}{i}},
\]
then the Memoizer op attempts to add only valid \realm{LIM}s,
and only to \VVelement{S}{i}.
\end{theorem}

\begin{proof}
Inspection of
Algorithm \Aref{alg:memoizer-op} shows
\begin{itemize}
\item that the Memoizer op
only attempts to add a \realm{LIM}
at line \ref{line:memoizer-op-9};
\item
that line \ref{line:memoizer-op-9} only attempts to add
\realm{LIM}s to \VVelement{S}{i}; and
\item
that \Aref{alg:memoizer-op}
enforces the requirements laid out in
the definition of \realm{LIM} validity \Dfref{lim-validity},
before the attempt by \Aref{alg:memoizer-op}
to add a \realm{LIM}
at line \ref{line:memoizer-op-9}.
\myqed
\end{itemize}
\end{proof}

\begin{theorem}[Memoizer op non-duplication]
\label{th:memoizer-op-non-duplication}
Let $\ell$ be a valid \realm{LIM}.
Every call of the Memoizer op attempts to add $\ell$
at most once.
\end{theorem}

\begin{proof}
Let $ell$ be an arbitrary valid LIM,
and let
\mypareq{eq:th-memoizer-op-non-duplication-100}{%
    \Veim{x} = \Source{\ell}.
}
By the Leo source bijection theorem \Thref{source-fn-injection},
we know that there is exactly one such \var{x}.
From line \ref{line:memoizer-op-3}
of \Aref{alg:memoizer-op},
we see that \Veim{source} is set to \var{x} in at most one
pass of the main loop of \Aref{alg:memoizer-op}.
Lines \ref{line:memoizer-op-3}--\ref{line:memoizer-op-8}
enforce \Eref{th-memoizer-op-non-duplication-100},
so that the pass
such that
$\Veim{source} = \var{x}$
is the only pass of the main
loop which could attempt to add $\ell$.
\myqed
\end{proof}

\begin{theorem}[Memoizer op maximum]
\label{th:memoizer-op-maximum}
If
\[
\forall\; 0 \le \Vnat{i} \le \Vnat{j} : \Valid{\VVelement{S}{i}},
\]
then each call of the Memoizer op
executes
line \ref{line:memoizer-op-9}
of \Aref{alg:memoizer-op}
at most
\size{\Vocab{\var{g}}} times.
\end{theorem}

\begin{proof}
\mypareq{eq:th-memoizer-op-maximum-100}{%
    There are at most \size{\Vocab{\var{g}}} valid \realm{LIM}s
    in \VVelement{S}{j}
    \linebreak[1]
    \cuz{} \Thref{es-lim-count}.
}
\mypareq{eq:th-memoizer-op-maximum-110}{%
    Every attempt to add a \realm{LIM} in a call of Memoizer op is
    an attempt to add a valid \realm{LIM} to \VVelement{S}{j}
    \cuz{} \Thref{memoizer-op-consistency}.
}
\mypareq{eq:th-memoizer-op-maximum-115}{%
    Where $\ell$ is a LIM,
    a call of the Memoizer op
    attempts to add $\ell$ at most once
    \cuz{} \Thref{memoizer-op-non-duplication}.
}
\mypareq{eq:th-memoizer-op-maximum-120}{%
    There are at most \size{\Vocab{\var{g}}}
    attempts to add a \realm{LIM} in any call of Memoizer op
    \cuz{} \Eref{th-memoizer-op-maximum-100},
        \Eref{th-memoizer-op-maximum-110},
        \Eref{th-memoizer-op-maximum-115}.
}
\mypareq{eq:th-memoizer-op-maximum-130}{%
    Each execution of
    line \ref{line:memoizer-op-9}
    is an attempt to
    add a \realm{LIM}
    \linebreak[1]
    \cuz{} \Aref{alg:memoizer-op}.
}
This theorem follows from
\Eref{th-memoizer-op-maximum-120}
and \Eref{th-memoizer-op-maximum-130}.
\myqed
\end{proof}

\section{Summary}

\todo{Rewrite operations summary}

\begin{observation}[\Marpa operation by type of result]
\label{obs:marpa-op-by-result}
Let \Vpim{rez} be the result of a \Marpa operation.
Every \realm{PIM} is the result of exactly \Marpa operation,
as follows:
\begin{itemize}
\item $\Current{\Vpim{rez}} = 0$
iff \Vpim{rez} is the result of the
\Marpa \var{Initializer} operation.
\item \Vpim{rez} is an \realm{EIM},
$\Predot{\Veim{rez}} = \Lambda$, and
$\Current{\var{rez}} > 0$,
iff
\var{rez} is the result of a
\Marpa \var{Predicter} operation.
\item \Vpim{rez} is an \realm{EIM},
$\Predot{\Veim{rez}} \in \Term{\var{g}}$, and
$\Current{\var{rez}} > 0$,
iff
\var{rez} is the result of a
\Marpa \var{Scanner} operation.
\item \Vpim{rez} is an \realm{EIM},
$\Predot{\Veim{rez}} \in \NT{\var{g}}$, and
$\Current{\var{rez}} > 0$
iff \var{rez} is the result of a
\Marpa \var{Reducer} operation.
\item \Vpim{rez} is a \realm{LIM}
iff
\var{rez} is the result of a
\Marpa \var{Memoizer} operation.
\end{itemize}
This observation follows from
\Aref{alg:initializer-op},
\Aref{alg:scanner-op},
\Aref{alg:reducer-op},
\Aref{alg:predicter-op}, and
\Aref{alg:memoizer-op}.
\obEnd
\end{observation}

\begin{observation}[\Earley{} operation by type of result]
\label{obs:earley-op-by-result}
The operation of the original \Earley{} algorithm can be determined
from the result as described in
\Obref{marpa-op-by-result},
except
in \Earley{} there is no \var{Memoizer} operation,
and there are no \realm{LIM}s.
This follows from
\Obref{earley-set-0},
\Obref{earley-scanner-op},
\Obref{earley-completer-op},
\Obref{earley-predicter-op},
\Obref{earley-memoizer-op}, and
\Obref{marpa-op-by-result}.
\obEnd
\end{observation}

\begin{observation}[\Marpa operation by direct ancestor]
\label{obs:marpa-op-by-direct-ancestor}
If a completion is a direct ancestor of a \Marpa
operation, that operation is the \var{Reducer} operation.
If a token is a direct ancestor of a \Marpa
operation, that operation is the \var{Scanner} operation.
This observation follows from
\Aref{alg:initializer-op},
\Aref{alg:scanner-op},
\Aref{alg:reducer-op},
\Aref{alg:predicter-op}, and
\Aref{alg:memoizer-op}.
\obEnd
\end{observation}

\begin{observation}[\Earley{} operation by direct ancestor]
\label{obs:earley-op-by-direct-ancestor}
If a completion is a direct ancestor of an \Earley{}
operation, that operation is the Completer operation.
If a token is a direct ancestor of a \Marpa
operation, that operation is the Scanner operation.
This observation follows from
\Obref{earley-set-0},
\Obref{earley-scanner-op},
\Obref{earley-completer-op},
\Obref{earley-predicter-op} and
\Obref{earley-memoizer-op}.
\obEnd
\end{observation}

\chapter{Ambiguity}
\label{chap:ambiguity}

\todo[prepend, caption={``Ambiguity'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Discuss the sources of this approach.}

We say that a parse $[ \Vcfg{g}, \Vstr{w1} ]$
is ambiguous, if there is more than one derivation tree for it.
Most derivation trees allow more than one derivation,
but for every derivation tree there is exactly one rightmost
derivation.
So a parse is ambiguous if and only if there is more than
one rightmost derivation tree for \Vstr{w1}.
A grammar is ambiguous if and only if there is some input
for which the parse is ambiguous.

We call a string of terminals derived from a sentential form
its \dfn{frontier}.
If a parse is unambiguous, the frontier of a sentential form
form derives is unique, so that
we can write the frontier of
\Vstr{sf} as $\Vfrontier{sf}$,
where we will have that
$\Vfrontier{sf} \in \Term{\var{g}}^\ast$
for the current choice of \Vcfg{g}.

We recognize two kinds of ambiguity:
horizontal and vertical.

\begin{definition}[Horizontal Ambiguity]
\label{def:h-ambig}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse,
and let \var{gallia} be a triple $[\Vrule{r}, \var{pos1}, \var{pos2}]$,
where $0 \le \var{pos1} < \var{pos2} \le \size{\RHS{\var{r}}}$,
indicating a rule whose RHS is partitioned
into three parts.
The 3-partitioned rule may be written as
\begin{equation}
\Vrule{r} = [ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} ],
\end{equation}
where
\begin{gather}
\Vstr{rhs1} = (\RHS{\var{r}})[0 \ldots \Vdecr{pos1}], \\
\Vstr{rhs2} = (\RHS{\var{r}})[\var{pos1} \ldots \Vdecr{pos2}], \;\; \text{and} \\
\Vstr{rhs3} = (\RHS{\var{r}})[\var{pos2} \ldots \decr{\size{\RHS{\var{r}}}}].
\end{gather}
A \dfn{horizontal ambiguity} is a 5-tuple,
\begin{equation}
\begin{gathered}
[ \var{gallia}, \Vloc{origin}, \Vloc{overlap1}
\Vloc{overlap2}, \Vloc{dot} ]
\end{gathered}
\end{equation}
such that
\begin{align}
\label{eq:def-h-ambig-21a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-h-ambig-21b}
& \destar \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c}
& \derives \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c2}
& \destar \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-h-ambig-21d}
& \destar \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:def-h-ambig-21e}
& \destar \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-h-ambig-21f}
& \destar \Vterm{w},
\end{align}
and also such that
\begin{gather}
\label{eq:def-h-ambig-30a}
\Vstr{rhs2} \destar \boldRangeDecr{w}{overlap2}{dot}
\;\; \text{and} \\
\label{eq:def-h-ambig-30b}
\var{overlap1} \neq \var{overlap2}.
\end{gather}

\var{gallia} is called the \dfn{partitioned rule},
\Vloc{orig} the \dfn{origin}, and
\Vloc{dot} the \dfn{dot position}
of the horizontal ambiguity.
The duple $[\var{overlap1}, \var{overlap2}]$ is
called its \dfn{overlap}.
The string
\[
\var{w}[\var{overlap1}\ldots\Vdecr{overlap2}]
\]
is also called its \dfn{overlap}.

If there is a horizontal ambiguity for a parse \var{p},
then
\var{p} is a horizontally ambiguous parse.
A grammar is \dfn{horizontally ambiguous} if and only if it has a
horizontally ambiguous parse. \dfEnd{}
\end{definition}

Note that if $\Vstr{rhs1} = \epsilon$, that
$\var{overlap1} = \var{overlap2}$.
Among other things, this implies that a unit rule
cannot be the rule of a horizontally ambiguity.

\begin{theorem}[Grammar of horizontally ambiguous parse is ambiguous]
\label{th:h-ambig-g}
If a parse is horizontally ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but contains a horizontal ambiguity as defined in \Dfref{h-ambig}.
Since the parse is unambiguous, we can write
\[
\Vstr{suffix} \destar \Vfrontier{suffix},
\]
where
$\Vfrontier{suffix} \in \Term{\var{g}}^\ast$.
Then we can rewrite
\Eref{def-h-ambig-21a}--\Eref{def-h-ambig-21f}
as the following right derivation
\begin{align}
\label{eq:th-h-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vfrontier{suffix} \\
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vfrontier{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{current}{w} \\
\label{eq:th-h-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:th-h-ambig-10-1-e}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-h-ambig-10-1-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We have replaced the concatenation operator
in \Eref{th-h-ambig-10-1-d}
with the $\spadesuit$ symbol
for convenience in referring to that location.

Since the parse is assumed to be unambiguous, the right derivation must be
unique.
The step which derives \Eref{th-h-ambig-10-1-d} in unique within its derivation
as the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$.
Therefore
\Eref{th-h-ambig-10-1-d}
is a unique step of a unique derivation.

Note that no assumption is made that any of
\Vstr{prefix}, \Vstr{rhs1}, \Vstr{rhs2}, \Vstr{rhs3},
and \Vstr{suffix} contain
non-terminals.
If they do not, the derivations of terminal strings are trivial,
and our claims about the presence
and elimination of non-terminals are trivially or vacuously true.
For example, if \Vstr{rhs1} contains no non-terminals,
then the step from
\Eref{th-h-ambig-10-1-d} to
\Eref{th-h-ambig-10-1-e}
is trivial and the sentential forms of
\Eref{th-h-ambig-10-1-d} and
\Eref{th-h-ambig-10-1-e}
are identical,
so that our claim that
\Eref{th-h-ambig-10-1-d} occurs prior to the elimination
of any non-terminals in \Vstr{rhs1} is vacuously true.

We go back
to \Dfref{h-ambig}, and write a second right derivation for the parse.
Because our parse is unambiguous, this derivation should be equivalent to the
derivation from
\Eref{th-h-ambig-10-1-a}
to
\Eref{th-h-ambig-10-1-f}.
\begin{align}
\label{eq:th-h-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:th-h-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w} \\
\label{eq:th-h-ambig-10-2-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
The step which derives
the sentential form of \Eref{th-h-ambig-10-2-d}
also the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$ and,
since they result from the same steps of identical derivations,
the two sentential forms \Eref{th-h-ambig-10-1-d}
and \Eref{th-h-ambig-10-2-d}
should be identical.
But $\var{overlap1} \neq \var{overlap2}$
\Eref{def-h-ambig-30b}
and therefore
\begin{equation}
\label{eq:th-h-ambig-50}
\begin{gathered}
\Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\neq \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w}
\end{gathered}
\end{equation}
From
\Eref{th-h-ambig-50}
we see that the
derivation from
\Eref{th-h-ambig-10-1-a}
to
\Eref{th-h-ambig-10-1-f}
is not the same as the derivation from
\Eref{th-h-ambig-10-2-a} to
\Eref{th-h-ambig-10-2-f}.
Since these are both right derivations,
the parse must be ambiguous, which shows
the reductio.
Since the parse is ambiguous, its grammar is ambiguous.
This shows the theorem. \myqed
\end{proof}

\begin{definition}[Vertical Ambiguity]
\label{def:v-ambig}
Let $[\Vcfg{g}, \Vstr{w}]$ be a parse.
A \dfn{vertical ambiguity} is a 4-tuple,
\begin{equation}
\label{eq:def-v-ambig-10a}
\left[ \Vrule{r1}, \Vrule{r2}, \Vloc{origin}, \Vloc{dot} \right],
\end{equation}
such that
\begin{gather}
\label{eq:def-v-ambig-15a}
\var{r1} = [ \var{lhs} \de \Vstr{rhs1} ], \\
\label{eq:def-v-ambig-15b}
\var{r2} = [ \var{lhs} \de \Vstr{rhs2} ], \\
\label{eq:def-v-ambig-15c}
\var{rhs1} \neq \var{rhs2}, \\
\label{eq:def-v-ambig-15d}
\var{rhs2} \destar \boldRangeDecr{w}{origin}{dot},
\end{gather}
and
\begin{align}
\label{eq:def-v-ambig-20a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-v-ambig-20b}
& \destar \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20c}
& \derives \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20d}
& \destar \;
\begin{aligned}
& \Vstr{prefix} \cat \boldRangeDecr{w}{origin}{dot} \\
& \qquad \cat \boldRangeSizeDecr{w}{dot}{w}
\end{aligned}
\\
\label{eq:def-v-ambig-20e}
& = \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-v-ambig-20f}
& \destar \Vterm{w}.
\end{align}

\var{r1} and \var{r2} are the \dfn{rules}
of the vertical ambiguity,
\Vloc{orig} is its \dfn{origin}, and
\Vloc{dot} is its \dfn{dot position}.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse.

If there is a vertical ambiguity for a parse \var{p},
then
\var{p} is a vertically ambiguous parse.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse. \dfEnd{}
\end{definition}

\begin{theorem}[Grammar of vertically ambiguous parse is ambiguous]
\label{th:v-ambig-g}
If a parse is vertically ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but has a vertical ambiguity as defined in \Dfref{v-ambig}.
Noting that
\Eref{def-v-ambig-20a} through
\Eref{def-v-ambig-20f}
in \Dfref{v-ambig} is a right derivation,
we can write
\begin{align}
\label{eq:th-v-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-1-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-1-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We also have a right derivation
\begin{align}
\label{eq:th-v-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-2-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-2-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
by
\begin{itemize}
\item letting the derivation of
\Eref{th-v-ambig-10-2-b} from
\Eref{th-v-ambig-10-2-a}
be identical to the derivation of
\Eref{th-v-ambig-10-1-b} from
\Eref{th-v-ambig-10-1-a};
\item
using the rule
\Eref{def-v-ambig-15b}
to derive
\Eref{th-v-ambig-10-2-c};
\item
using
\Eref{def-v-ambig-15d}
to derive
\Eref{th-v-ambig-10-2-d};
\item and letting the derivation of
\Eref{th-v-ambig-10-2-e} from
\Eref{th-v-ambig-10-2-d}
be identical to the derivation of
\Eref{th-v-ambig-10-1-e} from
\Eref{th-v-ambig-10-1-d};
\end{itemize}

Since \var{p} is assumed to be unambiguous,
the two right derivations
\Eref{th-v-ambig-10-1-a}--\Eref{th-v-ambig-10-1-e}
and
\Eref{th-v-ambig-10-2-a}--\Eref{th-v-ambig-10-2-e}
must be identical,
and therefore must be a sequence of identical sentential forms.

\Eref{th-v-ambig-10-1-c} and
\Eref{th-v-ambig-10-2-c}
must be sentential forms in the same position of their
identical derivations.
We know this
because both result from the production of the \Vsym{lhs}
when that \Vsym{lhs} is the rightmost non-terminal and
the underived part of sentential form is $\Vstr{prefix}\Vsym{lhs}$.
We know that this the only time that happens because otherwise
\[
\Vstr{prefix}\Vsym{lhs} \deplus \Vstr{prefix}\Vsym{lhs},
\]
in which case \var{p} contains a cycle,
which is contrary to assumption for this case.
Therefore
\Eref{th-v-ambig-10-1-c} and
\Eref{th-v-ambig-10-2-c} are sentential forms after the
same step of identical derivations,
and are therefore equal.

But from the definition of vertical ambiguity,
we have $\var{rhs1} \neq \var{rhs2}$,
so that
the sentential forms at
\Eref{th-v-ambig-10-1-c} and
\Eref{th-v-ambig-10-2-c}
are not equal.
Therefore the two right derivations differ,
and \var{p} is ambiguous,
which shows the reductio.
Since \var{p} is ambiguous, its grammar, \Vcfg{g} must
also be ambiguous.
This shows the theorem. \myqed
\end{proof}

\begin{definition}
\label{def:ambig-eim}
An confirmed \realm{EIM} is \dfn{ambiguous} if it is of the form
\begin{equation}
\label{eq:ambig-eim-05}
\left[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs3} ],
\Vloc{origin}
\right] @ \Vloc{dot}
\end{equation}
\Eref{ambig-eim-05}
and has two distinct and valid link pairs.
Without loss of generality,
the predecessor in the first link pair is
the valid \realm{EIM}
\begin{equation}
\label{eq:ambig-eim-10a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap1},
\end{equation}
and its cause is the valid \realm{EIM}
\begin{equation}
\label{eq:ambig-eim-10b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children1} \mydot ], \var{overlap1}
\right] @ \Vloc{dot}.
\end{equation}
Also without loss of generality,
the predecessor in the second link pair is the valid \realm{EIM}
\begin{equation}
\label{eq:ambig-eim-20a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap2},
\end{equation}
and the cause of the second link pair is the valid \realm{EIM}
\begin{equation}
\label{eq:ambig-eim-20b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children2} \mydot ], \var{overlap2}
\right] @ \Vloc{dot}.
\end{equation}

The \realm{EIM} is \dfn{vertically ambiguous} if
\begin{equation}
\label{eq:ambig-eim-02}
\Vloc{overlap1} = \Vloc{overlap2},
\end{equation}
and the \realm{EIM} is
\dfn{horizontally ambiguous} if
\begin{equation}
\label{eq:ambig-eim-03}
\Vloc{overlap1} \neq \Vloc{overlap2}. \quad \dfEnd
\end{equation}
\end{definition}

\begin{theorem}[Parse with horizontal ambiguity is ambiguous]
\label{th:h-eim-g-ambig}
If a partial parse has a horizontally ambiguous \realm{EIM},
then any full parse consistent with that partial parse is
horizontally ambiguous,
and the grammar shared by the partial and full parse is horizontally ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-h-eim-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Further assume that \var{partial}
has a horizontally ambiguous \realm{EIM} \Dfref{ambig-eim}.

Without loss of generality,
let
\begin{equation}
\label{eq:th-h-eim-g-ambig-20}
\begin{gathered}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\text{for some \Vterm{suffix}}
\end{gathered}
\end{equation}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a horizontal ambiguity in \var{full}
according to \Dfref{h-ambig}.
By assumption for the theorem,
\var{partial} has a horizontally ambiguous \realm{EIM} according to
\Dfref{ambig-eim}.
Let \Vloc{origin},
\Vloc{overlap1},
\Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs},
\Vsym{rhs1}, and
\Vstr{rhs3}
in \Dfref{h-ambig}
be their eponyms in \Dfref{ambig-eim}.
And let
\Vstr{rhs2} in \Dfref{h-ambig}
be the string of length 1 whose only character is
\Vsym{rhs2} in \Dfref{ambig-eim}.

In the horizontally ambiguous \realm{EIM}
let the 3-partitioned rule be
\[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \Vstr{rhs3} ].
\]
from \Dfref{ambig-eim}.
and let \Vloc{origin}, \Vloc{overlap1}, \Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs}, \Vstr{rhs1}, and \Vstr{rhs3}
in
\Dfref{h-ambig} be their eponyms in
\Dfref{ambig-eim}.
Let \Vstr{rhs2}
in \Dfref{h-ambig} be
the string of length 1 consisting of \Vsym{rhs2}
in \Dfref{ambig-eim}.

To show that
writing the derivation
\Eref{def-h-ambig-21a}--\Eref{def-h-ambig-21f}
in \Dfref{h-ambig}
is justified by the definition
\Dfref{ambig-eim}, we note that
\begin{itemize}
\item from the definition of validity
\Dfref{eim-validity}
for the Earley item
\Eref{ambig-eim-05},
we have the accessibility and productivity
of all the symbols in
\Eref{def-h-ambig-21a}-\Eref{def-h-ambig-21f};
\item from the validity of
the \realm{EIM} \Eref{ambig-eim-10b}, we have
the location of \Vloc{dot}
in \Eref{def-h-ambig-21d}
and of \Vloc{origin}
in \Eref{def-h-ambig-21f}; and
\item
from the definition of \realm{EIM} validity for
\Eref{ambig-eim-10b},
we have the location of \Vloc{overlap1}
in \Eref{def-h-ambig-21e}.
\end{itemize}

To show
the remaining elements of \Dfref{h-ambig}
we note that
\begin{itemize}
\item we have the location of \Vloc{overlap2}
in \Eref{def-h-ambig-30a}
from the definition of \realm{EIM} validity for
\Eref{ambig-eim-20a}; and
\item
we have \Eref{def-h-ambig-30b}
from \Eref{ambig-eim-03}.
\end{itemize}

We now have shown all the elements
in the definition of horizontal ambiguity \Dfref{h-ambig}.
Since our assumption of a full parse consistent with
\Eref{th-h-eim-g-ambig-15} is without loss of generality
\Eref{th-h-eim-g-ambig-20},
this shows that any full parse consistent with
\Eref{th-h-eim-g-ambig-15} is horizontally ambiguous.
And since by \Thref{h-ambig-g}, if a parse is horizontally
ambiguous,
its grammar is horizontally ambiguous,
we see that \Vcfg{g} is horizontally ambiguous. \myqed
\end{proof}

\begin{theorem}[Parse with vertical ambiguity is ambiguous]
\label{th:v-eim-g-ambig}
If a partial parse has a vertically ambiguous \realm{EIM},
then any full parse consistent with that partial parse is vertically ambiguous,
and the grammar shared by the partial and full parse is vertically ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-v-eim-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Further assume that \var{partial}
has a vertically ambiguous \realm{EIM} \Dfref{ambig-eim}.

Without loss of generality,
let
\begin{gather}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\label{eq:th-v-eim-g-ambig-20b}
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\nonumber
\text{for some \Vterm{suffix}}
\end{gather}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a vertical ambiguity in \var{full}
according to \Dfref{v-ambig}.
By assumption for the theorem,
\var{partial} has a vertically ambiguous \realm{EIM} according to
\Dfref{ambig-eim}.

Let
\begin{align}
\label{eq:th-v-eim-g-ambig-25a}
\text{\Vsym{rhs2} in \Dfref{ambig-eim}} & = \text{\Vsym{lhs} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25b}
\text{\Vstr{children1} in \Dfref{ambig-eim}} & = \text{\Vstr{rhs1} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25c}
\text{\Vstr{children2} in \Dfref{ambig-eim}} & = \text{\Vstr{rhs2} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25d}
\text{\Vloc{dot} in \Dfref{ambig-eim}} & = \text{\Vloc{dot} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25e}
\text{\Vloc{origin} in \Dfref{ambig-eim}} & = \text{\Vloc{origin} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25f}
\text{\Vloc{overlap1} in \Dfref{ambig-eim}} & = \text{\Vloc{origin} in \Dfref{v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25g}
\text{\Vloc{overlap2} in \Dfref{ambig-eim}} & = \text{\Vloc{origin} in \Dfref{v-ambig}}
\end{align}

To justify writing
\Eref{def-v-ambig-20a}--\Eref{def-v-ambig-20f}
after the renamings of
\Eref{th-v-eim-g-ambig-25a}--\Eref{th-v-eim-g-ambig-25g},
we note that we have
\begin{itemize}
\item
\Eref{def-v-ambig-15a} from
from
\Eref{th-v-eim-g-ambig-25a},
\Eref{th-v-eim-g-ambig-25b} and
\Eref{ambig-eim-10b};
\item
\Eref{def-v-ambig-15b} from
\Eref{th-v-eim-g-ambig-25a},
\Eref{th-v-eim-g-ambig-25c} and
\Eref{ambig-eim-20b};
\item
the accessibility and productivity of the variables in
\Eref{def-v-ambig-20a}--\Eref{def-v-ambig-20f}
from the definition of \realm{EIM} validity for
\Eref{ambig-eim-10b};
\item
the location of \Vloc{dot},
in \Eref{def-v-ambig-20b}
from the definition of \realm{EIM} validity for
\Eref{ambig-eim-10b}
and \Eref{th-v-eim-g-ambig-25d};
\item
the location of \Vloc{origin}
in \Eref{def-v-ambig-20d}
from the definition of \realm{EIM} validity for
\Eref{ambig-eim-10b}
and \Eref{th-v-eim-g-ambig-25f}; and
\item
\Eref{def-v-ambig-15d} follows from
\Eref{ambig-eim-20b},
\Eref{th-v-eim-g-ambig-25d}, and
\Eref{th-v-eim-g-ambig-25g}.
\end{itemize}

It remains to justify
$\var{rhs1} \neq \var{rhs2}$,
\Eref{def-v-ambig-15c}.
In \Dfref{ambig-eim}
the two link pairs are required to be distinct.
The predecessors of the two links,
\Eref{ambig-eim-10a} and
\Eref{ambig-eim-20a}, differ only in their dot locations:
\Vloc{overlap1} versus \Vloc{overlap2}.
In
\Eref{th-v-eim-g-ambig-25f} and
\Eref{th-v-eim-g-ambig-25g},
we have set
both of these
to \Vloc{origin}
in \Dfref{v-ambig},
so that
\begin{equation}
\label{eq:th-v-eim-g-ambig-40}
\begin{gathered}
\text{\Vloc{overlap1} in \Eref{ambig-eim-10a}} \\
= \text{\Vloc{overlap2} in \Eref{ambig-eim-20a}} \\
= \text{\Vloc{origin} in \Dfref{v-ambig}.}
\end{gathered}
\end{equation}
So, to be distinct, the two link pairs must differ in the causes.
The two causes,
\Eref{ambig-eim-10b} and
\Eref{ambig-eim-20b}
can differ only in their origins
and in their RHS's.
From
\Eref{th-v-eim-g-ambig-40}
we know that the origins of
\Eref{ambig-eim-10a} and
\Eref{ambig-eim-10b}
are identical,
so that
\Eref{ambig-eim-10b} must differ from
\Eref{ambig-eim-20b} in its RHS:
\begin{equation}
\label{eq:th-v-eim-g-ambig-43}
\begin{gathered}
\text{\Vstr{children1} in \Eref{ambig-eim-10a}} \\
\neq \text{\Vstr{children2} in \Eref{ambig-eim-20a}}.
\end{gathered}
\end{equation}
From
\Eref{th-v-eim-g-ambig-43},
\Eref{th-v-eim-g-ambig-25b}, and
\Eref{th-v-eim-g-ambig-25c}
we have
\begin{equation}
\label{eq:th-v-eim-g-ambig-46}
\Vstr{rhs1} \neq \Vstr{rhs2}
  \text{ in \Eref{def-v-ambig-15c}
    of \Dfref{v-ambig}. \myqed}
\end{equation}
\end{proof}

\begin{theorem}[Grammar of parse with ambiguity is ambiguous]
\label{th:ambig-eim-ambig-g}
If a partial parse has an ambiguous \realm{EIM},
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{partial} = [ \Vcfg{g}, \Vstr{prefix} ]$ be a partial
parse with an ambiguous \realm{EIM}.
Let $\var{full} = [ \Vcfg{g}, \Vstr{prefix}\Vterm{suffix} ]$ be a full
parse consistent with \var{partial}.
From \Dfref{ambig-eim}, we see that an ambiguous \realm{EIM}
is either a horizontally ambiguous \realm{EIM}, or
a vertically ambiguous \realm{EIM}.
We proceed by cases.

If the ambiguous \realm{EIM} is horizontally ambiguous then
by \Thref{h-eim-g-ambig}
any full parse consistent with the partial parse
is horizontally ambiguous,
and therefore \var{full} is horizontally ambiguous.
By \Thref{h-ambig-g} if \var{full} is horizontally ambiguous,
then \Vcfg{g} is ambiguous.

If the ambiguous \realm{EIM} is vertically ambiguous then
by \Thref{v-eim-g-ambig}
any full parse consistent with the partial parse
is vertically ambiguous,
and therefore \var{full} is vertically ambiguous.
By \Thref{v-ambig-g} if \var{full} is vertically ambiguous,
then \Vcfg{g} is ambiguous. \myqed
\end{proof}

\begin{theorem}[Unambiguous grammar does not have ambiguity]
\label{th:unambig-g-unambig-eim}
If a grammar is unambiguous, then it does not have
an ambiguous \realm{EIM}.
\end{theorem}

\begin{proof}
This theorem is the contrapositive of \Thref{ambig-eim-ambig-g}. \myqed
\end{proof}

\chapter{Per-set lists}
\label{chap:per-set-lists}

\todo[prepend, caption={``Per-set lists'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Document complexity claims for \Seen{} and \SetSeen{}}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\VVelement{S}{i}, \var{x}]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a direct link to the Earley set \VVelement{S}{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to,
or very similar to,
PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this book.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of the structure
that implements each Earley set.
While \Marpa is building a new Earley set,
\VVelement{S}{j},
a PSL is kept for every previous Earley set ---
that is, for every
$\Vloc{i} < \Vloc{j}$.

\Marpa uses a PSL for its \var{seen} data structure.
In the case of \var{seen},
there is one fixed-size entry for every dotted rule in the grammar,
so that the size of \var{seen} for each ES
is
\[
\bigorder{\size{\DottedRules{\var{g}}}}.
\]
\DottedRules{\var{g}}
is a constant which depends on \Vint{g},
and therefore the space consumed by \var{seen}
\Oc{}, all of which is treated as trimmed space.
For the same reasons, the time used to allocate \var{seen}
for each ES is \Oc{}.

While \Marpa is adjoining \realm{EIM}s into \VVelement{S}{j},
the PSL for \VVelement{S}{i}
keeps track of the Earley items in \VVelement{S}{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{g}.
Recall that \Vsize{g}
is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's
for all the Earley sets
every time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

Intuitively, the idea is avoid most re-computation
of the PSL's by keeping timestamps with the PSL data.
To illustrate the use of PSL's,
we will consider the case
where \Marpa is building \VVelement{S}{j}
and wants to check whether Earley item,
\begin{equation*}
\Veim{x} = [ \Vdr{x}, \Vorig{x} ],
\end{equation*}
is new,
or if \Veim{x} needs to be adjoined into the Earley sets.

Let
\begin{equation*}
\var{psl-entry} = \PSL{\VVelement{S}{i}}{\var{y}}
\end{equation*}
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}, such that
\begin{equation*}
\begin{alignedat}{3}
& \var{psl-entry} && \defined && \quad
\begin{cases}
\Lambda, \quad \text{if the entry at $\var{psl-entry}$ has never been used,} \\
[\var{time-stamp}, \Vbool{z}], \text{otherwise} \\
\end{cases} \\
%
\end{alignedat}
\end{equation*}
Here \var{time-stamp} is a time stamp which will be used to
avoid unnecessary re-computation,
and \Vbool{z} is \var{true} if \Veim{x}
is already present in \VVelement{S}{j},
and \var{false} otherwise.

\FloatBarrier

\begin{algorithm}[H]
\caption{Adjoin \realm{EIM}}
\label{alg:adjoin-eim}
\begin{algorithmic}[1]
\Procedure{Adjoin-EIM}{$[\Vdr{x},\Vorig{x}]@\VVelement{S}{j}$}
\If{$\PSL{\VVelement{S}{j}}{\ID{\Vdr{x}}} = [\Vloc{j}, \var{true}]$}
\State return \Comment Do nothing if \Veim{x} exists
\EndIf
\State $\VVelement{S}{j} \gets \VVelement{S}{j} \cup \set{[\Vdr{x]}, \Vorig{x}]}$
\State $\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets [\Vloc{j}, \var{true}]$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\Aref{alg:adjoin-eim} contains pseudocode
showing the use of PSL's.
The reader may notice that
\Aref{alg:adjoin-eim} could be simplified,
because the data to be kept for \realm{EIM}s is a single boolean.
Further, this boolean is never set to \var{false}, so that
if a PSL entry exists, its boolean may be assumed to be \var{true}.
This means that an explicit boolean is not actually needed.
The simpler approach, which omits the boolean,
is the one that
the \Marpa implementation sometimes uses.

\chapter{\Marpa is consistent}
\label{chap:consistent}

\todo[prepend, caption={``\Marpa is consistent'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Move consistency stuff here}

[ Important note:
To show consistency, we must show
\var{dotZeroUsed} and \myfn{Seen}{} CORRECTNESS,
not just consistency. ]

\chapter{\Marpa is correct}
\label{chap:correctness}

\todo[prepend, caption={``\Marpa is correct'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\myepi{\label{epi:ariane}%
    The exception which occurred was not due to random failure but a design
    error. The exception was detected, but inappropriately handled because
    the view had been taken that software should be considered correct until
    it is shown to be at fault. The Board has reason to believe that this
    view is also accepted in other areas of Ariane 5 software design. The
    Board is in favour of the opposite view, that software should be assumed
    to be faulty until applying the currently accepted best practice methods
    can demonstrate that it is correct.}
    {Ariane 5, Flight 501, Failure Report~\cite[section 2.2]{Lyons1996}}%

In this book,
we call a parsing algorithm \dfn{correct} if and
only if, for every parse, it is \dfn{consistent} and
\dfn{complete}.
We call a parsing algorithm \dfn{consistent} if
it produces only valid Earley items.
We call a parsing algorithm \dfn{complete} if
it produces all of the valid Earley items.

\begin{theorem}[\Earley{} is correct]
\label{th:earley-is-correct}
\Earley{} is correct.
\end{theorem}

\begin{proof}
\Earley{} is proved correct in
\cite[Theorem 4.9, pp. 323-325]{AU1972} and
\cite[pp. 18-25]{Earley1968}. \myqed
\end{proof}

The following is proved as part of
\Thref{earley-is-correct},
but we state and prove it separately
because we make special use of it.

\begin{theorem}[Earley completer consistency]
\label{th:earley-completer-is-consistent}
The \Earley{} completer operation preserves validity.
That is, if the predecessor and cause
of an Earley{} completer operation are valid,
the result of the Earley{} completer operation is valid.
\end{theorem}

\begin{proof}
Let $[\Vint{g}, \Vstr{prefix}]$ be a partial parse,
let
\[
[ \Veim{pred}, \Veim{cuz} ]_\realm{LP}
\]
be an arbitrary ancestry of an Earley{} completer operation,
and let the result of the operation be \Veim{rez}.
Then
\begin{equation}
\label{eq:earley-completer-is-consistent-20}
  \begin{gathered}
  \Vdr{pred} =
  \left[ \Vsym{lhsP} \de \Vstr{rhs1} \mydot \Vsym{postdot} \Vstr{rhs3}
  \right] \\
  \because \Eref{obs-earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-22}
  \begin{gathered}
  \Veim{pred} =
  \left[ \Vdr{pred}, \Vloc{orig} \right] @ \Vloc{meet} \\
  \because \Eref{obs-earley-completer-summary-2},
    \Eref{earley-completer-is-consistent-20}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-24}
\LHS{\Vdr{cuz}} = \Vsym{postdot}
\because \Eref{obs-earley-completer-summary-3},
  \Eref{earley-completer-is-consistent-20}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-26}
  \begin{gathered}
  \Veim{cuz} = \left[ \Vdr{cuz}, \Vloc{meet} \right]
    @ \Vloc{current} \\
  \because \Eref{earley-completer-is-consistent-22},
  \Eref{obs-earley-completer-summary-1},
  \Eref{obs-earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-28}
  \begin{gathered}
  \Veim{rez} =
  \left[
    \begin{gathered}
      \Next{\DR{\var{pred}}}, \\
      \Origin{\var{pred}}
    \end{gathered}
  \right]@\Vloc{current} \\
  \because
  \Eref{obs-earley-completer-summary-4},
  \Eref{earley-completer-is-consistent-22},
  \Eref{earley-completer-is-consistent-26}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-30}
\myparbox{\Veim{pred} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-32}
\myparbox{\Veim{cuz} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-34}
\Rule{\Veim{pred}} \in \Rules{\var{g}}
\because \Eref{eim-valid-22a},
  \Eref{earley-completer-is-consistent-30}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-36}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{pred}} \cat \Vstr{after} \\
  \because \Eref{eim-valid-22b},
    \Eref{earley-completer-is-consistent-30}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-38}
\begin{gathered}
  \Vstr{before} \destar \var{prefix}[0  \ldots  \Vdecr{orig}] \\
  \because \Eref{eim-valid-22c},
    \Eref{earley-completer-is-consistent-22},
    \Eref{earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-40}
\begin{gathered}
  \Vstr{rhs1} \destar \var{prefix}[\var{orig}  \ldots  \Vdecr{meet}] \\
  \because \Eref{eim-valid-22d},
  \Eref{earley-completer-is-consistent-20},
  \Eref{earley-completer-is-consistent-22},
  \Eref{earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-42}
\Rule{\Veim{pred}} = \Rule{\Veim{rez}}
\because \Eref{earley-completer-is-consistent-28}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-44}
\Rule{\Veim{rez}} \in \Rules{\var{g}}
\because \Eref{earley-completer-is-consistent-34},
\Eref{earley-completer-is-consistent-42}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-46}
\myparbox{$\LHS{\Veim{pred}} = \LHS{\Veim{rez}}$
$\because$ Def \var{Next} \Eref{def-next},
  \Eref{earley-completer-is-consistent-42}.
}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-48}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{rez}} \cat \Vstr{after} \\
  \because \Eref{earley-completer-is-consistent-36},
  \Eref{earley-completer-is-consistent-46}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-50}
\begin{aligned}
  & \Vdr{rez} = \left[ \Vsym{lhsP} \de \Vstr{rhs1} \Vsym{postdot} \mydot \Vstr{rhs3}
  \right] \\
  & \qquad \quad \because \Eref{earley-completer-is-consistent-20}, \Eref{earley-completer-is-consistent-28}.
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-52}
\myparbox{\Veim{cuz} is a completion
  $\because \Eref{obs-earley-completer-summary-1}$.}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-53}
  \begin{gathered}
    \LHS{\Vdr{cuz}} \destar \var{prefix}[\var{meet}  \ldots  \Vdecr{current}] \\
    \because \Eref{eim-valid-22d},
    \Eref{earley-completer-is-consistent-26},
    \Eref{earley-completer-is-consistent-32},
    \Eref{earley-completer-is-consistent-52}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-54}
  \begin{gathered}
  \Vstr{postdot} \destar
  \var{prefix}[\var{meet}  \ldots  \Vdecr{current}] \\
  \because
  \Eref{earley-completer-is-consistent-24},
  \Eref{earley-completer-is-consistent-53}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-56}
\begin{aligned}
  & \Vstr{rhs1} \Vstr{postdot} \\
  & \qquad \destar \var{prefix}[\var{orig}  \ldots  \Vdecr{current}] \\
  & \qquad \qquad \because \Eref{earley-completer-is-consistent-40},
    \Eref{earley-completer-is-consistent-54}
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-58}
\myparbox{\Veim{rez} is valid
  $\because$ \Dfref{eim-validity},
  \Eref{earley-completer-is-consistent-38},
  \Eref{earley-completer-is-consistent-44},
  \Eref{earley-completer-is-consistent-48},
  \Eref{earley-completer-is-consistent-56}.
  \myqed
}
\end{equation}
\end{proof}

% \TODO{Delete this?}
% Let \Ves{physical} be an Earley set as produced by \Marpa,
% and let \Vves{virtual} be \Ves{physical} with its Leo memoized
% items restored.
% We will call \Ves{physical} a \dfn{physical} Earley set, and
% we will call \Vves{virtual} a \dfn{virtual} Earley set
% (realm \realm{VES}).

\begin{definition}[Leo chain]
\label{def:leo-chain}
A \dfn{Leo chain},
or simply \dfn{chain} when the meaning is clear in context,
is a sequence of one or more \realm{LIM}s,
call it $\Vlimset{links}$,
where
\[
\PreviousLink{\var{links}[\Vlastix{links}]} = \emptyset
\]
and, for all $0 \le \var{n} < \Vlastix{links}$,
\[
\PreviousLink{\var{links}[\var{n}]} = \set{\var{links}[\var{n}+1]}.
\]
We say that \var{links} is the chain of \Vlim{first} if and
only if \var{links} is a Leo chain
such that $\var{links}[0] = \Vlim{first}$.

A chain is \dfn{maximal} if and only if it is not properly
contained in another chain.
A chain is \dfn{partial} if it is contained in another chain.
Note that a maximal chain is a special case of a partial chain.
Chains are maximal unless stated otherwise.
\dfEnd{}
\end{definition}

\begin{theorem}[LEO Chain containment]
\label{th:chain-containment}
Let
\begin{gather}
\label{eq:th-chain-containment-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-containment-02}
\myparbox{$\mylim{\ell} \in \var{longer}$ be a \realm{LIM},} \\
\label{eq:th-chain-containment-03}
\myparbox{\var{shorter} be the valid Leo chain of \mylim{\ell}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-containment-07}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer}
\end{gather}
\end{theorem}

\begin{proof}
Let \Vlimset{longer}, \mylim{\ell},
and \Vlimset{shorter} be as stated for the theorem.

We show \Eref{th-chain-containment-07} by induction on element index
of \var{shorter}.
\begin{equation}
\label{eq:th-chain-containment-10}
\begin{gathered}
0 \le \var{ix} \le \Vlastix{shorter} \implies \VVelement{shorter}{ix} \in \var{longer} \\
\text{$\because$ hypothesis for induction.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-12}
\myparbox{$\Velement{shorter}{0} \in \var{longer}$
  $\because$
  \Dfref{leo-chain}, \Eref{th-chain-containment-03}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-13}
\begin{gathered}
0 \le 0 \le \Vlastix{shorter} \implies \Velement{shorter}{0} \in \var{longer} \\
\myparbox{$\because$ \Eref{th-chain-containment-12}.
  This is the basis of an induction on
  hypothesis \Eref{th-chain-containment-10}, with $\var{ix} = 0$.%
}
\end{gathered}
\end{equation}
For the step of the induction
we assume
\begin{equation}
\label{eq:th-chain-containment-14}
\begin{gathered}
0 \le \var{n} \le \Vlastix{shorter} \implies \VVelement{shorter}{n} \in \var{longer} \\
\myparbox{$\because$ \Eref{th-chain-containment-10} with $\var{ix} = \var{n}$.%
}
\end{gathered}
\end{equation}
to show
\begin{equation}
\label{eq:th-chain-containment-14-10}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{th-chain-containment-10} with $\var{ix} = \var{n}+1$.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-15}
\begin{gathered}
0 \le \var{n} < \Vlastix{shorter} \\
\myparbox{$\because$ AF subcase.
Otherwise, we have \Eref{th-chain-containment-14-10} vacuously.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-16}
\myparbox{$\exists \; \var{m} : \VVelement{shorter}{n} = \VVelement{longer}{m}$
  $\because$
  \Eref{th-chain-containment-14},
  \Eref{th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-18}
\myparbox{$\PreviousLink{\VVelement{shorter}{n}} = \set{\Velement{shorter}{\var{n}+1}}$
  $\because$ \longDfref{Leo chain}{leo-chain},
  \Eref{th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-20}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \set{\Velement{shorter}{\var{n}+1}}$
  $\because$
  \Eref{th-chain-containment-15},
  \Eref{th-chain-containment-16},
  \Eref{th-chain-containment-18}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-22}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \set{\Velement{longer}{\var{m}+1}}$
  $\because$ \longDfref{Leo chain}{leo-chain},
  \Eref{th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-24}
\myparbox{$\VVelement{shorter}{\var{n}+1} = \Velement{longer}{\var{m}+1}$
  $\because$
  \Eref{th-chain-containment-20},
  \Eref{th-chain-containment-22}.
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-26}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{th-chain-containment-24}.
  This is \Eref{th-chain-containment-10} with $\var{ix} = \var{n}+1$
  and shows the step starting at \Eref{th-chain-containment-14}.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-28}
\begin{gathered}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer} \\
\because \text{ induction \Eref{th-chain-containment-10}--\Eref{th-chain-containment-26}.}
   \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[LEO Chain termination]
\label{th:chain-termination}
Let
\begin{gather}
\label{eq:th-chain-termination-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-termination-03}
\myparbox{\var{shorter} be the valid Leo chain contained in \var{longer}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-termination-08}
\Velement{shorter}{\Vlastix{shorter}} = \Velement{longer}{\Vlastix{longer}}
\end{gather}
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

\begin{theorem}[Memoizer consistency]
\label{th:memoizer-consistency}
The \Marpa \var{Memoizer} operation preserves validity.
\end{theorem}

\begin{proof}
Call the \realm{LIM} added by an arbitrary \Marpa \var{Memoizer}
operation \mylim{\ell},
and call its valid ancestry $[\Vlim{previous}, \Veim{source}]$.
\begin{equation}
\label{eq:th-memoizer-consistency-10}
\myparbox{\Vlim{previous} is $\Lambda$ or a valid \realm{LIM}, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-12}
\myparbox{\Veim{source} is valid, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-14}
  \begin{gathered}
  \Postdot{\Veim{source}} = \Transition{\ell} \\
  \because
  \text{TODO}
  \end{gathered}
\end{equation}
\todo{Add justification for this equation}

\todo{second proof merged in what follows}

% \begin{theorem}[LIM chain dotted rule and origin]
% \label{th:LIM-chain-dr-origin}
% In a \Marpa parse,
% let \Vlimset{chain} be a valid partial \realm{LIM} chain.
% For every element \mylim{\ell} of \var{chain},
% \begin{equation}
% \label{eq-th-LIM-chain-dr-origin-05}
% \begin{gathered}
% \text{$\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}}$ and} \\
% \Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}}
% \end{gathered}
% \end{equation}
% \end{theorem}

% \begin{proof}

The proof is by a reductio.
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-10}
\begin{gathered}
\exists \; \mylim{\ell}, \mylim{\ell} \in \var{chain} : \\
\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\lor \; \Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\because \text{AF reductio.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-12}
\myparbox{\realm{LIM}s are only added by the
\Marpa \var{Memoizer} operation
$\because$ \Obref{marpa-op-by-result}.%
}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-14}
\myparbox{\realm{LIM}s are added by either the
\var{Start-Leo-Chain} or
\var{Extend-Leo-Chain} sub-op
$\because$
\Eref{th-LIM-chain-dr-origin-12},
\text{TODO: Add ref to memoizer code}.%
}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-15-20}
\myparbox{Of the \realm{LIM}s added which
satisfy
\Eref{th-LIM-chain-dr-origin-10}
one, call it \Vlim{first} is added first
$\because$
\Eref{th-LIM-chain-dr-origin-14}.%
}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-15-25}
\myparbox{\Vlim{first} = \Velement{partial}{0},
where \Vlimset{partial} is the partial Leo chain
of \Vlim{first}
$\because$
\Dfref{leo-chain},
\text{TODO: Add ref to memoizer code}.%
}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}

We now proceed by subcase, starting with
\var{Start-Leo-Chain}.
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-16}
\myparbox{\Vlim{first} was added by a
\var{Start-Leo-Chain} sub-op
$\because$ AF subcase.%
}
\end{equation}
\todo{Finish proof from here.  Look at use of \var{LIMPredecessor}}
\myqed
\end{proof}

\begin{definition}[Leo chain--base match]
\label{def:leo-chain-base-match}
Let \Veim{base} be an \realm{EIM}
and let \Vlim{lim} be a \realm{LIM}.
We say \Veim{base} \dfn{matches} \Vlim{lim},
if and only if
\begin{itemize}
\item \Veim{base} is a completion;
\item $\Transition{\Vlim{lim}} = \LHS{\var{base}}$; and
\item $\Vlim{lim} \in \es{(\Origin{\var{base}})}$.
\end{itemize}
We say that \Veim{base} \dfn{matches} a Leo chain,
call it \Vlimset{chain},
if and only if \Veim{base} matches $\var{chain}[0]$.
\dfEnd{}
\end{definition}

\begin{definition}[Leo edge]
\label{def:leo-edge}
Let \Vint{g} be a grammar,
let \Vlimset{chain} be a valid Leo chain,
and let $\Veim{base} \in \Ves{current}$ be a valid \realm{EIM} which
matches \var{chain}.
Then the \dfn{Leo edge}, or just \dfn{edge},
of \var{chain} and \var{base}
is a sequence of complete \realm{EIM}s, call it \Veimset{edge},
such that
\begin{equation}
\label{eq:d-leo-edge-10}
\var{edge}[0] = \Veim{base}
\end{equation}
and, for all \var{n} such that $0 \le \var{n} < \Vsize{chain}$,
\begin{equation}
\label{eq:def-leo-edge-30}
\begin{gathered}
\Veim{source} \in \Source{\var{chain}[\var{n}]} \\
\land \; \left[ \var{source}, \var{edge}[\var{n}] \right]_\realm{LP}
  \in \LinkPairs{\var{edge}[\var{n}+1]}
\end{gathered}
\end{equation}

We say that an edge is \dfn{valid} if and only if all the elements of the edge
are valid.
The \dfn{top} of the edge is \Velement{edge}{\Vlastix{edge}}.
The \dfn{bottom} of the edge is \Velement{edge}{0}.
The \dfn{inside} of the edge is the sequence
\Velement{edge}{1 \ldots \decr{\Vlastix{edge}}}.
An \dfn{inside element} of an edge is a element of the
inside of the edge.
An \dfn{upper element} of an edge is a top or an inside element.
An \dfn{lower element} of an edge is a bottom or an inside element.

An edge is \dfn{maximal} if its Leo chain is maximal.
An edge is \dfn{partial} if its Leo chain is partial.
An edge is maximal, unless stated otherwise.
\dfEnd{}
\end{definition}

\begin{theorem}[Leo edge length]
\label{th:leo-edge-length}
Let \Vint{g} be a grammar;
let \Vlimset{chain} be a valid partial Leo chain;
let $\Veim{base} \in \Ves{current}$ be a valid \realm{EIM} which
matches \var{chain};
and let \Veimset{edge} be their edge.
Then
\[
\Vlastix{edge} = \Vlastix{chain} + 1.
\]
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-length-10}
\Vsize{chain} > 0 \because
\text{Def of ``Leo chain'' \Dfref{leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-14}
  \begin{aligned}
    & \Veim{source} \in \Source{\Velement{chain}{\Vlastix{chain}}} \\
    & \land \; \left[ \var{source}, \Velement{edge}{\Vlastix{chain}} \right]_\realm{LP} \\
    & \qquad \qquad \in \LinkPairs{\Velement{edge}{\Vlastix{chain}+1}} \\
    & \because \text{\Eref{def-leo-edge-30} in \Dfref{leo-chain}},
        \Eref{th-leo-edge-length-10}.
  \end{aligned}
\end{equation}
\todo{\var{Source} changed.  Revise from here.}
\begin{equation}
\label{eq:th-leo-edge-length-16}
\myparbox{edge index is monotonically increasing
as a function of chain index $\because$
\Eref{def-leo-edge-30} in \Dfref{leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-18}
\Vlastix{edge} = \Vlastix{chain} + 1
\because
\Eref{th-leo-edge-length-14},
\Eref{th-leo-edge-length-16}. \quad \myqed
\end{equation}
\end{proof}

\begin{theorem}[Leo chain source properties]
\label{th:leo-chain-source-properties}
In a \Marpa internal grammar,
the source of a Leo chain element is a penult in the same
Earley set as the element.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa internal grammar.
\begin{equation}
\label{eq:th-leo-chain-source-properties-10}
\myparbox{$\mylim{\ell}@\Vloc{j}$ is a Leo chain element
$\because$ AF theorem, WLOG.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-12}
\Veim{source} \in \Source{\ell}
\because
\Eref{th-leo-chain-source-properties-10},
\text{WLOG}.
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-20}
\begin{gathered}
\Veim{source} \in \LeoEligible{\Vloc{j}} \\
\qquad \because
\text{TODO: Add ref to memoizer code},
\end{gathered}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-22}
\begin{gathered}
\Veim{source} \in \LeoUnique{\Vloc{j}} \\
\because
\Eref{th-leo-chain-source-properties-20}, \Eref{def-leo-eligible}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-24}
\myparbox{\Veim{source} is a penult
  $\because$ \Eref{th-leo-chain-source-properties-22},
  Def ``Leo unique'' \Dfref{leo-unique}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-26}
\Current{\Veim{source}} = \Vloc{j}
  \because \Eref{th-leo-chain-source-properties-22},
  \Dfref{leo-unique}.
\end{equation}
The theorem follows directly from
\Eref{th-leo-chain-source-properties-24} and
\Eref{th-leo-chain-source-properties-26}. \myqed
\end{proof}

\begin{theorem}[Edge properties]
\label{th:edge-elements-are-complete}
In a \Marpa internal grammar,
let \Vlimset{chain} be a valid Leo chain,
and \Veim{base} a valid \realm{EIM} matching \var{chain}.
Every element of the edge of \var{chain} and \var{base}
is a valid completion in the same Earley set as \Veim{base},
and the edge is valid.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa internal grammar,
let \Vlimset{chain} be a valid Leo chain
and let \Veim{base} be a valid \realm{EIM} which matches \var{chain}.
Let \Veimset{edge} be the edge of \var{chain} and \var{base}.
We proceed by induction on the elements of the edge,
where the induction hypothesis is
\begin{equation}
\label{eq:edge-elements-are-complete-05}
\myparbox{
$\var{edge}[\var{ix}]$ a valid completion in the same Earley set as \Veim{base}.
}
\end{equation}

We use $\var{ix} = 0$ as the basis of the induction:
\begin{equation}
\label{eq:edge-elements-are-complete-10}
\var{edge}[0] = \Veim{base} \because \Eref{d-leo-edge-10}.
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-12}
\text{\Veim{base} is a completion $\because \Dfref{leo-chain-base-match}$.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-14}
\text{\Veim{base} is in the same Earley set as \Veim{base} $\because$ trivial.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-16}
\text{\Veim{base} is valid by
  assumption for the theorem.}
\end{equation}
\begin{gather}
\label{eq:edge-elements-are-complete-18}
\myparbox{\var{edge}[0] is a valid completion
  in the same Earley set as \Veim{base}
  $\because$
  \Eref{edge-elements-are-complete-10},
  \Eref{edge-elements-are-complete-12},
  \Eref{edge-elements-are-complete-14},
  \Eref{edge-elements-are-complete-16}.
  This is
  \Eref{edge-elements-are-complete-05} for $\var{ix} = 0$,
  the basis of the induction on \Eref{edge-elements-are-complete-05}.
  }
\end{gather}

For the step of the induction, we assume
the induction hypothesis \Eref{edge-elements-are-complete-05}
for $\var{ix}=\var{n}$
to show
the induction hypothesis
for $\var{ix}=\var{n}+1$.
\begin{equation}
\label{eq:edge-elements-are-complete-20}
\myparbox{
$\var{edge}[\var{n}]$ is a valid completion in the same Earley set as \Veim{base}
$\because$ assumption for step of the induction.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-30}
  \begin{gathered}
    \Veim{source} \in \Source{\var{chain}[\var{n}]} \\
    \land \; \left[ \var{source}, \var{edge}[\var{n}] \right]_\realm{LP}
      \in \LinkPairs{\var{edge}[\var{n}+1]} \\
    \because
      \text{\Eref{def-leo-edge-30} of Def ``Leo edge''
      \Dfref{leo-edge}}.
    \end{gathered}
\end{equation}
\todo{\var{Source} changed.  Recheck from here.}
\begin{equation}
\label{eq:edge-elements-are-complete-36}
\myparbox{$\var{edge}[\var{n}+1]$
is the result of an \Earley{} completer operation
whose ancestry includes
$\left[ \Veim{source},
  \var{edge}[\var{n}] \right]_\realm{LP}$
$\because$
\Obref{earley-op-by-direct-ancestor},
\Eref{edge-elements-are-complete-20},
\Eref{edge-elements-are-complete-30}.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-10}
\myparbox{
  \Vlimset{chain} is valid
  $\because$ AF theorem.}
\end{equation}
\todo{Fix next equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-15}
\myparbox{
  \Veim{source} is valid
  $\because$
  text{TODO \realm{LIM} validity definition},
  \Eref{edge-elements-are-complete-37-10}.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-20}
\myparbox{
  $\var{edge}[\var{n}]$ is valid
  $\because \Eref{edge-elements-are-complete-20}$.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-25}
\myparbox{
  $\var{edge}[\var{n}+1]$ is valid
  $\because$
  \Thref{earley-completer-is-consistent},
  \Eref{edge-elements-are-complete-36},
  \Eref{edge-elements-are-complete-37-15},
  \Eref{edge-elements-are-complete-37-20}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-38}
\myparbox{
  \Veim{source} is a penult
  $\because$ \Thref{leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-40}
\begin{gathered}
  \DR{\var{edge}[\var{n}+1]} = \Next{\Veim{source}} \\
  \because
  \Obref{earley-completer-summary}, \Eref{edge-elements-are-complete-36}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-42}
\myparbox{$\var{edge}[\var{n}+1]$ is a completion
  $\because$
  Def \var{Next} \Eref{def-next},
  \Eref{edge-elements-are-complete-38},
  \Eref{edge-elements-are-complete-40}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-44}
\myparbox{\Velement{edge}{\var{n}+1} is in the same Earley set as \VVelement{edge}{n}
  $\because$
\Eref{obs-earley-completer-summary-1}
  \Eref{obs-earley-completer-summary-4},
  \Eref{edge-elements-are-complete-36}. }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-46}
\myparbox{$\var{edge}[\var{n}+1]$ is in the same Earley set as \Veim{base}
  $\because$
  \Eref{edge-elements-are-complete-20},
  \Eref{edge-elements-are-complete-44}.  }
\end{equation}
The step and the induction follow
from
\Eref{edge-elements-are-complete-37-25},
\Eref{edge-elements-are-complete-42}, and
\Eref{edge-elements-are-complete-46}, so that
\begin{equation}
\label{eq:edge-elements-are-complete-50}
\myparbox{%
for all \var{n} such that
$0 \le \var{n} \le \Vlastix{edge}$,
\VVelement{edge}{n} is a valid completion in the same Earley set as \Veim{base}.%
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-52}
\myparbox{\Veimset{edge} is valid $\because$
\Dfref{leo-edge},
\Eref{edge-elements-are-complete-50}.}
\end{equation}
We have the theorem directly from
\Eref{edge-elements-are-complete-50} and
\Eref{edge-elements-are-complete-52}. \myqed
\end{proof}

\begin{theorem}[Edge Descendant Uniqueness]
\label{th:edge-descendant-uniqueness}
In an \Earley{} parse,
the descendant of a lower element of an edge
is unique.
\end{theorem}

\begin{proof}
We proceed by
assuming that an arbitrary lower element has
two distinct descendants,
for a reductio.
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-08}
\myparbox{Let \Veimset{edge} be an edge WLOG.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-10}
\myparbox{$\Veim{lo}=\VVelement{edge}{n}$ is a valid lower element of an edge
  $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-12}
\myparbox{$\Veim{desc} = \Velement{edge}{\var{n}+1}$
  is a valid direct descendant of \Veim{lo}
  $\because$ AF theorem.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-14}
\myparbox{\Veim{desc2} is a valid direct descendant of \Veim{lo}
$\because$ WLOG.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-16}
\myparbox{$\var{desc} \neq \Veim{desc2}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-18}
\myparbox{\Veim{lo} is complete
$\because$
\Thref{edge-elements-are-complete},
\Eref{th-edge-descendant-uniqueness-10}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-20}
\myparbox{\Veim{desc} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred}, \var{lo} \right]_\realm{LP}$
$\because$
\Obref{earley-op-by-direct-ancestor},
\Eref{th-edge-descendant-uniqueness-12},
\Eref{th-edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-21}
\begin{gathered}
\left[ \Veim{pred2}, \var{lo} \right]_\realm{LP}
  \in \LinkPairs{\Veim{desc2}} \\
  \land \;\; \Vlim{match} = \var{chain}[\var{n}] \\
  \land \;\; \Veim{pred2} \in \Source{\Vlim{match}} \\
  \because \text{Def of ``Leo edge''} \Dfref{leo-edge},
    \Eref{th-edge-descendant-uniqueness-12}
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-22}
\myparbox{\Veim{desc2} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred2}, \var{lo} \right]_\realm{LP}$
$\because$
\Obref{earley-op-by-direct-ancestor},
\Eref{th-edge-descendant-uniqueness-14},
\Eref{th-edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-24}
\myparbox{$\Current{\Veim{pred}} = \Origin{\var{lo}}$
  $\because$
  \Eref{obs-earley-completer-summary-1}
  and \Eref{obs-earley-completer-summary-2}
  in \Obref{earley-completer-summary};
  \Eref{th-edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-26}
\myparbox{$\Current{\Veim{pred2}} = \Origin{\var{lo}}$
  $\because$
  \Eref{obs-earley-completer-summary-1}
  and \Eref{obs-earley-completer-summary-2}
  in \Obref{earley-completer-summary};
  \Eref{th-edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-28}
\myparbox{$\Current{\Veim{pred2}} = \Current{\Veim{pred}}$
  $\because$
  \Eref{th-edge-descendant-uniqueness-24},
  \Eref{th-edge-descendant-uniqueness-26}.}
\end{equation}
%
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-30}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{lo}}$
  $\because$
  \Eref{obs-earley-completer-summary-3}
  in \Obref{earley-completer-summary};
  \Eref{th-edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-32}
\myparbox{$\Postdot{\Veim{pred2}} = \LHS{\var{lo}}$
  $\because$
  \Eref{obs-earley-completer-summary-3}
  in \Obref{earley-completer-summary};
  \Eref{th-edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-34}
\myparbox{$\Postdot{\Veim{pred}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{th-edge-descendant-uniqueness-30},
  \Eref{th-edge-descendant-uniqueness-32}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-36}
\myparbox{\var{pred2} is the source of \Vlim{match}
  $\because$
  \Eref{th-edge-descendant-uniqueness-21}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-38}
\myparbox{\var{pred2} is a penult
  $\because$
  \Eref{th-edge-descendant-uniqueness-36},
  \Thref{leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-40}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{def-penult},
  \Eref{th-edge-descendant-uniqueness-38}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-41}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred}}$
  $\because$
  \Eref{th-edge-descendant-uniqueness-34},
  \Eref{th-edge-descendant-uniqueness-40}.}
\end{equation}
%
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-42}
\begin{gathered}
\Veim{pred2} \in \LeoEligible{\Current{\Vlim{match}}} \\
\because \Eref{th-edge-descendant-uniqueness-36},
\text{TODO: Add ref to memoizer code}.
\end{gathered}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-44}
  \begin{gathered}
    \Veim{pred2} \in \LeoUnique{\Current{\Vlim{match}}} \\
    \because
    \Eref{th-edge-descendant-uniqueness-42}, \Eref{def-leo-eligible}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-46}
\begin{gathered}
\forall \; \Veim{x}
\left(
\begin{gathered}
\left(
  \begin{gathered}
  \Current{\var{pred2}} = \Current{\var{x}} \\
  \land \; \Penult{\var{pred2}} = \Postdot{\var{x}}
  \end{gathered}
\right)
\\
\implies \var{pred2} = \var{x}.
\end{gathered}
\right)
\\
\because \text{Def of \var{Leo-Unique} \Dfref{leo-unique}},
  \Eref{th-edge-descendant-uniqueness-44}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-48}
\Veim{pred2} = \Veim{pred} \because
\Eref{th-edge-descendant-uniqueness-28},
\Eref{th-edge-descendant-uniqueness-41},
\Eref{th-edge-descendant-uniqueness-46}.
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-50}
[\Veim{pred2}, \Veim{lo}] = [\Veim{pred}, \var{lo}] \because
\Eref{th-edge-descendant-uniqueness-48}.
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-52}
\myparbox{If two \Earley{} completer operations share
a link pair, they produce the same result
$\because$
\Obref{earley-completer-summary}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-descendant-uniqueness-54}
\Veim{desc2} = \Veim{desc} \because
\Eref{th-edge-descendant-uniqueness-20},
\Eref{th-edge-descendant-uniqueness-22},
\Eref{th-edge-descendant-uniqueness-50},
\Eref{th-edge-descendant-uniqueness-52}.
\end{equation}
With
\Eref{th-edge-descendant-uniqueness-52} we
have the reductio of
\Eref{th-edge-descendant-uniqueness-16}
and the theorem. \myqed
\end{proof}

\begin{theorem}[Edge Top is Border]
\label{th:edge-top-is-border}
If an \realm{EIM} is the non-trivial descendant of a lower edge \realm{EIM},
it is either another lower edge \realm{EIM},
or the descendant of the top of an edge.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-edge-top-is-border-22}
\myparbox{\Veim{lo} is a lower edge \realm{EIM} $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-24}
\myparbox{\Veim{no} is an \realm{EIM} which is not a lower edge $\because$ AF theorem, WLOG.}
\end{equation}
For the purposes of this proof, we define \Veimset{seq} as follows:
\begin{equation}
\label{eq:th-edge-top-is-border-26}
\myparbox{%
  \Veim{lo} and \Veim{no} are
  part of a
  \Veimset{seq}, which is such that
  \begin{itemize}
  \item $\Velement{seq}{0} = \Veim{lo}$;
  \item $\Velement{seq}{\Vlastix{seq}} = \Veim{no}$; and
  \item for all \var{i} such that $0 < \var{i} \le \Vlastix{seq}$,
  \eim{(\VVelement{seq}{i})}
  is the direct descendant of \eim{(\Velement{seq}{\Vdecr{i}})}.
  \end{itemize}%
}
\end{equation}
Note that the criteria in \Veimset{seq} do not necessarily determine \var{seq}
uniquely.
We make an arbitrary choice among the possible value of \var{seq},
without loss of generality.
\begin{equation}
\label{eq:th-edge-top-is-border-27}
\myparbox{\VVelement{seq}{t} is the first element
of \var{seq}
that is not a lower edge \realm{EIM}.
$\because$ \Eref{th-edge-top-is-border-26}, WLOG.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-28}
\myparbox{\VVelement{seq}{t}
is the top of an edge, or it is not an edge \realm{EIM}
$\because$
Def of ``Leo edge'' \Dfref{leo-edge},
\Eref{th-edge-top-is-border-27}.}
\end{equation}
%
We now consider
\Velement{seq}{\Vdecr{t}}.
%
\begin{equation}
\label{eq:th-edge-top-is-border-30}
\myparbox{
\Velement{seq}{\Vdecr{t}} is a lower edge \realm{EIM}
$\because$
\Eref{th-edge-top-is-border-27}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-32}
\myparbox{One of the descendants of a lower edge \realm{EIM}
must be either another lower edge \realm{EIM} or the top of
an edge.
$\because$ Def of ``Leo edge'' \Dfref{leo-edge}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-34}
\myparbox{%
The descendant of a lower edge \realm{EIM}
is unique
$\because$ \Thref{edge-descendant-uniqueness}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-36}
\myparbox{%
The unique descendant of
\Velement{seq}{\Vdecr{t}} is
another lower edge \realm{EIM} or the top of
an edge
$\because$
\Eref{th-edge-top-is-border-30},
\Eref{th-edge-top-is-border-32},
\Eref{th-edge-top-is-border-34}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-38}
\myparbox{\VVelement{seq}{t} is the unique descendant
of \Velement{seq}{\Vdecr{t}}.
$\because$
\Eref{th-edge-top-is-border-30}
\Eref{th-edge-top-is-border-34}.}`
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-40}
\myparbox{\VVelement{seq}{t} is the top of an edge
$\because$
\Eref{th-edge-top-is-border-28},
\Eref{th-edge-top-is-border-36},
\Eref{th-edge-top-is-border-38}.}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-42}
\myparbox{\Veim{no} is a descendant of
\VVelement{seq}{t}.
$\because$ \Eref{th-edge-top-is-border-26}
}
\end{equation}
\begin{equation}
\label{eq:th-edge-top-is-border-44}
\myparbox{\Veim{no} is a descendant of
the top of an edge.
$\because$ \Eref{th-edge-top-is-border-40}
\Eref{th-edge-top-is-border-42}.}
\end{equation}
Since the choice of \Veim{no} was without loss of generality,
with \Eref{th-edge-top-is-border-44} we have the
theorem. \myqed
\end{proof}

\begin{theorem}[Leo edge top]
\label{th:leo-edge-top}
In a \Marpa parse,
let \Vlimset{chain} be a valid \realm{LIM} chain,
and let \Veim{base} be a valid \realm{EIM} which matches \var{chain}.
Then the top of their edge is
\begin{equation}
\label{eq:th-leo-edge-top-05}
\begin{gathered}
\left[ \DR{\Veim{source}}, \Origin{\var{source}}
\right]@\Current{\var{base}}, \\
\text{where $\Veim{source} \in \Source{\Velement{chain}{\Vlastix{chain}}}$.}
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-top-10}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the top of the edge
$\because$ Def of ``Leo edge'' \Dfref{leo-edge}.}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-12}
\begin{gathered}
\left[ \Source{\Velement{chain}{\decr{\Vlastix{edge}}}}, \Velement{edge}{\decr{\Vlastix{edge}}} \right]_\realm{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \text{\Eref{def-leo-edge-30} in \Dfref{leo-edge}}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-1}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{chain}}}, \Velement{edge}{\Vlastix{chain}} \right]_\realm{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Thref{leo-edge-length}, \Eref{th-leo-edge-top-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-2}
\begin{gathered}
\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\realm{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Eref{th-leo-edge-top-05}, \Eref{th-leo-edge-top-13-1}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-14}
\myparbox{\Velement{edge}{\Vlastix{chain}} is a completion
$\because$ \Thref{edge-elements-are-complete}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-16}
\begin{gathered}
\Current{\Velement{edge}{\Vlastix{chain}}} = \Current{\var{base}} \\
\because \Thref{edge-elements-are-complete}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-18}
\begin{gathered}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the
result of an \Earley{} completer
operation with link pair
$\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\realm{LP}$%
} \\
\because \Obref{earley-op-by-direct-ancestor}, \Eref{th-leo-edge-top-13-2},
  \Eref{th-leo-edge-top-14}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-20}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Velement{edge}{\Vlastix{chain}}} \\
\because
\Obref{earley-completer-summary},
\Eref{th-leo-edge-top-18}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-22}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}} \\
\because \Eref{th-leo-edge-top-16}, \Eref{th-leo-edge-top-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-24}
\myparbox{The top of \Veimset{edge} is
  $\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}}$ \\
  $\because$ \Eref{th-leo-edge-top-10}, \Eref{th-leo-edge-top-22}.
  \myqed
}
\end{equation}
\end{proof}


\begin{theorem}[\op{LeoReducer} is consistent]
\label{th:leo-reducer-is-consistent}
The \op{LeoReducer} sub-op
of the \Marpa \op{Reducer} operation
preserves validity.
\end{theorem}
\begin{proof}
\todo{prove this}
\myqed
\end{proof}

\begin{theorem}[\Marpa is consistent]
\Marpa is consistent.
That is, every \realm{PIM} created by a \Marpa parser
is valid.
\end{theorem}

\begin{proof}
We assume for a reductio,
that there is \Marpa parse that creates an invalid \realm{PIM}.
This means that there is a first invalid \realm{PIM} created
by that \Marpa parse.
Inspection of the \Marpa algorithm
\Chref{algorithm} shows
that \realm{PIM}s are only created by operations.

We now proceed by elimination of subcases,
where our subcases are the operations
of the \Marpa algorithm.
We first note that an operation creates
the first invalid \realm{PIM} only if it does not preserve
validity.

The \Marpa initializer operation creates the
same \realm{PIM}s as the \Earley{} initializer operation
\Obref{earley-set-0}.
The \Earley{} initializer operation preserves
validity because the \Earley{} algorithm is
correct
\Thref{earley-is-correct}.
Note in the case of the Initializer operation,
validity is preserved vacuously ---
no \realm{PIM} is in the ancestry of the Initializer
operation.

The \Marpa scanner operation
creates the same \realm{PIM}s as the \Earley{}
scanner operations
\Obref{earley-scanner-op}.
The \Marpa predicter operation
creates the same \realm{PIM}s as the \Earley{}
predicter operations
\Obref{earley-predicter-op}.
These two operations preserve
validity because the \Earley{} algorithm is
correct
\Thref{earley-is-correct}.

We have shown that the memoizer operation
preserves validity
\Thref{memoizer-consistency}.
The remaining subcase is the \Marpa \op{Reducer} operation.

The \Marpa \op{Reducer} operation consists of two
sub-ops:
\op{Earley-reducer} and
\op{LeoReducer}.
We have shown that the
\op{LeoReducer} preserves validity
\Thref{leo-reducer-is-consistent}.

From
\Obref{earley-completer-op}
and \Aref{alg:earley-reducer-op},
\op{Earley-reducer}, creates the \realm{EIM} that
the \Earley{} completer operation would have
created, unless there is a \realm{LIM} that ``blocks''
\Marpa's \op{Earley-reducer}.
Therefore the \realm{PIM}s created by
the \Marpa \op{Earley-reducer} sub-op
are a subset of those created by the \Earley{}
completer operation.
We know the \Earley{} completer operation preserves
validity
\Thref{earley-completer-is-consistent},
and therefore any operation which creates some subset of the
\realm{EIM}s that
the \Earley{} completer operation creates also
preserves validity.

With this we have shown the last subcase,
and may conclude that no \Marpa operation creates the
first invalid \realm{PIM},
and therefore no \Marpa operation creates
an invalid \realm{PIM}.
Therefore, \Marpa is consistent.
\myqed
\end{proof}

\begin{definition}[Hidden and Visible \realm{PIM}s]
An \realm{EIM} is \dfn{hidden} if it is an element of the inside
of a Leo edge.
All other \realm{PIM}s are \dfn{visible}.
\end{definition}

\begin{theorem}[\Marpa visible \realm{PIM}s are complete]
\Marpa is complete, except for the hidden
\realm{PIM}s.
\end{theorem}

\begin{proof}
\todo{prove this}
\myqed
\end{proof}

Intuitively
the Earley tables are ``complete as far as X'',
where X is some point in running of the \Marpa algorithm,
if they contain all the visible valid \realm{PIM}s
for X and every prior to it in
in the \Marpa algorithm.
\todo{Replace DFPP ("definition for the purpose of presentation")
    throughout, with something more sensible and less awkward.}
\begin{equation}
\myparbox{The Earley tables of a \Marpa parse
are ``complete as far as''
Earley set \var{j},
if every Earley set, \VVelement{S}{i}, $0 \le \var{i} \le \var{j}$,
contains all the valid visible \realm{PIM}s
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa parse
are ``complete as far as''
the \Marpa \var{Scanner} operation of Earley set \var{j},
if they are complete as far as Earley set $\Vdecr{j}$,
and if they contain all the visible valid \realm{EIM}s with a predot terminal
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa parse
are ``complete as far as''
the \Marpa \op{Reducer} operation of Earley set \var{j},
if they are complete as far as Earley set $\Vdecr{j}$,
and if they contain all the visible valid confirmed \realm{EIM}s
$\because$ DFPP.
}
\end{equation}

Intuitively, ``visible depth''
is depth counted in terms of
visible confirmed \realm{EIM} causes.
We will write \var{Depth}(\Veim{x}) for the visible depth
of \Veim{x}.
When speaking of several \realm{EIM}s,
we will say that the one with the
numerically lowest depth value
is the ``deepest''.

\begin{equation}
\label{eq:def-complete-as-far-as-20}
\myparbox{$\var{Depth}(\Veim{x}) = 1$
if $\Predot{\Veim{x}} \in \Term{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-22}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{bottom})+1$,
if \var{x} is the top of a Leo edge,
where
\Veim{bottom} is the bottom of the Leo edge
$\because$ DFPP.}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-24}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{cuz})+1$,
where \Veim{cuz} is the ``deepest'' cause of \Veim{x},
if \var{x} is a visible confirmed \realm{EIM} where
\var{x} is not the top of a Leo edge and
$\Predot{\var{x}} \in \NT{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-26}
\myparbox{$\var{Depth}(\Vpim{pim}) = \Lambda$
if \Vpim{pim} is a \realm{LIM},
a hidden \realm{EIM},
or a prediction.
In other words,
visible depth is
defined only for visible confirmed \realm{EIM}s
$\because$ DFPP.
}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-25}
\begin{gathered}
\forall \; \Veim{x} \; ( \var{Depth}(\var{x}) \ge 1 \lor
 \var{Depth}(\var{x}) = \Lambda) \\
\because
\Eref{def-complete-as-far-as-20},
\Eref{def-complete-as-far-as-22},
\Eref{def-complete-as-far-as-24},
\Eref{def-complete-as-far-as-26}.
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-28}
\myparbox{The Earley tables are ``complete as far as''
visible depth \var{n} in Earley set \var{j},
if they are complete as far as Earley set $\Vdecr{j}$,
and if they contain all the visible valid \realm{EIM}s of visible depth
\var{n} or less
$\because$ DFPP.
}
\end{equation}

\begin{theorem}
\label{th:marpa-reducer-completeness}
Let \VVelement{S}{j} be an Earley set in a \Marpa parse,
$\var{j} > 0$.
If the parse is complete as far as the
\Marpa \var{Scanner} operation of
Earley set \VVelement{S}{j},
then the parse is complete as far as the
\Marpa \op{Reducer} operation of
Earley set \VVelement{S}{j}.
\end{theorem}

\begin{proof}
This proof will be by induction on
visible depth.

We take for the induction hypothesis
\begin{equation}
\label{th:marpa-reducer-completeness-28}
\myparbox{After a \Marpa \op{Reducer} operation at \VVelement{S}{j},
\VVelement{S}{j} is complete
with respect to the
the \realm{EIM}s of visible depth \var{ix}.}
\end{equation}

\todo{finish this proof}
\myqed
\end{proof}

\begin{theorem}
\label{th:marpa-visible-depth-completeness}
Let \VVelement{S}{j} be an Earley set in a \Marpa parse,
$\var{j} > 0$.
If the parse is complete as far as
visible depth \var{n}
in Earley set \VVelement{S}{j},
then the parse is complete as far as
visible depth $\var{n}+1$
in
Earley set \VVelement{S}{j}.
\end{theorem}
\begin{proof}
The proof is by a reductio.
\todo{presence in physical Earley set (PES?) vs theoretical {ES?}.}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-08}
\myparbox{Let $\Vpim{rez}@\VVelement{S}{j}$ be a valid \realm{PIM} where
$\var{Depth}(\var{rez}) = \var{n}+1 \land
  \neg \var{rez} \in \VVelement{S}{j}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-10}
\myparbox{$\Vpim{rez} = \Veim{rez} \because$ only \realm{EIM}s have
a visible depth
\Eref{def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-12}
\myparbox{\Veim{rez} is confirmed
$\because$ only confirmed \realm{EIM}s have a visible depth
\Eref{def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-14}
\begin{gathered}
\var{Depth}(\Veim{rez}) > 1 \because
\Eref{def-complete-as-far-as-25},
\Eref{th-marpa-visible-depth-completeness-08}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-16}
\Predot{\Veim{rez}} \in \NT{\var{g}} \because
\Eref{def-complete-as-far-as-20},
\Eref{th-marpa-visible-depth-completeness-12},
\Eref{th-marpa-visible-depth-completeness-14}.
\end{equation}
\todo{finish proof}
\myqed
\end{proof}

\todo{Prove Marpa + Reverse Leo = Original Earley}

\begin{theorem}[Useful completions are disjoint or contained]
In an unambiguous grammar,
in every pair of useful complete \realm{EIM}s,
one is either contained
in the other,
or they are disjoint
\end{theorem}
\begin{proof}
\todo{Need this proof?}
\myqed
\end{proof}

\begin{theorem}[Useful RR bound]
\label{th:useful-RR-count}
Let $\var{p} = [ \Vint{g}, \Vstr{w} ]$
be an unambiguous parse,
where $\var{n} = \size{\Vstr{w}}$.
The number of useful complete right recursive \realm{EIM}s
in \var{p} is less than or equal to $2 \times \Vdecr{n}$,
or \On{}.
\end{theorem}

\begin{proof}
\todo{Need this proof?}
\myqed
\end{proof}

\chapter{Reversing Leo memoization}
\label{chap:reverse-leo}

\todo[prepend, caption={``Reversing Leo memoization'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\todo{Finish writing this section}

At evaluation time,
it is desirable to expand the Leo memoizations,
but in almost all practical cases,
only the useful \realm{EIM}s
need to be expanded.

\begin{theorem}[Useful \realm{EIM} count]
Let $[\Vint{g}, \Vstr{partial}]$ be a \Marpa partial parse.
the number of useful \realm{EIM}s is \On{},
where $\var{n} = \Vsize{partial}$.
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

As a result,
even if the time and space
required to expand Leo memoization
during evaluation
are taken into account,
the time and space complexity of
a right recursion become
$\On{} + \On{} = \On{}$.

\begin{theorem}[Leo reversal is linear]
The time and space to reverse the Leo memoization
in an unambiguous parse is \On{}.
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

\chapter{LRR complexity}
\label{chap:lrr-complexity}

\todo[prepend, caption={``LRR complexity'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{theorem}[A grammar which allows an ambiguity is ambiguous: duplicate?]
\label{th:ambiguous-links-pair}
\footnote{This theorem and its proof are adapted from
Lemma 4.6 in
\cite[Vol. 1, p. 325-326]{AU1972}.
}%
Let \Vcfg{g} be a CFG.
If there is an input \Vstr{w} such that a valid
confirmed \realm{EIM} has more than one link pair,
then \var{g} is ambiguous.
\end{theorem}

\begin{proof}
For the theorem, we assume without loss of generality that
\begin{gather}
\label{eq:ambiguous-links-pair-10a}
\Veim{adjoined} = \left[ \left[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs2} \right], \Vorig{i} \right], \\
\label{eq:ambiguous-links-pair-10b}
\text{$\Veim{adjoined} @ \VVelement{S}{j}$ is valid, and} \\
\label{eq:ambiguous-links-pair-10d}
\LinkPairs{\Veim{adjoined}} \ge 1.
\end{gather}

The proof proceeds by eliminating subcases within a reductio.
We assume for the reductio that
\begin{equation}
\label{eq:ambiguous-links-pair-12}
\text{\var{g} is not ambiguous.}
\end{equation}

Confirmed \realm{EIM}s are only adjoined by the Scanner and the Completer.

\todo{Does this duplicate the proof in the ambiguity section?}
\myqed

\end{proof}

\section{Complexity of each Earley item}

% TODO: Is this theorem used?
\begin{theorem}[Right recursion bound]
\label{th:leo-right-recursion}
In a \Marpa internal grammar,
either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa internal grammar.
and let the constant $\var{c} = \NT{\var{g}}$.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
Since we assumed for the theorem that \var{g}
is a \Marpa internal grammar,
there will be no nulling symbols,
and therefore
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.
So part of the rightmost derivation must take the form
\begin{equation}
\label{eq:-th-leo-right-recursion-50}
\Vstr{earlierPrefix} \cat \Vsym{A} \deplus \Vstr{laterPrefix} \cat \Vsym{A}.
\end{equation}
But the first step of the derivation in
\Eref{-th-leo-right-recursion-50}
must use a rule of the form
\begin{equation*}
\Vsym{A} \de \Vstr{RHSprefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\myqed
\end{proof}

\section{The complexity results}

\begin{lemma}[Unique Handle]
\label{lem:handle-unique}
Let \Vint{g} be a LR($\pi$) grammar where $\pi$ is
a left congruence.
Let
\begin{gather}
\label{eq:handle-unique-10a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:handle-unique-10b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB},
\end{aligned}
\\
\label{eq:handle-unique-10c}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB},
\\
\label{eq:handle-unique-10d}
\text{and} \;\;
\Vterm{suffixA} \iff \Vterm{suffixB} (\text{mod $\pi$}),
\end{gather}
where
\begin{gather*}
\set{ \Vterm{suffixA}, \Vterm{suffixB},
\Vterm{postA}, \Vterm{postB},
} \subseteq \Term{\var{g}}^\ast
\;\; \\
\text{and $\set{ \Vsym{lhsA}, \Vsym{lhsB} } \in \NT{\var{g}}$}.
\end{gather*}
Then
\begin{gather}
\label{eq:handle-unique-15a}
\Vstr{lhsA} = \Vstr{lhsB},
\\
\label{eq:handle-unique-15b}
\Vstr{preA} = \Vstr{preB},
\\
\label{eq:handle-unique-15c}
\Vstr{rhsA} = \Vstr{rhsB},
\;\; \text{and} \\
\label{eq:handle-unique-15d}
\Vterm{postA} = \Vterm{postB}.
    \quad \thEnd
\end{gather}
\end{lemma}

\begin{proof}
Using \Eref{handle-unique-10c}
we may rewrite
\Eref{handle-unique-10b}
to
\begin{equation}
\label{eq:handle-unique-20}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixB},
\end{aligned}
\end{equation}
We assumed for the theorem that
\Vint{g} is a left congruence,
so from
\Eref{handle-unique-10d}
we have
\begin{equation}
\label{eq:handle-unique-23}
\Vterm{postA}\Vterm{suffixA} \iff \Vterm{postA}\Vterm{suffixB} (\text{mod $\pi$}).
\end{equation}
\Vint{g} is LR and from \Dfref{LR},
\Eref{handle-unique-10a},
\Eref{handle-unique-20}, and
\Eref{handle-unique-23}
we conclude that
\begin{gather}
\label{eq:handle-unique-26a}
\Vsym{lhsA} =
\Vsym{lhsB},
\\
\label{eq:handle-unique-26b}
\Vstr{preA} =
\Vstr{preB}, \;\; \text{and}
\\
\label{eq:handle-unique-26c}
\Vterm{postA}\Vterm{suffixB} =
\Vterm{postB}\Vterm{suffixB}.
\end{gather}
From
\Eref{handle-unique-26c}, we know that
\begin{equation}
\label{eq:handle-unique-30}
\Vterm{postA} = \Vterm{postB}
\end{equation}
and from
\Eref{handle-unique-10c},
\Eref{handle-unique-26b}, and
\Eref{handle-unique-30},
we have
\begin{equation}
\label{eq:handle-unique-33}
\Vstr{rhsA} = \Vstr{rhsB}.
\end{equation}

To show the theorem, we need
\Eref{handle-unique-15a},
\Eref{handle-unique-15b},
\Eref{handle-unique-15c}, and
\Eref{handle-unique-15d}.
We have
\Eref{handle-unique-15a} from \Eref{handle-unique-26a};
\Eref{handle-unique-15b} from \Eref{handle-unique-26b};
\Eref{handle-unique-15c} from \Eref{handle-unique-33};
and
\Eref{handle-unique-15d} from \Eref{handle-unique-30}.
\myqed
\end{proof}

\begin{theorem}
\label{th:addendum-lemma1}
This theorem and its proof are adapted from Lemma 1 of \cite{Leo1991a}.
Let
\Vint{g}
be an
$LR(\pi)$
grammar,
where the partition
$\pi$
is a left congruence
of
$\Term{\Vint{g}}^\ast$.
If for some
\[
\Vterm{suffixA}, \Vterm{suffixB} \in \Term{\var{g}}^\ast,
\]
we have
\begin{gather}
\label{eq:left-congruence1}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfA}\Vterm{suffixA} \\
                  & \destar \Vstr{prefix}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:left-congruence2}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfB}\Vterm{suffixB} \\
  & \destar \Vstr{prefix}\Vterm{suffixB},
\end{aligned}
\end{gather}
and
\begin{equation}
\label{eq:left-congruence3}
\Vterm{suffixA} \iff \Vterm{suffixB} \pmod \pi,
\end{equation}
then
\begin{equation}
\label{eq:left-congruence4}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA}
\end{equation}
\end{theorem}

\begin{proof}
Assume that
\Eref{left-congruence1},
\Eref{left-congruence2}, and
\Eref{left-congruence3} hold.
Expand \Eref{left-congruence1} into
\begin{equation}
\label{eq:left-congruence10a}
\begin{aligned}
& \var{sfseqA}[\var{lenA}]\cat\Vterm{suffixA} = \Vstr{sfA}\cat\Vterm{suffixA}
\\
& \qquad \xderives{R} \var{sfseqA}[\Vdecr{lenA}]\cat\Vterm{suffixA} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqA}[1]\cat\Vterm{suffixA}] = \Vstr{prefix}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
and expand \Eref{left-congruence2} into
\begin{equation}
\label{eq:left-congruence10b}
\begin{aligned}
& \var{sfseqB}[\var{lenB}]\cat\Vterm{suffixB} = \Vstr{sfB}\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \var{sfseqB}[\Vdecr{lenB}]\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqB}[1]\cat\Vterm{suffixB} = \Vstr{prefix}\Vterm{suffixB} \\
\end{aligned}
\end{equation}

Let
\begin{equation}
\label{eq:left-congruence10-2}
\var{len} = \min( \var{lenA} ,\var{lenB})
\end{equation}
We seek to show
\begin{equation}
\label{eq:left-congruence11}
\forall \; \var{k} \; \left( \;
1 \le \var{k} \le \var{len}
\implies
\var{sfseqA}[\var{k}] = \var{sfseqB}[\var{k}]
\; \right).
\end{equation}
We assume, for a reductio, that
\var{j}, $1 \le \var{j} \le \var{len}$,
is the smallest integer such that
\begin{equation}
\label{eq:left-congruence12a}
\var{sfseqA}[\var{j}] \neq \var{sfseqB}[\var{j}]
\end{equation}
so that
\begin{equation}
\label{eq:left-congruence12b}
\forall \; \var{i} \left( \;
1 \le \var{i} < \var{j}
\implies
\var{sfseqA}[\var{i}] = \var{sfseqB}[\var{i}]
\; \right).
\end{equation}

From \Eref{left-congruence10a}
and \Eref{left-congruence10b} we know that
$\var{sfseqA}[1] = \var{sfseqB}[1] = \Vstr{prefix}$,
so that $\var{j} > 1$.
With this, we can write
for some $\Vterm{postA} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqA}[\var{j}]\cat\Vterm{suffixA} \\
& \qquad = \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15a1}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\var{sfseqA}[\Vdecr{j}]
\end{equation}
and similarly, we can write
for some $\Vterm{postB} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqB}[\var{j}]\cat\Vterm{suffixB} \\
& \qquad = \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad = \var{sfseqB}[\Vdecr{j}]\cat\Vterm{suffixB}
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15b1}
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} =
\var{sfseqB}[\Vdecr{j}].
\end{equation}
Then
\begin{gather}
\label{eq:left-congruence18}
\begin{gathered}
\var{sfseqA}[\Vdecr{j}] = \var{sfseqB}[\Vdecr{j}], \\
\because \Eref{left-congruence12b}, \;\; \text{and}
\end{gathered}
\\[5pt]
\label{eq:left-congruence19}
\begin{gathered}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} \\
\because \Eref{left-congruence15a1},
\Eref{left-congruence15b1},
\Eref{left-congruence18}.
\end{gathered}
\end{gather}

Using
\Eref{left-congruence3},
\Eref{left-congruence15a},
\Eref{left-congruence15b},
\Eref{left-congruence19}, and
\Lmref{handle-unique} we
have
\begin{gather}
\label{eq:left-congruence25a}
\Vstr{lhsA} = \Vstr{lhsB}, \\
\label{eq:left-congruence25b}
\Vstr{preA} = \Vstr{preB}, \\
\label{eq:left-congruence25c}
\Vstr{rhsA} = \Vstr{rhsB}, \;\; \text{and} \\
\label{eq:left-congruence25d}
\Vterm{postA} = \Vterm{postB}.
\end{gather}
Therefore
\begin{gather}
\label{eq:left-congruence30a}
\begin{gathered}
\var{sfseqA}[\var{j}] = \Vstr{preA}\Vsym{lhsA}\Vterm{postA} \\
\because \Eref{left-congruence15a},
\end{gathered}
\\
\label{eq:left-congruence30b}
\begin{gathered}
\var{sfseqB}[\var{j}] = \Vstr{preB}\Vsym{lhsB}\Vterm{postB} \\
\because \Eref{left-congruence15b},
\;\; \text{and}
\end{gathered}
\\
\label{eq:left-congruence50}
\begin{gathered}
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \\
\because \Eref{left-congruence25a},
\Eref{left-congruence25b},
\Eref{left-congruence25d},
\Eref{left-congruence30a},
\Eref{left-congruence30b}.
\end{gathered}
\end{gather}

\Eref{left-congruence50}
contradicts
\Eref{left-congruence12a},
our assumption for the reductio,
and allows us to conclude that
\begin{equation}
\label{eq:left-congruence52}
\begin{gathered}
\forall \; \var{j} \left (1 \le \var{j} \le \var{len}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right).
\end{gathered}
\end{equation}

Assume, without loss of generality,
\begin{equation}
\label{eq:left-congruence53}
\var{lenA} \ge \var{lenB}.
\end{equation}
Then
\begin{equation}
\label{eq:left-congruence53-2}
\var{lenB} = \var{len} \because
\Eref{left-congruence10-2},
\Eref{left-congruence53},
\end{equation}
\begin{equation}
\label{eq:left-congruence54}
\begin{gathered}
\forall \; \var{j} \left (1 \le \var{j} < \var{lenB}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right) \\
\because \Eref{left-congruence52}, \Eref{left-congruence53-2},
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:left-congruence55a}
\begin{aligned}
& \Vstr{sfA}\Vterm{suffixA} = \var{sfseqA}[\var{lenA}]\Vterm{suffixA} \because
\Eref{left-congruence10a} \\
& \qquad \xderives{R\ast} \var{sfseqA}[\var{lenB}]\Vterm{suffixA}
\because
\Eref{left-congruence10b},
\Eref{left-congruence52},
\Eref{left-congruence53-2},
\\
& \qquad = \var{sfseqB}[\var{lenB}]\Vterm{suffixA}
\because \Eref{left-congruence52}, \Eref{left-congruence53-2}
\\
& \qquad = \Vstr{sfB}\Vterm{suffixA}
\because \Eref{left-congruence10b},
\end{aligned}
\end{equation}
\begin{equation}
\begin{gathered}
\label{eq:left-congruence60}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA} \\
\because \Eref{left-congruence55a}
\end{gathered}
\end{equation}
and from \Eref{left-congruence60}
we have \Eref{left-congruence4}
and the theorem.
\myqed
\end{proof}

\begin{theorem}
\label{th:addendum-lemma2}
\footnote{
This theorem and its proof are adapted from Lemma 2 of \cite{Leo1991a}.}%
Let
\Vint{g}
be an
unambiguous grammar,
and let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vint{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$.}
\end{gather*}
If, for some $\Vterm{input1}, \Vterm{input2}, \Vterm{input3A}, \Vterm{input3B} \in
\Term{\var{g}}^\ast$,
we have
\begin{align}
\label{eq:equal-prefix1}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \xderives{R/ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A} \\
\end{aligned}
\\
\intertext{and}
\label{eq:equal-prefix2}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3B} \\
\end{aligned}
\end{align}
and
\begin{equation}
\label{eq:equal-prefix3}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs} \\
\text{or} \;\; \Vstr{beforeB}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}
then
\begin{equation}
\label{eq:equal-prefix4}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
\end{theorem}

\begin{proof}
We assume, for a reductio, that
\Eref{equal-prefix1},
\Eref{equal-prefix2}, and
\Eref{equal-prefix3}
are all true,
but that
\Eref{equal-prefix4} is false.
By symmetry,
we may assume that
\[
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs},
\]
without loss of generality.
If
\begin{equation}
\label{eq:equal-prefix5}
\Vstr{beforeA}\Vsym{lhs} \xderives{(0)}
\Vstr{beforeB}\Vsym{lhs},
\end{equation}
we have
\Vstr{beforeA} = \Vstr{beforeB}
directly from
\Eref{equal-prefix5},
which contradicts our assumption.
For the rest of this proof, therefore,
we assume  that
\begin{equation}
\label{eq:equal-prefix8}
\Vstr{beforeA}\Vsym{lhs} \xderives{R+}
\Vstr{beforeB}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar like \Vint{g},
a right derivation is unique,
so that combining
\Eref{equal-prefix1},
\Eref{equal-prefix2}, and
\Eref{equal-prefix8},
we see that the right derivation of
must take  the form
\begin{align}
\label{eq:equal-prefix10a}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10b}
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10c}
                  & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10d}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10e}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
\label{eq:equal-prefix10f}
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{align}

Assume, for an inner reductio,
that, in the derivation from \Eref{equal-prefix10b}
to \Eref{equal-prefix10c},
\Vstr{rhs2} does not produce the \Vsym{lhs} of
\Eref{equal-prefix10c}.
Then we must have
\begin{equation}
\label{eq:equal-prefix11}
\Vstr{rhs2} \destar \epsilon.
\end{equation}
But, since \Vint{g} is a \Marpa internal grammar,
and there are no nullable symbols in a \Marpa  grammar,
we have \Eref{equal-prefix11} only if
\begin{equation}
\label{eq:equal-prefix12}
\Vstr{rhs2} = \epsilon,
\end{equation}
which is contrary to assumption for the theorem.
This shows the inner reductio,
and allows us to write
\begin{equation}
\label{eq:equal-prefix15}
\Vstr{rhs2} \destar \Vstr{shim}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar, a right derivation must be unique,
so we can substitute
\Eref{equal-prefix15} into
\Eref{equal-prefix1},
writing
\begin{align}
\label{eq:equal-prefix20a}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A}
\because \Eref{equal-prefix10a}
\\
\label{eq:equal-prefix20b}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{rhs2}\Vterm{input3A}
\because \Eref{equal-prefix10b}
\end{aligned}
\\
\label{eq:equal-prefix20c}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vsym{lhs}\Vterm{input3A}
\because \Eref{equal-prefix15}
\end{aligned}
\\
\label{eq:equal-prefix20d}
&
\begin{aligned}
& \xderives{R} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \qquad \because \Eref{equal-prefix10d}
\end{aligned}
\\
\intertext{where \Vstr{beforeA}\Vstr{rhs1}\Vstr{shim} = \Vstr{beforeB}}
\label{eq:equal-prefix20e}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A}
  \because \Eref{equal-prefix10e}
\end{aligned}
\\
\label{eq:equal-prefix20f}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vterm{input2}\Vterm{input3A}
\because \Eref{equal-prefix1}.
\end{aligned}
\end{align}

We have the derivation step
\Eref{equal-prefix20e}--\Eref{equal-prefix20f}
because
\Eref{equal-prefix20f}
occurs in \Eref{equal-prefix1}, and
\Eref{equal-prefix20f}
must occur in the derivation after
\Eref{equal-prefix20e}
because, in a right derivation,
any non-terminals in
\Eref{equal-prefix20e} must be expanded
before we reach step
\Eref{equal-prefix20f}.

In fact, comparing \Eref{equal-prefix20e}
to \Eref{equal-prefix20f},
we see that
\begin{equation}
\label{eq:equal-prefix30}
\Vstr{shim}\Vstr{rhs1} \destar \epsilon,
\end{equation}
which in a \Marpa internal grammar means the
\begin{equation}
\label{eq:equal-prefix31}
\Vstr{shim}\Vstr{rhs1} = \epsilon.
\end{equation}

We now proceed to show that
\Eref{equal-prefix30} implies that \Vint{g}
has a cycle.
\begin{align}
\label{eq:equal-prefix35a}
& \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
\\
\label{eq:equal-prefix35b}
& \begin{aligned}
& \xderives{\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \quad \because \text{derivation \Eref{equal-prefix20b}--\Eref{equal-prefix20d}}
\end{aligned}
\\
\label{eq:equal-prefix35c}
& \begin{aligned}
& = \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
  \because \Eref{equal-prefix31}
\end{aligned}
\end{align}
The derivation \Eref{equal-prefix35a}--%
\Eref{equal-prefix35c}
is a cycle, which can only happen if
\Vint{g} is ambiguous.
(Note that
\Eref{equal-prefix35a}--\Eref{equal-prefix35c}
is not and does not need to be a right-derivation.)
It is contrary to assumption for the theorem
for \Vint{g} to be ambiguous,
and this
gives us the reductio
and the theorem.
\myqed
\end{proof}

\begin{theorem}[Incompletions bound]
\label{th:incompletion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an LRR
grammar.
There is a constant \var{c} such
that the number of incomplete items
in each set \VVelement{S}{j} is at most \var{c}.
\end{theorem}

\begin{proof}
Let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vint{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$}.
\end{gather*}
Let \Vterm{input1},
\Vterm{input2},
\Vterm{input3A} and
\Vterm{input3B} be elements
of $\Term{\Vint{g}}^\ast$.

Let $\var{j} = \size{\Vterm{input1}}$.
If two distinct derivations both put incomplete \realm{EIM}s into
\VVelement{S}{j}, they have the form
\begin{align}
\label{eq:incompletion-bound-1}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{aligned}
\\
\intertext{and}
\label{eq:incompletion-bound-2}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3B}.
\end{aligned}
\end{align}

Assume that
\Eref{incompletion-bound-1} and
\Eref{incompletion-bound-2}
are in the same cell of $\pi$,
so that
\begin{equation}
\label{eq:incompletion-bound-3}
\Vterm{input3A} \iff \Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}
which, since $\pi$ is a left congruence,
is the same as
\begin{equation}
\label{eq:incompletion-bound-5}
\Vterm{input2}\Vterm{input3A} \iff \Vterm{input2}\Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}

Since an LRR grammar is
an LR($\pi$) grammar where
$\pi$ is a left congruence of $\Term{\Vint{g}}^\ast$,
we can use
\Thref{addendum-lemma1}.
We set equivalents between the variables of this
theorem and
\Thref{addendum-lemma1} as follows:

\begin{align}
& \begin{aligned}
& \text{\Vstr{sfA} in \Thref{addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeA}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixA} in \Thref{addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3A} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vstr{sfB} in \Thref{addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeB}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixB} in \Thref{addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3B} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{prefix} in \Thref{addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input1}\Vterm{input2} in this proof}
\end{aligned}
\end{align}

\Thref{addendum-lemma1} tells us that
\begin{equation}
\label{eq:incompletion-bound-10}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}
\\
\text{or} \;\;
\Vstr{beforeB}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}

From
\Eref{incompletion-bound-1},
\Eref{incompletion-bound-2},
\Eref{incompletion-bound-10}, and
\Thref{addendum-lemma2},
we have
\begin{equation}
\label{eq:incompletion-bound-15}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
From
\Eref{incompletion-bound-1},
\Eref{incompletion-bound-2}, and
\Eref{incompletion-bound-15},
we see that every derivation which adjoins an incompletion
into \VVelement{S}{j} for the cell of $\pi$ containing \Vterm{input3} is
of the form
\begin{align}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3}
\end{align}
and the incompletion is therefore
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for some \Vorig{i}.
\Vorig{i} is $\size{\Vterm{factor1}}$
where
\begin{gather*}
\Vterm{input1} = \Vterm{factor1}\Vterm{factor2}, \\
\Vstr{beforeA} \destar \Vterm{factor1}, \;\; \text{and} \\
\Vstr{rhs1} \destar \Vterm{factor2}.
\end{gather*}

% TODO: Justify in lemma that factoring must be unique.
If there were more than one factoring of
\Vterm{input1} into \Vterm{factor1}and \Vterm{factor2},
then \Vint{g} would be ambiguous.
LRR grammars are never ambiguous, so there is only one
$\Vorig{i} = \size{\Vterm{factor1}}$,
and only one
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for each cell of the partition.
Therefore the maximum number of incompletions at \VVelement{S}{j} is
less than
\begin{equation}
\label{eq:incompletion-bound-80}
\var{c} = \size{\Vint{g}} \times \size{\pi},
\end{equation}
where
$\size{\Vint{g}}$ is the number of dotted rules,
and $\size{\pi}$ is the cardinality of the regular partition.
$\size{\Vint{g}}$ is a constant depending on \Vint{g},
and, for any LRR grammar,
$\size{\pi}$ is a constant.
Therefore \var{c} in
\Eref{incompletion-bound-80}
is the constant required by the theorem.
\myqed
\end{proof}

\begin{theorem}[Completions bound medials]
\label{th:completion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an unambiguous
grammar.
Let \var{complete} be the number of complete Earley items
in an arbitrary Earley set \VVelement{S}{j},
and let \var{medial} be the number of confirmed incomplete Earley items.
Then
\begin{equation*}
\var{complete} = \order{\var{medial}}.
\end{equation*}
\end{theorem}

\begin{proof}
Since \Vint{g} is unambiguous, no \realm{EIM} will have more than one link
pair \Thref{unambig-g-unambig-eim},
and therefore no \realm{EIM} will have more than one cause \realm{EIM}.
In this proof,
let a ``cause chain'' be a sequence \var{chain} such that
for every \var{i}, $0 \le \var{i} < \decr{\Vsize{chain}}$,
$\var{chain}[\var{i}]$ is the cause of the link pair
of $\var{chain}[\var{i}+1]$.

\realm{EIM}s may not have more than one cause,
but they may share causes.
Call a cause chain where no \realm{EIM} is the cause of more than one other
\realm{EIM}, a ``single-cause chain''.  We will now consider the maximum length
of a single-cause chain in a \Marpa parse.

In a single-cause chain, every \realm{EIM} after the first has exactly one link pair
and therefore is Leo-unique.
In any single-chain longer than $\size{\NT{\var{g}}}$
an LHS will occur twice, and therefore the portion of the chain between
the duplicate occurrences will be Leo-unique and right recursive
and therefore Leo-eligible.
Any Leo-eligible stretch of a single-cause chain is memoized and reduced to
two \realm{EIM}s -- the bottom and the top.
So
\begin{equation}
\label{eq:th-completion-bound-20}
\text{the longest single-cause chain is $\size{\NT{\var{g}}}+1$.}
\end{equation}

We now look at the \realm{EIM}s in a single Earley set, call it \VVelement{S}{j},
seeking to establish the relationship between the count of complete and incomplete
items.
We exclude predictions and organize the rest into a ``cause chain tree''.

We call an \realm{EIM} of the form,
\begin{equation}
\left[\Accept{\var{g}} \de \Vstr{top}], 0\right]
\end{equation}
the ``accept completion''.
Let the ``leaves'' of the cause tree be the confirmed incompletions,
plus the accept completion, if it exists in \VVelement{S}{j}.
These leaves have in common that they cannot be the cause of any other
\realm{EIM}.
The remaining nodes are the completions, other than the accept completion.
We call these the ``interior'' nodes of the cause tree.
In this proof,
we think of a tree as having its ``leaves'' at the top,
so that the ``interior'' nodes are below the ``leaves''.

For our result, we seek to show the ``worst case''
and therefore to maximize the number of completions.
To do this we maximize the number of interior nodes.
Every leaf will have at most one cause, so we assume every leaf is at the
end of a single-cause chain.
We call the single-cause chains that end in a ``leaf'',
the first layer of the tree.

Below the first layer layer, the bottom of every single-cause chain must share
a cause, because otherwise it would not be the bottom of the single-cause chain.
To maximize the number of interior nodes, we assume that pairs of first-layer bottom
nodes share their causes, and that the causes of the first-layer
bottom notes are in turn the top of a second layer
of single-cause chains.
For the second layer, we again maximize by assuming that pairs of bottom \realm{EIM}s of the second
layer share causes.

Let \var{leaves} be the number of ``leaves'' in the cause tree,
and letting \var{chains} be the number of single-cause chains in the cause tree,
Stating the above reasoning more precisely, we have
\begin{gather}
\label{eq:th-completion-bound-22}
\var{chains} \le \var{leaves} + \frac{\var{leaves}}{2} + \frac{\var{leaves}}{4} + \ldots + 1
\\
\label{eq:th-completion-bound-23}
\var{chains} = 2 \times \var{leaves} - 1 \because \Eref{th-completion-bound-22}.
\end{gather}
Letting \var{interior} be the number of interior nodes in the cause tree,
\begin{gather}
\label{eq:th-completion-bound-25a}
\var{interior} \le \var{chains} \times (\size{\NT{\var{g}}}+1)
\because \Eref{th-completion-bound-20}
\\
\label{eq:th-completion-bound-25b}
\begin{gathered}
\var{interior} \le (2 \times \var{leaves} - 1) \times (\size{\NT{\var{g}}}+1) \\
\because \Eref{th-completion-bound-23},
\Eref{th-completion-bound-25a}.
\end{gathered}
\end{gather}
Noting that
\size{\NT{\var{g}}} is a constant which depends on \var{g},
\Eref{th-completion-bound-25b}
simplifies to
\begin{equation}
\label{eq:th-completion-bound-30}
\var{interior} = \order{\var{leaves}}.
\end{equation}

There may not be an accept completion in \VVelement{S}{j},
but to maximize the number of completions,
we assume there is one,
so that
\begin{gather}
\label{eq:th-completion-bound-32a}
\text{$\var{complete} = \var{interior}+1$ and} \\
\label{eq:th-completion-bound-32b}
\var{medial} = \var{leaves} - 1.
\end{gather}
Then
\begin{equation}
\var{complete} = \order{\var{medial}} \because
\Eref{th-completion-bound-30},
\Eref{th-completion-bound-32a},
\Eref{th-completion-bound-32b}.
\end{equation}
and we have the theorem.
\myqed
\end{proof}

\begin{theorem}[\Marpa LRR time and space is linear]
For every LRR grammar,
\Marpa runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\chapter{General complexity}
\label{chap:complexity}

\todo[prepend, caption={``General complexity'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\begin{theorem}[\Marpa unambiguous time and space is quadratic]
For every unambiguous grammar,
\Marpa runs in $\order{n^2}$ time and space.
\thEnd
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\begin{theorem}[\Marpa context-free time is cubic]
For every context-free grammar,
\Marpa runs in $\order{\var{n}^3}$ time.
\thEnd
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\begin{theorem}[\Marpa context-free space is cubic]
\label{th:cfg-space-is-cubic}
For every context-free grammar,
\Marpa runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\thEnd
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

The result of
\Thref{cfg-space-is-cubic} is worse than the one usually reported
in the literature,
because the literature traditionally reports the space consumed without
link pairs.
Space without link pairs is more relevant when \Marpa is used
as a language recognizer,
but this is rarely the case in practice.
In practice, \Marpa and its competitors are almost used to
parse.
The next theorem
\Thref{cfg-space-is-quadratic-wo-links}
shows that, in an apples-to-applies, \Marpa has
the same space bound for CFG's as \Earley{}.

\begin{theorem}[\Marpa context-free space is quadratic without links]
\label{th:cfg-space-is-quadratic-wo-links}
For every context-free grammar,
\Marpa runs in $\order{\var{n}^2}$ space,
if it does not track links.
\thEnd
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in
\Thref{cfg-space-is-quadratic-wo-links}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}[\Marpa context-free space is cubic]
For every context-free grammar,
\Marpa runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\thEnd
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\chapter{The \Marpa input model}
\label{chap:input}

\todo[prepend, caption={``The \Marpa Input Model'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

In this book,
up to this point,
the traditional input stream model
has been assumed.
As implemented,
\Marpa generalizes the idea of
input streams beyond the traditional
model.

\Marpa's generalized input model
replaces the input \Cw{}
with a set of tokens,
\var{tokens},
whose elements are triples of symbol,
start location and length:
\begin{equation*}
    [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
$\var{length} \ge 1$
and
$\Vloc{start} \ge 0$.
The size of the input, \size{\Cw},
is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

Multiple tokens can start at a single location.
(This is how \Marpa supports ambiguous tokens.)
The variable-length,
ambiguous and overlapping tokens
of \Marpa
bend the conceptual framework of ``parse location''
beyond its breaking point,
and a new term for parse location is needed.
Start and end of tokens are described in terms
of \dfn{earleme} locations,
or simply \dfn{earlemes}.
Token length is also measured in earlemes.

Like standard parse locations, earlemes start at 0,
and run up to \size{\Cw}.
Unlike standard parse locations,
there is not necessarily a token ``at'' any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
properly inside at least one token.
Here ``properly inside'' means after the token's start earleme
and before the token's end earleme.

In the \Marpa input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is, for all \Vloc{i} such
that $0 \le \Vloc{i} < \size{\Cw}$,
there must exist
\begin{equation*}
         \var{token} = [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
\begin{gather*}
         \var{token} \in \var{tokens} \quad \text{and} \\
         \Vloc{start} \le \Vloc{i} < \Vloc{start}+\var{length}.
\end{gather*}

The intent of \Marpa's generalized input model is to allow
users to define alternative input models for special
applications.
An example that arises in current practice is natural
language, features of which are most
naturally expressed with ambiguous tokens.
The traditional input stream can be seen as the special case of
the \Marpa input model where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength},
if we have both of
\begin{align*}
    [\Vsym{x}, \Vloc{x}, \var{xlength}] & \in \var{tokens} \quad \text{and} \\
    [\Vsym{y}, \Vloc{y}, \var{ylength}] & \in \var{tokens},
\end{align*}
then we have both of
\begin{gather*}
\var{xlength} = \var{ylength} = 1 \quad \text{and} \\
     \Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}.
\end{gather*}

The correctness results hold for \Marpa input streams,
but to preserve the time complexity bounds,
restrictions must be imposed.
In stating them,
let it be understood that
\begin{equation*}
        \Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \in \var{tokens}
\end{equation*}
We require that,
for some constant \var{c},
possibly dependent on the grammar \Cg{},
that every token length be less than \var{c},
\begin{equation}
\label{eq:restriction1}
\forall \; \Vtoken{[\Vsym{x}, \Vloc{x}, \var{length}]},
\; \var{length} < \var{c},
\end{equation}
and that
the cardinality of the set of tokens starting at any
one location
be less than \var{c},
\begin{equation}
\label{eq:restriction2}
 \forall \; \Vloc{i}, \;
 \Bigl|
 \bigl \lbrace
        \Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \bigm|
        \Vloc{x} = \Vloc{i}
  \bigr \rbrace
  \Bigr| < \var{c}
\end{equation}
\Eref{restriction1}
and \Eref{restriction2}
impose little or no obstacle
to the practical use
of \Marpa's generalized input model.
And with them,
the complexity results for \Marpa stand.

\chapter{Acknowledgments}
\label{chap:Acknowledgments}

\todo[prepend, caption={``Acknowledgments'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\myepi{\label{epi:hamlet}%
    Beggar that I am, I am even poor in thanks; but I thank you;
    and sure, dear friends, my thanks are too dear a halfpenny.}
    {Hamlet}

While we always feared that this section would
be an exhibition of our negligence and ingratitude as anything else,
there had been plans for it to be otherwise.
We took care to thank those who helped us
in \Marpa's IRC channel,
expecting to use its backlog
as a source for this chapter.

In 2018, the European Union (EU),
via its General Data Protection Regulation (GDPR),
suddenly brought these plans to nought.
Under the GDPR,
logs like that for \Marpa's IRC channel could not possibly be made
conformant in a way that kept their administrators safe from
legal action.
In theory this applied to similar, highly-profitable databases maintained
by large corporations,
as much as it did to the volunteers
generously maintaining the backlog for our IRC channel.
In practice large corporations have been able to largely neutralize the GDPR,
deftly parrying most enforcement,
with their worst possible outcome being a cost-of-doing-business fine.
For the typical volunteer, however, even winning a legal action is ruinous,
given the costs.

Whether or not some were aware of
GDPR's highly discriminatory effects
when it was being drafted,
realization of the GDPR's practical implications
for ourselves
came to the open source community very abruptly,
and after it was already in effect.
Our backlogging was done in EU jurisdiction,
and no measure to save our IRC backlogs
could be taken
without inviting legal action.

Many members of the \Marpa community
have helped me in many ways,
and it is risky
to single out one of them.
But, from almost its beginning,
Ron Savage has been unstinting in
his support.

Jean-Damien Durand's assistance included several ambitious
\Marpa applications.
Deyan Ginev provided advice on \LaTeX{} and
on deeper matters which proved essential for this book.
Lenz Moritz quietly and effectively maintained our IRC channel.
Andrew Rodland stood in for me as the face of \Marpa,
and he pioneered the use of one \Marpa grammar
to feed another.
Ruslan Shvedov provided many hours of assistance,
ranging from linguistic insights to details of code testing.
Luc St-Louis moderated the \Marpa IRC channel,
saving me time,
and the channel's other users from having to put up with
my less than fully competent,
and often too heavy-handed efforts.
Peter Stuifzand wrote the prototype on which \Marpa's
SLIF interface is based.
Ruslan Zakirov was a very early \Marpa supporter
who made many useful suggestions
that are incorporated in this book.
Larry Wall has always made himself available,
and his wise and experienced advice saved us much trouble.
An anonymous member of the Hoon community was generous
financially.

Additional help came from
Mohammad S Anwar,
Lukas Atkinson,
Peter Blackson,
Anton Dyudin,
B. Fraser,
Zaki Mughal,
Omar Roth,
Aria Stewart,
Flaviu Tomas,
and David Whitten.

\chapter{Asides}
\label{chap:asides}

\todo[prepend, caption={``Asides'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

This chapter contains material
that would impair the flow if incorporated directly into
the main presentation, but which it is useful or essential
to include.
Traditionally, these ``asides'' take the form of long footnotes.

Long footnotes do occur closer to the material they reference,
but a long list of problems outweighs this one advantage.
Long footnotes interrupt the text flow.
They present sometimes insoluable page break issues.
If typeset in the traditional smaller font, they are hard to read.
And they restrict, or make awkward,
the use of tables, lists and other readability aids.

\section{Chiron}
\label{sec:aside-chiron}

William Farmer's 2012 paper~\cite{Farmer2012}
is our primary reference for Chiron.
The material we use as our foundation is in Chapters 1 through 5,
pages 1-60 of~\cite{Farmer2012}.
\cite{Farmer2007} is an earlier version of \cite{Farmer2012}.

Farmer calls his approach to ill-definedness the ``traditional
approach'' because it handles ill-definedness in a way that conforms
to the usually implicit practice of the literature.
In~\cite{FG2000} Farmer introduced his ``traditional approach'',
and in~\cite{Farmer2004} he applied it
to examples taken from a basic calculus text.
\cite{Farmer2001}~is an earlier effort by Farmer
which provides insight into the thinking that led to
Chiron.

\todo{Add more Farmer sources here?}

Many of the features of Chiron are not used in this
book, and may be ignored for our purposes.
These include the
\begin{itemize}
\item the \mname{eval} built-in operator;
\item the Axiom of Choice;
\item ``indefinite descriptions'';
\item the ``choice function'', or $\xi$ element,
    of the structure defining
    a Chiron language~\cite[p. 25]{Farmer2012});
\item dependent function types; and
\item type application.
\end{itemize}

Dependent function types (also called dependent product types)
would allow the type of the result
of a function application to vary with its argument.
For our purposes the type of the result of a function application
does not depend on its argument.
This is the traditional, and far more common, approach.

Type application is used to implement dependent functions types.
It allows the application of a type to a class to produce another type.
Since we do not use dependent function types, we do not need or use
type application.

\section{Meta-language notation}
\label{sec:aside-meta-language-notation}

In fact, the literature generally uses
the same terms and notation whether
talking at the meta-language or object language level.
While uneasy about this tradition,
we follow it.
The advantage of the traditional practice is that it avoids a very
very tedious set of quasi-duplicate definitions and explanations,
with an attendant proliferation of vocabulary and notations.
The added complexity risks the same confusion it would be intended to prevent.

The disadvantage is that a burden is placed on the reader
to track whether meta-language or object-language is in use.
But in practice, it seems that most readers fall into one of two classes:
those who care little about foundational issues,
and are therefore willing to ignore the distinction
between languages;
and those who are confident about their ability to
track the distinction and regard this as a small price
to pay for
a less cluttered presentation.

\chapter{About the epigraphs}
\label{chap:epigraphs}

\todo[prepend, caption={``Epigraphs'' chapter is FRAGMENTARY}]{%
This chapter is fragmentary and inconsistent
and much of it may be deleted.
Non-author readers are not encouraged.
Filing pull requests
will usually be a waste of time.}

\newcommand{\epiSource}[2]{%
    {\bf Page} \pageref{epi:#2}, {#1}:}

This section provides sourcing for the epigraphs.
It also contains additional information about them which,
while we hope it is interesting,
would clutter the main presentation if
included there.

In our the use of epigraphs,
many readers will detect the influence of Knuth \cite{Knuth1986}.
Knuth's epigraphs are often jokes or puns
and act as as contrast to his exposition,
which can be very demanding.
While we enjoy Knuth's sense of humor,
and benefited by his example,
our epigraphs are never primarily intended
as comic relief.
Every epigraph is hoped to
expand the reader's
understanding of the motivation for,
implications of, or background to this book.
Usually an epigraph is particularly relevant to the surrounding material.

Those epigraph sources which also contribute directly to the
main presentation of this book
are included in the traditional bibliographic apparatus.
To avoid clutter,
those sources which only contribute indirectly,
though the epigraphs,
are documented in this chapter.
In what follows, {\bf Duhem} is
Pierre Duhem,
{\em The Aim and Structure of Physical Theory},
Princeton University Press, 1991.

\epiSource{John Dewey}{dewey}
From Chapter 12, ``Activity and the Training of Thought'',
of John Dewey's book
{\em How We Think}, the Gutenberg HTML edition.
According to Gutenberg this is on pages 159-160
of the D.C.~Heath 1910 edition.

\epiSource{Kurt G{\"o}del}{goedel}
From page 75 of Hao Wang's book,
{\em A Logical Journey: From G{\"o}del to Philosophy},
The MIT Press, 1997.
% TODO: From Wikipedia citation.  Confirm sometime by looking
% at my copy of Wang.

\epiSource{Pierre Duhem}{duhem}
From page 133 in Chapter 3, Part II of {\bf Duhem}.

\epiSource{Lewis Carroll}{looking-glass}
In Chapter 6 of
{\em Through the Looking Glass}.

\epiSource{Hamlet}{hamlet}
William Shakespeare's "Hamlet", Act~2, scene~2.

\bibliographystyle{plain}

{
% Ragged right, do not hyphenate.
\RaggedRight
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Silence current hbox warnings, but allow them if
% badness increases further
\hbadness=2000

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock {\em The Theory of Parsing, Translation, and Compiling}.
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing.
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Brabrand2007}
Claus~Brabrand, Robert~Giegerich and Anders~M{\"o}ller.
\newblock Analyzing ambiguity of context-free grammars.
\newblock {\em Proc. 12th International Conference
on Implementation and Application of Automata}.
\newblock CIAA â€™07, Prague, Czech Republic, July 2007.
\newblock \url{http://www.brics.dk/~amoeller/papers/ambiguity/journal.pdf}

\bibitem{Corcoran2005}
John~Corcoran.
\newblock Schemata: The Concept of Schema in the history of logic.
\newblock {\em The Bulletin of Symbolic Logic}.
\newblock Vol. 12, No. 2, June 2006.

\bibitem{Cormen2009}
Thomas~H.~Cormen, Charles~E.~Leiserson, Ronald~L.~Rivest, and
    \hbox{Clifford~Stein.}
\newblock {\em Introduction to Algorithms.}
\newblock Third Edition (3rd. ed.), The MIT Press, 2009.

\bibitem{Culik1973}
Karel~{\v{C}}ulik, and Rina~Cohen.
\newblock LR-Regular grammars --- an extension of {LR($k$)} grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Cummings2021}
Cummings~Jay.
\newblock {\em Proofs: A Long-Form Mathematics Textbook}.
\newblock ``13 May 2021, Middletown DE''.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{Lyons1996}
J.~L.~Lyons.
\newblock {\em Ariane 5, Flight 501 Failure: Report by Inquiry Board}
\newblock{Paris, 19 July 1996}

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H~Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{FG2000}
William~M.~Farmer and Joshua~D.~Guttman.
\newblock A Set Theory with Support for Partial Functions.
\newblock {\em Studia Logica: An International Journal for Symbolic Logic},
    Vol. 66, No. 1, pp. 59-78, Oct 2000.

\bibitem{Farmer2001}
William~M.~Farmer.
\newblock {STMM}: A {S}et {T}heory for {M}echanized {M}athematics.
\newblock {\em Journal of Automated Reasoning}, 26:269--289, 2001.

\bibitem{Farmer2004}
William~M.~Farmer.
\newblock Formalizing undefinedness arising in calculus.
\newblock In D.~Basin and M.~Rusinowitch, editors, {\em Automated
  Reasoning---IJCAR 2004}, volume 3097 of {\em Lecture Notes in Computer
  Science}, pages 475--489. Springer-Verlag, 2004.

\bibitem{Farmer2007}
William~M.~Farmer.
\newblock Chiron: {A} multi-paradigm logic.
\newblock In R.~Matuszewski and A.~Zalewska, editors, {\em From Insight to
  Proof: Festschrift in Honour of Andrzej Trybulec}, volume 10(23) of {\em
  Studies in Logic, Grammar and Rhetoric}, pages 1--19. University of
  Bia{\l}ystok, 2007.

\bibitem{Farmer2012}
William~M.~Farmer.
\newblock Chiron: A Set Theory with Types, Undefinedness, Quotation, and Evaluation.
\newblock SQRL Report No. 38, McMaster University, 2007 (revised 2012)
\newblock arXiv:1305.6206

\bibitem{Goedel1931}
Kurt~G{\"o}del.
\newblock {\"U}ber formal unentscheidbare {S}\"atze der {P}rincipia
  {M}athematica und verwandter {S}ysteme {I}.
\newblock {\em Monatshefte f\"ur {M}athematik und {P}hysik}, 38:173--198, 1931.

\bibitem{Goedel1940}
Kurt~G{\"o}del.
\newblock {\em The Consistency of the Axiom of Choice and the Generalized
  Continuum Hypothesis with the Axioms of Set Theory}, volume~3 of {\em Annals
  of Mathematical Studies}.
\newblock Princeton University Press, 1940.

\bibitem{Halmos1960}
Paul~R.~Halmos.
\newblock {\em Naive Set Theory}.
\newblock van Nostrand Reinhold, 1960.

\bibitem{Harrison1978}
Michael~A.~Harrison.
\newblock {\em Introduction to Formal Language Theory}.
\newblock Addison-Wesley, 1978.

\bibitem{HU1979}
John E.~Hopcroft and Jeffrey D.~Ullman.
\newblock{\em Introduction to Automata Theory,
    Language and Computation}.
\newblock Addison-Wesley, 1979.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Kegler2019}
Jeffrey~Kegler.
\newblock Marpa, A practical general parser: the recognizer.
\\ % fix underfull HBOX
\newblock arXiv:1910.08129v1.
\newblock \url{https://arxiv.org/abs/1910.08129v1}

\bibitem{Knuth1986}
Donald~Ervin Knuth.
\newblock{\em The TeXbook}.
\newblock Addison-Wesley, 1986.

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Timeline}
Jeffrey~Kegler.
\newblock Parsing:~a~timeline.
\newblock Version 3.1, 2019.
\\ % fix underfull HBOX
\newblock \url{https://jeffreykegler.github.io/personal/timeline_v3}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\bibitem{Leo1991a}
J.~M. I.~M. Leo.
\newblock Addendum.
\newblock An undated 2-page addendum to \cite{Leo1991}.

\bibitem{Mendelson1997}
E.~Mendelson.
\newblock {\em Introduction to Mathematical Logic}.
\newblock Chapman \& Hall/CRC, fourth edition, 1997.

\bibitem{Tarski1933}
Alfred~Tarski.
\newblock Poj\rc{e}cie prawdy w j\rc{e}zykach nauk dedukcyjnych ({The} concept
  of truth in the languages of the deductive sciences).
\newblock {\em Prace Towarzystwa Naukowego Warszawskiego}, 3(34), 1933.

\bibitem{Tarski1935}
Alfred~Tarski.
\newblock {Der Wahrheitsbegriff in den formalisierten Sprachen}.
\newblock {\em Studia Philosophica}, 1:261--405, 1935.
\newblock Translation, with revisions, of \cite{Tarski1933}.

\bibitem{Tarski1935a}
Alfred~Tarski.
\newblock The concept of truth in formalized languages.
\newblock In J.~Corcoran, editor, {\em Logic, Semantics, Meta-Mathematics},
  pages 152--278. Hackett, second edition, 1983.
\newblock Translation, with revisions, of \cite{Tarski1935}.

\bibitem{Tarski1944}
Alfred~Tarski.
\newblock The Semantic Conception of Truth and the Foundations of Semantics.
\newblock {\em Philosophy and Phenomenological Research}, Vol. 4, Issue 3, pp. 341-376,
    March 1944.

\bibitem{Tarski1994}
Alfred~Tarski.
\newblock {\em Introduction to Logic
and to the Methodology
of the Deductive Sciences}.
\newblock Fourth edition edited by Jan Tarski.
\newblock Oxford University Press, 1994.

\end{thebibliography}

} % RaggedRight

\clearpage
\def\indexname{Index of defined terms}
\indexcomment{This index lists defienda,
    together with related notions, such as scopes
    and context.
    Notions which do not conveniently index using the Latin
    alphabet are in a separate index.}
\printindex{recce-defienda}

\clearpage
\def\indexname{Index of symbols}
\indexcomment{This index contains symbols
    which do not conveniently
    index using the Latin alphabet.}
\printindex{recce-symbol}

\clearpage
\phantomsection
\listofalgorithms

\clearpage
\phantomsection
\renewcommand{\listtheoremname}{List of global theoremoids}
% \renewcommand{\listtheoremname}{Theorems,
    % Definitions, Remarks and Observations}
\pdfbookmark[0]{\listtheoremname}{listoftheorems}
% swapnumber would be cool, but is not in the thmtools in
% the latest Raspbian
\listoftheorems[ignoreall,onlynamed=observation,
show={theorem,remark,example,definition}]

\clearpage
\phantomsection
\renewcommand{\listtablename}{List of tables}
\listoftables

\ifdraft{
    \clearpage
    \phantomsection
    \listoftodos
}{}

\renewcommand{\listtheoremname}{List of all theoremoids}
\clearpage
\phantomsection
\pdfbookmark[0]{\listtheoremname}{listoflemmas}
% swapnumber would be cool, but is not in the thmtools in
% the latest Raspbian
\listoftheorems[ignoreall,onlynamed=observation,
    show={lemma,ldef,theorem,example,remark,definition}]

\clearpage
\def\indexname{General index}
\indexcomment{This index includes concepts which
    it is convenient to index
    outside of our specialized lists and indexes,
    such as names and dates.}
\printindex{recce-general}

\clearpage
\tableofcontents

\end{document}
% vim: expandtab shiftwidth=4:
