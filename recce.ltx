% Copyright 2013 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[draft,12pt]{amsbook}
\usepackage{ifdraft}
\usepackage[obeyDraft]{todonotes}
\ifdraft{
    \usepackage{datetime2}
    \usepackage{draftwatermark}
}{}

% Fix conflict between todonotes and amsart packages
% See http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/todonotes/todonotes.pdf,
% p. 10.
\makeatletter
\providecommand\@dotsep{5}
\def\listtodoname{List of Todos}
\def\listoftodos{\@starttoc{tdo}\listtodoname}
\makeatother

\emergencystretch=3em

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{url}
\usepackage{varioref}
\usepackage[final]{hyperref} % ignore draft status
\hypersetup{pdfborder=0 0 .1}
\usepackage{ragged2e}
\usepackage{placeins}
\usepackage{thmtools}
\usepackage{multirow}
\renewcommand{\listtheoremname}{List of Displayed Statements}
% \usepackage[notcite,notref]{showkeys}
\allowdisplaybreaks[3]

\makeatletter
\renewcommand{\listofalgorithms}{\@starttoc{loa}{List of Algorithms}}
\let\l@algorithm=\l@figure
\makeatother

% In this document, we have many multi-line displays throughout.  Tex often
% tries to deal with these by stretching vertical space to an annoying
% degree.  In any case, good page breaks for flush bottoms are just
% not to be found for many of our pages.  We therefore simply give up,
% and let page bottoms be ragged.
\raggedbottom

% Re hbox's, see
% https://tex.stackexchange.com/questions/241343/what-is-the-meaning-of-fussy-sloppy-emergencystretch-tolerance-hbadness

% * Reference meta tags ("classifying prefixes") use in this document:
% * From fancyref:
% Chapter chap:
% Section sec:
% Equation eq:
% Figure fig:
% Table tab:
% Enumeration enum:
% Footnote fn:
%
% * Not in fancyref
% Definition def:
% Theorem th:
% Lemma lem:
% Observation obs:
% Assumption asm:

% The auto-placement of qed's fails a lot and manual
% replacement with \qedhere often fails as well.  I go with
% 100% manual placement, which turns out to be no more trouble,
% and which allows me to quarantee the QED's wind up where I
% want them.
\renewcommand{\qedsymbol}{}
\newcommand{\myqed}{%
  \ifmmode\square
  \else$\square$
  \fi%
}

% Hack the algorithmicx package
\algdef{S}[FOR]{ForNoDo}[1]{\algorithmicfor\ #1}%
\algtext*{EndIf}
\algtext*{EndFor}
\algtext*{EndProcedure}
\algtext*{EndWhile}

\delimitershortfall=0pt
\delimiterfactor=1100

% Shorten the distane between equations.  This is not
% to save space for its own sake, but because
% we often equations into series, and these
% do not look good with too much space.
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength\abovedisplayshortskip{3pt}
\setlength\belowdisplayshortskip{3pt}

\newcommand{\fnset}[2]{\ensuremath{{#1}\to{#2}}}
\newcommand{\finseq}[1]{\ensuremath{
    \set{ \var{f} : \exists \; \var{X} \in \naturals : \var{f} \in \fnset{\var{X}}{\var{#1}} }
}}
\newcommand{\infseq}[1]{\ensuremath{
    \set{ \var{f} : \exists \; \var{X} \in (\naturals \cup \set{\naturals}) :
        \var{f} \in \fnset{\var{X}}{\var{#1}} }
}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\bigorder}[1]{\ensuremath{{\mathcal O}\left(#1\right)}}
\newcommand{\powerset}[1]{\ensuremath{\mathcal{P}(#1)}}
\newcommand{\riota}{\mathop{\scalebox{1.5}{\rotatebox[origin=c]{180}{$\iota$}}}}
\newcommand{\existsUniq}{\ensuremath{\mathop{\exists!}}}

% \newcommand{\TODO}[1]{\par{\bf TODO}: #1\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\mathtt{#1}}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\op}[2][]{%
  \ifmmode\texttt{#2}(#1)%
  \else\texttt{#2}(#1)%
  \fi%
}
\newcommand{\proc}[2][]{%
  \ifmmode\texttt{#2}(#1)%
  \else\texttt{#2}(#1)%
  \fi%
}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\fixedsize}[1]{\ensuremath{|{#1}|}}
\newcommand{\size}[1]{\ensuremath{\left| {#1} \right|}}
\newcommand{\enRange}[2]{\ensuremath{#1\text{--}#2}}
\newcommand{\enRefs}[2]{\enRange{\ref{#1}}{\ref{#2}}}
\newcommand{\overlap}{\mathrlap{\raisebox{.5ex}{$\vee$}}\cap}
\newcommand{\plusplus}{\mathord{++}}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\lastix{\var{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}[#2]}}
\newcommand{\Velement}[2]{\element{\var{#1}}{#2}}
\newcommand{\VVelement}[2]{\Velement{#1}{\var{#2}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\#\var{#1}}}
\newcommand{\dispEnd}{
  \ifmmode\bigcirc
  \else$\bigcirc$
  \fi%
}
\newcommand{\cuz}{
  \ifmmode
    \because
  \else
    $\because$
  \fi%
}
\newcommand{\tightMath}[1]{%
    {%
    \medmuskip=0mu
    \thinmuskip=0mu
    \thickmuskip=0mu
    \ensuremath{#1}%
    }%
}

\newcommand{\colonpageref}[1]{%
\vrefpagenum{\temprefpage}{#1}%
 \ifthenelse{\equal{\temprefpage}{\thepage}}%
  {}{:\pageref{#1}}%
}

\newcommand{\Aref}[1]{(a\ref{#1}\colonpageref{#1})}
\newcommand{\Asmref}[1]{(\textrm{Asm}\ref{#1}\colonpageref{#1})}
\newcommand{\Dfref}[1]{(\textrm{Df}\ref{#1}\colonpageref{#1})}
\newcommand{\Eref}[1]{(e\ref{#1}\colonpageref{#1})}
\newcommand{\Lref}[1]{(L\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Lmref}[1]{(\textrm{Lm}\ref{#1}\colonpageref{#1})}
\newcommand{\Obref}[1]{(\textrm{Ob}\ref{#1}\colonpageref{#1})}
\newcommand{\Sref}[1]{(S\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Chref}[1]{(Ch\ref{#1}:\pageref{#1})} % always include page
\newcommand{\Tbref}[1]{(Tb\ref{#1}\colonpageref{#1})}
\newcommand{\Thref}[1]{(\textrm{Th}\ref{#1}\colonpageref{#1})}

\newcommand{\longThref}[2]{%
  \ifmmode \text{``#1''\Thref{#2}}%
  \else``#1'' \Thref{#2}%
  \fi%
}
\newcommand{\longDfref}[2]{%
  \ifmmode \text{``#1''\Dfref{#2}}%
  \else``#1'' \Dfref{#2}%
  \fi%
}
\newcommand{\longEref}[2]{
  \ifmmode \text{``#1''\Eref{#2}}%
  \else``#1'' \Eref{#2}%
  \fi%
}

% "full" refs -- that is, always include page
% These exist because if, for example, both refs are to floats
% the question of whether both are on the same page becomes
% difficult for Tex, and it is easiest to just force use of
% the page number.
\newcommand{\ThFref}[1]{(\textrm{Th}\ref{#1}:\pageref{#1})}
\newcommand{\AFref}[1]{(a\ref{#1}:\pageref{#1})}
\newcommand{\AsmFref}[1]{(Asm\ref{#1}:\pageref{#1})}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\varprime}[2]{\ensuremath{\texttt{#1}#2}}
\newcommand{\boldvar}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mapsto}
\newcommand{\nderives}[1]{
\mathrel{%
  \ifthenelse{\equal{#1}{}}{%
    {\not\mapsto}%
  }{%
    {\mbox{$\:\stackrel{\!{#1}}{\not\mapsto\!}\:$}}%
  }%
}}%
\newcommand{\xderives}[1]
    {\mathrel{\mbox{$\:\stackrel{\!{#1}}{\mapsto\!}\:$}}}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\mapsto\!}\:$}}}
\newcommand{\ndestar}{\nderives{\ast}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\mapsto\!}\:$}}}
\newcommand{\ndeplus}{\nderives{+}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\mapsto\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\mapsto\!}\:$}}}

\newcommand{\decr}[1]{#1\subtract1}
\newcommand{\Vdecr}[1]{\var{#1}\subtract1}
\newcommand{\seq}[1]{{\left[ #1 \right]} }
\newcommand{\set}[1]{%
    \ensuremath{{\left\lbrace #1 \right\rbrace}}%
}
% \newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\tuple}[1]{\ensuremath{\left[ #1 \right]} }
\newcommand{\Dom}[1]{\ensuremath{\mymathop{Dom}(#1)}}
\newcommand{\Cod}[1]{\ensuremath{\mymathop{Cod}(#1)}}
\newcommand{\Ran}[1]{\ensuremath{\mymathop{Ran}(#1)}}
\newcommand{\frontier}[1]{{\left\lfloor #1 \right\rfloor} }
\newcommand{\Vfrontier}[1]{\frontier{\boldvar{#1}}}
\newcommand{\boldElement}[2]{\textbf{#1[#2]}}
\newcommand{\boldRange}[3]{\textbf{#1[#2 \ldots #3]}}
\newcommand{\boldRangeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\textbf{#3}\subtract 1$)}]}%
}
\newcommand{\boldRangeSizeDecr}[3]{\textbf{#1[#2 \ldots %
\textbf{($\left|\textbf{#3}\right|\subtract 1$)}%
]} }

\newcommand{\wRange}[2]{\boldRange{w}{#1}{#2}}
\newcommand{\Tvar}[2]{\ensuremath{\var{#1}_{\var{#2}}}}
\newcommand{\Tvarset}[2]{\ensuremath{\var{#1}_{\set{\var{#2}}}}}
\newcommand{\Tvarseq}[2]{\ensuremath{\var{#1}_{\seq{\var{#2}}}}}

\newcommand{\eim}[1]{\ensuremath{#1_\type{EIM}}}
\newcommand{\es}[1]{\ensuremath{#1_\type{ES}}}
\newcommand{\et}[1]{\ensuremath{#1_\type{ET}}}
\newcommand{\mylim}[1]{\ensuremath{#1_\type{LIM}}}
\newcommand{\lss}[1]{\ensuremath{#1_\type{LSS}}}
\newcommand{\les}[1]{\ensuremath{#1_\type{LES}}}
\newcommand{\mes}[1]{\ensuremath{#1_\type{MES}}}
% Delete NSS?
\newcommand{\nss}[1]{\ensuremath{#1_\type{NSS}}}
\newcommand{\str}[1]{\ensuremath{\langle\langle{#1}\rangle\rangle}}
\newcommand{\term}[1]{\ensuremath{\lfloor{#1}\rfloor}}
\newcommand{\sym}[1]{\ensuremath{\langle{#1}\rangle}}
\newcommand{\Vbool}[1]{\Tvar{#1}{BOOL}}
\newcommand{\Vcfg}[1]{\Tvar{#1}{CFG}}
\newcommand{\dr}[1]{\ensuremath{#1_\type{DR}}}
\newcommand{\Vdr}[1]{\dr{\var{#1}}}
\newcommand{\Vdrset}[1]{\Tvarset{#1}{DR}}
\newcommand{\Veim}[1]{\eim{\var{#1}}}
\newcommand{\Veimseq}[1]{\Tvarseq{#1}{EIM}}
\newcommand{\Veimset}[1]{\Tvarset{#1}{EIM}}
\newcommand{\ves}[1]{\ensuremath{#1_\type{VES}}}
\newcommand{\Ves}[1]{\es{\var{#1}}}
\newcommand{\Vesseq}[1]{\Tvarseq{#1}{ES}}
\newcommand{\Vet}[1]{\et{\var{#1}}}
\newcommand{\Vext}[1]{\Tvar{#1}{EXT}}
\newcommand{\Vint}[1]{\Tvar{#1}{INT}}
\newcommand{\Vlim}[1]{\mylim{\var{#1}}}
\newcommand{\Vlss}[1]{\lss{\var{#1}}}
\newcommand{\Vlssset}[1]{\Tvarset{#1}{LSS}}
\newcommand{\Vlp}[1]{\Tvar{#1}{LP}}
\newcommand{\Vlimset}[1]{\Tvarset{#1}{LIM}}
\newcommand{\Vloc}[1]{\Tvar{#1}{LOC}}
\newcommand{\Vles}[1]{\les{\var{#1}}}
\newcommand{\Vmes}[1]{\mes{\var{#1}}}
% Delete Vnss?
\newcommand{\Vnss}[1]{\nss{\var{#1}}}
\newcommand{\Vcurr}[1]{\Tvar{#1}{CURR}}
\newcommand{\Vorig}[1]{\Tvar{#1}{ORIG}}
\newcommand{\Vpim}[1]{\Tvar{#1}{PIM}}
\newcommand{\Vpimset}[1]{\Tvarset{#1}{PIM}}
\newcommand{\Vrule}[1]{\Tvar{#1}{RULE}}
\newcommand{\Vruleset}[1]{\Tvarset{#1}{RULE}}
\newcommand{\Vstr}[1]{\str{\var{#1}}}
\newcommand{\Vstrset}[1]{\Tvarset{#1}{STR}}
\newcommand{\Vterm}[1]{\term{\boldvar{#1}}}
\newcommand{\Vtermset}[1]{\Tvarset{\boldvar{#1}}{TERM}}
\newcommand{\Vsym}[1]{\sym{\var{#1}}}
\newcommand{\Vsymset}[1]{\Tvarset{#1}{SYM}}
\newcommand{\Vtoken}[1]{\Tvar{#1}{TOKEN}}
\newcommand{\Vves}[1]{\ves{\var{#1}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\nat}[1]{\ensuremath{#1_\type{\naturals}}}
\newcommand{\Vnat}[1]{\nat{\var{#1}}}

\newcommand{\naturals}{\ensuremath{\omega}}
\newcommand{\algo}[1]{\texorpdfstring{\ensuremath{\mathbb{\uppercase{#1}}}}{#1}}
\newcommand{\Earley}{\algo{Earley}}
\newcommand{\Leo}{\algo{Leo}}
\newcommand{\Marpa}{\algo{Marpa}}
\newcommand{\Rtwo}{\texttt{Marpa::R2}}

% I want to use 'call' outside of pseudocode
\newcommand\mycallname[1]{\textproc{#1}}
\newcommand\call[2]{\mycallname{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newcommand{\Cg}{\var{g}}
\newcommand{\Cw}{\var{w}}
\newcommand{\CVw}[1]{\ensuremath{\Vsym{\Cw[\var{#1}]}}}

\newcommand{\myfnname}[1]{\ensuremath{\mymathop{#1}}}
\newcommand{\myfn}[2]{\ensuremath{\myfnname{#1}(#2)}}

\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\Cause}[1]{\ensuremath{\mymathop{Cause}(#1)}}
\newcommand{\Current}[1]{\ensuremath{\mymathop{Current}(#1)}}
\newcommand{\DotPos}[1]{\ensuremath{\mymathop{DotPos}(#1)}}
\newcommand{\DottedRules}[1]{\ensuremath{\mymathop{Dotted-Rules}(#1)}}
\newcommand{\DR}[1]{\ensuremath{\mymathop{DR}(#1)}}
\newcommand{\GOTO}{\ensuremath{\mymathop{GOTO}}}
\newcommand{\ID}[1]{\ensuremath{\mymathop{ID}(#1)}}
\newcommand{\Invocs}[1]{\ensuremath{\mymathop{Invocs}(#1)}}
\newcommand{\LeoEligible}[1]{\ensuremath{\mymathop{Leo-Eligible}(#1)}}
\newcommand{\LeoUnique}[1]{\ensuremath{\mymathop{Leo-Unique}(#1)}}
\newcommand{\LHS}[1]{\ensuremath{\mymathop{LHS}(#1)}}
\newcommand{\Lim}[1]{\ensuremath{\mymathop{Lim}(#1)}}
\newcommand{\LIMPredecessor}[1]{\ensuremath{\mymathop{LIM-Predecessor}(#1)}}
\newcommand{\LinkPairs}[1]{\ensuremath{\mymathop{Link-Pairs}(#1)}}
\newcommand{\Matches}[1]{\ensuremath{\mymathop{Matches}(#1)}}
\newcommand{\Maximal}[1]{\ensuremath{\mymathop{Maximal}(#1)}}
\newcommand{\myL}[1]{\ensuremath{\mymathop{L}(#1)}}
\newcommand{\Next}[1]{\ensuremath{\mymathop{Next}(#1)}}
\newcommand{\NextSource}[1]{\ensuremath{\mymathop{Next-Source}(#1)}}
\newcommand{\NT}[1]{\ensuremath{\mymathop{NT}(#1)}}
\newcommand{\Origin}[1]{\ensuremath{\mymathop{Origin}(#1)}}
\newcommand{\Passes}[1]{\ensuremath{\mymathop{Passes}(#1)}}
\newcommand{\Penult}[1]{\ensuremath{\mymathop{Penult}(#1)}}
\newcommand{\Pfx}[1]{\ensuremath{\mymathop{Pfx}(#1)}}
\newcommand{\Postdot}[1]{\ensuremath{\mymathop{Postdot}(#1)}}
\newcommand{\Predecessor}[1]{\ensuremath{\mymathop{Predecessor}(#1)}}
\newcommand{\Predict}[1]{\ensuremath{\mymathop{Predict}(#1)}}
\newcommand{\Predot}[1]{\ensuremath{\mymathop{Predot}(#1)}}
\newcommand{\PreviousLink}[1]{\ensuremath{\mymathop{Previous-Link}(#1)}}
\newcommand{\PrPfx}[1]{\ensuremath{\mymathop{PrPfx}(#1)}}
\newcommand{\PrSfx}[1]{\ensuremath{\mymathop{PrSfx}(#1)}}
\newcommand{\PSL}[2]{\ensuremath{\mymathop{PSL}[#1][#2]}}
\newcommand{\RHS}[1]{\ensuremath{\mymathop{RHS}(#1)}}
\newcommand{\Rightmost}[1]{\ensuremath{\mymathop{Rightmost}(#1)}}
\newcommand{\RightRecursive}[1]{\ensuremath{\mymathop{Right-Recursive}(#1)}}
\newcommand{\Rule}[1]{\ensuremath{\mymathop{Rule}(#1)}}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Seen}[1]{\ensuremath{\mymathop{Seen}(#1)}}
\newcommand{\SetSeen}[1]{\ensuremath{\mymathop{SetSeen}(#1)}}
\newcommand{\Sfx}[1]{\ensuremath{\mymathop{Sfx}(#1)}}
\newcommand{\Source}[1]{\ensuremath{\mymathop{Source}(#1)}}
\newcommand{\Space}[1]{\ensuremath{\mymathop{Space}(#1)}}
\newcommand{\Symbol}[1]{\ensuremath{\mymathop{Symbol}(#1)}}
\newcommand{\Term}[1]{\ensuremath{\mymathop{Term}(#1)}}
\newcommand{\Time}[1]{\ensuremath{\mymathop{Time}(#1)}}
\newcommand{\Transition}[1]{\ensuremath{\mymathop{Transition}(#1)}}
\newcommand{\Transitions}[2]{\ensuremath{\mymathop{Transitions}(#1,#2)}}
\newcommand{\Valid}[1]{\ensuremath{\mymathop{Valid}(#1)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}

% I use parboxes in equations.  This sets a useful width for them.
\newlength{\mathparwidth}
\newlength{\longtagwidth}
\settowidth{\longtagwidth}{(99999)\quad}
\setlength{\mathparwidth}{\dimexpr\textwidth-\longtagwidth}
\newcommand{\myparbox}[2][\mathparwidth]{%
  \parbox[t]{#1}{%
  \linespread{1.15}\selectfont
  %\raggedright#2\par
  \centering#2\par
  \vspace{-\prevdepth} % remove the depth of the last line
  \vspace{1ex} % add a fixed vertical space
  }%
}
\newcommand{\cuzbox}[1]{%
   \myparbox{\cuz{} #1}
}
% A "My paragraph-based equation"
\newcommand{\mypareq}[2]{%
   \ifthenelse{\equal{#1}{}}%
   {%
       \begin{equation*}
   }{%
       \begin{equation}
       \label{#1}
   }%
   \begin{gathered}%
   \myparbox{#2}%
   \end{gathered}%
   \ifthenelse{\equal{#1}{}}%
   {%
       \end{equation*}
   }{%
       \end{equation}
   }%
}
\newcommand{\rawparcomment}[2][\mathparwidth]{%
  \myparbox[#1]{#2}%
}
\newcommand{\parcomment}[2][\mathparwidth]{%
  \myparbox[#1]{$\triangleright$ #2}%
}
\newcommand{\lcomment}[1]{\hspace\algorithmicindent$\triangleright$ #1}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
% \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{assumption}[theorem]{Assumption}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

\begin{document}


\title{The Marpa Book}
\author{Jeffrey Kegler}
\ifdraft{
    % legacy draftwatermark options -- update?
    \SetWatermarkLightness{0.1}
    \SetWatermarkText{draft \DTMnow}
    \SetWatermarkAngle{0}
    \SetWatermarkScale{.5}
    \SetWatermarkHorCenter{1.5in}
    \SetWatermarkVerCenter{.5in}
}{}
\thanks{\ifdraft{%
    DRAFT as of \DTMnow%
}{%
    TODO: No date set
}}
\thanks{%
Copyright \copyright\ 2020 Jeffrey Kegler.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

% \begin{abstract}
% The \Marpa{} recognizer is described.
% \Marpa{} is
% a fully implemented
% algorithm for the recognition,
% parsing and evaluation of context-free grammars.
% \Marpa{} is an Earley parser
% which includes the first
% practical implementation
% of the improvements
% in Joop Leo's 1991 paper.
% In addition,
% \Marpa{} tracks the full state of the parse,
% at run-time,
% in a form accessible by the application.
% This greatly improves error detection;
% enables event-driven parsing;
% and allows techniques such as
% ``Ruby Slippers'' parsing,
% in which
% the input is altered in response
% to the parser's expectations.
% \end{abstract}

\maketitle

\chapter{Introduction}

The \Marpa{} project was intended to create
a practical and highly available tool
to generate and use general context-free
parsers.
Tools of this kind
had long existed
for LALR~\cite{Johnson} and
regular expressions.
But, despite an encouraging academic literature,
no such tool had existed for context-free parsing.

The first stable version of \Marpa{} was uploaded to
a public archive on Solstice Day 2011.
This revision of this paper
describes the algorithm used
in the most recent version of \Marpa{},
\Rtwo~\cite{Marpa-R2}.
It is a simplification of the algorithm presented
in the first version of this paper.

While the presentation in this paper is theoretical,
the approach is practical.
The \Rtwo implementation is widely available
and has seen considerable use.

% TODO Revise once chapters are settled
\Chref{chap:features}
describes the important features of \Marpa{}.
(The reference \Chref{chap:features} is to
section \ref{chap:features}
on page \pageref{chap:features}.)
\Chref{chap:using-features}
goes into detail about their application.

Section \Chref{chap:preliminaries}
begins the formal part of this paper
with preliminary remarks
on notation and conventions.
Section \Chref{chap:CFGs} defines context-free grammars.
Section \Chref{chap:lr-grammars} defines LR grammars.
Section \Chref{chap:EXTs} defines \Marpa{}'s external grammars.
Section \Chref{chap:useless-symbols} describes \Marpa{}'s handling of ``useless''
symbols.

Section \Chref{chap:rewrite} describes the way in which \Marpa{}
rewrites its external grammars into internal grammars.
Section \Chref{chap:INTs} defines \Marpa{}'s internal grammars.
Sections \Chref{chap:earley-item} and
\Chref{chap:earley-table} introduce Earley parsing.
Section \Chref{chap:leo} describes Joop Leo's modification
of Earley's algorithm.
Section \Chref{chap:ancestry} describes the tracking
of the ``ancestry'' of Earley items.
\Chref{chap:complexity-preliminaries} lays our strategy
for showing the time and speed complexity of \Marpa{}.

\Chref{chap:marpa} describes the \Marpa{} algorithm.
It includes pseudocode, as well as the low-level complexity analysis.

Section \Chref{chap:ambiguity} contains a treatment of parsing ambiguity,
with particular relevance to \Marpa{}.
Section \Chref{chap:per-set-lists} describes \Marpa{}'s
per-set lists.
Section \Chref{chap:correctness} contains
a correctness proof for \Marpa{}.
Section \Chref{chap:reverse-leo} describes our method
for reversing the Leo memoizations.
Section \Chref{chap:lrr-complexity} shows that Marpa
has linear complexity for for LRR grammars.
Section \Chref{chap:complexity} lays out Marpa's non-linear
complexity results, which match those of \Earley{}.
Finally,
section \Chref{chap:input}
generalizes \Marpa{}'s input model.

We assume familiarity with the theory of parsing,
as well as Earley's algorithm.
\cite{Timeline}
contains a full description of \Marpa{}'s relationship to
prior work.
Readers may find it helpful to read
\cite{Timeline}
before this paper.

\chapter{Features}
\label{chap:features}

\section{General context-free parsing}
The \Marpa{} algorithm is capable of parsing all
context-free grammars.
As implemented,
Marpa parses
all cycle-free context-free
grammars.\footnote{
Earlier implementations of \Marpa{} allowed cycles,
but support for them
was removed because
practical interest seems to be non-existent.
For more detail, see \Chref{chap:EXTs}.
}
Worst case time bounds are never worse than
those of Earley~\cite{Earley1970},
and therefore never worse than $\order{\var{n}^3}$.

\section{Linear time for practical grammars}
Currently, the grammars suitable for practical
use are thought to be a subset
of the deterministic context-free grammars (DCFG's).
\Marpa{} uses a technique discovered by
Leo~\cite{Leo1991}
to run in
\On{} time for LR-regular (LRR) grammars,
a superset of the the DCFG's.
\Marpa{}
also parses many ambiguous grammars in linear
time.

\section{Left-eidetic}
The original \Earley{} algorithm kept full information
about the parse ---
including partial and fully
recognized rule instances ---
in its tables.
At every parse location,
before any symbols
are scanned,
\Marpa{}'s parse engine makes available
its
information about the state of the parse so far.
This information is
in useful form,
and can be accessed efficiently during the parse.

\section{Recoverable from read errors}
When
\Marpa{} reads a token which it cannot accept,
the error is fully recoverable.
An application can attempt to read another
token,
repeatedly if that is desirable.
Once the application provides
a token that is accepted by the parser,
parsing will continue
as if the unsuccessful read attempts had never been made.

\section{Ambiguous tokens}
\Marpa{} allows ambiguous tokens.
These are often useful in natural language processing
where, for example,
the same word might be a verb or a noun.
Use of ambiguous tokens can be combined with
recovery from rejected tokens so that,
for example, an application could react to the
rejection of a token by reading two others.

\chapter{Using the features}
\label{chap:using-features}

\section{Error reporting}
An obvious application of left-eideticism is error
reporting.
\Marpa{}'s abilities in this respect are
ground-breaking.
For example,
users typically regard an ambiguity as an error
in the grammar.
\Marpa{}, as currently implemented,
can detect an ambiguity and report
specifically where it occurred
and what the alternatives were.

\section{Event driven parsing}
As implemented,
\Rtwo\cite{Marpa-R2}
allows the user to define ``events''.
Events can be defined that trigger when a specified rule is complete,
when a specified rule is predicted,
when a specified symbol is nulled,
when a user-specified lexeme has been scanned,
or when a user-specified lexeme is about to be scanned.
A mid-rule event can be defined by adding a nulling symbol
at the desired point in the rule,
and defining an event which triggers when the symbol is nulled.

\section{Ruby slippers parsing}
Left-eideticism, efficient error recovery,
and the event mechanism can be combined to allow
the application to change the input in response to
feedback from the parser.
In traditional parser practice,
error detection is an act of desperation.
In contrast,
\Marpa{}'s error detection is so painless
that it can be used as the foundation
of new parsing techniques.

For example,
if a token is rejected,
the lexer is free to create a new token
in the light of the parser's expectations.
This approach can be seen
as making the parser's
``wishes'' come true,
and we have called it
``Ruby Slippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a clean
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As part of \Rtwo~\cite{Marpa-R2},
the author\footnote{
We refer to the author both as ``the author''
and as ``we'', despite this paper having a single author.
The use of ``we''
follows the tradition in which a mathematician
speaks as a representative of a community of inquiry,
one which includes the reader.
The author refers to himself as such in those cases
where the reference only makes sense
as being to a specific individual.
}
has implemented an HTML parser,
based on a grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a simple and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
in the worst case
treating them as highly defective HTML.

\section{Ambiguity as a language design technique}
In current practice, ambiguity is avoided in language design.
This is very different from the practice in the languages humans choose
when communicating with each other.
For example,
the language of this paper, English, is notoriously
ambiguous.
Human languages exploit ambiguity in order to allow expression
to be more flexible and powerful.

Ambiguity of course can present a problem.
A sentence in an ambiguous
language may have undesired meanings.
But note that this is not a reason to ban potential ambiguity ---
it is only a problem with actual ambiguity.

Consider, for comparison purposes,
the standard treatment of syntax errors.
Syntax errors are undesired, but nobody tries
to design languages that make syntax errors impossible.
A language in which every input was well-formed and meaningful
would be not just cumbersome, but dangerous.
A parser would never warn the user about typos,
because all typos in such a language would have unintended
meanings.

With \Marpa{}, ambiguity can be dealt with in the same way
that syntax errors are dealt with in current practice.
A \Marpa{}-powered
language can be designed to allow ambiguity,
and any actual ambiguity will be detected
and reported at parse time.
This exploits \Marpa{}'s ability
to report exactly where
and what the ambiguity is.
\Rtwo's own parser description language, the SLIF,
uses ambiguity in this way.

\section{Auto-generated languages}
\cite[pp. 6-7]{Culik1973} points out that the ability
to efficiently parse LRR languages
opens the way to
auto-\linebreak[4]generated % LINEBREAK HACK
languages.
In particular,
\cite{Culik1973} notes that a parser which
can parse any LRR language will be
able to parse a language generated using syntax macros.

\section{Second order languages}
In the literature, the term ``second order language''
is usually used to describe languages with features
which are useful for second-order programming.
True second-order languages --- languages which
are auto-generated
from other languages ---
have not been seen as practical,
since there was no guarantee that the auto-generated
language could be efficiently parsed.

With \Marpa{}, this barrier is raised.
As an example,
\Rtwo's own parser description language, the SLIF,
allows ``precedenced rules''.
Precedenced rules are specified in an extended BNF.
The BNF extensions allow precedence and associativity
to be specified for each RHS.

\Rtwo's precedenced rules are implemented as
a true second order language.
The SLIF representation of the precedenced rule
is parsed to auto-generate a pure BNF grammar which is equivalent,
and which has the desired precedence and associativity.

In creating the auto-generated BNF grammar,
the SLIF does a standard parsing textbook transformation,
translating explicitly specified precedence and associativity
into pure BNF which implicitly implements
that precedence and associativity
Since the SLIF is powered by \Marpa{}, which is
linear for LRR,
the SLIF can easily ensure that the grammar
that it auto-generates will
be parsed in linear time.

Notationally, \Marpa{}'s precedenced rules
are an improvement over
similar features
in LALR-based parser generators like
yacc or bison.
In the SLIF,
there are two important differences.
First, in the SLIF's precedenced rules,
precedence is generalized, so that it does
not depend on the operators:
there is no need to identify operators,
much less class them as binary, unary, etc.
This more powerful and flexible precedence notation
allows the definition of multiple ternary operators,
and multiple operators with arity greater than three.

Second, and more important, a SLIF user is guaranteed
to get exactly the language that the precedenced rule specifies.
The user of the yacc equivalent must hope their
syntax falls within the limits of LALR.

\chapter{Preliminaries}
\label{chap:preliminaries}

The conventions of the parsing literature were formed
when its disseminated relied on printed media.
Mathematical publishing was a zero-sum game in which
every line printed came at the expense of other mathematics,
mathematics which might well be lost forever.

To mitigate these losses, mathematicians enforced a
ruthless brevity on themselves.
Many of the details of the reasoning
were left out,
and the reader was expected to reconstruct them.
Compact notations were accepted even when
hard on the eyes.
Helpful explanations were left implicit.
Historical narrative and motivation were almost
entirely excluded.

Narrative and motivation may be seen as expendable,
but with the passage of time their omission has produced
an irrecoverable loss to history of the field.%
\footnote{%
   An exception to this was Donald Knuth,
   who went out of his way to preserve this material,
   not only for his own work,
   but for the work of others.
   The field itself owes much to Knuth's work,
   but so does its history.
}
In every field, judgements must be made as
to what is important and what is not.
Oo the extent they lack explanations of why previous workers
made the judgements they made,
new investigators must accept previous judgements
uncritically,
or risk rejecting them out of ignorance.
We believe this erase of the field's history is much
of the explanation for its slowed progress
after the 1970s.

This book is fated to a primarily electronic
dissemination,
and this liberates us from the cruel trade-offs of
printed dissemination.
We detail our arguments as long as the extra depth seems
to add insight into,
or increase confidence in the correctness of,
our reasoning.
We stop when additional detail seems to degenerate
into mere symbol-pushing.

We will be willing to include explanations,
or adopt conventions,
which increase the length of this book,
if they shorten the time needed for a careful reading.
We include narrative and motivation if they
seem likely to be helpful in the preservation of
useful history.
And we are careful with our use of superscripts
and subscripts to avoid those traditional uses
which can pose a challenge to the eyesight.

Mathematics in this book may be written \dfn{verbally} or
in \dfn{notation}.
We say that a mathematical statement is written ``verbally'' if it uses words
of the English language.
We say that a mathematical statement is ``in notation'' if it uses mathematical
symbols.
For example, the statement ``six times seven equals forty-two'' is written
verbally --- its equivalent in notation is $6\times7=42$.

In this paper,
internal references will be of the form
({\it Xn:p}),
where {\it X} is the reference type,
{\it n} is the number of the referent,
and
{\it p} is a page reference.
Reference types are
``a'' (algorithm),
``e'' (equation),
``S'' (section)
``L'' (a location within a section,
where {\it n} is the section number),
``t'' (table),
``Df'' (definition),
% ``Lm'' (lemma),
``Ob'' (observation), and
``Th'' (theorem).
In this paper,
following the convention in much
the literature of mathematical typography,
any displayed mathematical statement is
called an ``equation'',
regardless of whether it is
an equation in the dictionary sense.

For example
\Eref{eq:restriction1} refers to equation
\ref{eq:restriction1} on page \pageref{eq:restriction1};
\Chref{chap:features} refers to
section \ref{chap:features}
on page \pageref{chap:features};
and
\Lref{loc-brick-depth} refers
to page \pageref{loc-brick-depth}
inside section \ref{loc-brick-depth}.
Where the page reference would be to the current page,
it is omitted.

Definitions occur throughout.
Verbal defienda are in boldface.
For emphasis,
or for convenient reference,
definitions may be
in numbered sections.
The end of a definitions section is indicated with a circle (\dispEnd).
Often one or more shorter verbal phrases are defined as a synonyms of a longer one.
For example, \Dfref{def:parse} states ``full parse'' and ``parse'' mean
the same thing.
Shorter synonyms are intended to be used for brevity,
and only in cases where they are not expected to cause confusion.

The basis of our formal presentation is
a naive set theory,
consistent with that of
Halmos 1960~\cite{Halmos1960}.
Particularly, all of our mathematical objects are sets.

\chapter{Notation}
\label{chap:notation}

In this chapter we deal with
matters of notation.
We will usually not restate notation
that is standard,
unless the standard has an ambiguity
relevant for our purposes,
or unless the standard notation is a modernization
which differs from
Halmos 1960~\cite{Halmos1960}.

Functions are partial unless stated otherwise.
An onto function is a surjection.
A one-to-one function is an injection.
A function that is both one-to-one and
onto is a bijection.
$\var{A} \subseteq \var{B}$ means that \var{A} is a subset of \var{B}, while
$\var{A} \subset \var{B}$ means that \var{A} is a proper subset of \var{B}.
$\Dom{\var{f}}$ is the domain of \var{f},
$\Cod{\var{f}}$ is the codomain of \var{f},
and $\Ran{\var{f}}$ is the range of \var{f}.
$\powerset{\var{A}}$ is the powerset of the set \var{A}.
\naturals{} is the set of non-negative integers.
The abbreviation ``iff'' is used for ``if and only if''.

Logical quantification is
always with respect to a specific set,
which is specified explicitly
or through the quantified variable's type,
as described below.
Where the set is obviously \naturals{},
and the expression has a single variable,
the specification may be abbreviated so that, for example,
\[
\forall \; \var{j} \in \set{ \var{i} \in \naturals : 0 \le \var{i} \le 42 } :
    \myfn{P}{\var{j}}
\]
would be written as
\[
\forall \; 0 \le \var{j} \le 42 : \myfn{P}{\var{j}}.
\]

We use $\existsUniq$ as a uniqueness quantification operator,
so that
\[
    \existsUniq \var{x} \in \var{A} : \myfn{P}{\var{x}}
\]
is equivalent to
$$\exists \; \var{x} \in \var{A} : \myfn{P}{\var{x}} \land \forall \; \var{y} : \myfn{P}{\var{y}} \implies \var{y}=\var{x}.$$
We use Russell's iota operator,
so that
\begin{equation}
\label{eq:def-riota}
\riota \var{x} \in \var{A} : \myfn{P}{x}
\end{equation}
is the unique \var{x} in the set \var{A} such that
\myfn{P}{x} is true.
\Eref{eq:def-riota}
is defined iff
$\existsUniq \var{x} \in \var{A} : \myfn{P}{\var{x}}$.

We write \fnset{\var{X}}{\var{Y}}
for the set of functions from \var{X} to \var{Y},%
\footnote{%
    The set of functions from
    \var{Y} to \var{X} is often written
    $\var{X}^\var{Y}$, but we prefer to avoid
    the optometric and typographic difficulties
    that superscripts introduce.
}
so that $\var{f}\in\fnset{\var{X}}{\var{Y}}$ states
that \var{f} is a function with domain \var{X}
and codomain \var{Y}.%
\footnote{%
    $\var{f} \in \fnset{\var{X}}{\var{Y}}$
    is often written
    $\var{f}:\fnset{\var{X}}{\var{Y}}$ but we prefer to avoid
    overloading the colon operator.
}
In our naive set theory, a sequence of \var{Y}'s is identical to a function from
integers to \var{Y}, so that \infseq{Y} is the set of sequences of \var{Y}'s,
and \finseq{Y} is the set of finite sequences of \var{Y}'s.

As a way of avoiding tedious special cases,
logical implication makes an opportunistic use
of ternary logic.
An logical implication
with a \var{false} antecedant
is allowed to have an undefined consequent,
and the value of such an implication is \var{true}.
(Typically the ``undefined'' expressions are
iota operators
or sequence values with out-of-bounds indexes.)
Other uses of undefined expressions
are banned,
except when explained in the text.
\begin{table}[t]
\centering
\caption{Logical implication convention}
\begin{tabular}{|c|c|c|}
\hline
%\rule{0pt}{3ex}%
\multicolumn{3}{|c|}{$\var{a} \implies \var{b}$} \\
\hline
\multicolumn{1}{|c|}{Value of \var{a}} &
    \multicolumn{1}{|c|}{Value of \var{b}} &
    \multicolumn{1}{|c|}{Value of} \\
\multicolumn{1}{|c|}{} &
    \multicolumn{1}{|c|}{} &
    \multicolumn{1}{|c|}{implication} \\
\hline
\var{true} & \var{true} & \var{true} \\
\var{true} & \var{false} & \var{false} \\
\var{true} & Undefined & Undefined \\
\var{false} & \var{true} & \var{true} \\
\var{false} & \var{false} & \var{true} \\
\var{false} & Undefined & \var{true} \\
Undefined & \var{true} & Undefined \\
Undefined & \var{false} & Undefined \\
Undefined & Undefined & Undefined \\
\hline
\end{tabular}
\label{tab:ternary-implication}.
\end{table}

Alongside the set theoretical notions,
and in concept based on them,
are algorithmic notions.
The algorithmic notions are better for describing implementation
on modern computer architectures,
and will be essential for our complexity analysis.

The algorithmic notions are, in concept,
ultimately reducible to sequences of the primitive operations
of a random access machine (RAM), like the one described by Earley
but with two addition understandings.%
\footnote{Jay Earley
  describes his random access machine model in
  \cite[p. 61]{Earley1970}.
}
First, the word size of our machine must be less than $\var{c}\times\var{n}$,
where \var{c} is a constant and \var{n} is the size of the problem under
discussion.
Second, the primitive operations of our RAM
are understood to be those typical of a modern computer.%
\footnote{Jay Earley's primitive operations only included addition --
  his algorithm and his data structure (linked lists) did not need multiplication.
  Our changes bring us in line with the more modern
  approach of Cormen et al. 2009~\cite[p. 23--24]{Cormen2009}.
  In their RAM, multiplication and bit shifts are primitive operations,
  but their RAM does not have an exponentiation instruction.%
}
In fact, we never break any algorithm down even close
to the primitive operation level.
Once a piece is small enough that we can conveniently
characterize its complexity by other means,
we will break it down no further.\footnote{%
  Those who want more implementation details will find
  most of the procedures in this paper,
  including all of
  the procedures required for a practical implementation,
  broken further down in our C language
  implementation
  \cite{Marpa-R2}.%
}

In this paper,
a \dfn{function} is never an algorithm ---
it always refers to the set theoretic notion.
Algorithms are also referred to as \dfn{procedures}.

We consider it clearer to
avoid representing relations in the form
\begin{equation}
\label{eq:relation-example}
\var{x} \var{R} \var{y},
\end{equation}
where \var{R} is a binary relation between \var{x} and \var{y}.
Instead we prefer to deal with relations in the form of the equivalent
set,
or via the boolean function they induce.
For example we might discuss
\Eref{eq:relation-example} via the boolean binary function
\myfn{R}{\var{x}, \var{y}}.

\Earley{} will refer to the Earley's original
recognizer\cite{Earley1970}
with zero characters of lookahead.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\Marpa{} will refer to the recognizer described in
this paper, and implemented in
\Rtwo~\cite{Marpa-R2}.

The procedures which are major building blocks
of \Marpa{} will be called \dfn{ops}.
A second level of \Marpa{} building blocks will
be called \dfn{sub-ops}.
The building blocks of \Earley{} will be
called \dfn{operations}.
This overloads a term with a mathematical meaning
but we feel that following Jay Earley's terminology in \cite{Earley1970}
justifies any risk of confusion.

\begin{table}[t]
\centering
\caption{Type notations}
\begin{tabular}{ll}
\hline
$\var{X}_\var{T}$ & The variable \var{X} of type \type{T} \\
$\var{agg-one}_\set{\var{T}}$ & The variable \var{set-one} of type ``set of \type{T}'' \\
$\var{agg-one}_\seq{\var{T}}$ & The variable \var{seq-one} of type ``sequence of \type{T}'' \\
\type{SYM} & The type for a symbol \\
\type{STR} & The type for a string \\
\type{EIM} & The type for an Earley item \\
\type{RULE} & The type for the rule of a grammar\\
\Tvar{a}{SYM} & Variable \var{a} of type \type{SYM}, subscripted form \\
\Vsym{a} & Variable \var{a} of type \type{SYM}, angle bracket form \\
\Tvar{a}{STR} & Variable \var{a} of type \type{STR}, subscripted form \\
\Vstr{a} & Variable \var{a} of type \type{STR}, angle bracket form \\
\Veim{a} & Variable \var{a} of type \type{EIM} \\
\Vrule{a} & Variable \var{a} of type \type{RULE} \\
\Vsymset{agg-two} & Variable \var{agg-two}, an set  of strings \\
\Vstrset{strings} & Variable \var{strings}, an set of strings \\
\Veimset{agg-three} & Variable \var{agg-three}, an set of Earley items \\
\Veimseq{agg-three} & Variable \var{agg-three}, an sequence of Earley items \\
\Vruleset{agg-four} & Variable \var{agg-four}, an set of grammar rules \\
$\var{i}_\type{\naturals}$ & Variable \var{i}, a non-negative integer \\
\hline
\end{tabular}
\label{tab:type-notation}
\end{table}

Parsing theory examples usually require a cumbersome ``setup''
during which the various variables are introducted and related to
each other.
These setups are repetitive, tedious,
and usually obvious in their details.
In this paper we try to avoid them in favor of
a system of ``contextual binding''.
We believe
that the reader will find this system intuitive
to the degree that the reader is able to ignore it.

Variables in this paper,
as in most of the parsing literature,
have a ``scope''.
Usually scope conventions are left implicit,
but we will find it helpful to make some of our
conventions explicit.
When a new variable is introduced,
unless otherwise stated,
it is bound to,
and visible only within,
the most recently opened scope.

Every definition is a scope,
called a \dfn{definition scope}.
The defiens is visible only within a definition's
scope, but the defiendum is visible outside it.

Every theorem is a scope,
called a \dfn{theorem scope}.
The proof associated with a theorem
is within the scope of the theorem.
Only the theorem itself is visible outside
that scope.
An observation is a scope, with only the observation
itself being visible outside the scope.

The entire paper is the \dfn{global scope}.
Other scopes will be introduced
explicitly.

\todo{Rewrite type system description, add "context" discussion}
In order to abbreviate certain recurring definitions,
for the purposes of this paper,
we use a simple and opportunistic ``type system''.%
\footnote{%
    We might have used the term ``tag'' instead of
    ``type'', in order to avoid raising certain kinds
    of expectation.
    Our types are not intended to help with automated theorem
    proving.
    Also,
    though a \Marpa{} implementation might be able
    to use some of the
    same type names and concepts,
    the types in this paper are not intended to be taken
    as a model of the type system a \Marpa{} implementation
    should follow.
}
Our types are certain sets of values,
which will be specifically defined.
Not all values have a type ---
for example, the types themselves usually do not
have a type.%
\footnote{
   We do not need to assign a type to all of the sets
   we use,
   because the objective of our type
   system is limited to abbreviating
   a specific set of definitions for readability purposes.
}

This paper will
use subscripts to indicate commonly occurring types,
as shown in
\Tbref{tab:type-notation}.

Since a type is a set,
we may express facts about types using set notation.
For example, writing
\[
   \var{x} \in \type{T}
\]
means that \var{x} has type T,
and is alternative to writing $\var{x}_\type{T}$.
We may say that type \type{T1} is a synonym of type \type{T2}
by writing $\type{T1} = \type{T2}$.
And if we write that
\[
   \type{T1} = \type{T2} \cup \type{T3} \land (\type{T2} \cap \type{T3} = \emptyset)
\]
we indicate that a variable type \type{T1} is either
a variable of type \type{T2}
or a variable of type \type{T3},
but that no variable can be both
of type \type{T2} and of type \type{T3}.

Type subscripts are often omitted when the type
is clear from the context.
Symbols and strings
have an alternate type notation,
which uses angle brackets.
Type names are often used in the text
as a convenient way to refer to
their type.

Occasionally we will have two or more distinct variables
in a scope that have the same name and differ only only in type.
This is helpful, for example, when two tuples of the same type are
in scope.
The elements of each tuple can share a variable name,
and be distinguished by type.
We overload in this fashion only when the advantages for clarity
seem to outweigh the disadvantages.

Types are defined explicitly, when they are introduced.
In addition, for every type \var{T},
\begin{itemize}
\item \powerset{\var{T}} is a type; and
\item the sequence whose range is \var{T} and
whose index set is
$\naturals$ is a type.
\end{itemize}

Multi-character variable names will be common.
When the angle bracket notation is used,
concatenation of strings and symbols may be implicit.
Otherwise, operations are never implicit

Since multiplication is never implicit,
we overload the Cartesian product operator
so that it represents multiplication when its
operands are integers:
$\var{n} \times 6$.
Concatenation may also be made explicit:
$\var{a} \cat \var{b}$.
We find it helpful to allow hyphenated variable names,
and therefore we distinguish subtraction typographically:
\[
\var{symbol-count} \subtract \var{terminal-count}.
\]
Floor brackets are a very intuitive way to indicate terminal
symbols:
\Vterm{terminal}.
We therefore represent the floor of a number
as a function:
$\myfn{floor}{\var{a}}$.

\begin{table}[t]
\caption{
    \label{tab:op-precedence}
    Operator precedence cheatsheet}
\begin{tabular}{|p{1.5in}|p{3in}|}
\hline
{$\times$} & Multiplication \\
{$+$} {$\subtract$} & Addition, subtraction \\
   {$\le$} {$<$}
   {$\ge$} {$>$}
   & Equality, comparison \\
{$\cat$} & Concatenation ($a\cat b$) \\
{$\cup$ $\cap$ $\subtract$} & Set union, intersection, difference \\
{$\times$} & Cartesian product (for sets) \\
{$\subset$} {$\subseteq$}
    {$\supset$} {$\supseteq$}
    & Set inclusion, set containment \\
{$\in$} {$\notin$}
    & Set inclusion \\
{$=$} {$\neq$} & Equality, inequality \\
{$\neg$} & Logical negation \\
{$\land$} & Logical and \\
{$\lor$} & Logical or \\
{$\implies$} & Implication \\
{$\iff$} & Logical equivalence \\
{$:$}    & ``such that'' \\
{$\riota$ $\exists$ $\existsUniq$ $\forall$} & Quantifiers \\
{$\defined$} & Definition \\[12pt]
{$\because$} & ``because'' (hint separator) \\[12pt]
\hline
\multicolumn{2}{|l|}{
    \myparbox[4.5in]{%
        Precedence
        is from tightest to loosest,
        so that the loosest precedence is last.
    }
} \\
\hline
\end{tabular}
\end{table}

Arity, associativity and precedence follow tradition,
but precedence presents a difficulty: varying traditions
need to be merged.
We state our choice of precedence conventions
in a ``cheatsheet'' in Table \Tbref{tab:op-precedence}.
Otherwise tradition is followed: bracketing symbols override precedence,
and where an operator is written out in verbal form (``iff'') it has looser
precedence than one written in math symbols $\iff$.

In this paper, we will only need to describe countable cardinalities.
\Vsize{set} is the cardinality of the set \var{set}.
The notation $\Vsize{set} \in \naturals$
indicates that \var{set} is finite.
The notation $\Vsize{set} = \naturals$
indicates that \var{set} is countably infinite.%
\footnote{
    In this case, $\naturals$ is used as a cardinal,
    and many texts would indicate this by writing it $\aleph_0$.
    But $\naturals = \aleph_0$ and,
    since we will not mention any other transfinite cardinals,
    we see no need to have two different notations
    for the same set.%
}

Where \var{seq} is a sequence,
the first element of \var{seq} is \Velement{seq}{0}.%
\footnote{%
   Our use of \VVelement{S}{i} instead of the more traditional
   $\var{S}_\var{i}$ is one of the choices that we
   made hoping to save
   eyesight as well as reading time.
   We are particularly motivated by our memory of
   a paper in which the traditional notation
   required a subscript to a subscript, and
   turned the statement of one of the paper's
   major results
   into an exercise,
   the solution to which required the reader
   to divine whether one of the symbols represented
   the numeral 1 or the letter l.
}
Where \var{seq} is a finite sequence,
and the last element is \Velement{seq}{\Vlastix{seq}}.
Note that this implies that $\Vlastix{seq} = \Vsize{seq}\subtract 1$.

We write \Dom{\var{fn}} for the domain of a function \var{fn},
\Cod{\var{fn}} for the codomain of a function \var{fn},
and \Ran{\var{fn}} for the range of \var{fn}.
A sequence is a function whose domain is a set of 0-based consecutive non-negative
integers so that, where \var{seq} is a sequence,
\Dom{\var{seq}} is the set of valid indexes of \var{seq}
and \Ran{\var{seq}} is the set of valid elements of \var{seq}.

We will write
$\var{s}[\var{a} \ldots \var{z}]$
for the subsequence
\[
    \var{s}[\var{a}], \;\;
    \var{s}[\var{a}+1], \;\;
    \ldots \;\;
    \var{s}[\var{z}].
\]
We require of the subsequence indices that
$\var{z} \in \Dom{\var{seq}}$,
except in one special case.
If
$\var{z} < \var{a}$ then
the values of \var{i} and \var{j} may be negative,
and $\var{s}[\var{a} \ldots \var{z}]$
is the empty sequence, which we may write as $\epsilon$:
\[
\var{z} < \var{a} \implies \var{s}[\var{a} \ldots \var{z}] = \epsilon.
\]

\begin{definition}[Prefix]
\label{def:prefix}
Let \var{x} and \var{y} be sequences.
\begin{gather*}
\Pfx{\var{x},\var{y}} \defined
\exists \; \Vnat{j} : \var{j} \le \Vlastix{y}
\land \var{x} = \Velement{y}{0\ldots \var{j}}.
\\
\PrPfx{\var{x},\var{y}} \defined
\exists \; \Vnat{j} : \var{j} < \Vlastix{y}
\land \var{x} = \Velement{y}{0\ldots \var{j}}.
\end{gather*}
We say
that \var{x} is a \dfn{prefix} of \var{y} if \Pfx{\var{x}, \var{y}};
and that \var{x} is a \dfn{proper prefix} of \var{y} if \PrPfx{\var{x}, \var{y}}.
\dispEnd
\end{definition}

\begin{definition}[Suffix]
\label{def:suffix}
\begin{gather*}
\Sfx{\var{x},\var{y}} \defined
\exists \; \Vnat{j} : \var{j} \le \Vlastix{y}
\land \var{x} = \Velement{y}{\var{j}\ldots \Vlastix{y}}.
\\
\PrSfx{\var{x},\var{y}} \defined
\exists \; \Vnat{j} : 0 < \var{j} \le \Vlastix{y}
\land \var{x} = \Velement{y}{\var{j}\ldots \Vlastix{y}}.
\end{gather*}
We say
that \var{x} is a \dfn{suffix} of \var{y} if \Sfx{\var{x}, \var{y}};
and that \var{x} is a \dfn{proper suffix} of \var{y} if \PrSfx{\var{x}, \var{y}}.
\dispEnd
\end{definition}

Let \Vsymset{alphabet} be a non-empty set of symbols.
Symbols are of type \type{SYM}.
Let \Tvar{x}{SYM} be a symbol.
More commonly we write
\Tvar{x}{SYM} as \Vsym{x}.

let $\var{alphabet}^\ast$ be the set of all strings
(type \type{STR}) formed
from those symbols.
Let \Tvar{s}{STR} be a string.
More commonly we write
\Tvar{s}{STR} as \Vstr{s}.
\size{\Vstr{s}} is the length of \Vstr{s},
counted in symbols.
Let $\var{alphabet}^+$ be
\begin{equation*}
\left\{ \Vstr{x}
\; \middle| \;
\Vstr{x} \in \var{alphabet}^\ast
\; \land \;
\Vsize{\Vstr{x}} > 0
\right\}.
\end{equation*}

A string can be seen as sequence of symbols,
and we use sequence index
to represent individual symbols of a string,
and subsequence notation
to represent substrings.
Thus,
the \var{i}'th character of the string,
is \sym{\var{str}[\var{i}]};
and
\begin{equation*}
\str{\var{str}[\var{a} \ldots \var{z}]} \defined
\str{\lbrace
    \var{str}[\var{a}], \;\;
    \var{str}[\var{a}+1], \;\;
    \ldots \;\;
    \var{str}[\var{z}]
\rbrace}.
\end{equation*}
We sometimes describe the last character of a
string as its rightmost:
\[
\Rightmost{\str{\var{str}[\var{a} \ldots \var{z}]}} = \var{z}.
\]

$\Lambda$ is often used as a special ``null'' value.
$\Lambda$ is untyped, but a variable of any type may have
a $\Lambda$ value.
Except when explicitly stated otherwise,
the only operations allowed for $\Lambda$ are
assignment to a variable,
and comparison.
When used in a comparison,
$\Lambda$ compares equal to itself
and unequal to any other value.

Explicit ``Definition'' sections and boldface are reserved
for definitions that are of significance throughout
this paper.
In some contexts,
as for example in stating and proving
some theorems,
it is convenient to have definitions
which are ``local'' ---
useful only within
a particular presentation.
Local definitions are given in double quotes,
(``definition''),
and in numbered equations.

Equations are sometimes stated verbally,
at least in part.
Whether stated verbally or symbolically,
equations are often followed by a ``hint''.
The ``hint'' is a list of considerations,
such as theorems, definitions
and other equations,
intended to justify the assertion of the equation
in that context.
For clarity, the ``hint'' is separated
from the body of the equation by a ``because'' symbol (``$\because$'').

Where an equation is a definition,
its justification will state that is a
``definition for the purpose of
presentation'',
which may be abbreviated ``DFPP''.
``WLOG'' abbreviates ``assumed without loss of generality''.
``Def'' abbreviates ``definition''.
``AF'' abbreviates ``assumed for''.
For example, ``AF theorem'' abbreviates ``assumed for the theorem''.

A definition usually does not raise problematic existence
considerations,
and the consistency of a definition usually follows immediately from
its defiens.
Where existence or consistency is non-obvious,
a definition is stated as part
of a theorem, with a proof.
Definitions stated as theorems are often definitions
of functions,
as the definition of a function implies existence,
and proving the existence of the function can be non-trivial.

\chapter{Context-free grammars}
\label{chap:CFGs}

\begin{definition}[Context-free grammar]
\label{def:context-free-grammar}
A \dfn{context free grammar} (CFG) is a 4-tuple,
\begin{equation}
\begin{gathered}
\text{$[\Vsymset{nt}, \Vsymset{term}, \Vruleset{rules}, \Vsym{accept}]$ such that} \\
\Vsymset{nt} \cup \Vsymset{term} = \emptyset, \\
\text{$\Vsym{accept} \in \Vsymset{nt}$, and} \\
\text{\Vruleset{rules} is a set of elements of type \type{RULE}.}
\end{gathered}
\end{equation}
We will define type \type{RULE} shortly.
Context-free grammars have the type CFG.

For convenience,
for a CFG, \Vcfg{g},
we write
\begin{equation*}
\begin{alignedat}{3}
& \NT{\Vcfg{g}} && \defined && \quad \Vsymset{nt}, \\
& \Term{\Vcfg{g}} && \defined && \quad \Vsymset{term}, \\
& \Accept{\Vcfg{g}} && \defined && \quad \Vsym{accept}, \\
& \Rules{\Vcfg{g}} && \defined && \quad \Vruleset{rules}, \quad \text{and} \\
& \Vocab{\Vcfg{g}} && \defined && \quad \Vsymset{nt} \cup \Vsymset{term}.
\end{alignedat}
\end{equation*}
The elements of the set \NT{\var{g}}, are non-terminal symbols,
or simply \dfn{non-terminals}.
The elements of the set \Term{\var{g}}, are terminal symbols,
or simply \dfn{terminals}.
\Accept{\var{g}} is the \dfn{accept symbol},
and in the parsing literature is often called
the ``start symbol''.
\Vocab{\var{g}} is
the \dfn{vocabulary} of \Vext{g}.

A \dfn{rule}
(type \type{RULE})
is a duple
\begin{equation*}
\left[ \Vsym{lhs} \de \Vstr{rhs} \right]
\end{equation*}
such that
\begin{equation*}
\Vsym{lhs} \in \NT{\var{g}} \; \land \;
\Vstr{rhs} \in \Vocab{\var{g}}^\ast.
\end{equation*}
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
The LHS and RHS of \Vrule{r} may also be
referred to as
$\LHS{\var{r}}$ and $\RHS{\var{r}}$, respectively.
\dispEnd
\end{definition}

\begin{definition}[Context grammar]
Definitions, theorems
and other statements in this document will imply
the existence of a grammar.
If not stated otherwise,
all statements are in context of the \dfn{context grammar}.
A context grammar is always in scope.
The global-scope context grammar, until redefined,
is \Vcfg{g}.
\dispEnd
\end{definition}

\begin{definition}[Rule size]
\label{def:rule-size}
The size of \var{r}, $\size{\Vrule{r}}$,
is the number of symbols on its RHS,
so that
\begin{equation*}
\size{\Vrule{r}} = \size{\RHS{\var{r}}}.
\end{equation*}
If $\size{\Vrule{r}} = 0$, then \Vrule{r}
is an \dfn{empty rule}.
If $\size{\Vrule{r}} = 1$, then \Vrule{r}
is a \dfn{unit rule}.
\end{definition}

\begin{definition}[Terminal string convention]
\label{def:terminal-string}
When a string
is a string of terminal symbols,
we will write it in boldface and enclose it
between a pair of the brackets traditionally used
for the ``floor'' operation: for example,
\Vterm{terminal}.
\end{definition}

Traditionally in the parsing literature,
introducing a new scope
frequently requires a tedious and repetitive setup.
We will avoid these in favor of \dfn{binding assumptions}.
The binding assumptions are a set of assumptions
that are implicitly made,
unless stated otherwise,
whenever a new variable is introduced.
The following definition
introduces the first of our binding assumptions.

\begin{definition}[SYM, STR and RULE binding assumptions]
\label{def:sym-str-rule-binding}
Let \var{cg} be the context grammar of the most recent scope
when a variable is bound.
Unless otherwise stated,
\begin{itemize}
\item a variable of \type{SYM} is assumed to be an element of \Vocab{\var{cg}};
\item a variable of \type{STR} is assumed to be an element of $\Vocab{\var{cg}}^\ast$; and
\item a variable of \type{RULE} is assumed to be an element of \Rules{\var{cg}}.
\end{itemize}
\dispEnd
\end{definition}

As examples of
\Dfref{def:sym-str-rule-binding},
\begin{align*}
& \text{\Vsym{sym} will imply that $\Vsym{sym} \in \Vocab{\var{g}}$}, \\
& \text{$\var{sym}_\type{SYM}$ will imply that $\Vsym{sym} \in \Vocab{\var{g}}$}, \\
& \text{\Vstr{str} will imply that $\Vstr{str} \in \Vocab{\var{g}}^\ast$}, \\
& \text{$\var{str}_\type{STR}$ will imply that $\Vstr{str} \in \Vocab{\var{g}}^\ast$, and} \\
& \text{\Vterm{str} will imply that $\Vterm{str} \in \Term{\var{g}}^\ast$}, \\
& \text{\Vrule{r} will imply that $\Vrule{r} \in \Rules{\var{g}}$}.
\end{align*}

A grammar describes a rewriting system.
We consider
\begin{equation}
\label{eq:rewrite-step}
\Vstr{prefix}\Vsym{lhs}\Vstr{suffix} \derives \Vstr{prefix}\Vstr{rhs}\Vstr{suffix}
\end{equation}
a valid rewrite step of \Vcfg{g} iff
\begin{gather*}
\Vstr{prefix} \in \Vocab{\Vcfg{g}}, \\
\Vstr{suffix} \in \Vocab{\Vcfg{g}}, \quad \text{and} \\
[\Vsym{lhs} \de \Vstr{rhs}] \in \Rules{\Vcfg{g}}.
\end{gather*}
The valid rewrite step of
\Eref{eq:rewrite-step}
is a \dfn{right rewrite step} iff
\Vsym{lhs} is the rightmost non-terminal.

A sequence of iterated rewrite steps of \Vcfg{g}, such that
\begin{equation}
\label{eq:derivation}
\Vstr{s1} \derives \Vstr{s2} \derives \Vstr{s3} \ldots \Vstr{sec-n}
\end{equation}
is called a \dfn{derivation} of \Vcfg{g},
and a rewrite step that is part of a derivation is called
a \dfn{derivation step}.
A derivation is a \dfn{right derviation} iff
all of its rewrite steps are right rewrite steps.

% TODO: Do I need the definition of \dfn{derivation sequence}.
The sequence of strings in a derivation, its sequence of strings
is called its \dfn{derivation sequence}.
The derivation sequence of \Eref{eq:derivation} is
\begin{equation*}
\Vstr{s1}, \Vstr{s2}, \Vstr{s3} \ldots \Vstr{sec-n}.
\end{equation*}

We explicitly indicate that some \Vstr{x} derives \Vstr{y} in \var{k} steps
by writing
\begin{equation*}
\Vstr{x} \xderives{\var{k}} \Vstr{y}.
\end{equation*}
Writing $\Vstr{x} \xderives{1} \Vstr{y}$ is equivalent
to writing $\Vstr{x} \derives \Vstr{y}$.

By convention,
every string derives itself in zero steps.
\begin{equation*}
\Vstr{x} = \Vstr{y} \iff \Vstr{x} \xderives{0} \Vstr{x}.
\end{equation*}
A derivation of zero steps is called a
\dfn{trivial derivation}.
A derivation of more than zero steps is called a
\dfn{non-trivial derivation}.

We write
\begin{equation*}
\Vstr{x} \deplus \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in one or more steps.
We write
\begin{equation*}
\Vstr{x} \destar \Vstr{y}
\end{equation*}
to say that \Vstr{x} derives \Vstr{y} in zero or more steps.
Where
\begin{equation*}
\var{d} = \Vstr{first} \destar \Vstr{last}
\end{equation*}
is a derivation and
\Vstrset{dseq} is the derivation sequence of \var{d},
then saying that
\begin{equation*}
\var{dseq}[\var{i}] = \Vstr{s}
\end{equation*}
is equivalent to saying that
\begin{equation*}
\var{d} = \Vstr{first} \xderives{\var{i}} \Vstr{s} \destar \Vstr{last}.
\end{equation*}
Note that, as a result of the above definitions,
$\epsilon \xderives{0} \epsilon$ and
$\epsilon \destar \epsilon$.

We write
\begin{equation*}
\Vstr{x} \xderives{R} \Vstr{y}.
\end{equation*}
to say that \Vstr{x} right derives \Vstr{y}.
The ``$R$'' superscript may be combined with any of the other superscripts
so that, for example,
\begin{equation*}
\Vstr{x} \xderives{R+} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in one or more steps;
and
\begin{equation*}
\Vstr{x} \xderives{R(\var{k})} \Vstr{y}
\end{equation*}
states that that \Vstr{x} right derives \Vstr{y} in \var{k} steps.

Let \Vcfg{g} be a CFG.
A \dfn{sentential form} is a string which can be derived from the start
symbol, \Accept{\Vcfg{g}}.
If the sentential form \Vterm{sf} is a string of terminals,
then \Vterm{sf} is a \dfn{sentence} of \Vcfg{g}.

Let $\Vstr{a} \in \Vocab{\var{g}}^\ast$.
The \dfn{language} of \var{g} and \Vstr{a},
is
\begin{equation}
\label{eq:language}
\myL{\Vcfg{g}, \Vstr{a}}
\defined
\set{
\quad
\begin{aligned}
& \Vterm{sentence} \quad \text{such that} \\
& \qquad \begin{aligned}
  & \Vstr{a} \destar \Vterm{sentence} \\
  & \land \Vterm{sentence} \in \Term{\Vcfg{g}}^\ast.
  \end{aligned}
\end{aligned}
\quad
}
\end{equation}
The \dfn{language} of the grammar \Vcfg{g} is its set
of sentential forms:
\[
\myL{\var{g}} \defined \myL{\var{g}, \Accept{\var{g}}}.
\]

\begin{definition}[Parse]
\label{def:parse}
A \dfn{full parse} is a duple \tuple{ \Vcfg{g}, \Vterm{w} } such that
$\Vterm{w} \in \myL{\var{g}}$.
A \dfn{partial parse} is a duple \tuple{ \Vcfg{g}, \Vterm{inp} } such that,
for some \Vterm{lookahead},
\[
\tuple{ \Vcfg{g}, \Vterm{inp}\Vterm{lookahead} }
\]
is a full parse.
A partial parse,
\tuple{ \Vcfg{g}, \Vterm{inp} }
is \dfn{consistent} with a full parse \tuple{ \Vcfg{g}, \Vterm{w} }
iff
\[
\exists \; \Vterm{lookahead} : \Vterm{inp}\Vterm{lookahead} = \Vterm{w}.
\]
A \dfn{partial parse} is often called, simply, a \dfn{parse}.
Note that, by this definition,
a full parse is a special case of a partial parse.
\dispEnd
\end{definition}

\begin{definition}[Context parse scopes]
\label{def:context-parse-scope}
From this point on, the global scope
and every new
definition or theorem scope
will be a \dfn{context parse scope}.
Unless otherwise stated,
the \dfn{context parse} of these scopes
will be a partial parse $[\Vcfg{g}, \Vstr{w}]$.
the \dfn{context input} will be \Vstr{w},
and the \dfn{context grammar} will be \Vcfg{g}.
\dispEnd
\end{definition}

\begin{definition}[Context input]
From this point on, unless otherwise stated,
the \dfn{context input} is the input of the context parse.
\dispEnd
\end{definition}

\begin{definition}[Context grammar redefined]
From this point on, unless otherwise stated,
the \dfn{context grammar} is the grammar of the context parse.
\dispEnd
\end{definition}

We say that symbol \Vsym{x} is \dfn{nullable} iff
$\Vsym{x} \destar \epsilon$.
\Vsym{x} is \dfn{nulling} iff it always derives the null
string, that is, for all \Vstr{y}
\begin{equation*}
\Vsym{x} \destar \Vstr{y} \implies \Vstr{y} \destar \epsilon.
\end{equation*}
\Vsym{x} is \dfn{non-nullable} iff it is not nullable.
\Vsym{x} is a \dfn{proper nullable} iff it is nullable,
but not nulling.

Locations in the input will be of type \type{LOC}.
By tradition, the input to the parse is written as \Cw{},
where $\Cw \in \Term{\var{g}}^\ast$.
\Vsize{w} will be the length of the input, counted in symbols.
When we state our complexity results later,
they will often be in terms of $\var{n}$,
where $\var{n} = \Vsize{w}$.

Unless \Vstr{w}, the context input, is otherwise specified,
use of the type \type{LOC} will bind
its variable,
so that
\begin{equation*}
\myparbox{\Vloc{j} will imply that $0 \le \var{j} \le \size{\Vterm{w}}$.} \\
\end{equation*}

\begin{definition}[Token]
\label{def:token}
In discussion that are closer to the implementation,
the contents of the location \Vloc{j}
in an input \Vstr{w}
will be treated as a \dfn{token}.
A token is a duple
\[
  \left[ \Vsym{token}, \var{token-value} \right],
\]
where \Vsym{token} = \Velement{w}{j} is the \dfn{token symbol},
and \var{token-value} is the \dfn{token value}.

The token value is irrelevant to parsing --- it exists
in order to be passed on to the semantics.
In \Marpa{},
the implementation of the token is a pointer to a structure
containing a pointer to the token symbol,
and a pointer to the token value,
which is a value of arbitrary type.\footnote{
In C language,
these can be implemented as so-called ``void pointers''.}
\dispEnd
\end{definition}

Where an input location, \Vloc{j},
is the token, $\var{token} = [\Vsym{sym}, \var{value}]$,
we will write
\begin{gather*}
\Symbol{\Vloc{j}} \defined \Vsym{sym} = \VVelement{w}{j} \\
\Symbol{\var{token}} \defined \Vsym{sym} = \VVelement{w}{j} \\
\Origin{\Vloc{j}} \defined \var{j} \\
\Origin{\var{token}} \defined \var{j} \\
\Current{\Vloc{j}} \defined \var{j}+1 \\
\Current{\var{token}} \defined \var{j}+1.
\end{gather*}
The usefulness of these notations will be clearer later, when
we introduce parse items.

\chapter{LR grammars}
\label{chap:lr-grammars}

\begin{definition}[Handle]
Let \Vstr{sf} be a sentential form of \Vcfg{g}.
A \dfn{handle} of \Vstr{sf} is an ordered pair $[\Vrule{r}, \var{i}]$ such
that,
for some $\Vterm{lookahead} \in \Term{\var{g}}^\ast$,
\begin{gather}
0 \le \var{i} \le \size{\Vstr{sf}}, \\
\begin{aligned}
\Accept{\var{g}} \xderives{R\ast} & \Vstr{stack}\Vsym{lhs}\Vterm{lookahead} \\
   \xderives{R} & \Vstr{stack}\Vsym{rhs}\Vterm{lookahead} \\
   = & \Vstr{sf},
\end{aligned}
\\
\Vrule{r} = \var{lhs} \de \Vstr{rhs}, \;\; \text{and} \\
\var{i} = \size{\Vstr{stack}\Vstr{rhs}}. \quad \dispEnd
\end{gather}
\end{definition}

Let $\Vtermset{phrases} = \Term{\var{g}}^\ast$.
In this paper, a \dfn{partition}, call it $\pi$,
is a subset of \powerset{\var{phrases}},
where all of the elements of $\pi$ are non-empty and disjoint,
and where the elements of $\pi$ ``cover'' \var{phrases} in the sense that
their union is equal to \var{phrases}.
That is,
\begin{gather}
\pi \subseteq \powerset{\var{phrases}}, \\
\forall \; \var{x} \in \pi : \var{x} \neq \emptyset, \\
\var{phrases} = \set{\var{phr2} \in \Vtermset{X} : \var{X} \in \pi }
\end{gather}
and
\begin{gather}
\forall \; \Vtermset{x} \in \pi : \forall \; \Vtermset{y} \in \pi : \var{x} \cap \var{y} = \emptyset \lor \var{x} = \var{y}
\end{gather}
Note that in this paper $\pi$ will usually, but not necessarily,
be of finite cardinality.

We call an elements of $\pi$, a \dfn{cell}.
When two terminal strings, \Vterm{ss1} and \Vterm{ss2},
are in the same cell of $\pi$,
we say that \var{ss1} and \var{ss2} are ``equivalent modulo $\pi$'', or
\[
\var{ss1} \iff \var{ss2} \pmod\pi.
\]

Informally, an LR($\pi$) grammar is a grammar where two
sentential forms that differ only in their lookaheads
have the same handle,
if those lookaheads are in the same cell of $\pi$.
More formally,

\begin{definition}[LR grammar]
\label{def:LR}
\begin{align}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{history1}\Vsym{lhs1}\Vstr{lookahead1} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs1}\Vstr{lookahead1} \\
& = \Vstr{behind}\Vstr{lookahead1},
\end{aligned}
\\
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{history2}\Vsym{lhs2}\Vstr{lookahead2} \\
& \xderives{R} \Vstr{history2}\Vstr{rhs2}\Vstr{lookahead2} \\
& = \Vstr{behind}\Vstr{lookahead2},
\end{aligned}
\\
\Vstr{lookahead1} \iff \Vstr{lookahead2} \pmod \pi,
\end{align}

We say that an LR($\pi$) grammar is LR-regular (LRR)
iff $\pi$ is a finite regular partition.
We say that an LR($\pi$) grammar is LR($\var{k}$)
iff
\begin{equation}
\pi =
\begin{gathered}
\left\lbrace
\left\lbrace
\var{u}\var{v}
\; \middle| \;
\var{v} \in \Term{\var{g}}^\ast
\right\rbrace
\; \middle| \;
(
\var{u} \in \Term{\var{g}}^\ast \land \Vsize{u} = \var{k}
)
\right\rbrace
\\
\cup
\left\lbrace
\left\lbrace
\var{u}
\right\rbrace
\; \middle|\;
( \var{u} \in \Term{\var{g}}^\ast \land \Vsize{u} \le \var{k} )
\right\rbrace
\end{gathered}
\end{equation}

Every LR($\pi$) grammar is unambiguous.
If a grammar is unambiguous then it is LR($\pi$)
for the partition of infinite cardinality,
\[
\pi = \set{\set{\Vstr{x}} : \var{x} \in \Term{\var{g}}^\ast}. \quad \dispEnd
\]

\end{definition}

\Thref{th:LR}
presents \Dfref{def:LR}
in a way that is easier to apply.
\begin{theorem}[LR($\pi$) grammar]
\label{th:LR}
\footnote{
The statement of the theorem follows \cite[p. 70]{Culik1973}.
}%
A grammar, \Vcfg{g}, is LR($\pi$) for a partition $\pi$,
if, given
\begin{gather}
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history1}\Vsym{lhs1}\Vstr{lookahead1} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead1},
\end{aligned}
\\
\begin{aligned}
\Accept{\Vcfg{g}} & \xderives{R\ast} \Vstr{history2}\Vsym{lhs2}\Vstr{lookahead3} \\
& \xderives{R} \Vstr{history1}\Vstr{rhs}\Vstr{lookahead2},
\end{aligned}
\;\; \text{and}
\\
\Vstr{lookahead1} \iff \Vstr{lookahead2} ( \text{mod $\pi$} )
\end{gather}
we have
\begin{gather*}
\Vsym{lhs1} =
\Vsym{lhs2},
\\
\Vstr{history1} =
\Vstr{history2}, \;\; \text{and}
\\
\Vstr{lookahead3} =
\Vstr{lookahead2}.
\end{gather*}
\end{theorem}

\begin{proof}
Directly from \Dfref{def:LR}. \myqed
\end{proof}

\chapter{External grammars}
\label{chap:EXTs}

\section{Cycles}

An \dfn{external grammar} (type \type{EXT}) is a cycle-free CFG.
A CFG is \dfn{cycle-free} if it contains no cycles.
A \dfn{cycle string}, or \dfn{cycle} is a sentential form
which non-trivially derives itself.
Intuitively, a cycle is the parsing equivalent of an infinite loop.
A \dfn{cycle derivation} is any derivation of the form
\begin{equation*}
\label{eq:cycle}
 \Vstr{cycle} \deplus \Vstr{cycle}.
\end{equation*}
Cycle strings and cycle derivations are also often called
simply ``cycles''.

Cycles are recursions but most recursions
are not cycles.
In a recursion, a symbol \Vsym{recurse} derives itself as part of a string.
That is, \Vsym{recurse} is recursive iff
\begin{equation*}
 \Vsym{recurse} \deplus \Vstr{pre} \cat \Vsym{recurse} \cat \Vstr{post}.
\end{equation*}
\Vsym{recurse} is a cycle only if
both \Vstr{pre} and \Vstr{post} are the null string:
\begin{equation*}
 \Vstr{pre} = \Vstr{post} = \epsilon.
\end{equation*}

Earlier implementations of \Marpa{} have supported cycles,
but the current implementation of \Marpa{} does not.
When the current implementation finds a cycle in a grammar,
it print a message describing the cycle and throws
a fatal error.
This seems to be the behavior users desire.

There is no indication that cycles are of practical use.
Nor does the theoretical literature suggest potential uses for
them.
Support for cycles did require complicated extra code,
and removing the support of cycles seems to be cost-free.

When \Marpa{} did support cycles,
it did so by cutting the cycle short at cycle depth 1.
Intuitively, the cycle depth is the number of times the cycle
string returns to itself.
More formally, if \Vstrset{d} is the derivation sequence
of the cycle derivation
$\Vstr{cycle} \xderives{\var{n}} \Vstr{cycle}$
then the cycle depth is
\begin{equation}
\left| \;
\left\lbrace \;
\var{i}
\quad \middle| \quad
\begin{gathered}
0 \le \var{i} \le \var{n} \quad \text{and} \; \\
\var{d}[\var{i}] = \Vstr{cycle}
\end{gathered}
\; \right\rbrace
\; \right|
\end{equation}
It follows that
a cycle of cycle depth 1 begins and ends with the
cycle string, but does not derive the cycle string
in any of its intermediate steps.

\chapter{Useless symbols}
\label{chap:useless-symbols}

\section{Unproductive symbols}
The current implementation of
\Marpa{} supports unproductive
symbols.
By default, \Marpa{} treats an unproductive symbol
as a fatal error,
but the user may bypass the fatal error
by marking the unproductive symbol as a ``unicorn''.

A \dfn{productive symbol}
is one which derives a terminal string.
That is, \Vsym{prod} is productive if,
for some \Vterm{phrase},
\begin{equation*}
\Vterm{phrase} \in \Term{\var{g}}^\ast
\;  \land \;
  \Vsym{prod} \destar \Vterm{phrase}
\end{equation*}
It is vacuously true that
the empty string is a terminal string,
and therefore all nullable
symbols are productive.

A symbol is \dfn{unproductive} if it is not
productive.
In the current version of \Marpa{},
unproductive symbols can be implemented by
defining \dfn{unicorn} terminals.
A unicorn terminal is a terminal symbol
which can never be
found in the input.

In the \Marpa{} implementation,
a terminal can be made into a unicorn by
requiring that
a terminal match a regular expression that does not match
any string in $\Vocab{\Vint{g}}^\ast$.
For example,
the POSIX expression
\texttt{[\char`^ \char`\\ D\char`\\ d]}
indicates that the terminal
must match a single character which
is both a digit and a non-digit.
This is, of course, not possible
and therefore any terminal required to match that POSIX expression
will never match anything in the input.

Unproductive symbols often prove useful in \Marpa{}
applications.
One example of the utility of unproductive symbols
is interaction with custom lexers.
Marpa allows custom lexers, and these can feed lexemes
to the parser under application control,
observe the result,
and react accordingly.
The custom lexer can hand control back to \Marpa{}'s
own lexer at any point.

It is often convenient for \Marpa{} to use its internal lexer
in most situations,
switching over to custom lexer only at certain points.
To do this, unicorn terminals can
be defined and placed in the grammar at points where the
custom lexer should take over.
Marpa will never recognize a unicorn in the input,
but it does keep track of when unicorns are expected.
Marpa can be configured to trigger a run-time event
when the unicorn is expected.
The application will receive control when the event occurs,
and it can then invoke the custom lexer.

For our theoretical results in this paper,
we treat all symbols as if they were productive.
This is consistent with being conservative,
since a ``unicorn'' can always be feed to the parser
by a custom lexer and,
in that sense, all \Marpa{} symbols are productive.

\section{Inaccessible symbols}

An \dfn{inaccessible symbol} is one which can never be derived from
the accept symbol.
That is, \Vsym{noacc} is inaccessible in \Vext{g} iff
\begin{equation*}
\neg \; \exists \; \Vstr{pre}, \Vstr{post} \; \mid \; \Accept{\var{g}} \destar \Vstr{pre} \Vsym{noacc} \Vstr{post}.
\end{equation*}

The current implementation of \Marpa{} supports inaccessible symbols.
The user can specify \Marpa{}'s treatment
of inaccessible symbols:
inaccessible symbols can be silently ignored,
they can be reported as warnings,
or they can be treated as fatal errors.
Treatment of inaccessible symbols can be specified
symbol by symbol,
and the default treatment can also be specified.
The ``factory default'' is
to report inaccessible symbols as warnings.

Inaccessible symbols have been found useful in \Marpa{} applications
that use higher-level languages ---
languages which generate other languages.
Often the generated grammar consists of
\begin{itemize}
\item ``custom''
rules which vary, and
\item ``boilerplate'' rules,
which are the same in every generated grammar.
\end{itemize}
It can be useful to have sections of the ``boilerplate''
be optional,
so that some of the ``boilerplate'' rules are only used
if the ``custom'' rules call for them.
If the custom rules do not call for a set of optional rules
within the boilerplate,
those boilerplate rules become inaccessible.

Optional boilerplate can be implemented
by silencing inaccessibility errors for the symbols in the optional boilerplate.
Usually it is possible to locate a topmost symbol for
each section of optional boilerplate,
in which case the warning need only be silenced for those
topmost symbols.

\chapter{Rewriting the grammar}
\label{chap:rewrite}

\section{The Aycock-Horspool parser}

A parser that claims to be practical must accept
grammars with nullable and nulling symbols
and empty rules.
But difficulties in handling these correctly
have bedeviled previous attempts
at Earley parsing.

Marpa follows Aycock and Horspool\cite{AH2002} in using
grammar rewrites to handle these issues.
Marpa's recognizer works using
Marpa internal grammars, but
Marpa's grammar rewriting allows the user to specify
their parse in terms of an external grammar.
In \Marpa{}, parsing, evaluation and
run-time events are performed
in such a way that the internal grammar
is invisible to
the user.

We defined \Marpa{}'s external grammars in
\Chref{chap:EXTs}.
Recall that an external grammar may be any cycle-free grammar.
Marpa internal grammars are described by construction
in this section.
They will be defined more carefully
in \Chref{chap:INTs}.
This section describes the rewrite
from an external grammar
to an internal grammar.

Grammar rewriting was not the focus
of \cite{AH2002} ---
instead that paper centered on revising Earley's algorithm
to use LR(0) states.
The first version of \Marpa{}\cite{Kegler2019}
used the LR(0) states of \cite{AH2002},
combining them with Joop Leo's improvements\cite{Leo1991}.

Adding the LR(0) states to the first version of \Marpa{}
made its theory much more complex,
and the change offered
no complexity gains in terms of Landau notation.
The hope was that benefits would be seen in practice.

Unfortunately, these benefits did not emerge.
Marpa's experience in using, developing and maintaining
LR(0) states in \Marpa{}'s early versions showed
that LR(0) states added
greatly to the complexity of the code;
were an obstacle to the implementation of run-time features
in the parser;
and produced no real improvement in speed or in space requirements.
On the other hand,
Aycock and Horspool's idea
of using grammar rewriting
to deal with
with nullable symbols became
fundamental to the \Marpa{} algorithm.

Rewriting grammars is a common
technique
in the parsing literature,
but many of these rewrites are almost totally without
usefulness in practice.
In the academic literature,
a rewritten grammar may recognize the same language,
but often it will not preserve the semantics
of the grammar.

Very few users of CFG's are
content with recognizing a language.
Almost always the user of a grammar-driven parser
formulated the rules and symbols of that grammar
in terms of a semantics,
and that user wants the parser
to preserve their semantics.
If a rewrite scrambles the user's semantics,
then that rewrite cannot play an essential role
in a general-purpose practical parsing tool.

Marpa restricts itself to using rewrites that
are \dfn{semantics-safe}.
The intent behind
semantics-safe rewrites
is that they
allow the rules and symbols of the external grammar
to be reconstructed from a parse using the internal grammar,
not just at evaluation time,
but at run-time as well.
\Sref{sec:augmentation},
\Sref{sec:eliminating-proper_nullables},
\Sref{sec:removing-nulling-symbols},
and
\Sref{sec:trivial-grammars}
describe the rewrites
applied by \Marpa{}.
We will state \Marpa{}'s definition
of semantics-safe more precisely
in section \Sref{semantics-safe}.

\section{Augmenting the grammar}
\label{sec:augmentation}

Let \Vint{g} be a \Marpa{} internal grammar
such that $\Vsym{accept} = \Accept{\Vint{g}}$.
There must be
a dedicated \dfn{accept rule} for \Vint{g},
\begin{equation}
\label{eq:def-accept-rule}
\begin{gathered}
  \Vrule{accept} = \tuple{\Vsym{accept} \de \Vsym{top}} \text{ such that} \\
  \Vsym{top} \in \NT{\Vint{g}} \\
  \land \; \forall \; \Vrule{r} \left(\Vsym{accept} \notin \RHS{\Vrule{r}} \right)
  \\
  \land \; \forall \; \Vrule{r}
      \left(
        \LHS{\Vrule{r}} = \Vsym{accept} \implies \Vrule{r} = \Vrule{accept}
      \right)
\end{gathered}
\end{equation}

The dedicated accept rule is also called
the \dfn{augment rule}.
The semantics for the augment rule are simple ---
the semantics of \Vsym{top} are passed up to
\Vsym{accept}.
This is obviously semantics-safe.

In the \Marpa{} implementation,
the general method
described in section \Sref{semantics-safe}
for dealing with the semantics of rewritten rules
is not applied to the accept rule.
Instead, the accept rule is
dealt with as a special case.

\section{Eliminating proper nullables}
\label{sec:eliminating-proper_nullables}

Recall that a proper nullable
is a nullable symbol which is not nulling.
Marpa eliminates proper nullables with an improved
version of the rewrite
of~\cite[Algorithm 2.10, p. 148]{AU1972}
and~\cite{AH2002}.
In \Marpa{}'s rewrite,
rules with proper nullables are identified
and a new grammar is created
in which the external grammar's rules are divided
up so that no rule has more than two proper nullables.
This is similar to a rewrite into Chomsky form.

Every proper nullable symbol is then cloned into two others:
one nulling and one non-nullable.
All occurrences of the original proper nullable symbol are then replaced
with one of the two new symbols.
New rules are created as necessary to ensure that all possible combinations
of nulling and non-nullable symbols are accounted for.

The rewrite in~\cite{AH2002}
was similar, but did not do the Chomsky-style
rewrite before replacing the proper nullables.
As a result the number of rules in the post-rewrite grammar of \cite{AH2002}
could be \order{e^\ell}, where $\ell$ is the maximum length of a rule in the
the pre-rewrite grammar of~\cite{AH2002},
\[
\ell = \max\left(\set{ \Vsize{r} : \Vrule{r} \in \Rules{\Vext{pre-rewrite}} }\right).
\]
In our version, the worst case growth in the number of rules is \order{\ell}.

In traditional complexity terms,
because $\ell$ is a constant which
depends on the grammar, and often a very reasonable one,
minimizing the rule count in terms of $\ell$ does not
appear to be a important consideration.
In practice, however, languages and language statements which have many optional clauses are important.
The SELECT statement of SQL is one example.
A straight-forward BNF representation of the syntax of a statement
with many optional clauses has many proper nullables on the RHS,
so that, in these cases,
going exponential on $\ell$ has a cost
which it is worth some trouble to avoid.

\section{Removing nulling symbols}
\label{sec:removing-nulling-symbols}

After the rewrite of
\Sref{sec:eliminating-proper_nullables},
all the nullable symbols in the grammar are also
nulling symbols.
Note that, if \Vext{g} is an external grammar,
and
if \varprime{g}{'} is \Vext{g}
revised to eliminate
all nulling symbols,
that
the language of the two grammars is the same: $\myL{\Vext{g}} = \myL{\varprime{g}{'}}$.
Similarly,
other than
the presence and absence of the nodes for the nulling symbols themselves,
the parse trees generated from \Vext{g} are the same as those generated from \varprime{g}{'}.

Thus, \Marpa{} may rewrite its grammars to eliminate nulling symbols.
For the purposes of evaluation and to allow the generation of parse-time events
based on nulled symbols,
Marpa records the locations of the eliminated nulling symbols.

A corner case occurs when eliminating nulling symbols from two distinct
rules makes them identical.
In this paper,
we assume a rewrite that creates
a new LHS symbol for each rule that must remain distinct;
and that adds new rules to derive the new LHS symbols from the original ones.
Less clean theoretically,
but easier in practice,
is to have the implementation treat rules as distinct
even if they differ only in their recorded nulling symbols.
This is what the \Marpa{} implementation does.

\section{Trivial grammars and null parses}
\label{sec:trivial-grammars}

Null parses are parses of zero length inputs.
The \Marpa{} implementation
handles null parses by treating them
as a special case.

\dfn{Degenerate grammars} are grammars which do not accept any string.
A degenerate grammar is treated as a fatal error.

\dfn{Trivial grammars} are grammars which only accept
zero length inputs.
No \Marpa{} internal grammar can be a trivial grammar.
Trivial grammars are handled as a special case.

\section{Semantics-safe rewriting}
\label{semantics-safe}

This section describes the restrictions
that rewrites must follow in order to be semantics-safe.
Consider an input \Cw{} and
an external grammar, \Vext{g}.
Let \Vint{g} be the internal grammar that is
the rewritten version of \Vext{g}.

Let
\begin{equation}
\label{eq:parse-tree}
\var{pt} = \var{tree}(\var{g},\Vstr{w})
\end{equation}
be the parse tree that results from a parse of \Vstr{w}
by a grammar \var{g}.
If \var{g} is an internal grammar,
\var{pt} is an \dfn{internal parse}.
If \var{g} is an external grammar,
\var{pt} is an \dfn{external parse}.

Every node of \var{pt} is a \dfn{symbol instance}.
The symbol instances of the parse correspond one-to-one with 4-tuples
of the form
\begin{equation*}
[ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ].
\end{equation*}
where
\begin{itemize}
\item \Vsym{s} is the node's symbol.
\item \boldRange{w}{\Vloc{start}}{(\Vloc{start}+\var{length}-1)}
is the terminal string derived from \var{s}.
\item \var{depth}
is the ``brick depth''~\Lref{loc-brick-depth}.
\end{itemize}

In an internal grammar, there are
brick and mortar symbols.
Internal \dfn{brick} symbols correspond, many-to-one, to
external symbols.
Internal \dfn{mortar} symbols exist entirely for the purposes
of the internal grammar.
Only brick symbols have semantics attached to them.
All terminal symbols are bricks.

Recall \var{pt} from \Eref{eq:parse-tree}.
If the symbol of an instance in \var{pt} is a brick,
the instance is a \dfn{brick instance}.
If the symbol of an instance in \var{pt} is a mortar symbol,
the instance is a \dfn{mortar instance}.

The
\dfn{brick depth}~\label{loc-brick-depth}
of an instance in a external grammar is
the depth of the node of the instance measured in the traditional way --
the distance in nodes to the root.
In an internal grammar, the brick depth is more complicated.
Intuitively, it is the distance of the node
in brick nodes.
More formally,
\begin{itemize}
\item The brick depth of the root instance of an internal grammar is undefined.
\item Since an internal grammar is augmented, the root instance must
have only one child, a brick instance whose brick depth is defined to be zero.
\item For all other brick nodes,
the brick depth is one plus the brick depth of its parent node.
\item For mortar nodes, the brick depth is the brick depth of its parent node.
\end{itemize}

For evaluation, it is useful to convert the internal parse tree to
a \dfn{brick tree}.
To create a brick tree, we traverse the internal parse tree in pre-order,
ignoring internal nodes,
and creating instances for the brick nodes.
Note that this implies that the root node (the node
whose symbol is \Accept{\Vint{g}}) is ignored.

When, while constructing a brick tree,
a brick node is encountered that is marked with memoized
nulling symbols,
new instances must be created for the nulling symbols.
The new instances are called \dfn{nulling instances}.

Let \var{marked} be a node in a parse tree.
Assume that \var{marked} has
one or more recorded nulling instances.
Recorded with each nulling instance will be whether it occurs
before or after \var{marked}.
In the brick tree, each nulling instance will take the form
\begin{equation*}
[ \Vsym{nulling}, \Vloc{bound}, 0, \var{depth} ]
\end{equation*}
where
\begin{itemize}
\item \Vsym{nulling} is the memoized nulling symbol.
\item \Vloc{bound} is the location of the nulling symbol.
If the nulling instance occurs before \var{marked},
\var{bound} is the start location of \var{marked}.
If the nulling instance occurs after \var{marked},
\var{bound}
is the location
immediately after
\var{marked}.
\item \var{depth} is the same as the depth of \var{marked}.
\end{itemize}
Nulling instances are brick instances.

In what follows, we will want to match instances from an internal
parse
to instances of an external parse of the same input.

\begin{definition}[Equality of symbol instances]
Symbol instances are \dfn{equal} if their elements
are equal.  This is, if \var{node1} and \var{node2} are
instances,
\begin{equation*}
\begin{aligned}
& \var{node1} = \var{node2} \quad \defined \\
& \quad
\left(
\begin{gathered}
\exists \; \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} : \\
\var{node1} = [ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ] \\
\land \;\; \var{node2} = [ \Vsym{s}, \Vloc{start}, \var{length}, \var{depth} ]
\end{gathered}
\right) \quad \dispEnd
\end{aligned}
\end{equation*}
\end{definition}
Duplicate symbol instances do not occur within a parse tree,
so we will usually be speaking of the equality of symbol
instances in different trees.

\begin{definition}[Semantics-safe]
Let
\begin{itemize}
\item
$\var{Etree} = \var{parse}(\Vext{g},\Vstr{w})$
be the parse from
an external grammar,
\item $\var{Itree} = \var{parse}(\Vint{g},\Vstr{w})$
be the parse from
an internal grammar, and
where \Vint{g} is a rewrite of \Vext{g},
\item \var{Btree} be the brick tree
produced from \var{Itree}.
\end{itemize}
Let \var{Instance-matches} be the relation defined by
the equality of symbol instances
in \var{Btree} and \var{Etree}:
\begin{equation*}
\var{Instance-matches} \; \defined \;
\left\lbrace
\begin{gathered}
[ \var{Enode}, \var{Bnode} ] : \\
\begin{aligned}
& \var{Enode} \in \var{Etree} \\
\land \; & \var{Bnode} \in \var{Btree} \\
\land \; & \var{Enode} = \var{Bnode}
\end{aligned}
\end{gathered}
\right\rbrace
\end{equation*}

A rewrite is \dfn{semantics-safe}, iff
\var{Instance-matches}
is a one-to-one correspondence
(bijection). \dispEnd{}
\end{definition}

\chapter{\Marpa{} internal grammars}
\label{chap:INTs}

\begin{definition}[Internal grammar]
\label{def:internal-grammar}
A Marpa internal grammar (type \type{INT}) is
a Marpa external grammar which is augmented,
and which has no nullable symbols.
\dispEnd
\end{definition}

The notations for CFG's \Chref{chap:CFGs}
carry over to internal grammars,
although with the changes imposed
by the restricted form of INT's.
Noteworthy among these changes are
that \Marpa{} internal grammars

\begin{itemize}
\item do not allow nulling symbols,
\item do not allow the input to a parse to be of length zero, and
\item do not allow the RHS of a rule to be empty.
\end{itemize}

In the rest of this paper,
unless otherwise stated,
we will be assuming that the grammar
we are discussing
is a \Marpa{} internal grammar.
The restrictions imposed on INT's allow us
to state some
definitions, for example \Dfref{def:rr},
more simply than we could if they had to apply
to EXT's, or to CFG's,

\begin{definition}[Right recursion]
\label{def:rr}
Let \Vint{g} be an internal grammar.
Let \Vstr{sf} be a sentential form of \var{g} non-trivially derived from
\Vsym{rr}, so that
\begin{equation}
\label{eq:rr}
\Vsym{rr} \derives \Vstr{rhs} \destar \Vstr{sf}.
\end{equation}
The derivation of \Eref{eq:rr}
is \dfn{right-recursive} iff
\begin{equation}
\Rightmost{\Vstr{sf}} = \Vsym{rr}.
\end{equation}
We also say that the rule used in the first step of \Eref{eq:rr},
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{[\Vsym{rr} \de \Vstr{rhs}]}
\end{equation*}
and that the symbol $\Vsym{rr}$
is \dfn{right-recursive},
\begin{equation*}
\RightRecursive{\Vsym{rr}}. \quad \dispEnd
\end{equation*}
\end{definition}
\Dfref{def:rr} implies that, for any rule \var{r}, $\var{r} \in \Rules{\var{g}}$,
we have
\begin{equation}
\begin{gathered}
\RightRecursive{\var{r}} \implies \RightRecursive{\LHS{\var{r}}}.
\end{gathered}
\end{equation}

\begin{theorem}[Right-recursive step uses right recursive rule]
Let \Vint{g} be a \Marpa{} internal grammar.
If \Vrule{r} is a rule used in a step of a right recursive derivation,
then \Vrule{r} is right recursive.
\end{theorem}

\begin{proof}
Note that for the case of a right recursion of length 1,
the theorem is trivially true.
We now consider the case of an arbitrarily chosen step
in a right recursion in \var{g} of
length 2 or more:
\begin{align}
\label{rr-proof-10a}
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{rr-proof-10b}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{rr-proof-10c}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\label{rr-proof-10d}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}
The arbitrarily chosen step is from
\Eref{rr-proof-10b} to
\Eref{rr-proof-10c}
and uses the rule $[ \Vsym{symX} \de \Vstr{rhsX} ]$.
We may ``pump'' the derivation of
\Eref{rr-proof-10a}--\Eref{rr-proof-10d}
to produce another right recursion in \var{g}
\begin{align}
\nonumber
\Vsym{rr1} \derives & \; \Vstr{rhs1} \\
\label{rr-proof-20a}
 \destar & \; \Vstr{preX} \Vsym{symX} \\
\label{rr-proof-20b}
 \derives & \; \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vsym{rr1} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{rhs1} \\
\label{rr-proof-20c}
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vsym{symX} \\
\nonumber
 \derives & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{rhsX} \\
\nonumber
 \destar & \; \Vstr{preX} \Vstr{leftX} \Vstr{preX} \Vstr{leftX} \Vsym{rr1}.
\end{align}

Isolating the derivation from \Eref{rr-proof-20a} to
\Eref{rr-proof-20c} we have
\begin{equation}
 \Vsym{symX} \derives \Vstr{rhsX} \destar
 \Vstr{leftX} \Vstr{preX} \Vsym{symX}
\end{equation}
so that
\begin{equation}
\RightRecursive{[ \Vsym{symX} \de \Vstr{rhsX} ]} \because \Dfref{def:rr}. \quad \myqed
\end{equation}
\end{proof}

\chapter{Earley items}
\label{chap:earley-item}

\begin{definition}[Context parse redefined]
\label{def:context-parse-int}
Recall that the global scope
and every new
definition or theorem scope
are context parse scopes.
From this point on,
and unless otherwise stated,
the \dfn{context parse} of these scopes
will be a partial parse $[\Vint{g}, \Vstr{w}]$.
\dispEnd
\end{definition}

Since this paper is intended to describe \Marpa{},
many of our results take advantage of Marpa{}'s rewriting,
and simplify the presentation by
assuming \Marpa{} internal grammars.
Descriptions of
\Earley{} for more general CFGs
are plentiful.%
\footnote{%
    Focusing on the classic Earley-related literature,
    these include
    \cite[pp. 320-321]{AU1972},
    \cite{AH2002},
    \cite{Earley1968},
    \cite{Earley1970},
    \cite{GJ2008}, and
    \cite{Leo1991}.%
}

Let $\Vrule{r} \in \Rules{\Vint{g}}$
be a rule.
Recall that $\Vsize{r}$
is the length of the RHS of \var{r}.
A \dfn{dotted rule} (type \type{DR}) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 \le \var{pos} \le \size{\Vrule{r}}$.
The position, \var{pos}, indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \Vsym{Z}]
\end{equation*}
is a rule,
\begin{equation*}
[\Vsym{A} \de \Vsym{X} \Vsym{Y} \mydot \Vsym{Z}]
\end{equation*}
is the dotted rule with the dot at
$\var{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.
\DottedRules{\Vcfg{g}} is the set of dotted rules
in \Vcfg{g}, that is,
\begin{align*}
& \DottedRules{\Vcfg{g}} \defined \\
& \qquad \qquad \left\lbrace \;
\begin{aligned}
& \hspace{0pt} [ \Vrule{r}, \var{pos} ] \quad \text{such that} \; \\
& \begin{aligned}
         & \Vrule{r} \in \Rules{\Vint{g}} \\
\qquad \land \quad & 0 \le \var{pos} \le \size{\RHS{\Vrule{r}}} \\
\end{aligned} \\
\end{aligned}
\; \right\rbrace .
\end{align*}
Where the choice of \Vcfg{g} is clear in context,
and unless otherwise specified,
\Vdr{dr} will imply that $\var{dr} \in \DottedRules{\var{g}}$.
For convenience, where
\[
\Vdr{x} = [\Vrule{r}, \var{pos}]
\]
is a dotted rule,
\begin{equation*}
\begin{gathered}
\text{$\Rule{\Vdr{x}} \defined \Vrule{r}$ and} \\
\DotPos{\Vdr{x}} \defined \var{pos}.
\end{gathered}
\end{equation*}

\begin{definition}[Size of grammar]
\label{def:grammar-size}
The size of a grammar, \Vsize{\var{g}} is
the number of distinct dotted rules it allows.
That is,
\[
\size{\Vcfg{g}} = \size{\var{Dotted-Rules}[\var{g}]}.
\]
\end{definition}

If we let \Vdr{x} be a dotted rule, such that
\begin{equation}
\Vdr{x} = \left[ \Vrule{x}, \var{pos} \right],
\end{equation}
then
\begin{align}
& \LHS{\Vdr{x}} && \defined && \quad
\LHS{\Vrule{x}} \\
%
& \Predot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\var{pos}\subtract 1], \; \text{if $\var{pos} > 0$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-postdot}
& \Postdot{\Vdr{x}} && \defined && \quad
\begin{cases}
\RHS{\Vrule{x}}[\var{pos}], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-next}
& \Next{\Vdr{x}} && \defined && \quad
\begin{cases}
[\Vrule{x}, \var{pos}+1], \; \text{if $\var{pos} < \size{\Vrule{x}}$} \\
\Lambda, \; \text{otherwise}
\end{cases} \\
\label{eq:def-penult}
& \Penult{\Vdr{x}} && \defined && \quad
\begin{cases}
\Postdot{\Vdr{x}}, \; \text{if $\var{pos} = \size{\Vrule{x}}\subtract 1$} \\
\Lambda, \quad \text{otherwise}
\end{cases}
%
\end{align}

\begin{definition}[Penult]
\label{def:penult}
A dotted rule, \Vdr{d},
is a \dfn{penult} if it is such that $\Penult{\var{d}} \neq \Lambda$.
The \dfn{initial dotted rule} is
\begin{equation*}
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ].
\quad \dispEnd
\end{equation*}
\end{definition}
\begin{definition}[Prediction]
\label{def:prediction}
A \dfn{predicted dotted rule},
or \dfn{prediction} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = [\Vsym{A} \de \mydot \Vstr{rhs} ].
\quad \dispEnd
\end{equation*}
\end{definition}
\begin{definition}[Completion]
\label{def:completion}
A \dfn{completed dotted rule},
or \dfn{completion},
is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = [\Vsym{A} \de \Vstr{rhs} \mydot ].
\end{equation*}
A dotted rule which is not a completion is called an
\dfn{incomplete rule},
or an \dfn{incompletion}.
\dispEnd{}
\end{definition}
\begin{definition}[Confirmation]
\label{def:confirmation}
A \dfn{confirmed dotted rule},
or \dfn{confirmation},
is a dotted rule
with a dot position greater than zero.
A dotted rule is \dfn{unconfirmed} iff
it is not confirmed.
\dispEnd{}
\end{definition}
\begin{definition}[Medial]
\label{def:medial}
A rule which is both an incompletion and a confirmation
is called a \dfn{medial rule}, or \dfn{medial}.
\dispEnd
\end{definition}

\begin{definition}[EIM]
\label{def:eim}
A Earley item (type \type{EIM}) is a triple
\begin{equation}
\label{eq:d-eim}
    [\Vdr{dotted}, \Vorig{i}, \Vcurr{j}]
\end{equation}
of dotted rule, origin and current location.
\dispEnd
\end{definition}

The origin is the location where recognition of the rule
started.
In the Earley parsing literature,
the origin is often called the ``parent''.
For convenience, the types \type{ORIG} and \type{CURR} will be synonyms
for \type{LOC}, so that
\[
\type{LOC} = \type{ORIG} = \type{CURR}
\]
The type \type{ORIG} will
indicate that the variable designates
an origin element of an Earley item; and
the type \type{CURR} will
indicate that the variable designates
a current location element of an Earley item.
where
\[
\Veim{x} = [\Vdr{x}, \Vorig{x}, \Vcurr{x}]
\]
is an arbitrary \type{EIM},
\begin{gather}
\label{eq:def-dr}
\DR{\Veim{x}} \defined \Vdr{x}, \\
\label{eq:def-origin}
\Origin{\Veim{x}} \defined \Vorig{x}, \text{ and} \\
\label{eq:def-curr}
\Current{\Veim{x}} \defined \Vcurr{x}.
\end{gather}

When a dotted rule concept is applied to an \type{EIM},
it is the same as applying that concept to the \type{EIM}s
dotted rule.
When a rule concept is applied to an dotted rule
it is the same as applying that concept to the rule
of the dotted rule.
When a rule concept is applied to an \type{EIM}
it is the same as applying that concept to the rule
of the dotted rule of the \type{EIM}.

For example, let
\begin{equation*}
\Veim{x} = [\Vdr{x}, \Vorig{x}, \Vcurr{x}] =
\left[[\Vrule{x}, \var{pos}], \Vorig{x} , \Vcurr{x} \right].
\end{equation*}
Then
\begin{equation*}
\begin{gathered}
\Penult{\Veim{x}} = \Penult{\Vdr{x}}, \\
\Postdot{\Veim{x}} = \Postdot{\Vdr{x}}, \\
\LHS{\Veim{x}} = \LHS{\Vdr{x}} = \LHS{\Vrule{x}}, \\
\RHS{\Veim{x}} = \RHS{\Vdr{x}} = \RHS{\Vrule{x}}, \\
\end{gathered}
\end{equation*}
and so forth.

Similarly, we say that
an \type{EIM} is a \dfn{penult} if its dotted rule is a penult,
an \type{EIM} is a \dfn{prediction} if its dotted rule is a prediction,
and so on.

A \dfn{parse item} is either an Earley or a Leo item.
Parse items are of type \type{PIM}.
Leo items (type \type{LIM})
will be defined in \Chref{chap:leo}.


\todo{Revisit productivity, accessiblity.  Require in INT?}
\begin{definition}[EIM validity]
\label{def:eim-validity}
Let
\begin{equation}
\label{eq:eim-valid-10}
\Veim{x} = \left[ \left[ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2}
   \right], \Vorig{i}, \Vcurr{j} \right]
\end{equation}
be an arbitrary \type{EIM}.
\Veim{x} is \dfn{valid}
iff
\begin{gather}
\label{eq:eim-valid-20}
\var{i} \le \var{j} \le \Vlastix{inp}, \\
\label{eq:eim-valid-22a}
\left[ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \right] \in \Rules{\var{g}},
\\
\label{eq:eim-valid-22b}
\Accept{\var{g}} \destar \Vstr{before} \Vsym{lhs} \Vstr{after}, \\
\label{eq:eim-valid-22c}
\Vstr{before} \destar \var{inp}[0  \ldots  (\var{i}\subtract 1)], \text{ and} \\
\label{eq:eim-valid-22d}
\Vstr{rhs1} \destar \var{inp}[\var{i}  \ldots  (\var{j}\subtract 1)].
\end{gather}
\Veim{x}
is \dfn{useful}
iff
\begin{equation*}
\label{eq:eim-valid-30}
\Vstr{rhs2}\Vstr{after} \destar \var{inp}[\var{j}  \ldots  \Vlastix{inp}].
\end{equation*}
The boolean function \Valid{\Veim{x}} is true iff \Veim{x}
is valid.
\dispEnd
\end{definition}

\begin{definition}[Symbol of an \type{EIM}]
\begin{equation*}
\Symbol{\Veim{eim}} \defined \LHS{\var{eim}}. \quad \dispEnd
\end{equation*}
\end{definition}

\begin{theorem}[EIM direction]
\label{th:eim-direction}
If \Veim{x} is a valid \type{EIM},
then $\Origin{\var{x}} + \size{\RHS{\var{x}}} = \Current{\var{x}}$.
Also,
the origin of \Veim{x} is at or before its current location.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-10}
\begin{gathered}
\text{Let \Veim{x} be a valid \type{EIM}, where} \\
\Veim{x} = \left[ \left[ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2}
   \right], \Vorig{i}, \Vcurr{j} \right] \\
   \because \text{ WLOG, \Eref{eq:eim-valid-10} in \Dfref{def:eim-validity}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-12}
\begin{gathered}
\Vorig{i} + \size{\Vstr{rhs1}} = \Vloc{j} \\
   \because \Eref{eq:eim-valid-22d} \text{ in } \Dfref{def:eim-validity},
    \Eref{eq:th-eim-origin-not-after-current-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-14}
\begin{gathered}
\Origin{\var{x}} + \size{\Vstr{rhs1}} = \Current{\var{x}} \\
\because
\longEref{Origin}{eq:def-origin},
\longThref{EIM direction}{th:eim-direction},
\Eref{eq:th-eim-origin-not-after-current-10},
\Eref{eq:th-eim-origin-not-after-current-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-eim-origin-not-after-current-16}
\Origin{\var{x}} \le \Current{\var{x}} \because
    \Eref{eq:th-eim-origin-not-after-current-14}.
    \quad \myqed
\end{equation}
The theorem is
\Eref{eq:th-eim-origin-not-after-current-14}
and
\Eref{eq:th-eim-origin-not-after-current-16}.
\end{proof}

\begin{definition}[Matching]
\label{def:matching-1}
We say that an \var{l} of some type \dfn{matches}
a \var{r} of some type if
\Matches{\var{l},\var{r}},
where \var{Matches} is an overloaded boolean function.
Where \Matches{\var{l},\var{r}},
we say that \var{l} is a \dfn{left match} of \var{r},
and that \var{r} is a \dfn{left match} of \var{l}.

Three of the loadings of \var{Matches} are as follows.
The match of an \type{EIM} with another \type{EIM},
\begin{equation}
\label{eq:def-matches-eim-eim}
\begin{gathered}
\Matches{\Veim{left},\Veim{right}} \defined \\
\Postdot{\var{left}} = \Symbol{\var{right}} \\
\land \; \Current{\var{left}} = \Origin{\var{right}}.
\end{gathered}
\end{equation}
The match of an \type{EIM} with a token,
\begin{equation*}
\begin{gathered}
\Matches{\Veim{left},\var{token}} \defined \\
         \Postdot{\var{left}} = \Symbol{\var{token}} \\
\land \; \Current{\var{left}} = \Origin{\var{token}}.
\end{gathered}
\end{equation*}
The match of an \type{EIM} with a location,
\begin{equation*}
\begin{gathered}
\Matches{\Veim{left},\Vloc{j}} \defined \\
         \Postdot{\var{left}} = \Symbol{\Vloc{j}} \\
\land \; \Current{\var{left}} = \Origin{\Vloc{j}}.
\quad \dispEnd
\end{gathered}
\end{equation*}
\end{definition}

\begin{theorem}[Matching \type{EIM}s direction]
\label{th:matching-eims-direction}
\begin{equation}
\label{eq:th-matching-eims-direction-03}
\begin{gathered}
\Matches{\Veim{left},\Veim{right}} \\
\implies \Current{\var{left}} \le \Current{\var{right}}.
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
By \Eref{eq:th-matching-eims-direction-03} and
\Eref{eq:def-matches-eim-eim} in the definitions of
``matching'' \Dfref{def:matching-1}, we have
\begin{equation}
\label{eq:th-matching-eims-direction-10}
\Current{\var{left}} = \Origin{\var{right}}.
\end{equation}
The theorem follows from
\Eref{eq:th-matching-eims-direction-10}
and \Thref{th:eim-direction}.
\end{proof}

\begin{theorem}[Reduction lemma]
\label{th:reduction}
let
\begin{equation}
\label{eq:th-reduction-lemma-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \type{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{equation}
\label{eq:th-reduction-lemma-04}
\Vsym{postdot} \destar \boldRangeDecr{inp}{\Vloc{j}}{\Vloc{k}}.
\end{equation}
Then
\begin{equation}
\label{eq:th-reduction-lemma-06}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \type{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{k}.
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-reduction-lemma-10}
\begin{gathered}
\Rule{\Veim{pred}} \in \Rules{\var{g}} \\
\because \Eref{eq:eim-valid-22a} \text{ in }
    \longDfref{EIM validity}{def:eim-validity},
    \Eref{eq:th-reduction-lemma-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-12}
\begin{gathered}
\Rule{\Veim{pred}} =
\Rule{\Veim{reduced}}
\because
    \Eref{eq:th-reduction-lemma-02}
    \Eref{eq:th-reduction-lemma-06}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-14}
\begin{gathered}
\Rule{\Veim{reduced}} \in \Rules{\var{g}} \\
\because
    \Eref{eq:th-reduction-lemma-10},
    \Eref{eq:th-reduction-lemma-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-16}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \Vsym{lhs} \cat \Vstr{after} \\
  \because \Eref{eq:eim-valid-22b} \text{ in }
    \longDfref{EIM validity}{def:eim-validity},
    \Eref{eq:th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-18}
\begin{gathered}
  \Vstr{before} \destar \boldRangeDecr{inp}{0}{\var{i}} \\
  \because \Eref{eq:eim-valid-22c} \text{ in }
    \Dfref{def:eim-validity},
    \Eref{eq:th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-20}
\begin{gathered}
  \Vstr{pt1} \destar \boldRangeDecr{inp}{\var{i}}{\var{j}} \\
  \because \Eref{eq:eim-valid-22d} \text{ in }
    \Dfref{def:eim-validity},
    \Eref{eq:th-reduction-lemma-02}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-lemma-22}
\begin{gathered}
  \Vstr{pt1}\Vsym{postdot} \destar \boldRangeDecr{inp}{\var{i}}{\var{k}} \\
  \because
    \Eref{eq:th-reduction-lemma-04},
    \Eref{eq:th-reduction-lemma-20}.
  \end{gathered}
\end{equation}
The validity of
\Veim{reduced} \Eref{eq:th-reduction-lemma-06} follows from
\Eref{eq:th-reduction-lemma-14},
\Eref{eq:th-reduction-lemma-16},
\Eref{eq:th-reduction-lemma-18}, and
\Eref{eq:th-reduction-lemma-22}.
\myqed
\end{proof}

\begin{theorem}[Reduction by completion]
\label{th:reduction-by-completion}
Let
\begin{equation}
\label{eq:th-reduction-by-completion-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \type{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{gather}
\label{eq:th-reduction-by-completion-04}
\text{\Veim{cuz} be a valid \type{EIM} such that} \\
\label{eq:th-reduction-by-completion-05}
\Postdot{\var{cuz}} = \Lambda, \\
\label{eq:th-reduction-by-completion-06}
\Matches{\var{pred}, \var{cuz}}, \text{ and} \\
\label{eq:th-reduction-by-completion-08}
\Current{\var{cuz}} = \Vloc{k}.
\end{gather}
Then
\begin{equation}
\label{eq:th-reduction-by-completion-10}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \type{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{k}.
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-reduction-by-completion-22}
\Origin{\Veim{cuz}} = \Vloc{j} \because
    \Eref{eq:th-reduction-by-completion-02},
    \Eref{eq:th-reduction-by-completion-06}.
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-24}
\LHS{\Veim{cuz}} = \Vsym{postdot} \because
    \Eref{eq:th-reduction-by-completion-02},
    \Eref{eq:th-reduction-by-completion-06}.
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-26}
\begin{gathered}
\exists \; \Vstr{rhs} : \Veim{cuz} = \tuple{\tuple{\Vsym{postdot} \de \var{rhs} \mydot}, \Vorig{k}}@\Vloc{j} \\
\because \Eref{eq:th-reduction-by-completion-05},
        \Eref{eq:th-reduction-by-completion-08},
        \Eref{eq:th-reduction-by-completion-22},
        \Eref{eq:th-reduction-by-completion-24}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-reduction-by-completion-28}
\begin{gathered}
\Vsym{postdot} \destar \boldRangeDecr{inp}{\Vloc{j}}{\Vloc{k}} \\
   \because
        \Eref{eq:th-reduction-by-completion-04},
        \Eref{eq:th-reduction-by-completion-26}
        \longDfref{EIM validity}{def:eim-validity}.
\end{gathered}
\end{equation}
The theorem follows from
\Lmref{th:reduction},
\Eref{eq:th-reduction-by-completion-02}, and
\Eref{eq:th-reduction-by-completion-28}.
\myqed
\end{proof}

\begin{theorem}[Reduction by token]
\label{th:reduction-by-token}
Let
\begin{equation}
\label{eq:th-reduction-by-token-02}
\begin{gathered}
  \Veim{pred} \text{ be a valid \type{EIM}, where } \Veim{pred} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \mydot \Vsym{postdot} \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @\Vloc{j}
\end{gathered}
\end{equation}
and let
\begin{gather}
\label{eq:th-reduction-by-token-04}
\Vsym{postdot} = \boldElement{inp}{\var{j}}
\end{gather}
Then
\begin{equation}
\label{eq:th-reduction-by-token-10}
\begin{gathered}
  \Veim{reduced} \text{ is a valid \type{EIM}, where } \Veim{reduced} = \\
  \left[
      \left[ \Vsym{lhs} \de \Vstr{pt1} \Vsym{postdot} \mydot \Vstr{pt3}
      \right],
      \Vloc{i}
  \right] @(\var{j}+1)
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The theorem follows from
\Lmref{th:reduction},
\Eref{eq:th-reduction-by-token-02}, and
\Eref{eq:th-reduction-by-token-04}.
\myqed
\end{proof}

\chapter{Earley tables}
\label{chap:earley-table}

The definitions of this section
assume
the partial parse $[\Vint{g}, \Vstr{w}]$.

\begin{definition}[Earley set {[}ES{]}]
\label{def:earley-set}
An Earley set
(type \type{ES})
is as a set of \dfn{parse items}.
That is,
\[
\type{ES} = \powerset{\type{PIM}}. \quad \dispEnd
\]
\end{definition}
Note that, since an Earley set is a set
in the mathematical sense,
we do not say that \type{PIM}s are ``added'' to
an Earley set,
but that they are ``adjoined'' to it.
The result of adjoining a \type{PIM} \var{p} to the Earley set \var{es}
is $\Ves{es} \cup \set{\Vpim{p}}$, so that if \var{p} is already
an element of \var{es},
\var{es} is unchanged:
\[
    \Vpim{p} \in \Ves{es} \implies \var{es} \cup \set{\var{p}} = \var{es}.
\]

\begin{definition}[ES validity]
\label{def:es-validity}
\begin{equation}
\begin{gathered}
\Valid{\Ves{x}} \defined
    \forall \; \Vpim{pim} \in \var{x} : \Valid{\var{pim}}.
\end{gathered}
\end{equation}
If \Valid{\Ves{x}}, we say that \var{x}
is \dfn{valid}.
\dispEnd
\end{definition}

Note that in \Dfref{def:es-validity}, we have yet to
define \Valid{\Vpim{lim}}, where \var{lim} is a \type{LIM}.
We will do so later \Dfref{def:lim-validity}.

\begin{definition}[Earley table {[}ET{]}]
\label{def:et}
An Earley table, type \type{ET},
is a finite sequence of Earley sets.
That is,
\[
    \type{ET} = \finseq{\type{ES}}. \quad \dispEnd
\]
\end{definition}

\begin{definition}[Earley table validity]
\label{def:et-validity}
An Earley table is \dfn{valid} iff
\begin{itemize}
\item the sequence contains an Earley set for every symbol in the context input
    plus an initial Earley set,
\item all of these Earley sets are valid, and
\item the current location of every \type{EIM} in each of the Earley sets
   is the same as the index of that Earley set in the Earley table.
\end{itemize}
That is, where \var{tab} is an Earley table,
\begin{equation}
\label{eq:def-et-validity-10}
\begin{gathered}
\Valid{\Vet{tab}} \defined \Vlastix{tab} = \size{\Vstr{w}}
    \\ \land \; \forall \; \Vnat{ix} \in \Dom{\var{et}} :
    \\ \left( \begin{gathered}
            \Valid{\VVelement{tab}{ix}}
            \\ \land \; \forall \; \Veim{eim} \in
            \VVelement{tab}{ix} : \Current{\var{eim}} = \var{ix}
        \end{gathered} \right).
\end{gathered}
\end{equation}
\end{definition}

Note that, by definition \Dfref{def:et-validity},
an Earley table may be vacuously valid.
That is, an Earley table all of whose Earley sets are empty is valid,
as long as it is of the correct length.

\begin{definition}[Context Earley table and sets]
\label{def:context-earley-table}
From this point on,
and unless stated otherwise,
\begin{itemize}
\item every context parse scope contains its own \dfn{context Earley table},
or \dfn{context table},
\item the context table is valid for the context parse,
\item the context table is \Vet{S}.%
\footnote{%
   The use of \var{S} for the Earley table goes back to Earley~\cite{Earley1968}.
   The items in an Earley set at location \Vloc{j} can be seen
   as encoding the state of the parse at \var{j},
   so that ``S'' stands for ``state set''.
}
\dispEnd
\end{itemize}
\end{definition}

\begin{definition}[ES binding assumption]
\label{def:es-binding}
The binding assumption for an ES, call it \Ves{es},
is that,
where \Vet{cxt} is the context table,
\[
    \exists \; \Vloc{i} \in \Dom{\Vet{cxt}}: \VVelement{cxt}{i} = \var{es}.
        \quad \dispEnd
\]
\end{definition}

\begin{definition}[ES current location]
\label{def:es-current}
Let \var{es} be a valid ES and let \var{table}
be a valid ET.
Then
\begin{equation}
\label{eq:def-es-current-10}
    \Current{\var{es}} \defined \riota \; \Vnat{i} : \VVelement{table}{i} = \var{es}.
\end{equation}
Where the value of \Vet{table} is not specified,
it is understood to be the context table.
The validity of \var{table} quarantees the uniqueness of \var{i} in
\Eref{eq:def-es-current-10}, so that \Current{\var{es}} exists for
any valid \Ves{es}.
We say that \Current{\Ves{es}} is the
\dfn{current location} of \var{es}.
\dispEnd
\end{definition}

\begin{theorem}[ET--EIM current location]
\label{def:et-eim-current}
Let \VVelement{tab}{j} be a valid Earley set
in a valid table.  Then
\[
    \begin{gathered}
    \forall \; \Veim{eim} \in \Current{\VVelement{tab}{j}} :
    \\ \Current{\Veim{eim}} = \Current{\VVelement{tab}{j}} = \var{j}.
    \end{gathered}
\]
\end{theorem}

\begin{proof}
This theorem follows from
the definition of Earley table validity
\Dfref{def:et-validity}
and the definition of ES current location
\Thref{def:es-current}.
\end{proof}

\begin{theorem}[ES--EIM current location]
\label{def:es-eim-current}
Let \Ves{es} be an Earley set.  Then
\[
    \forall \; \Veim{eim} \in \Current{\var{es}} : \Current{\Veim{eim}} = \Current{\var{es}}.
\]
\end{theorem}

\begin{proof}
This theorem follows from
the ES binding assumption \Dfref{def:es-binding},
and the ``ET--EIM current location'' theorem
\Thref{def:et-eim-current}.
\myqed
\end{proof}

An Earley parser works by building an Earley table.
Earley set 0 is the initial Earley set.
Earley set $\Vloc{i}+1$ describes the state of
the parse after processing the input at location \Vloc{i}.

\begin{definition}[ES by location]
\label{def:es-by-location}
Let \Vet{cxt} be the context table.
It is often convenient to refer to an Earley set
by its location instead of a name.
In such cases we could abuse our
notation slightly by writing
$\es{(\Vloc{j})}$ instead of $\Vet{cxt}[\Vloc{j}]$.
In fact, when our intent is clear,
we will carry the abuse a bit further,
and abbreviate $\es{(\Vloc{j})}$ to \VVelement{S}{j}.
\dispEnd
\end{definition}

We will require an idea of direction
within an Earley table.

\begin{definition}[Earley table directionality]
\label{def:et-directionality}
A sequence of locations within a Earley table is \dfn{left-to-right}
if it is non-decreasing.
A sequence of locations within a Earley table is \dfn{right-to-left}
if it is non-increasing.
\dispEnd
\end{definition}

\begin{definition}[Input acceptance]
\label{def:input-acceptance}
Recall that
there was a unique accept symbol,
\Accept{\Vint{g}}, in \Vint{g}.
The Earley table \var{table}
\dfn{accepts} the input \Vterm{inp} iff
\begin{gather*}
\tuple{\Vdr{accept}, 0} \in \Vet{table}\left[ \Vsize{\Vterm{inp}} \right]
\\ \land \; \LHS{\Vdr{accept}} = \Accept{\Vint{g}}.
    \quad \dispEnd
\end{gather*}
\end{definition}

In the proofs of the next two theorems,
we include explicit derivation steps stating the context
and binding assumptions,
and we derive basic facts from these steps.
After that, we will usually introduce facts directly
when they are obvious
from the context and binding assumptions.

\begin{theorem}[Prediction locations]
\label{th:predictions-locations}
Let \Veim{pred} be a prediction in Earley set \Ves{es}.
Then
\[
\Origin{\Veim{pred}} = \Current{\Veim{pred}} = \Current{\var{es}}.
\]
\end{theorem}

\begin{proof}
\mypareq{eq:th-predictions-location-10-05}{%
    \Valid{\Vet{S}} \cuz{}
    \longDfref{Context Earley table and sets}{def:context-earley-table}.
}
\mypareq{eq:th-predictions-location-10-10}{%
    $\Ves{es} \in \var{S}$ \cuz{}
    \longDfref{Context table}{def:context-earley-table},
    \longDfref{ES binding assumption}{def:es-binding},
    \Ves{es} from statement of theorem.
}
\mypareq{eq:th-predictions-location-10-15}{%
    \Valid{\Ves{es}} \cuz{}
    \longDfref{ET validity}{def:et-validity},
    \Eref{eq:th-predictions-location-10-05},
    \Eref{eq:th-predictions-location-10-10}.
}
\begin{equation}
\label{eq:th-predictions-location-10-70}
\myparbox{\Veim{pred} is a prediction
    in \Ves{es}
    \cuz{} \text{AF theorem}.
}
\end{equation}
\mypareq{eq:th-predictions-location-10-75}{%
    $\Current{\Veim{pred}} = \Current{\Ves{es}}$ \linebreak
    \cuz{} \longDfref{ET validity}{def:et-validity},
        \Eref{eq:th-predictions-location-10-05},
        \Eref{eq:th-predictions-location-10-10},
        \Eref{eq:th-predictions-location-10-70}.
}
\begin{equation}
\label{eq:th-predictions-location-10-80}
\Valid{\Veim{pred}}
    \because \longDfref{ES validity}{def:es-validity},
    \Eref{eq:th-predictions-location-10-15},
    \Eref{eq:th-predictions-location-10-70}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-10-90}
\begin{gathered}
\Veim{pred} = \tuple{
        \tuple{ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2} },
        \Vorig{i}, \Vloc{j} }
   \\ \myparbox{\cuz{}
       \longDfref{EIM}{def:eim},
        \Eref{eq:th-predictions-location-10-70},
       new free \Vsym{lhs}, \Vstr{rhs1}, \Vstr{rhs2}, \Vorig{i},
       \Vloc{j}.
   }
\end{gathered}
\end{equation}
\mypareq{eq:th-predictions-location-12}{%
$\Vstr{rhs1} = \epsilon$ \cuz{} \longDfref{prediction}{def:prediction},
    \Eref{eq:th-predictions-location-10-70},
    \Eref{eq:th-predictions-location-10-90}.
}
\begin{equation}
\label{eq:th-predictions-location-14}
\begin{gathered}
\Vstr{rhs1} \destar \var{inp}[\var{i}  \ldots  (\var{j}\subtract 1)]
\\ \myparbox{\cuz{}
    \Eref{eq:eim-valid-22d} in \longDfref{EIM validity}{def:eim-validity},
    \Eref{eq:th-predictions-location-10-90},
    new \Vstr{inp} for convenience.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-16}
\var{inp}[\var{i}  \ldots  (\var{j}\subtract 1)] = \epsilon
\because
\Eref{eq:th-predictions-location-12},
\Eref{eq:th-predictions-location-14}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-17}
\var{j} \le \var{i}
\because \Eref{eq:th-predictions-location-16}.
\end{equation}
\mypareq{eq:th-predictions-location-18}{%
    $\var{i} \le \var{j}$ \cuz{}
    \Eref{eq:eim-valid-20} in \longDfref{EIM validity}{def:eim-validity},
    \Eref{eq:th-predictions-location-10-90}.
}
\begin{equation}
\label{eq:th-predictions-location-20}
\var{i} = \var{j} \because
  \Eref{eq:th-predictions-location-17},
  \Eref{eq:th-predictions-location-18}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-22}
\Origin{\Veim{pred}} = \Current{\Veim{pred}}
  \because \Eref{eq:th-predictions-location-10-90},
  \Eref{eq:th-predictions-location-20}.
\end{equation}
\begin{equation}
\label{eq:th-predictions-location-24}
\begin{gathered}
\Origin{\Veim{pred}} = \Current{\Veim{pred}} = \Current{\Ves{es}}
  \\ \because \Eref{eq:th-predictions-location-10-75},
      \Eref{eq:th-predictions-location-22}.
  \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[ES subset count]
\label{th:es-subset-count}
Let \Veimset{eims} be a subset of \Ves{es}:
$\var{eims} \subseteq \Ves{es}$.
Let \var{origs} be the set of origins of
the elements of \var{eims}:
\[
    \var{origs} = \set{ \Vloc{orig} :
        \exists \; \Veim{eim} \in \var{eims} : \var{orig} = \Origin{\var{eim}}
    }.
\]
Then
\[
    \size{\Veimset{eims}} = \bigorder{\size{\Veimset{origs}}}.
\]
\end{theorem}

\begin{proof}
The proof proceeds by noting that every \type{EIM} is a tuple;
noting that the number of possible values for every tuple is on the order of
the product of the number of possible values for each of that tuple's elements;
and calculating this product.
More specificly,
the number of possible values for every \type{EIM}
is the product of
\begin{itemize}
\item the number of possible dotted rules;
\item the number of possible current locations for \type{EIM}s in \Veimset{eims}; and
\item the number of possible origin locations for \type{EIM}s in \Veimset{eims}.
\end{itemize}
This means that
\begin{equation*}
\begin{aligned}
    \size{\Veimset{eims}} & = \bigorder{
        \begin{gathered}
        \size{\DottedRules{\Vcfg{g}}}
            \times \size{\Veimset{origs}}
        \\ \times \size{\set{\Current{\Ves{es}}}}
        \end{gathered}
    }
    \\ & = \size{\DottedRules{\Vcfg{g}}} \times \size{\set{\Current{\Ves{es}}}}
    \\ & \quad\quad \times \bigorder{\size{\Veimset{origs}}}
\end{aligned}
\end{equation*}
$\size{\DottedRules{\Vint{g}}}$ is a constant
that depends on the grammar,
and \set{\Current{\Ves{es}}} is obviously a singleton,
so that
\begin{equation}
\begin{aligned}
    \size{\Veimset{eims}} & =
        \Oc \times 1 \times
        \bigorder{\size{\Veimset{origs}}}
    \\ & = \bigorder{\size{\Veimset{origs}}}.
        \quad \myqed
\end{aligned}
\end{equation}
\end{proof}

\begin{theorem}[ES predictions count]
\label{th:es-prediction-count}
The count of valid predictions in an Earley set is \Oc{}.
\end{theorem}

\begin{proof}
Let \Ves{es} be an arbitrary Earley set.
From the context and the binding assumptions,
we know that \Ves{es} is valid and therefore all \type{EIM}s in \var{es} are valid.

Let \var{origs} be the set of origins
of valid predictions in an arbitrary Earley set \Ves{es}, so that
\begin{equation}
\label{eq:th-es-prediction-count-10}
\var{origs} = \set{
    \begin{gathered}
    \Vloc{orig} : \exists \; \Veim{eim} \in \Ves{es} :
    \\ \DotPos{\var{eim}} = 0 \land \var{orig} = \Origin{\var{eim}}
    \end{gathered}
    }.
\end{equation}
\mypareq{eq:th-es-prediction-count-12}{%
    $\Veim{eim} \in \Ves{es} \land \DotPos{\var{eim}} = 0 \implies \Current{\var{es}} = \Origin{\var{eim}}$
    \cuz{} \longThref{Prediction locations}{th:predictions-locations}.
}
The theorem follows trivially if $\Vsize{origs} = 0$,
so that for the rest of this
proof we assume that $\Vsize{origs} \ge 1$.
\mypareq{eq:th-es-prediction-count-20}{%
$\var{origs} = \set{ \Current{\var{es}} }$
\cuz{} \Eref{eq:th-es-prediction-count-10},
    \Eref{eq:th-es-prediction-count-12}.
}
\mypareq{eq:th-es-prediction-count-22}{%
    $\Vsize{origs} = 1 = \Oc{}$
        \cuz{} \Eref{eq:th-es-prediction-count-20}.
}
This theorem follows immediately
from \Eref{eq:th-es-prediction-count-22}
and
the ``ES subset count'' theorem \Thref{th:es-subset-count}.
\myqed
\end{proof}

\begin{theorem}[ES \type{EIM} count]
\label{th:es-eim-count}
The count of valid \type{EIM}s in the Earley set at \Vloc{j} is \order{\var{j}}.
\end{theorem}

\begin{proof}
From the context and the binding assumptions,
we know that
\VVelement{S}{j} is the Earley set at \Vloc{j},
that \VVelement{S}{j} is valid
and therefore that all \type{EIM}s in \VVelement{S}{j} are valid.

Let \var{origs} be the set of origins of \type{EIM}s in an arbitrary Earley set \VVelement{S}{j}, so that
\begin{equation}
\label{eq:th-es-eim-count-10}
\var{origs} = \set{
    \begin{gathered}
    \Vloc{orig} : \exists \; \Veim{eim} \in \VVelement{S}{j} : \var{orig} = \Origin{\var{eim}}
    \end{gathered}
    }.
\end{equation}
\mypareq{eq:th-es-eim-count-12}{%
    $\Origin{\var{eim}} \le \Current{\Veim{eim}}$
    \cuz{} \Eref{eq:eim-valid-20} in \longDfref{EIM validity}{def:eim-validity}.
}
\mypareq{eq:th-es-eim-count-14}{%
    $\var{eim} \in \VVelement{S}{j} \implies \Origin{\var{eim}} \le \Current{\VVelement{S}{j}}$
    \cuz{} \longDfref{ES--EIM current location}{def:es-eim-current},
        \Eref{eq:th-es-eim-count-12}.
}
\mypareq{eq:th-es-eim-count-16}{%
    $\var{eim} \in \VVelement{S}{j} \implies \Origin{\var{eim}} \le \var{j}$
        \cuz{} \longDfref{ES Current location}{def:es-current}
        \Eref{eq:th-es-eim-count-14}.
}
The theorem follows trivially if $\Vsize{origs} = 0$,
so that for the rest of this
proof we assume that $\Vsize{origs} \ge 1$.
\mypareq{eq:th-es-eim-count-20}{%
$\var{origs} \subseteq \set{ 0 \ldots \var{j} }$
\cuz{} \Eref{eq:th-es-eim-count-10},
    \Eref{eq:th-es-eim-count-16}.
}
\mypareq{eq:th-es-eim-count-22}{%
    $\Vsize{origs} \le \var{j}+1$
        \cuz{} \Eref{eq:th-es-eim-count-20}.
}
\mypareq{eq:th-es-eim-count-24}{%
    $\Vsize{origs} = \order{\var{j}}$.
        \cuz{} \Eref{eq:th-es-eim-count-22}.
}
This theorem follows immediately
from \Eref{eq:th-es-eim-count-24}
and
the ``ES subset count'' theorem \Thref{th:es-subset-count}.
\myqed
\end{proof}

\chapter{The Leo algorithm}
\label{chap:leo}

\section{Joop Leo's discovery}

In his 1968 Ph.D. thesis~\cite[p. 60]{Earley1968},
Jay Earley conjectured
that \Earley\! could be
modified to be \On{} for
the deterministic context-free grammars.
Recall that the
deterministic context-free grammars (DCFG's)
are
the union of the
\var{LR}(\var{k})
grammars for all \var{k}.
Earley gave no details of the method he had in mind
and left the field shortly thereafter.
By 1973 Jay Earley had earned a second Ph.D. and
was a practicing psychotherapist in California.

Earley had been correct.
In a 1991 paper, Joop Leo~\cite{Leo1991}
modified the Earley's algorithm so that
it was was linear,
not just for all DCFG's,
but for the LR-regular grammars,
a superset.
Leo did his work while unaware of the 1968 conjecture ---
Leo first encountered Earley's surmise in the course of writing up his
own discovery.%
\footnote{Leo, Joop, personal email to Jeffrey Kegler,
11 September, 2015.}

The problem both investigators noticed was that,
while \Earley{} is \On{}
for left recursion,
it is $\order{\var{n}^2}$ for right recursion.
This is because \Earley\! creates \type{EIM}s
to represent a right recursion at every parse location
where the right recursion might end.
At those locations where the right recursion does not end,
most of these \type{EIM}s will be useless.

\section{\Marpa{} and right recursion}

Leo's solution was to
memoize penults.
Leo restricted the memoization to situations where
the penult is unambiguous.
Every right recursion involves a penult,
so that Leo's method also memoizes right recursions.
Recall that we
call a dotted rule \Vdr{d},
a \dfn{penult}, if $\Penult{\var{d}} \neq \Lambda$.
In Leo's original algorithm, any penult
was treated as a potential right-recursion.

\Marpa{} applies the Leo memoizations in more restricted circumstances.
For \Marpa{} to consider a dotted rule
\Vdr{candidate} for Leo memoization,
not only must \Vdr{candidate} be a penult
but \Rule{\var{candidate}} must be right-recursive.

By restricting Leo memoization to right-recursive rules,
\Marpa{} incurs the cost of Leo memoization only in cases
where Leo sequences could be arbitrarily
long.
This more careful targeting of the memoization is for efficiency reasons.
If all penults are memoized,
many memoizations will be performed where
the maximum length of a Leo sequence is less than a constant,
often a small constant.
For limited-length sequences,
the payoff of Leo memoization is also limited.
In practice, the payoff seldom justifies
even the modest overhead of Leo memoization.

A future extension might be to identify
non-right-recursive rules
which generate Leo sequences long enough to
reward the trouble of including
them in Leo memoizations.
Such cases would be unusual,
but they may occur often enough to be worth
providing for.

% TODO Refer to proof that omission memoization does not affect correctness
Omission of a Leo memoization does not affect correctness,
so \Marpa{}'s restriction of Leo memoization
preserves the correctness as shown in Leo\cite{Leo1991}.
% TODO Add ref instead of "Later"
Later in this paper
we will prove this,
and we will also
show that this change also leaves
the complexity results of
Leo\cite{Leo1991} intact.

The current \Marpa{} implementation does not perform
Leo memoization in Earley set 0.
Earley set 0 memoization
would have to be implemented as a special case,
while omission of $\es{(0)}$ memoization
results in at most one extra \type{EIM} per ES.

\section{Leo directionality}

Leo's memoization method is simple from one point of view ---
it involves a sequence of memos, called Leo items.
The Leo items are matched to certain \type{EIM}s in a straightforward way.
An \type{EIM} matched to an Leo item is called a ``source \type{EIM}''.

Nonetheless, many have found the Leo method
challenging to the intuition.
One reason may be that
the natural order of the sequence of \type{EIM}s runs
in a direction opposite to the direction of
the intuitive order of the Leo items.
To understand
the building of the sequences of matched source \type{EIM}s
and Leo items, one must think
in both directions at once.
In the hopes of orienting the reader prior to our journey
into the details of the Leo method,
this section will give a high-level overview
of Leo directionality.

Recall from
\Dfref{def:et-directionality}
that we defined the
``left-to-right'' direction
within an \type{ET} as the direction
in which input location is non-decreasing.
Similarly, we defined
the ``right-to-left'' direction
as that in in which input location is non-increasing.

The natural direction of the source \type{EIM}s is
the same as that of the Earley sets --- left-to-right.
The intuitive direction of the Leo items, however, is the one
in which the right recursion must be unfolded.
The conceptual order of the right recursion,
and therefore the order in which a memoized right recursion must be rebuilt,
is from the bottom up.

The trickiness occurs because,
as the Earley sets are built,
left-to-right, each new Leo item becomes the bottom of a new potential
right recursion.
Therefore the intuitive, bottom-up order of Leo items
is right-to-left in terms of the Earley sets.

For the implementation, this reversal works out fine --
\type{EIM}s and their Leo memos are both created left-to-right.
But an investigator studying the implementation
must think simultaneously
in right-to-left and left-to-right terms.

\chapter{Leo items}

\begin{definition}[Leo items]
\label{def:lims}

In \cite{Leo1991}, penults were memoized
using what Leo called ``transitive items''.
In this paper Leo's ``transitive items''
will be called \dfn{Leo items}.
Recall that every parse item (\type{PIM})
is either
an Earley item or a Leo item.

A Leo item is of type \type{LIM},
where
\[
\type{LIM} = \set{ [ \Vdr{top}, \Vsym{transition}, \Vorig{top}, \Vcurr{j} ] }.
\]
If
\[
\Vlim{x} =
\tuple{ \Vdr{top}, \Vsym{transition}, \Vorig{top}, \Vcurr{j} }
\]
is an arbitrary \type{LIM},
we say that
\Vsym{transition} is the \dfn{transition symbol} of \Vlim{x},
and
\begin{align}
\DR{\Vlim{x}} & \defined \Vdr{top}, \\
\Transition{\Vlim{x}} & \defined \Vsym{transition}, \\
\Origin{\Vlim{x}} & \defined \Vorig{top} \text{, and} \\
    \Current{\Vlim{x}} & \defined \Vcurr{j}.
       \quad \dispEnd
\end{align}
\end{definition}

\begin{definition}[Transition of a \type{PIM}]
\label{def:transition}
Let \Vpim{pim} be an arbitrary \type{PIM}.
The \dfn{transition symbol},
or \dfn{transition},
of \var{pim} is
\mypareq{}{%
    \Transition{\var{pim}} if \var{pim} is a \type{LIM}, and
    \linebreak \Postdot{\var{pim}} if \var{pim} is an \type{EIM}.
}
In either case, the transition symbol
of \var{pim}
may be written
\Transition{\var{pim}}. \dispEnd
\end{definition}

\begin{definition}[Current location of a \type{PIM}]
\label{def:pim-current}
Let \Vpim{pim} be an arbitrary \type{PIM}.
The \dfn{current location},
or \dfn{location},
of \var{pim} is
\mypareq{}{%
    \Current{\var{pim}} if \var{pim} is a \type{LIM}, and
    \linebreak \Current{\var{pim}} if \var{pim} is an \type{EIM}.
}
In either case, the current location
of \Vpim{pim}
may be written
\Current{\var{pim}}. \dispEnd
\end{definition}

\begin{definition}[Leo unique]
\label{def:leo-unique}
\begin{equation}
\label{eq:def-leo-unique}
\begin{gathered}
\LeoUnique{\Vloc{j}} \defined \\
  \left\lbrace
    \begin{gathered}
    \Veim{x} \in \VVelement{S}{j} :
        \Penult{\var{x}} \neq \Lambda
        \\ \land \; \forall \; \Veim{y} \in \VVelement{S}{j} :
        \\ \Postdot{\var{x}} = \Postdot{\var{y}}
            \implies \var{x} = \var{y}
    \end{gathered}
  \right\rbrace .
\end{gathered}
\end{equation}
Note in
\Eref{eq:def-leo-unique}
that,
while \Veim{x} is restricted to penults,
\Veim{y} ranges over all the
\type{EIM}s of Earley set \VVelement{S}{j},
including those \type{EIM}s which are not penults.
We say that
an \type{EIM} \Veim{uniq}
is \dfn{Leo unique} in \VVelement{S}{j}
iff
$\Veim{uniq} \in \LeoUnique{\VVelement{S}{j}}$.
\dispEnd{}
\end{definition}

\begin{definition}[Leo Eligible]
\label{def:leo-eligible}
\begin{equation*}
\label{eq:def-leo-eligible}
\LeoEligible{\Vloc{i}} \defined \\
\left\lbrace \;\;
\begin{gathered}
\Veim{eligible} \in \LeoUnique{\var{i}} :
\\ \RightRecursive{\Veim{eligible}}
\end{gathered}
\;\; \right\rbrace
\end{equation*}
and
\begin{equation*}
\label{eq:def-is-leo-eligible}
\myfn{Is-Leo-Eligible}{\Veim{x}} \defined \\
\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\var{i}}.
\end{equation*}
We say that
an \type{EIM} \Veim{eligible}
is \dfn{Leo eligible}
iff
\myfn{Is-Leo-Eligible}{\var{eligible}}.
\dispEnd{}
\end{definition}

\begin{theorem}[Leo eligible location]
\label{th:leo-eligible-location}
\[
\myfn{Is-Leo-Eligible}{\Veim{x}} \iff
\Veim{x} \in \LeoEligible{\Current{\var{x}}}.
\]
\end{theorem}

\begin{proof}
For the forward direction we assume the antecedant to
show the consequent.
\begin{equation}
\label{eq:th-leo-eligible-location-10}
\myparbox{%
    $\myfn{Is-Leo-Eligible}{\Veim{x}}$ \cuz{}
    AF forward direction.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-12}
\myparbox{%
   $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\var{i}}$
   \linebreak
   \cuz{}
       \longDfref{\var{Is-Leo-Eligible}}{def:leo-eligible},
       \Eref{eq:th-leo-eligible-location-10}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-14}
\myparbox{$\Veim{x} \in \LeoEligible{\Vloc{j}}$
   \cuz{} \Eref{eq:th-leo-eligible-location-12},
   existential instantiation, arbitrary \Vloc{j}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-15}
\myparbox{$\Veim{x} \in \LeoUnique{\var{j}}$ \cuz{}
       \longDfref{Leo Eligible}{def:leo-eligible},
        \Eref{eq:th-leo-eligible-location-14}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-16}
\myparbox{$\Veim{x} \in \VVelement{S}{j}$ \cuz{}
       \longDfref{Leo Unique}{def:leo-unique},
        \Eref{eq:th-leo-eligible-location-15}.%
}
\end{equation}
\mypareq{eq:th-leo-eligible-location-18}{
    $\Vloc{j} = \Current{\Veim{x}}$
    \cuz{} \longDfref{ET--EIM current location}{def:et-eim-current},
        \Eref{eq:th-leo-eligible-location-16}.
}
\begin{equation}
\label{eq:th-leo-eligible-location-20}
\Veim{x} \in \LeoEligible{\Current{\var{x}}} \because
    \Eref{eq:th-leo-eligible-location-14},
    \Eref{eq:th-leo-eligible-location-18}.
\end{equation}
Equation \Eref{eq:th-leo-eligible-location-20} is the consequent of
the forward direction, so that the derivation
\Eref{eq:th-leo-eligible-location-10}--%
\Eref{eq:th-leo-eligible-location-20} shows
the forward direction.

For the reverse direction, again we assume the antecedant
to show the consequent.
\begin{equation}
\label{eq:th-leo-eligible-location-30}
\myparbox{%
    $\Veim{x} \in \LeoEligible{\Current{\var{x}}}$
    \linebreak \cuz{}
    AF reverse direction.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-32}
\myparbox{%
    $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\var{i}}$
    \linebreak \cuz{}
    \Eref{eq:th-leo-eligible-location-30},
    existential generalization.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-location-34}
\myparbox{%
   \myfn{Is-Leo-Eligible}{\Veim{x}}
   \linebreak \cuz{}
       \longDfref{\var{Is-Leo-Eligible}}{def:leo-eligible},
       \Eref{eq:th-leo-eligible-location-32}.%
}
\end{equation}
Equation \Eref{eq:th-leo-eligible-location-34} is the consequent of
the reverse direction, so that the derivation
\Eref{eq:th-leo-eligible-location-30}--%
\Eref{eq:th-leo-eligible-location-34} shows
the reverse direction.
\myqed
\end{proof}

\begin{theorem}[Leo eligible penult]
Every Leo eligible \type{EIM} is a penult.  That is,
\label{th:leo-eligible-penult}
\[
  \myfn{Is-Leo-Eligible}{\Veim{x}} \implies \Penult{\var{x}}.
\]
\end{theorem}

\begin{proof}
The proof is direct.
We assume the hypothesis to
show the conclusion.
\begin{equation}
\label{eq:th-leo-eligible-penult-10}
\myparbox{%
    $\myfn{Is-Leo-Eligible}{\Veim{x}}$ \cuz{}
    hypothesis of theorem.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-12}
\myparbox{%
   $\exists \; \Vloc{i} : \Veim{x} \in \LeoEligible{\var{i}}$
   \linebreak
   \cuz{}
       \longDfref{\var{Is-Leo-Eligible}}{def:leo-eligible},
       \Eref{eq:th-leo-eligible-penult-10}.%
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-14}
\myparbox{$\Veim{x} \in \LeoEligible{\Vloc{j}}$
   \cuz{} \Eref{eq:th-leo-eligible-penult-12},
   existential instantiation, arbitrary \Vloc{j}.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-penult-15}
\myparbox{$\Veim{x} \in \LeoUnique{\var{j}}$ \cuz{}
       \longDfref{Leo Eligible}{def:leo-eligible},
        \Eref{eq:th-leo-eligible-penult-14}.
}
\end{equation}
\mypareq{eq:th-leo-eligible-penult-16}{%
$\Penult{\Veim{x}}$ \cuz{}
       \longDfref{Leo Unique}{def:leo-unique},
        \Eref{eq:th-leo-eligible-penult-15}.
}
\Eref{eq:th-leo-eligible-penult-16} is the conclusion of
the theorem and the derivation
\Eref{eq:th-leo-eligible-penult-10}--%
\Eref{eq:th-leo-eligible-penult-16}
show the theorem.
\myqed
\end{proof}

\begin{theorem}[Leo eligible postdot uniqueness]
\label{th:leo-eligible-postdot-uniqueness}
If \Veim{x} is a Leo eligible \type{EIM},
then it is the only \type{EIM} at that location
with that postdot symbol.
That is, if
\begin{gather}
  \label{eq:th-leo-eligible-postdot-uniqueness-02}
    \myfn{Is-Leo-Eligible}{\Veim{x}} \\
  \label{eq:th-leo-eligible-postdot-uniqueness-04}
    \land \; \Current{\var{x}} = \Current{\Veim{y}} \\
  \label{eq:th-leo-eligible-postdot-uniqueness-06}
    \land \; \Postdot{\var{x}} = \Postdot{\var{y}}
\end{gather}
then
\begin{gather}
  \label{eq:th-leo-eligible-postdot-uniqueness-08}
  \var{x} = \var{y}.
\end{gather}
\end{theorem}

\begin{proof}
The proof strategy is direct,
assuming the hypothesis to show the conclusion.
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-14}
\begin{gathered}
\Veim{x} \in \LeoEligible{\Current{\var{x}}} \\
\because \longThref{Leo eligible location}{th:leo-eligible-location},
    \Eref{eq:th-leo-eligible-postdot-uniqueness-02}.
\end{gathered}
\end{equation}
\mypareq{eq:th-leo-eligible-postdot-uniqueness-22}{%
    $\Veim{x} \in \LeoUnique{\Current{\var{x}}}$
    \linebreak \cuz{} \longDfref{Leo Eligible}{def:leo-eligible},
    \Eref{eq:th-leo-eligible-postdot-uniqueness-14}.
}
\mypareq{eq:th-leo-eligible-postdot-uniqueness-24}{%
    $\forall \; \Veim{y} \in \VVelement{S}{\Current{\var{x}}} :$
    \linebreak $\Postdot{\var{x}} = \Postdot{\var{y}} \implies \var{x} = \var{y}$
    \linebreak \cuz{} \Eref{eq:th-leo-eligible-postdot-uniqueness-22},
        \longDfref{Leo unique}{def:leo-unique}.
}
%
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-25-20}
\begin{gathered}
    \myparbox{%
        $\forall \; \Veim{z} \in \Velement{S}{\Current{\var{y}}} :$ \linebreak
        $\Postdot{\var{x}} = \Postdot{\var{z}} \implies \var{x} = \var{z}$ \linebreak
        \cuz
            \Eref{eq:th-leo-eligible-postdot-uniqueness-04},
            \Eref{eq:th-leo-eligible-postdot-uniqueness-24}.
    }
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-26}
\begin{gathered}
    \myparbox{%
        $\Postdot{\var{x}} = \Postdot{\var{y}}
            \implies \var{x} = \var{y}$
        \linebreak \cuz{} \Eref{eq:th-leo-eligible-postdot-uniqueness-25-20},
        instantiating \Veim{z}
        as \Veim{y}.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-eligible-postdot-uniqueness-28}
\begin{gathered}
    \var{x} = \var{y}
    \because
        \Eref{eq:th-leo-eligible-postdot-uniqueness-06},
        \Eref{eq:th-leo-eligible-postdot-uniqueness-26}.
        \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[Leo eligible \type{EIM} count]
\label{th:leo-eligible-eim-count}
There are at most \size{\Vocab{\Vint{g}}} Leo eligible
\type{EIM}s in any ES.
That is,
\[
\begin{gathered}
\forall \; \Vloc{j} \in \Dom{\Vet{S}} :
\\ \size{
   \set{
            \Veim{x} \in \Velement{S}{\var{j}} :
    \myfn{Is-Leo-Eligible}{\var{x}}
   }
}
\le \size{\Vocab{\Vint{g}}}.
\end{gathered}
\]
\end{theorem}

\begin{proof}
Let \Veim{elig} be a Leo eligible \type{EIM} in \Velement{S}{\var{j}}.
By \Thref{th:leo-eligible-postdot-uniqueness},
\var{elig} is the only \type{EIM} in \Velement{S}{\var{j}}
with \Postdot{\var{elig}} as its postdot symbol.
This means there cannot be more Leo eligible \type{EIM}s
in \Velement{S}{\var{j}} than there are postdot symbols.
There are only
$\size{\Vocab{\var{g}}}$ symbols in \Vint{g},
so the number of postdot symbols,
and therefore the number of Leo eligible \type{EIM}s,
cannot exceed
$\size{\Vocab{\var{g}}}$.
\myqed
\end{proof}

\begin{definition}[Next eligible \type{EIM}]
\label{def:next-eligible-eim}
\[
\begin{gathered}
\myfn{Next-Eligible}{\Veim{next},\Veim{prev}} \\
\defined \; \Matches{\var{next},\var{prev}} \\
\land \; \myfn{Is-Leo-Eligible}{\var{next}} \land \myfn{Is-Leo-Eligible}{\var{prev}}.
\end{gathered}
\]
We say that \Veim{next} is the \dfn{next Leo eligible \type{EIM}},
or \dfn{next eligible \type{EIM}}, of \Veim{prev}.
We say that \Veim{prev} is the \dfn{previous Leo eligible \type{EIM}},
or \dfn{previous eligible \type{EIM}}, of \Veim{next}.
\dispEnd
\end{definition}

\begin{theorem}[Next eligible connection]
\label{th:next-eligible-connection}
\begin{gather*}
\myfn{Next-Eligible}{\Veim{next},\Veim{prev}} \\
\implies
    \left( \begin{gathered}
    \Postdot{\var{next}} = \Symbol{\var{prev}} \\
    \land \; \Current{\var{next}} = \Origin{\var{prev}}
    \end{gathered} \right).
\end{gather*}
\end{theorem}

\begin{proof}
The theorem follows from
\longDfref{Next eligible \type{EIM}}{def:next-eligible-eim}
and \longDfref{Matching \type{EIM}s}{def:matching-1}.
\myqed
\end{proof}

\begin{theorem}[Next eligible direction]
\label{th:next-eligible-direction}
The current location of eligible \type{EIM}s in previous-to-next order
is non-increasing.  That is,
\begin{gather*}
    \myfn{Next-Eligible}{\Veim{next},\Veim{prev}} \\
    \implies \Current{\var{next}} \le \Current{\var{prev}}.
\end{gather*}
\end{theorem}

\begin{proof}
The theorem follows from
\longThref{Next eligible connection}{th:next-eligible-connection}
and
\longThref{EIM direction}{th:eim-direction}.
\myqed
\end{proof}

\begin{theorem}[Next eligible uniqueness]
\label{th:next-eligible-uniqueness}
\begin{gather}
    \label{eq:th-next-eligible-uniqueness-02}
    \myfn{Next-Eligible}{\Veim{next1},\Veim{prev}} \\
    \label{eq:th-next-eligible-uniqueness-04}
    \land \; \myfn{Next-Eligible}{\Veim{next2},\Veim{prev}} \\
    \nonumber \implies \\
    \label{eq:th-next-eligible-uniqueness-06}
    \var{next1} = \var{next2}.
\end{gather}
\end{theorem}

\begin{proof}
We assume the hypothesis of the theorem to show the conclusion.
\begin{equation}
\label{eq:th-next-eligible-uniqueness-10}
\begin{gathered}
    \Postdot{\var{next1}} = \Symbol{\var{prev}} \\
    \land \; \Current{\var{next1}} = \Origin{\var{prev}}
    \\ \because
    \longThref{Next eligible connection}{th:next-eligible-connection},
    \Eref{eq:th-next-eligible-uniqueness-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-uniqueness-12}
\begin{gathered}
    \Postdot{\var{next2}} = \Symbol{\var{prev}} \\
    \land \; \Current{\var{next2}} = \Origin{\var{prev}}
    \\ \because
    \Thref{th:next-eligible-connection},
    \Eref{eq:th-next-eligible-uniqueness-04}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-uniqueness-14}
\begin{gathered}
    \Postdot{\var{next1}} = \Postdot{\var{next2}} \\
    \land \; \Current{\var{next1}} = \Current{\var{next2}}
    \\ \because
    \Eref{eq:th-next-eligible-uniqueness-10},
    \Eref{eq:th-next-eligible-uniqueness-12}.
\end{gathered}
\end{equation}
\mypareq{eq:th-next-eligible-uniqueness-16}{%
    $\myfn{Is-Leo-Eligible}{\var{next1}}$ \linebreak
        \cuz{}
        \longDfref{Next eligible \type{EIM}}{def:next-eligible-eim},
        \Eref{eq:th-next-eligible-uniqueness-02}.
}
\mypareq{eq:th-next-eligible-uniqueness-18}{%
    $\var{next1} = \var{next2}$ \linebreak
    \cuz{}
    \longThref{Leo eligible postdot uniqueness}{th:leo-eligible-postdot-uniqueness},
    \Eref{eq:th-next-eligible-uniqueness-14},
    \Eref{eq:th-next-eligible-uniqueness-16}.
    This is the conclusion of the theorem. \myqed
}
\end{proof}

\chapter{Leo eligible sequences}
\label{ch:les}

\begin{definition}[Next eligible sequence]
\label{def:next-eligible-sequence}
A \dfn{next eligible sequence} (type \type{LES}) is
a non-empty sequence of valid, Leo eligible \type{EIM}s
such that every \type{EIM}
except the top \type{EIM}
is the next eligible \type{EIM} of the
previous \type{EIM} in the sequence.  That is,
\[
\type{LES} = \set{
   \begin{gathered}
       \Veimseq{les} : \forall \; \Vnat{i} \in \Dom{\var{les}} :
       \\ \Valid{\VVelement{les}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{les}{i}}
       \\ \land \; \left(
           \var{i} < \Vlastix{les} \implies \myfn{Next-Eligible}{\Velement{les}{\var{i}+1}, \VVelement{les}{i}}
       \right)
   \end{gathered}
   }.
\]
If \var{les} is an \type{LES},
we say that \Velement{les}{0} is the \dfn{base} of \var{les},
and that \Velement{les}{\Vlastix{les}} is the \dfn{top} of \var{les}.
\dispEnd
\end{definition}

\begin{theorem}[Next Leo eligible rule]
\label{th:next-eligible-rule}
If
\begin{gather}
\label{eq:th-next-eligible-rule-03a}
\Current{\Veim{middle}} = \Current{\Veim{next}} \\
\label{eq:th-next-eligible-rule-03b}
\land \; \myfn{Next-Eligible}{\var{middle},\var{prev}} \\
\label{eq:th-next-eligible-rule-03c}
\land \; \myfn{Next-Eligible}{\var{next},\var{middle}}
\end{gather}
then
\begin{gather}
\label{eq:th-next-eligible-rule-06a}
\tuple{\Vsym{lhs} \de \Vsym{rhs}} \in \Rules{\Vint{g}} \\
\label{eq:th-next-eligible-rule-06b}
\land \; \Symbol{\var{middle}} = \Vsym{lhs} \\
\label{eq:th-next-eligible-rule-06c}
\land \; \Symbol{\var{prev}} = \Vsym{rhs}.
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct, assuming the hypothesis to
show the conclusion.
\begin{equation}
\label{eq:th-next-eligible-rule-10}
\begin{gathered}
\myparbox{\Matches{\var{middle},\var{prev}} \linebreak
\cuz{} \longDfref{Next Leo eligible}{def:next-eligible-eim},
    \Eref{eq:th-next-eligible-rule-03b}.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-12}
\Postdot{\var{middle}} \neq \Lambda \because
    \Eref{eq:th-next-eligible-rule-10}.
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-14}
\begin{gathered}
\myparbox{%
\DR{\var{middle}} =
    $\tuple{\Vsym{lhs} \de \Vstr{before} \mydot \Vsym{rhs} \Vstr{after}}$
    \linebreak \cuz{} \Eref{eq:th-next-eligible-rule-12}, for arbitrary
    \Vsym{lhs}, \Vstr{before}, \Vsym{rhs}, \Vstr{after}.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-16}
\Vsym{lhs} = \Symbol{\var{middle}} \because \Eref{eq:th-next-eligible-rule-14}.
\end{equation}
%
\mypareq{eq:th-next-eligible-rule-17}{%
    $\Postdot{\var{middle}} = \Symbol{\var{prev}}$ \linebreak
    \cuz{} \Eref{eq:th-next-eligible-rule-10},
    \Eref{eq:def-matches-eim-eim} in \longDfref{Matching}{def:matching-1}.%
}
\begin{equation}
\label{eq:th-next-eligible-rule-18}
\Vsym{rhs} = \Symbol{\var{prev}} \because \Eref{eq:th-next-eligible-rule-14},
    \Eref{eq:th-next-eligible-rule-17}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-20}{%
    \myfn{Is-Leo-Eligible}{\var{middle}} \linebreak
    \cuz{}
    \longDfref{Next leo eligible}{def:next-eligible-eim},
    \Eref{eq:th-next-eligible-rule-03b}.
}
\mypareq{eq:th-next-eligible-rule-24}{%
    $\var{middle} \in \LeoEligible{\Current{\var{middle}}}$ \linebreak
        \cuz{} \longThref{Leo eligible location}{th:leo-eligible-location},
            \Eref{eq:th-next-eligible-rule-20}.
}
\mypareq{eq:th-next-eligible-rule-26}{%
    $\var{middle} \in \LeoUnique{\Current{\var{middle}}}$ \linebreak
        \cuz{} \longDfref{Leo Eligible}{def:leo-eligible},
        \Eref{eq:th-next-eligible-rule-24}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-28}
\Penult{\var{middle}} \neq \Lambda
    \because \longDfref{Leo unique}{def:leo-unique},
    \Eref{eq:th-next-eligible-rule-26}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-29}{\Vint{g} has no nullable symbols
    \cuz{} \longDfref{Marpa internal grammar}{def:internal-grammar}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-30}
    \Vstr{after} = \epsilon \because
        \Eref{eq:th-next-eligible-rule-14},
        \Eref{eq:th-next-eligible-rule-28},
        \Eref{eq:th-next-eligible-rule-29}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-32}{%
    \Matches{\var{next},\var{middle}} \linebreak
    \cuz{} \longDfref{Next Leo eligible}{def:next-eligible-eim},
    \Eref{eq:th-next-eligible-rule-03c}.
}
\mypareq{eq:th-next-eligible-rule-34}{%
    $\Current{\var{next}} = \Origin{\var{middle}}$ \linebreak
    \cuz{} \Eref{eq:def-matches-eim-eim} in \longDfref{Matching}{def:matching-1},
    \Eref{eq:th-next-eligible-rule-32}.
}
\begin{equation}
\label{eq:th-next-eligible-rule-36}
    \Current{\var{middle}} = \Origin{\var{middle}} \because
    \Eref{eq:th-next-eligible-rule-03a},
    \Eref{eq:th-next-eligible-rule-34}.
\end{equation}
\begin{equation}
\label{eq:th-next-eligible-rule-38}
    \Vstr{before} = \epsilon \because
        \Eref{eq:th-next-eligible-rule-14},
        \Eref{eq:th-next-eligible-rule-29},
        \Eref{eq:th-next-eligible-rule-36}.
\end{equation}
\mypareq{eq:th-next-eligible-rule-40}{%
    $\DR{\var{middle}} = \tuple{\Vsym{lhs} \de \mydot \Vsym{rhs}}$
    \linebreak \cuz{}
    \Eref{eq:th-next-eligible-rule-14},
    \Eref{eq:th-next-eligible-rule-30},
    \Eref{eq:th-next-eligible-rule-38}.%
}
\begin{equation}
\label{eq:th-next-eligible-rule-42}
    \tuple{\Vsym{lhs} \de \Vsym{rhs}} \in \Rules{\Vint{g}}
    \because \Eref{eq:th-next-eligible-rule-40}.
\end{equation}
The theorem is
    \Eref{eq:th-next-eligible-rule-16},
    \Eref{eq:th-next-eligible-rule-18}, and
    \Eref{eq:th-next-eligible-rule-42}.
\myqed
\end{proof}

\begin{theorem}[LES subsequence]
\label{th:les-subsequence}
Every subsequence of an \type{LES} is an \type{LES}.
That is,
\begin{gather}
\label{eq:th-les-subsequence-02}
\var{seq} \in \type{LES}
\\ \label{eq:th-les-subsequence-04}
\land \; \Vnat{a} \le \Vnat{b} \le \Vlastix{seq}
\\ \label{eq:th-les-subsequence-06}
\land \; \var{subseq} = \tuple{\VVelement{seq}{a} \ldots \VVelement{seq}{b}}
\\ \nonumber
\implies
\\ \label{eq:th-les-subsequence-08}
\var{subseq} \in \type{LES}.
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct, assuming the hypothesis of
the theorem to show the conclusion.
\mypareq{eq:th-les-subsequence-10}{%
   \Vles{seq} is an \type{EIM} sequence \linebreak
   \cuz{} \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-subsequence-02}.%
}
\mypareq{eq:th-les-subsequence-12}{%
   \Vles{subseq} is an \type{EIM} sequence \linebreak
   \cuz{} \Eref{eq:th-les-subsequence-06},
    \Eref{eq:th-les-subsequence-10}.
}
\begin{equation}
\label{eq:th-les-subsequence-14}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\var{seq}} :
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{seq} \implies \myfn{Next-Eligible}{\Velement{seq}{\var{i}+1}, \VVelement{seq}{i}}
   \right)
   \\ \because \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-subsequence-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-subsequence-16}
\begin{gathered}
   \forall \; \Vnat{i} \in \set{ \Vnat{ii} : \var{a} \le \var{ii} \le \var{b} }
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{seq} \implies \myfn{Next-Eligible}{\Velement{seq}{\var{i}+1}, \VVelement{seq}{i}}
   \right)
    \\ \because \Eref{eq:th-les-subsequence-04},
    \Eref{eq:th-les-subsequence-14}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-subsequence-18}{%
    $\forall \; \var{i} : \var{i} < \var{b} \implies \var{i} < \Vlastix{seq}$
        \cuz{} \Eref{eq:th-les-subsequence-04}.
}
\begin{equation}
\label{eq:th-les-subsequence-20}
\begin{gathered}
   \forall \; \Vnat{i} \in \set{ \Vnat{ii} : \var{a} \le \var{ii} \le \var{b} }
   \\ \Valid{\VVelement{seq}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{seq}{i}}
   \\ \land \; \left(
       \var{i} < \var{b} \implies \myfn{Next-Eligible}{\Velement{seq}{\var{i}+1}, \VVelement{seq}{i}}
   \right)
    \\ \because \Eref{eq:th-les-subsequence-16},
    \Eref{eq:th-les-subsequence-18}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-subsequence-30}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\var{subseq}} :
   \\ \Valid{\VVelement{subseq}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{subseq}{i}}
   \\ \land \; \left(
       \begin{gathered}
       \var{i} < \Vlastix{subseq} \implies
       \\ \myfn{Next-Eligible}{\Velement{subseq}{\var{i}+1}, \VVelement{subseq}{i}}
       \end{gathered}
   \right)
    \\ \because \Eref{eq:th-les-subsequence-06},
        \Eref{eq:th-les-subsequence-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-subsequence-40}
\var{subseq} \in \type{LES}
  \because \longDfref{LES}{def:next-eligible-sequence},
   \Eref{eq:th-les-subsequence-12},
   \Eref{eq:th-les-subsequence-30}.
\end{equation}
\Eref{eq:th-les-subsequence-40} is the conclusion
of this theorem.
\myqed
\end{proof}

\begin{theorem}[LES extension]
\label{th:les-extension}
Let \var{les} be an \type{EIM} sequence,
and let \var{next} be an \type{EIM}.
Then
\mypareq{eq:th-les-extension-02}{%
   $\tuple{\Velement{\var{les}}{0}\ldots
       \Velement{les}{\Vlastix{les}}, \var{next}} \in \type{LES}$
}
iff
\begin{gather}
\label{eq:th-les-extension-04}
    \var{les} \in \type{LES}
\\ \label{eq:th-les-extension-05}
    \land \; \Valid{\var{next}}
\\ \label{eq:th-les-extension-06}
    \land \; \myfn{Is-Leo-Eligible}{\var{next}}
\\ \label{eq:th-les-extension-07}
    \land \; \myfn{Next-Eligible}{\var{next},\Velement{les}{\Vlastix{les}}}.
\end{gather}
\end{theorem}

\begin{proof}
For the forward direction,
we assume
\Eref{eq:th-les-extension-02}
to show
\Eref{eq:th-les-extension-04}--\Eref{eq:th-les-extension-07}.
Equation \Eref{eq:th-les-extension-04} follows via
the ``\type{LES} subsequence'' theorem
\Thref{th:les-subsequence}.
Equations \Eref{eq:th-les-extension-05}--\Eref{eq:th-les-extension-07}
follow immediately from the definition of \type{LES} \Dfref{def:next-eligible-sequence}.

It remains to show the reverse direction.
For this we assume \Eref{eq:th-les-extension-04}--\Eref{eq:th-les-extension-07}
to show
\Eref{eq:th-les-extension-02}.
\begin{equation}
\label{eq:th-les-extension-10}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\var{les}} :
   \\ \Valid{\VVelement{les}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{les}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{les} \implies \myfn{Next-Eligible}{\Velement{les}{\var{i}+1}, \VVelement{les}{i}}
   \right)
   \\ \because \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-extension-04}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-extension-12}
\begin{gathered}
   \var{ext} = \left [
       \Velement{\var{les}}{0}\ldots
       \Velement{les}{\Vlastix{les}}, \var{next}
   \right ]
   \\ \because \text{new \Vles{ext} for convenience.}
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-13-10}{%
   \var{les} is an \type{EIM} sequence
       \cuz{} \longDfref{LES}{def:next-eligible-sequence},
        \Eref{eq:th-les-extension-04}.
}
\mypareq{eq:th-les-extension-13-20}{%
   \var{next} is an \type{EIM}
       \cuz{} AF theorem.
}
\mypareq{eq:th-les-extension-13-30}{%
   \var{ext} is an \type{EIM} sequence
   \cuz{} \Eref{eq:th-les-extension-13-10},
        \Eref{eq:th-les-extension-13-20}.
}
\begin{equation}
\label{eq:th-les-extension-14}
\begin{gathered}
   \forall \; \Vnat{i} \in (\Dom{\var{ext}} \subtract \set{\Vlastix{ext}}) :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{ext}\subtract 1 \implies \myfn{Next-Eligible}{\Velement{ext}{\var{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \Eref{eq:th-les-extension-10},
    \Eref{eq:th-les-extension-12}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-16}{%
   $\Velement{ext}{\Vlastix{ext}} = \var{next}$ \cuz{}
        \Eref{eq:th-les-extension-12}.
}
\mypareq{eq:th-les-extension-18}{%
    $\Valid{\Velement{ext}{\Vlastix{ext}}}$
    \cuz{} \Eref{eq:th-les-extension-05},
        \Eref{eq:th-les-extension-16}.
}
\mypareq{eq:th-les-extension-20}{%
    $\myfn{Is-Leo-Eligible}{\Velement{ext}{\Vlastix{ext}}}$
    \cuz{} \Eref{eq:th-les-extension-06}.
        \Eref{eq:th-les-extension-16}.
}
\begin{equation}
\label{eq:th-les-extension-22}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\var{ext}} :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{ext}\subtract 1 \implies \myfn{Next-Eligible}{\Velement{ext}{\var{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-extension-14},
    \Eref{eq:th-les-extension-18},
    \Eref{eq:th-les-extension-20}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-24}{%
    $\myfn{Next-Eligible}{\Velement{ext}{\Vlastix{ext}},\Velement{les}{\Vlastix{les}}}$
    \cuz{} \Eref{eq:th-les-extension-07}, \Eref{eq:th-les-extension-16}.
}
\mypareq{eq:th-les-extension-26}{%
    $\myfn{Next-Eligible}{\Velement{ext}{\Vlastix{ext}},\Velement{ext}{\Vlastix{ext}\subtract 1}}$
    \cuz{} \Eref{eq:th-les-extension-12}, \Eref{eq:th-les-extension-24}.
}
\begin{equation}
\label{eq:th-les-extension-28}
\begin{gathered}
   \forall \; \Vnat{i} \in \Dom{\var{ext}} :
   \\ \Valid{\VVelement{ext}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{ext}{i}}
   \\ \land \; \left(
       \var{i} < \Vlastix{ext} \implies \myfn{Next-Eligible}{\Velement{ext}{\var{i}+1}, \VVelement{ext}{i}}
   \right)
   \\ \because \Eref{eq:th-les-extension-22}, \Eref{eq:th-les-extension-26}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-extension-30}{%
    $\var{ext} \in \type{LES}$
        \cuz{} \longDfref{LES}{def:next-eligible-sequence},
        \Eref{eq:th-les-extension-13-30}, \Eref{eq:th-les-extension-28}.
}
The equation \Eref{eq:th-les-extension-30} is the conclusion of the
reverse direction.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES adjacent direction]
\label{lm:les-adjacent-direction}
If \Veim{prev} is an element of an \type{LES},
and \Veim{next} is the next element,
then the current location of \var{next}
is less than or equal to the current location of \var{prev}.
That is, if
\[
\var{les} \in \type{LES}
\land \; \Vnat{i} \le (\Vlastix{les}\subtract 1)
\]
then
\[
\Current{\Velement{les}{\var{i}+1}} \le
\Current{\VVelement{les}{i}}.
\]
\end{theorem}

\begin{proof}
The lemma follows from
   \longDfref{LES}{def:next-eligible-sequence},
   and \longThref{next eligible \type{EIM} direction}{th:next-eligible-direction}.
\myqed
\end{proof}

\begin{theorem}[LES direction]
\label{th:les-direction}
The current location of
every element of an \type{LES} is at or before
the current location of every lesser-indexed element
of the \type{LES}.
That is,
\[
\begin{gathered}
\forall \; \var{les} \in \type{LES} :
\forall \; \Vnat{hi} \in \Dom{\var{les}} :
\forall \; \Vnat{lo} \le \var{hi} :
\\ \Current{\VVelement{les}{hi}} \le
\Current{\VVelement{les}{lo}}.
\end{gathered}
\]
\end{theorem}

\begin{proof}
The proof is by induction
on the higher-indexed
of the two \type{LES} elements to be compared.
We will find that every \type{LES} is finite,
but we have yet to prove this,
so we allow the \type{LES} to be either finite
or countably infinite.
We let the induction index, \Vnat{indx},
be the higher
of the two \type{LES} indexes.
The induction hypothesis is
\begin{equation}
\label{eq:th-les-direction-10}
\begin{gathered}
\myfn{INDH}{\Vnat{indx}} \defined
\\ \forall \; \var{les} \in \type{LES} : \forall \; \Vnat{lo} \le \var{indx} :
    \var{indx} \in \Dom{\var{les}}
\\  \implies \Current{\VVelement{les}{indx}} \le \Current{\VVelement{les}{lo}}.
\end{gathered}
\end{equation}

We first show the induction hypothesis with an
induction index of 0,
as the basis of the induction.
\mypareq{eq:th-les-direction-12}{%
    $\var{indx} = 0$ \cuz{} AF basis,
        \Eref{eq:th-les-direction-10}.
}
\mypareq{eq:th-les-direction-14}{%
    $\var{lesB} \in \type{LES}$ \cuz{} new free \var{lesB}.
}
\mypareq{eq:th-les-direction-15}{%
    $\Vsize{lesB} \ge 1$
        \cuz{} \longDfref{LES}{def:next-eligible-sequence}.
}
\mypareq{eq:th-les-direction-16}{%
    $\var{indx} \in \Dom{\var{lesB}}$ \cuz{}
        \Eref{eq:th-les-direction-12},
        \Eref{eq:th-les-direction-15}.
}
\mypareq{eq:th-les-direction-17-10}{%
    $\var{loB} \le \var{indx}$ \cuz{}
        new free \var{loB}.
}
\mypareq{eq:th-les-direction-17-20}{%
    $\var{loB} = \var{indx}$ \cuz{}
        \Eref{eq:th-les-direction-12},
        \Eref{eq:th-les-direction-17-10}.
}
\mypareq{eq:th-les-direction-18}{%
    $\Current{\VVelement{lesB}{indx}} = \Current{\VVelement{lesB}{loB}}$
     \cuz{} \Eref{eq:th-les-direction-16},
     \Eref{eq:th-les-direction-17-20}.
}
\mypareq{eq:th-les-direction-19-10}{%
    $\var{indx} \in \Dom{\var{lesB}}$
    \linebreak $\implies \Current{\VVelement{lesB}{indx}} \le \Current{\VVelement{lesB}{loB}}$
    \linebreak \cuz{} \Eref{eq:th-les-direction-16}, \Eref{eq:th-les-direction-18}.
}
\begin{equation}
\label{eq:th-les-direction-19-20}
\begin{gathered}
\forall \; \Vnat{lo} \le \var{indx} : \var{indx} \in \Dom{\var{lesB}}
\\ \implies \Current{\VVelement{lesB}{indx}} \le \Current{\VVelement{lesB}{lo}}.
\\ \myparbox{\cuz{}
    universal generalization of \var{loB},
    \Eref{eq:th-les-direction-17-10},
    \Eref{eq:th-les-direction-19-10}.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-direction-19-30}
\begin{gathered}
\forall \; \Vles{les} \in \type{LES} : \forall \; \Vnat{lo} \le \var{indx} : \var{indx} \in \Dom{\var{les}}
\\ \implies \Current{\VVelement{les}{indx}} \le \Current{\VVelement{les}{lo}}
\\ \myparbox{\cuz{}
    universal generalization of \var{lesB},
    \Eref{eq:th-les-direction-14},
    \Eref{eq:th-les-direction-19-20}.
    This is \myfn{INDH}{0}, and the basis of the induction.
}
\end{gathered}
\end{equation}
%
For the step, we assume
\myfn{INDH}{\var{n}} to show \myfn{INDH}{\var{n}+1}.
\begin{equation}
\label{eq:th-les-direction-30}
\begin{gathered}
    \forall \; \var{les} \in \type{LES} : \forall \; \Vnat{lo} \le \var{n} :
\\ \var{n} \in \Dom{\var{les}} \implies \Current{\VVelement{les}{n}} \le \Current{\VVelement{les}{lo}}.
\\ \because \myfn{INDH}{\var{n}} \text{ AF step}.%
\end{gathered}
\end{equation}
\mypareq{eq:th-les-direction-31}{%
   $\var{indx} = \var{n}+1$ \cuz{} AF step.
}
\mypareq{eq:th-les-direction-32}{%
    $\var{lesP} \in \type{LES}$ \cuz{} new free \var{loP}.
}
\mypareq{eq:th-les-direction-34}{%
    $\var{loP} \in \naturals$ \cuz{} new free \var{loP}.
}
\mypareq{eq:th-les-direction-36}{%
    $\Vnat{loP} \le \var{n} \land \var{n}+1 \in \Dom{\var{lesP}}$
    \cuz{} AF derivation.
}
\mypareq{eq:th-les-direction-37}{%
    $\Vnat{loP} \le \var{n}$
        \cuz{} \Eref{eq:th-les-direction-36}.
}
\mypareq{eq:th-les-direction-37-10}{%
    $\var{n}+1 \in \Dom{\var{lesP}}$
        \cuz{} \Eref{eq:th-les-direction-36}.
}
\mypareq{eq:th-les-direction-38}{%
    $\var{n} \in \Dom{\var{lesP}}$ \cuz{} \var{lesP} is a sequence,
        \Eref{eq:th-les-direction-37-10}.
}
\mypareq{eq:th-les-direction-40}{%
    $\Current{\VVelement{lesP}{n}} \le \Current{\VVelement{lesP}{loP}}$
    \linebreak \cuz{} \Eref{eq:th-les-direction-30},
        \Eref{eq:th-les-direction-32},
        \Eref{eq:th-les-direction-34},
        \Eref{eq:th-les-direction-37},
        \Eref{eq:th-les-direction-38}.
}
\mypareq{eq:th-les-direction-42}{%
    $\Current{\VVelement{lesP}{\var{n}+1}} \le \Current{\VVelement{lesP}{n}}$
    \cuz{}
    \longThref{LES adjacent direction}{lm:les-adjacent-direction},
        \Eref{eq:th-les-direction-32},
        \Eref{eq:th-les-direction-37-10},
        \Eref{eq:th-les-direction-38}.
}
\mypareq{eq:th-les-direction-44}{%
    $\Current{\VVelement{lesP}{\var{n}+1}} \le \Current{\VVelement{lesP}{loP}}$
    \linebreak \cuz{}
        \Eref{eq:th-les-direction-40}, \Eref{eq:th-les-direction-42}.
}
\begin{equation}
\label{eq:th-les-direction-46}
\begin{gathered}
    \Vnat{loP} \le \var{n} \land \var{n}+1 \in \Dom{\var{lesP}}
    \\ \implies \Current{\VVelement{lesP}{\var{n}+1}} \le \Current{\VVelement{lesP}{loP}}
    \\ \because \text{derivation
        \Eref{eq:th-les-direction-36}--\Eref{eq:th-les-direction-44}.}
\end{gathered}
\end{equation}
\mypareq{eq:th-les-direction-50}{%
    $\Current{\VVelement{lesP}{\var{n}+1}} \le \Current{\VVelement{lesP}{\var{n}+1}}$
    \linebreak \cuz{} reflexivity, \Eref{eq:th-les-direction-32},
        \Eref{eq:th-les-direction-37-10}.
}
\begin{equation}
\label{eq:th-les-direction-52}
\begin{gathered}
    \Vnat{loP} = \var{n}+1 \land \var{n}+1 \in \Dom{\var{lesP}}
    \\ \implies \Current{\VVelement{lesP}{\var{n}+1}} \le \Current{\VVelement{lesP}{loP}}
    \\ \because \Eref{eq:th-les-direction-46},
        \Eref{eq:th-les-direction-50}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-direction-54}
\begin{gathered}
    \forall \; \var{les} \in \type{LES} : \forall \; \var{lo} \le \var{n} + 1: \var{n}+1 \in \Dom{\var{les}}
    \\ \implies \Current{\VVelement{les}{\var{n}+1}} \le \Current{\VVelement{les}{lo}}
    \\ \myparbox{\cuz{}
        universal generalization of \var{loP} as \var{lo} and
        of \var{lesP} as \var{les},
        \Eref{eq:th-les-direction-32}, \Eref{eq:th-les-direction-34},
        \Eref{eq:th-les-direction-52}.
        This is \myfn{INDH}{\var{n}+1}.
    }
\end{gathered}
\end{equation}
Equations \Eref{eq:th-les-direction-30}--\Eref{eq:th-les-direction-54}
derive \myfn{INDH}{\var{n}+1} from \myfn{INDH}{\var{n}},
which shows the step of the induction.
With the step and its basis
\Eref{eq:th-les-direction-19-30},
we have the induction and theorem.
\myqed
\end{proof}

\begin{theorem}[LES current location run]
\label{th:les-current-location-run}
If two elements of an \type{LES} are at the same
current location, they bound a run of \type{LES} elements
at that current location.
That is, if
\begin{gather}
\label{eq:th-current-location-run-02}
\Vles{les} \in \type{LES}
\\ \label{eq:th-current-location-run-04}
\land \; 0 \le \Vnat{start} \le \Vnat{end} \le \Vlastix{les}
\\ \label{eq:th-current-location-run-07}
\land \; \Vloc{loc} = \Current{\VVelement{les}{start}} = \Current{\VVelement{les}{end}}
\end{gather}
then
\begin{equation}
\label{eq:th-current-location-run-08}
\forall \; \var{i} \in \set{ \var{ii} : \var{start} \le \var{ii} \le \var{end}} :
    \Current{\VVelement{les}{i}} = \var{loc}.
\end{equation}
\end{theorem}

\begin{proof}
The proof strategy is to assume the hypothesis to show a reductio.
\mypareq{eq:th-current-location-run-10-10}{%
    $\Vsize{les} \ge 1$
        \cuz{} \longDfref{LES}{def:next-eligible-sequence},
        \Eref{eq:th-current-location-run-02}.
}
\begin{equation}
\label{eq:th-current-location-run-10-20}
\begin{gathered}
\exists \; \Vnat{i} \in \set{ \Vnat{ii} : \var{start} \le \var{ii} \le \var{end}} :
    \Current{\VVelement{les}{i}} \neq \var{loc}
    \\ \myparbox{\cuz{} Contrary of
        \Eref{eq:th-current-location-run-08},
        \Eref{eq:th-current-location-run-10-10},
        AF reductio.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-current-location-run-12}
\begin{gathered}
\var{start} \le \var{bad} \le \var{end} \land
    \Current{\VVelement{les}{bad}} \neq \var{loc}
    \\ \myparbox{\cuz{} \Eref{eq:th-current-location-run-10-20},
        instantiating \var{i} as \var{bad}.%
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-current-location-run-16}{%
    $\Current{\VVelement{les}{end}} \le \Current{\VVelement{les}{bad}} \le \Current{\VVelement{les}{start}}$
    \linebreak
    \cuz{} \longThref{LES direction}{th:les-direction},
    \Eref{eq:th-current-location-run-12}.%
}
\mypareq{eq:th-current-location-run-18}{%
    $\Current{\VVelement{les}{bad}} \neq \var{loc}$
    \cuz{} \Eref{eq:th-current-location-run-12}.%
}
\mypareq{eq:th-current-location-run-20}{%
    $\Current{\VVelement{les}{end}} \le \Current{\VVelement{les}{bad}} < \Current{\VVelement{les}{start}}$
    \linebreak
    \cuz{} \Eref{eq:th-current-location-run-07},
    \Eref{eq:th-current-location-run-16},
    \Eref{eq:th-current-location-run-18}.%
}
\mypareq{eq:th-current-location-run-22}{%
    $\Current{\VVelement{les}{end}} < \Current{\VVelement{les}{start}}$
    \cuz{} \Eref{eq:th-current-location-run-20}.%
}
\mypareq{eq:th-current-location-run-24}{%
    $\var{loc} < \var{loc}$
    \cuz{} \Eref{eq:th-current-location-run-07},
    \Eref{eq:th-current-location-run-22}.%
}
\Eref{eq:th-current-location-run-24} is a contradiction and shows the
reductio and the theorem.
\myqed
\end{proof}

\begin{theorem}[LES unit derivation]
\label{th:les-unit-derivation}
If an \type{LES} is of finite length at least 3,
and begins and ends at the same current location,
then there is a non-trivial unit derivation from
the symbol of its penultimate element to the
symbol of its first element. That is, if
\begin{gather}
    \label{eq:th-les-unit-derivation-02}
    \var{les} \in \type{LES}
    \\ \label{eq:th-les-unit-derivation-04}
    \land \;\; 3 \le \Vsize{les} \; \land \; \Vsize{les} \in \naturals
    \\ \label{eq:th-les-unit-derivation-06}
    \land \; \Current{\Velement{les}{0}} =
    \Current{\Velement{les}{\Vlastix{les}}}
\end{gather}
then
\begin{equation}
    \label{eq:th-les-unit-derivation-08}
    \Symbol{\Velement{les}{\Vlastix{les}\subtract 1}} \deplus
    \Symbol{\Velement{les}{0}}.
\end{equation}
\end{theorem}

\begin{proof}
The proof is by induction.
The induction variable is the length of the \type{LES},
offset by 3.
The induction hypothesis is
\begin{equation}
\label{eq:th-les-unit-derivation-10}
\begin{gathered}
    \myfn{INDH}{\Vnat{offX}} \defined
    \\ \forall \; \var{les} \in \set{
        \begin{gathered}
        \Vles{ines} :
            \Vsize{ines} = \var{offX} + 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\Vlastix{ines}\subtract 1}} \deplus
    \Symbol{\Velement{ines}{0}}.
\end{gathered}
\end{equation}

We take \myfn{INDH}{0} as the basis,
showing it directly.
\mypareq{eq:th-les-unit-derivation-11}{%
    $\size{\Vles{lesB}} = 3 \land
    \Current{\Velement{lesB}{\Vlastix{lesB}}}
        = \Current{\Velement{lesB}{0}}$
    AF derivation,
    new \Vles{lesB}.
}
\mypareq{eq:th-les-unit-derivation-12}{%
    $\Vles{lesB} = \tuple{\Veim{prev}, \Veim{middle}, \Veim{next}}$
    \cuz{} \Eref{eq:th-les-unit-derivation-11},
    new \Veim{prev}, \Veim{middle}, \Veim{next}
    for convenience.
}
\mypareq{eq:th-les-unit-derivation-14}{%
    \Current{\var{prev}} = \Current{\var{next}}
    \cuz{} \Eref{eq:th-les-unit-derivation-11},
        \Eref{eq:th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-16}{%
    \Current{\var{prev}} =
    \Current{\var{middle}} =
    \Current{\var{next}}
    \cuz{} \longThref{LES current location run}{th:les-current-location-run},
        \Eref{eq:th-les-unit-derivation-14}.
}
\mypareq{eq:th-les-unit-derivation-18}{%
    \myfn{Next-Eligible}{\var{next},\var{middle}}
    \cuz{} \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-20}{%
    \myfn{Next-Eligible}{\var{middle},\var{prev}}
    \cuz{} \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-unit-derivation-12}.
}
\mypareq{eq:th-les-unit-derivation-22}{%
    $\tuple{\Symbol{\var{middle}} \de \Symbol{\var{prev}}}
     \in \Rules{\Vint{g}}$
    \linebreak \cuz{} \Eref{eq:th-les-unit-derivation-16},
        \Eref{eq:th-les-unit-derivation-18},
        \Eref{eq:th-les-unit-derivation-20},
        \longThref{LES rule}{th:next-eligible-rule}.
}
\mypareq{eq:th-les-unit-derivation-24}{%
    $\Symbol{\Velement{lesB}{\Vlastix{lesB}\subtract 1}} \deplus
        \Symbol{\Velement{lesB}{0}}$
    \linebreak \cuz{} \Eref{eq:th-les-unit-derivation-12},
        \Eref{eq:th-les-unit-derivation-22}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-25-10}
\begin{gathered}
    \size{\Vles{lesB}} = 3 \land
    \Current{\Velement{lesB}{\Vlastix{lesB}}}
        = \Current{\Velement{lesB}{0}}
    \\ \implies \Symbol{\Velement{lesB}{\Vlastix{lesB}\subtract 1}} \deplus
        \Symbol{\Velement{lesB}{0}}
    \\ \myparbox{\cuz{}
        derivation \Eref{eq:th-les-unit-derivation-11}--%
        \Eref{eq:th-les-unit-derivation-24}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-unit-derivation-26}
\begin{gathered}
    \forall \; \var{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{2}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{1}} \deplus
    \Symbol{\Velement{ines}{0}}.
    \\ \myparbox{%
    \cuz{} universal generalization of arbitrary \var{lesB},
        \Eref{eq:th-les-unit-derivation-25-10}.
        This is \myfn{INDH}{0}, the basis of the induction.
    }
\end{gathered}
\end{equation}

For the step we assume
\myfn{INDH}{\var{n}}
to show \myfn{INDH}{\var{n}+1}.
\begin{equation}
\label{eq:th-les-unit-derivation-30}
\begin{gathered}
    \forall \; \var{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = \var{n} + 3
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\Vlastix{ines}\subtract 1}} \deplus
    \Symbol{\Velement{ines}{0}}
    \\ \myparbox{\cuz{} \myfn{INDH}{\var{n}}, AF step.%
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-unit-derivation-32}
\begin{gathered}
    \Vsize{\Vles{lesP}} = \var{n} + 3 + 1
    \\ \land \; \Current{\Velement{lesP}{\Vlastix{lesP}}} =
        \Current{\Velement{lesP}{0}}
    \\ \myparbox{\cuz{}
        AF derivation; new \var{lesP}.
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-36}{%
    $\forall \; \Vnat{i} < \Vlastix{lesP} : \Current{\VVelement{lesP}{i}} = \var{locP}$
    \linebreak \cuz{}
    \longThref{LES current location run}{th:les-current-location-run},
    \Eref{eq:th-les-unit-derivation-32},
     new \var{locP} for convenience.
}
\mypareq{eq:th-les-unit-derivation-38}{%
    $\Vles{pre} = \tuple{\Velement{lesP}{0} \ldots \Velement{lesP}{\var{n}+2}}$
    \linebreak \cuz{} \longThref{LES subsequence}{th:les-subsequence},
    \Eref{eq:th-les-unit-derivation-32};
    new \Vles{pre} for convenience.
}
\mypareq{eq:th-les-unit-derivation-39}{%
    $\Vsize{pre} = \var{n} + 3$
    \cuz{} \Eref{eq:th-les-unit-derivation-38}.%
}
\mypareq{eq:th-les-unit-derivation-40}{%
    $\forall \; \Vnat{i} \le \var{n}+2 : \VVelement{pre}{i} = \VVelement{lesP}{i}$
    \cuz{} \Eref{eq:th-les-unit-derivation-38}.
}
\mypareq{eq:th-les-unit-derivation-42}{%
    $\Current{\Velement{pre}{0}} = \Current{\Velement{pre}{\Vlastix{pre}}}$
    \linebreak \cuz{} \Eref{eq:th-les-unit-derivation-36},
        \Eref{eq:th-les-unit-derivation-39},
        \Eref{eq:th-les-unit-derivation-40}.
}
\mypareq{eq:th-les-unit-derivation-46}{%
    $\Symbol{\Velement{pre}{\Vlastix{pre}\subtract 1}} \deplus
    \Symbol{\Velement{pre}{0}}$
    \linebreak \cuz{} \Eref{eq:th-les-unit-derivation-30},
    \Eref{eq:th-les-unit-derivation-39},
    \Eref{eq:th-les-unit-derivation-42}.
}
\mypareq{eq:th-les-unit-derivation-47-10}{%
    $\Vlastix{pre}\subtract 1 = \Vsize{pre}\subtract 2
        = \var{n}+1$
    \cuz{} \Eref{eq:th-les-unit-derivation-39}.
}
\mypareq{eq:th-les-unit-derivation-47-20}{%
    $\var{n}+1
    = \Vsize{lesP}\subtract 3 =
    \Vlastix{lesP}\subtract 2$
    \cuz{} \Eref{eq:th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-47-30}{%
    $\Vlastix{pre}\subtract 1 = \Vlastix{lesP}\subtract 2$
    \cuz{} \Eref{eq:th-les-unit-derivation-47-10},
    \Eref{eq:th-les-unit-derivation-47-20}.
}
\mypareq{eq:th-les-unit-derivation-48}{%
    $\Symbol{\Velement{lesP}{\Vlastix{lesP}\subtract 2}} \deplus
    \Symbol{\Velement{lesP}{0}}$
    \cuz{} \Eref{eq:th-les-unit-derivation-40},
    \Eref{eq:th-les-unit-derivation-46},
    \Eref{eq:th-les-unit-derivation-47-30}.
}
\mypareq{eq:th-les-unit-derivation-50}{%
    \myfn{Next-Eligible}{
        \Velement{lesP}{\Vlastix{lesP}},
        \Velement{lesP}{\Vlastix{lesP}\subtract 1}}
    \linebreak \cuz{} \longDfref{LES}{def:next-eligible-sequence},
    \Eref{eq:th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-52}{%
    \myfn{Next-Eligible}{
        \Velement{lesP}{\Vlastix{lesP}\subtract 1},
        \Velement{lesP}{\Vlastix{lesP}\subtract 2}}
    \linebreak \cuz{} \Dfref{def:next-eligible-sequence},
    \Eref{eq:th-les-unit-derivation-32}.
}
\mypareq{eq:th-les-unit-derivation-54}{%
    $\Current{\Velement{lesP}{\Vlastix{lesP}\subtract 1}} = \Current{\Velement{lesP}{\Vlastix{lesP}\subtract 2}}$
    \linebreak \cuz{} \Eref{eq:th-les-unit-derivation-32},
        \Eref{eq:th-les-unit-derivation-36}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-56}
\begin{gathered}
    \tuple{
    \begin{gathered}
    \Symbol{\Velement{lesP}{\Vlastix{lesP}\subtract 1}}
    \\ \de \Symbol{\Velement{lesP}{\Vlastix{lesP}\subtract 2}}
    \end{gathered}
    }
     \in \Rules{\Vint{g}}
    \\ \because \longThref{LES rule}{th:next-eligible-rule},
    \Eref{eq:th-les-unit-derivation-50},
    \Eref{eq:th-les-unit-derivation-52},
    \Eref{eq:th-les-unit-derivation-54}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-58}{%
    $\Symbol{\Velement{lesP}{\Vlastix{lesP}\subtract 1}} \deplus
    \Symbol{\Velement{lesP}{0}}$
    \cuz{} \Eref{eq:th-les-unit-derivation-48},
    \Eref{eq:th-les-unit-derivation-56}.
}
\begin{equation}
\label{eq:th-les-unit-derivation-59-10}
\begin{gathered}
    \Vsize{\Vles{lesP}} = \var{n} + 3 + 1
    \\ \land \; \Current{\Velement{lesP}{\Vlastix{lesP}}} =
        \Current{\Velement{lesP}{0}}
    \\ \implies \Symbol{\Velement{lesP}{\Vlastix{lesP}\subtract 1}} \deplus
        \Symbol{\Velement{lesP}{0}}
    \\ \myparbox{\cuz{}
        derivation \Eref{eq:th-les-unit-derivation-32}--%
        \Eref{eq:th-les-unit-derivation-58}.
    }
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-les-unit-derivation-60}
\begin{gathered}
    \forall \; \var{les} \in \set{
        \begin{gathered}
        \Vles{ines} : \Vsize{ines} = \var{n} + 3 + 1
        \\ \land \; \Current{\Velement{ines}{0}} =
        \Current{\Velement{ines}{\Vlastix{ines}}}
        \end{gathered}
    } :
    \\ \Symbol{\Velement{ines}{\Vlastix{ines}\subtract 1}} \deplus
    \Symbol{\Velement{ines}{0}}
    \\ \myparbox{\cuz{}
        universal generalization of arbitrary \Vles{lesP},
        \Eref{eq:th-les-unit-derivation-59-10}.
        This is \myfn{INDH}{\var{n} + 1}.
    }
\end{gathered}
\end{equation}
\mypareq{eq:th-les-unit-derivation-62}{%
    $\myfn{INDH}{\var{n}} \implies \myfn{INDH}{\var{n}+1}$
    \cuz{} derivation
        \Eref{eq:th-les-unit-derivation-30}--\Eref{eq:th-les-unit-derivation-60}.
        This is the step of the induction.
}

We have the basis of the induction
\Eref{eq:th-les-unit-derivation-26}
and its step \Eref{eq:th-les-unit-derivation-62}.
With the induction, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES finiteness]
\label{th:les-finiteness}
Let \var{les} be an arbitrary \type{LES}.
Then
\begin{gather}
\label{eq:th-les-finiteness-02}
\Vsize{les} \le \var{max} \times (\Current{\Velement{les}{0}}+1),
\\ \label{eq:th-les-finiteness-04}
    \text{where } \Vnat{max} = \size{\Vocab{\Vint{g}}}+1.
\end{gather}
Also, every \type{LES} is finite:
\begin{equation}
\label{eq:th-les-finiteness-06}
\forall \; \var{les} \in \type{LES} : \var{les} \in \naturals.
\end{equation}
\end{theorem}

\begin{proof}

The proof proceeds by reductio.
Before embarking on the reductio proper,
we note two facts.
First,
\mypareq{eq:th-les-finiteness-10}{%
    $\size{\Vles{les}} \ge 1$
        \cuz{} \longDfref{LES}{def:next-eligible-sequence}.
}
Equation \Eref{eq:th-les-finiteness-10} will allow us to assume
that the element \Velement{les}{0} exists.
Second,
\begin{equation}
\label{eq:th-les-finiteness-12}
\begin{gathered}
\left( \begin{gathered}
    \forall \; \Vloc{i} \le \Current{\Velement{les}{0}} :
    \\ \size{\set{\Veim{eim} \in \var{les}: \Current{\var{eim}} = \var{i}}}
    \le \var{max}
\end{gathered} \right)
\\ \implies \Vsize{les} \le \var{max} \times (\Current{\Velement{les}{0}}+1).
\\ \because \Eref{eq:th-les-finiteness-02}, \Eref{eq:th-les-finiteness-04}.
\end{gathered}
\end{equation}
We now begin the reductio itself.
\mypareq{eq:th-les-finiteness-14}{%
    $\Vsize{les} > \var{max} \times (\Current{\Velement{les}{0}}+1)$
    \cuz{} negation of \Eref{eq:th-les-finiteness-02}, AF reductio.%
}
\mypareq{eq:th-les-finiteness-16}{%
    $\exists \; \Vloc{i} \le \Current{\Velement{les}{0}} : \size{\set{\Veim{eim} \in \var{les}: \Current{\var{eim}} = \var{i}}}
        > \var{max}$
        \cuz{}
        contrapositive of \Eref{eq:th-les-finiteness-12},
        \Eref{eq:th-les-finiteness-14}.
}
\mypareq{eq:th-les-finiteness-18}{%
    $\size{\set{\var{eim} \in \var{les}: \Current{\var{eim}} = \var{i}}} > \var{max}$
    \cuz{} \Eref{eq:th-les-finiteness-16}, existential instantiation of \var{i}.
}

The sequence \var{les} is countable,
and from \Eref{eq:th-les-finiteness-18} we know that there are at
least $\var{max}+1$ elements
of \var{les} whose current location is \var{i}.
Therefore we can count out in \var{les} until we hit the
$\var{max}+1$'th element whose current location is \Vloc{i}.
Call this element \VVelement{les}{z}, so that
\begin{equation}
\label{eq:th-les-finiteness-20}
\begin{gathered}
   \Current{\VVelement{les}{z}} = \var{i}
   \\ \land \; \size{\set{\Vnat{ix} : \var{ix} \le \var{z} \land \Current{\VVelement{les}{ix}} = \var{i}}}
       = \var{max}+1.
   \\ \myparbox{\cuz{} \Eref{eq:th-les-finiteness-18}, new \var{z}.}
\end{gathered}
\end{equation}

Let \Vles{les1} be the prefix of \var{les} such that
\begin{equation}
\label{eq:th-les-finiteness-22}
\Vles{les1} = \Velement{les}{0\ldots \var{z}}
    \because \longThref{LES subsequence}{th:les-subsequence},
        \Eref{eq:th-les-finiteness-20}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-22-50}
    \forall \; \Vnat{ix} \le \var{z} : \VVelement{les1}{ix} = \VVelement{les}{ix}
    \because \Eref{eq:th-les-finiteness-22}.
\end{equation}
There are \var{max} + 1 elements in \var{les1} whose
current location is \Vloc{i}:
\begin{equation}
\label{eq:th-les-finiteness-23b}
\begin{gathered}
    \size{\set{\Vnat{ix} \in \Dom{\var{les1}} : \Current{\VVelement{les1}{ix}} = \var{i}}}
       = \var{max}+1
    \\ \because \Eref{eq:th-les-finiteness-20},
            \Eref{eq:th-les-finiteness-22},
            \Eref{eq:th-les-finiteness-22-50}.
\end{gathered}
\end{equation}
Since \var{les1} is a sequence
and contains at least one element whose current location is \Vloc{i}
\Eref{eq:th-les-finiteness-23b},
there clearly is a first element of \var{les1}
whose current location is \Vloc{i}.
Call that first element \VVelement{les1}{a},
so that
\begin{gather}
\label{eq:th-les-finiteness-24a}
   \Vnat{a} \le \Vlastix{les1},
\\ \label{eq:th-les-finiteness-24b}
   \Current{\VVelement{les1}{a}} = \var{i}\text{, and}
\\ \label{eq:th-les-finiteness-26}
   \forall \; \var{ii} < \var{a} : \Current{\VVelement{les1}{ii}} \neq \var{i}.
\end{gather}

We let \var{run} be a new \type{LES}:
\begin{equation}
\label{eq:th-les-finiteness-28}
\Vles{run} = \Velement{les1}{\var{a}\ldots \var{z}}
    \because \Thref{th:les-subsequence},
        \Eref{eq:th-les-finiteness-22},
        \Eref{eq:th-les-finiteness-24a}.
\end{equation}
\Vles{run} is so called because it is
a run of \type{EIM}s with the property that
they share the same current location,
though we have yet to prove this.
We do so next.
\begin{equation}
\label{eq:th-les-finiteness-28-10}
\begin{gathered}
   \Current{\VVelement{les1}{a}} = \var{i} = \Current{\VVelement{les1}{z}}
       \\ \because \Eref{eq:th-les-finiteness-20}, \Eref{eq:th-les-finiteness-22-50},
       \Eref{eq:th-les-finiteness-24b}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-28-20}
\begin{gathered}
    \forall \; \Vnat{ix} \in \set{ \Vnat{ix1} : \var{a} \le \var{ix1} \le \var{z}} :
    \\ \Current{\VVelement{les1}{ix}} = \Vloc{i}
    \\ \because \longThref{LES current location run}{th:les-current-location-run},
    \Eref{eq:th-les-finiteness-28-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-28-30}
   \forall \; \Vnat{ix} \in \Dom{\var{run}} :
       \Current{\VVelement{run}{ix}} = \var{i}
   \because
    \Eref{eq:th-les-finiteness-28},
    \Eref{eq:th-les-finiteness-28-20}.
\end{equation}
We next characterize the cardinality of \var{run}.
\begin{equation}
\label{eq:th-les-finiteness-30}
\begin{gathered}
   \forall \; \var{ii} \in \Dom{\var{les1}} :
   \\ \Current{\VVelement{les1}{ii}} = \var{i}
   \iff \var{a} \le \var{ii} \le \var{z}
   \\ \because \Eref{eq:th-les-finiteness-22},
    \Eref{eq:th-les-finiteness-26},
    \Eref{eq:th-les-finiteness-28-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-33}
   \Vsize{run} = \var{max}+1
   \because
    \Eref{eq:th-les-finiteness-23b},
    \Eref{eq:th-les-finiteness-28},
    \Eref{eq:th-les-finiteness-30}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-34}
   \Vsize{run} =
     \size{\Vocab{\Vint{g}}}+2
\\ \because \Eref{eq:th-les-finiteness-04},
    \Eref{eq:th-les-finiteness-33}.
\end{equation}

We now look at a prefix of \var{run}.
\begin{equation}
\label{eq:th-les-finiteness-36}
\Veimseq{prerun} = \var{run}\left[ 0\ldots \size{\Vocab{\Vint{g}}} \right].
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-37}
\forall \; \var{i} \in \Dom{\var{prerun}} : \VVelement{prerun}{i} = \VVelement{run}{i}
\because \Eref{eq:th-les-finiteness-36}
\end{equation}
Every \type{EIM} in \var{prerun} has a symbol,
and since
\mypareq{}{%
there are only \size{\Vocab{\Vint{g}}} symbols in \Vint{g}, and
}
\mypareq{}{%
\var{prerun} contains \size{\Vocab{\Vint{g}}+1} \type{EIM}s
    \Eref{eq:th-les-finiteness-36},
}
at least one symbol must be used twice as the symbol of an
\type{EIM} in \var{prerun}.
Let this symbol be \Vsym{cycle-lhs},
and let the first and second \type{EIM}s with \Vsym{cycle-lhs} as their symbol be
\VVelement{prerun}{aa} and
\VVelement{prerun}{zz}, respectively:
\begin{equation}
\label{eq:th-les-finiteness-38}
\begin{gathered}
    \Symbol{\VVelement{prerun}{aa}} = \Symbol{\VVelement{prerun}{zz}}
    \land \; \var{aa} < \var{zz}
    \\ \text{\cuz{} arbitrary \Vnat{aa}, \Vnat{zz}.}%
\end{gathered}
\end{equation}

Let
\begin{equation}
\label{eq:th-les-finiteness-40}
\begin{gathered}
\Vles{cycle} = \Velement{run}{\var{aa}\ldots \var{zz}+1}
    \\ \myparbox{\cuz{}
        \longThref{LES subsequence}{th:les-subsequence},
        \Eref{eq:th-les-finiteness-28},
        \Eref{eq:th-les-finiteness-37},
        \Eref{eq:th-les-finiteness-38}.
    }
\end{gathered}
\end{equation}
In \Eref{eq:th-les-finiteness-40},
we know that \Velement{run}{\var{zz}+1} exists because of
\Eref{eq:th-les-finiteness-34} and \Eref{eq:th-les-finiteness-36}.
\begin{equation}
\label{eq:th-les-finiteness-42}
    \Symbol{\VVelement{run}{aa}} = \Symbol{\VVelement{run}{zz}}
    \because \Eref{eq:th-les-finiteness-37},
        \Eref{eq:th-les-finiteness-38}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-44}
\begin{gathered}
    \Symbol{\Velement{cycle}{0}} = \Symbol{\Velement{cycle}{\Vlastix{cycle}\subtract 1}}
    \\ \because \Eref{eq:th-les-finiteness-40}, \Eref{eq:th-les-finiteness-42}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-46}
    \var{aa} < \var{zz} \because \Eref{eq:th-les-finiteness-38}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-48}
    \Vsize{cycle} \ge 3 \because \Eref{eq:th-les-finiteness-40}, \Eref{eq:th-les-finiteness-46}.
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-52}
\begin{gathered}
        \forall \; \Vnat{ix} \in \Dom{\var{cycle}} : \Current{\VVelement{cycle}{ix}} = \Vloc{i}
        \\ \because \Eref{eq:th-les-finiteness-28-30},
            \Eref{eq:th-les-finiteness-40}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-54}
\begin{gathered}
    \Symbol{\Velement{cycle}{\Vlastix{cycle}\subtract 1}} \deplus
    \Symbol{\Velement{cycle}{0}}
    \\ \myparbox{\longThref{LES unit derivation}{th:les-unit-derivation},
        \Eref{eq:th-les-finiteness-40},
        \Eref{eq:th-les-finiteness-48},
        \Eref{eq:th-les-finiteness-52}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-finiteness-56}
\begin{gathered}
    \Symbol{\Velement{cycle}{0}} \deplus
    \Symbol{\Velement{cycle}{0}}
    \\ \because
        \Eref{eq:th-les-finiteness-44},
        \Eref{eq:th-les-finiteness-54}.
\end{gathered}
\end{equation}
\mypareq{eq:th-les-finiteness-58}{\Vint{g} does not contain a cycle
    \cuz{} \longDfref{Marpa internal grammar}{def:internal-grammar}.
    This contradicts \Eref{eq:th-les-finiteness-56} and concludes
    the reductio beginning at \Eref{eq:th-les-finiteness-14}.%
}
\begin{equation}
\label{eq:th-les-finiteness-60}
\begin{gathered}
    \Vsize{les} \le \var{max} \times (\Current{\Velement{les}{0}}+1)
    \\ \myparbox{\cuz{} negation of \Eref{eq:th-les-finiteness-14}
        by reductio \Eref{eq:th-les-finiteness-14}--\Eref{eq:th-les-finiteness-58}.%
    }
\end{gathered}
\end{equation}

The equation \Eref{eq:th-les-finiteness-60}
is \Eref{eq:th-les-finiteness-02} of the theorem.
We conclude \Eref{eq:th-les-finiteness-06} of
the theorem immediately
from \Eref{eq:th-les-finiteness-02},
completing the proof.
\myqed
\end{proof}

\begin{theorem}[LES uniqueness]
\label{th:les-uniqueness}
Two \type{LES}'s of the same length have the same base element
iff they are identical.
That is,
\begin{gather}
\label{eq:th-les-uniqueness-02}
   \forall \; \var{les1} \in \type{LES} :
       \forall \; \var{les2} \in \type{LES} :
\\ \label{eq:th-les-uniqueness-04}
   \Vsize{les1} = \Vsize{les2}
\\ \label{eq:th-les-uniqueness-06}
   \land \; \Velement{les1}{0} = \Velement{les2}{0}
\\ \nonumber \iff
\\ \label{eq:th-les-uniqueness-08}
   \var{les1} = \var{les2}.
\end{gather}
\end{theorem}

\begin{proof}
If two \type{LES}'s are identical, they obviously have the same
base element and length.
This shows the reverse direction of the theorem trivially.

For the forward direction, we divide the proof into two subcases,
based on the value of \Vsize{les1}.
We show the first subcase directly.
\mypareq{eq:th-les-uniqueness-10-12}{%
    $\Vsize{les1} = 1$ \cuz{} AF first subcase.
}
\mypareq{eq:th-les-uniqueness-10-14}{%
    $\Vsize{les2} = 1$ \cuz{} \Eref{eq:th-les-uniqueness-04},
        \Eref{eq:th-les-uniqueness-10-12}.
}
\mypareq{eq:th-les-uniqueness-10-16}{%
   $\var{les1} = \tuple{\Velement{les1}{0}}
       = \tuple{\Velement{les2}{0}} = \var{les2}$
   \linebreak \cuz{} \Eref{eq:th-les-uniqueness-06},
        \Eref{eq:th-les-uniqueness-10-12},
        \Eref{eq:th-les-uniqueness-10-14}.
}
\mypareq{eq:th-les-uniqueness-10-18}{%
   $\var{les1} = \var{les2}$ \cuz{} \Eref{eq:th-les-uniqueness-10-16}.
   This is the consequent \Eref{eq:th-les-uniqueness-08}
      and shows the first subcase of the foward direction.
}

For the second subcase, we proceed via a reductio.
\mypareq{eq:th-les-uniqueness-10-20}{%
    $\Vsize{les1} > 1$ \cuz{} AF second subcase.
}
\mypareq{eq:th-les-uniqueness-10-22}{%
    $\Vsize{les2} > 1$ \cuz{} \Eref{eq:th-les-uniqueness-04},
        \Eref{eq:th-les-uniqueness-10-20}.
}
For our reductio,
we assume that the two \type{LES}'s differ.
\mypareq{eq:th-les-uniqueness-10-24}{%
   $\var{les1} \ne \var{les2}$ \cuz{} AF reductio.
}
If \var{les1} and \var{les2} differ, they must differ
in one of their elements.
Therefore, there will be a first index at which the elements differ.
\begin{gather}
\label{eq:th-les-uniqueness-11-10}
        \VVelement{les1}{\Vnat{first}} \neq \VVelement{les2}{\var{first}}
\\ \label{eq:th-les-uniqueness-11-20}
        \land \; \forall \; \Vnat{ix} < \var{first} : \VVelement{les1}{ix} = \VVelement{les2}{ix}
\\ \nonumber
    \because \Eref{eq:th-les-uniqueness-10-24}, \text{new \Vnat{first}.}
\end{gather}
%
Since, by hypothesis for the theorem,
\var{les1} and \var{les2} share the same base
element, we know that
\begin{equation}
\label{eq:th-les-uniqueness-20}
\var{first} > 0 \because \Eref{eq:th-les-uniqueness-06},
    \Eref{eq:th-les-uniqueness-11-10}.
\end{equation}
%
\begin{equation}
\label{eq:th-les-uniqueness-22}
\begin{gathered}
    \myfn{Next-Eligible}{
        \VVelement{les1}{first},
        \Velement{les1}{\var{first}\subtract 1}}
    \\ \land \; \myfn{Next-Eligible}{
        \VVelement{les2}{first},
        \Velement{les2}{\var{first}\subtract 1}}
\\ \because
        \longDfref{LES}{def:next-eligible-sequence},
        \Eref{eq:th-les-uniqueness-10-20},
        \Eref{eq:th-les-uniqueness-10-22},
        \Eref{eq:th-les-uniqueness-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-les-uniqueness-24}
\begin{gathered}
        \Velement{les1}{\var{first}\subtract 1}
        \neq \Velement{les2}{\var{first}\subtract 1}
    \\ \because \longThref{Next eligible uniqueness}{th:next-eligible-uniqueness},
    \Eref{eq:th-les-uniqueness-22}.
\end{gathered}
\end{equation}
Equation \Eref{eq:th-les-uniqueness-24}
contradicts \Eref{eq:th-les-uniqueness-10-18},
which shows the reductio, the second subcase
and the forward direction.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LES prefix]
\label{th:les-prefix}
Consider two \type{LES}'s.
They have different lengths but the same base element
iff the shorter one is a proper prefix of the longer one.
That is,
\begin{gather}
\label{eq:th-les-prefix-02}
   \forall \; \var{les1} \in \type{LES} :
       \forall \; \var{les2} \in \type{LES} :
\\ \label{eq:th-les-prefix-04}
   \Vsize{les1} < \Vsize{les2}
\\ \label{eq:th-les-prefix-06}
   \land \; \Velement{les1}{0} = \Velement{les2}{0}
\\ \nonumber \iff
\\ \label{eq:th-les-prefix-08}
   \PrPfx{\var{les1},\var{les2}}.
\end{gather}
\end{theorem}

\begin{proof}
For the reverse direction,
\Eref{eq:th-les-prefix-04} and
\Eref{eq:th-les-prefix-06}
follow immediately from
\Eref{eq:th-les-prefix-08},
via the definition of proper prefix \Dfref{def:prefix}.

It remains to show the forward direction.
Let
\mypareq{eq:th-les-prefix-20}{%
    $\Vles{subLes2} = \Velement{les2}{0\ldots \Vlastix{les1}}$
    \linebreak \cuz{} \longThref{LES subsequence}{th:les-subsequence},
       antecedant of forward direction
           \Eref{eq:th-les-prefix-02}--\Eref{eq:th-les-prefix-06},
           new \Vles{subLes2} for convenience.
}
\mypareq{eq:th-les-prefix-21}{%
    \PrPfx{\var{subLes2},\var{les2}} \cuz{}
        \longDfref{Proper prefix}{def:prefix},
        \Eref{eq:th-les-prefix-04}, \Eref{eq:th-les-prefix-20}.
}
\begin{equation}
\label{eq:th-les-prefix-22}
\Vsize{subLes2} = \Vsize{les1} \because \Eref{eq:th-les-prefix-20}
\end{equation}
\mypareq{eq:th-les-prefix-24}{%
    $\Velement{subLes2}{0} = \Velement{les2}{0}$ \cuz{} \Eref{eq:th-les-prefix-20}.
}
\mypareq{eq:th-les-prefix-26}{%
    $\Velement{subLes2}{0} = \Velement{les1}{0}$
    \linebreak \cuz{}
       antecedant of forward direction \Eref{eq:th-les-prefix-06},
       \Eref{eq:th-les-prefix-24}.
}
\mypareq{eq:th-les-prefix-27}{%
    $\var{subLes2} = \var{les1}$
    \linebreak \cuz{} \longThref{LES uniqueness}{th:les-prefix},
        \Eref{eq:th-les-prefix-22},
        \Eref{eq:th-les-prefix-26}.
}
\mypareq{eq:th-les-prefix-28}{%
    $\PrPfx{\Vles{les1}, \Vles{les2}}$
    \cuz{} \Eref{eq:th-les-prefix-21},
    \Eref{eq:th-les-prefix-27}.
}
Equation \Eref{eq:th-les-prefix-28}
is the consequent of the forward direction,
which shows the forward direction.
With both directions, we have the proof.
\myqed
\end{proof}

\begin{theorem}[LES trichotomy]
\label{th:les-trichotomy}
If two \type{LES}'s have the same base element,
then exactly one of the following three statements is true:
\begin{itemize}
\item The two \type{LES}'s are identical.
\item The first \type{LES} is a proper prefix of the second.
\item The second \type{LES} is a proper prefix of the first.
\end{itemize}
That is, if
\begin{gather}
\label{eq:th-les-trichotomy-02}
\var{les1} \in \type{LES}
\\ \label{eq:th-les-trichotomy-04}
\; \land \; \var{les2} \in \type{LES}
\\ \label{eq:th-les-trichotomy-06}
\land \; \Velement{les1}{0} = \Velement{les2}{0}
\end{gather}
then
\mypareq{eq:th-les-trichotomy-08}{%
   \size{\var{fn} \in \set{\var{Eq},\var{Lt},\var{Gt}} :
       \myfn{fn}{\var{les1},\var{les2}}} = 1
}
where \var{Eq}, \var{Lt} and \var{Gt} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-les-trichotomy-09a}
    \myfn{Eq}{\Vles{x},\Vles{y}} \iff \var{x} = \var{y},
    \\ \label{eq:th-les-trichotomy-09b}
    \myfn{Lt}{\Vles{x},\Vles{y}} \iff \PrPfx{\var{les1},\var{les2}}\text{, and}
    \\ \label{eq:th-les-trichotomy-09c}
    \myfn{Gt}{\Vles{x},\Vles{y}} \iff \PrPfx{\var{les2},\var{les1}}.
\end{gather}
\end{theorem}

\begin{proof}
The proof is direct.
We first note that,
by the Trichotomy Law for the ordering of the natural numbers,
\mypareq{eq:th-les-trichotomy-10}{%
   \size{\var{fn} \in \set{\var{EqSz},\var{LtSz},\var{GtSz}} :
       \myfn{fn}{\var{les1},\var{les2}}} = 1
}
where \var{EqSz}, \var{LtSz} and \var{GtSz} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-les-trichotomy-12}
   \myfn{EqSz}{\Vles{x},\Vles{y}} \iff \Vsize{x} = \Vsize{y},
\\ \label{eq:th-les-trichotomy-14}
    \myfn{LtSz}{\Vles{x},\Vles{y}} \iff \Vsize{les1} < \Vsize{les2}\text{, and}
\\ \label{eq:th-les-trichotomy-16}
    \myfn{GtSz}{\Vles{x},\Vles{y}} \iff \Vsize{les1} > \Vsize{les2}.
\end{gather}

We now show three derivations, one for each of the functions
\var{EqSz}, \var{LtSz} and \var{GtSz}.
\mypareq{eq:th-les-trichotomy-17-18}{
   \myfn{EqSz}{\Vles{x},\Vles{y}} \cuz{} AF derivation;
   new arbitrary \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-20}{
   $\Vsize{x} = \Vsize{y}$ \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-18}
   using \Eref{eq:th-les-trichotomy-12}.
}
\mypareq{eq:th-les-trichotomy-17-22}{
   $\Vles{x} = \Vles{y}$ \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-20}
   using \longThref{LES uniqueness}{th:les-uniqueness},
    \Eref{eq:th-les-trichotomy-06}.
}
\mypareq{eq:th-les-trichotomy-17-24}{
   \myfn{Eq}{\Vles{x},\Vles{y}} \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-22}
   using \Eref{eq:th-les-trichotomy-09a}.
}
\mypareq{eq:th-les-trichotomy-17-26}{
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{EqSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Eq}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} equivalence \Eref{eq:th-les-trichotomy-17-18}--\Eref{eq:th-les-trichotomy-17-24},
   \linebreak universal generalization of \Vles{x}, \Vles{y}.
}

\mypareq{eq:th-les-trichotomy-17-28}{
   \myfn{LtSz}{\Vles{x},\Vles{y}} \cuz{} AF derivation;
   new arbitrary \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-30}{
   $\Vsize{x} < \Vsize{y}$ \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-28}
   using \Eref{eq:th-les-trichotomy-14}.
}
\mypareq{eq:th-les-trichotomy-17-32}{
   $\Vles{x} = \Vles{y}$ \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-30}
   using \longThref{LES prefix}{th:les-prefix},
    \Eref{eq:th-les-trichotomy-06}.
}
\mypareq{eq:th-les-trichotomy-17-34}{
   \myfn{Lt}{\Vles{x},\Vles{y}} \cuz{}
   equivalence with \Eref{eq:th-les-trichotomy-17-32}
   using \Eref{eq:th-les-trichotomy-09b}.
}
\mypareq{eq:th-les-trichotomy-17-36}{
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{LtSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Lt}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} equivalence \Eref{eq:th-les-trichotomy-17-28}--\Eref{eq:th-les-trichotomy-17-34},
   \linebreak universal generalization of \Vles{x}, \Vles{y}.
}
\mypareq{eq:th-les-trichotomy-17-40}{
   $\forall \; \Vles{x} : \forall \; \Vles{y} :$
   $\myfn{GtSz}{\Vles{x},\Vles{y}}
   \iff \myfn{Gt}{\Vles{x},\Vles{y}}$
   \linebreak \cuz{} symmetry with
   \Eref{eq:th-les-trichotomy-17-28}--\Eref{eq:th-les-trichotomy-17-36}.
}

The theorem follows from
\Eref{eq:th-les-trichotomy-10}--\Eref{eq:th-les-trichotomy-14},
\Eref{eq:th-les-trichotomy-17-26},
\Eref{eq:th-les-trichotomy-17-36} and \Eref{eq:th-les-trichotomy-17-40}.
\myqed
\end{proof}

\begin{definition}[LIM validity]
\label{def:lim-validity}
The valid \type{LIM}s of an \type{EIM} are defined recursively.
Let \Veim{src} be an arbitrary \type{EIM}.
Then the set of \type{LIM}s of \var{src} is
\begin{equation}
\nonumber
\begin{gathered}
\myfn{lims}{\var{src}} \defined
\\ \left\lbrace
    \begin{gathered}
    \tuple{ \Vdr{dr}, \Postdot{\var{src}}, \Vorig{orig}, \Current{\var{src}}} :
    \\ \Valid{\var{src}} \land \myfn{Is-Leo-Eligible}{\var{src}}
    \\ \land \;
        \left( \begin{gathered}
            \size{\myfn{next-lims}{\var{src}}} = 0
            \\ \implies \var{dr} = \DR{\var{src}} \land \var{orig} = \Origin{\var{src}}
        \end{gathered} \right)
    \\ \land \;
        \left( \begin{gathered}
            \size{\myfn{next-lims}{\var{src}}} > 0
            \\ \implies \mylim{\ell} \in \myfn{next-lims}{\var{src}}
            \\ \land \; \var{dr} = \DR{\ell} \land \var{orig} = \Origin{\ell}
        \end{gathered} \right)
    \end{gathered}
\right\rbrace,
\end{gathered}
\end{equation}
where
\begin{equation}
\nonumber
\begin{gathered}
\myfn{next-lims}{\var{src}} \defined
\\ \set{ \ell \in \type{LIM} :
    \myfn{Next-Eligible}{\Veim{next},\Veim{src}}
    \land \; \ell \in \myfn{lims}{\var{next}}
}.
\end{gathered}
\end{equation}
We overload \myfn{Valid}{} so that
\begin{equation}
\nonumber
\begin{gathered}
    \Valid{\mylim{\ell}} \defined \exists \; \Veim{src} : \ell \in \myfn{lims}{\var{src}}
\end{gathered}
\end{equation}
and we say that \mylim{\ell} is \dfn{valid} if \Valid{\ell}.
If $\mylim{\ell} \in \myfn{lims}{\Veim{src}}$,
we say that \type{EIM} \var{src} is the \dfn{source} of $\ell$,
and that the \type{LIM} $\ell$ is the \dfn{LIM} of \Veim{src}.
\dispEnd
\end{definition}

\begin{definition}[Leo source \type{EIM}]
\label{def:leo-source-eim}
\[
    \myfn{Is-source}{\Veim{src}} \defined \size{\myfn{lims}{\var{src}}} > 0.
\]
We say that \var{src} is
a \dfn{Leo source \type{EIM}}, or a \dfn{source \type{EIM}},
if \myfn{Is-source}{\Veim{src}}.
\dispEnd
\end{definition}

\begin{theorem}[Leo source function]
\label{th:leo-source-fn}
The total function
\begin{gather}
\label{eq:th-leo-source-fn-05}
\var{Source} \in \fnset{
    \set{\mylim{\ell} : \Valid{\ell}}
}{
    \set{\Veim{src} : \myfn{Is-source}{\var{src}}}
}
\\ \label{eq:th-leo-source-fn-07}
    \text{such that } \Source{\mylim{\ell}} \defined \riota \Veim{src} : \ell \in \myfn{lims}{\var{src}}
\end{gather}
exists and is a surjection.
\end{theorem}

\begin{proof}
The proof strategy is to
define a relation \var{Source-Rel},
to show that \var{Source-Rel} is the equivalent of \myfnname{Source},
and to show that the required properties hold for \var{Source-Rel}.
\mypareq{eq:th-leo-source-fn-10-01}{%
   $\var{Source-Rel} = \set{ \tuple{ \ell, \var{src} } \in \type{LIM} \times \type{EIM} :
         \ell \in \myfn{lims}{\var{src}}
   }$ exists \cuz obvious.
}
By the definition of \type{LIM} validity,
\mypareq{eq:th-leo-source-fn-10-02}{%
    $\forall \; \mylim{\ell} : \Valid{\ell} \iff \exists \; \Veim{src} : \ell \in \myfn{lims}{\var{src}}$
    \cuz{} \Dfref{def:lim-validity}.
}
\mypareq{eq:th-leo-source-fn-10-03}{%
    $\Dom{\var{Source-Rel}} = \Dom{\var{Source}}$
    \cuz{} \Eref{eq:th-leo-source-fn-05},
        \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-02}.
}
By the definition of Leo source \type{EIM},
\mypareq{eq:th-leo-source-fn-10-04}{%
    $\forall \; \Veim{src} : \myfn{Is-source}{\var{src}}
    \iff \exists \; \ell  : \ell \in \myfn{lims}{\var{src}}$
    \cuz{} \Dfref{def:leo-source-eim}.
}
\mypareq{eq:th-leo-source-fn-10-05}{%
    $\Ran{\var{Source-Rel}} = \Cod{\var{Source}}$
    \cuz{} \Eref{eq:th-leo-source-fn-05},
        \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-04}.
}
\mypareq{eq:th-leo-source-fn-10-09-10}{%
    If \var{Source-Rel} is a function,
    then it is
    equivalent to the function \myfnname{Source}
    \cuz{}
        \Eref{eq:th-leo-source-fn-05},
        \Eref{eq:th-leo-source-fn-07},
        \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-03},
        \Eref{eq:th-leo-source-fn-10-05}.
}
\mypareq{eq:th-leo-source-fn-10-09-20}{%
    If \var{Source-Rel} is a function,
    then it is
    a total function
    \linebreak \cuz{} \Eref{eq:th-leo-source-fn-10-03},
        \Eref{eq:th-leo-source-fn-10-09-10}.
}
\mypareq{eq:th-leo-source-fn-10-09-30}{%
    If \var{Source-Rel} is a function,
    then it is a surjection
    \linebreak \cuz{} \Eref{eq:th-leo-source-fn-10-05},
        \Eref{eq:th-leo-source-fn-10-09-10}.
}
It remains to show that \var{Source-Rel} is a function.
To this we assume, for a derivation, that two tuples of
\var{Source-Rel} have the same argument.
\begin{gather}
\label{eq:th-leo-source-fn-10-09-32}
\tuple{ \mylim{\ell}, \Veim{src1} } \in \var{Source-Rel}
\\ \label{eq:th-leo-source-fn-10-09-34}
\land \; \tuple{ \ell, \Veim{src2} } \in \var{Source-Rel}
\\ \nonumber
\text{\cuz{} AF derivation,
    new arbitrary \mylim{\ell}, \Veim{src1}, \Veim{src2}.}
\end{gather}
\mypareq{eq:th-leo-source-fn-10-09-36}{%
    $\ell \in \myfn{lims}{\var{src1}}$
    \cuz{} \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-09-32}.
}
\mypareq{eq:th-leo-source-fn-10-09-38}{%
    $\ell \in \myfn{lims}{\var{src2}}$
    \cuz{} \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-09-34}.
}
\mypareq{eq:th-leo-source-fn-10-09-40}{%
    \myfn{Is-Leo-Eligible}{\var{src1}}
    \cuz{} \longDfref{LIM validity}{def:lim-validity},
        \Eref{eq:th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-10}{%
    $\Transition{\ell} = \Postdot{\var{src1}}$
    \cuz{} \Dfref{def:lim-validity},
        \Eref{eq:th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-12}{%
    $\Current{\ell} = \Current{\var{src1}}$
    \cuz{} \Dfref{def:lim-validity},
        \Eref{eq:th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-22}{%
    $\Transition{\ell} = \Postdot{\var{src2}}$
    \cuz{} \Dfref{def:lim-validity},
        \Eref{eq:th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-24}{%
    $\Current{\ell} = \Current{\var{src2}}$
    \cuz{} \Dfref{def:lim-validity},
        \Eref{eq:th-leo-source-fn-10-09-36}.
}
\mypareq{eq:th-leo-source-fn-28}{%
    $\Postdot{\var{src1}} = \Postdot{\var{src2}}$
    \cuz{} \Eref{eq:th-leo-source-fn-10},
        \Eref{eq:th-leo-source-fn-22}.
}
\mypareq{eq:th-leo-source-fn-30}{%
    $\Current{\var{src1}} = \Current{\var{src2}}$
    \cuz{} \Eref{eq:th-leo-source-fn-12},
        \Eref{eq:th-leo-source-fn-24}.
}
\mypareq{eq:th-leo-source-fn-34}{%
    $\var{src1} = \var{src2}$
    \cuz{} \longThref{Leo eligible postdot uniqueness}{th:leo-eligible-postdot-uniqueness},
        \Eref{eq:th-leo-source-fn-10-09-40},
        \Eref{eq:th-leo-source-fn-28},
        \Eref{eq:th-leo-source-fn-30}.
}
\begin{equation}
\begin{gathered}
\label{eq:th-leo-source-fn-36}
    \forall \; \mylim{\ell} : \forall \; \Veim{src1} : \forall \; \Veim{src2} :
    \\ \tuple{ \mylim{\ell}, \Veim{src1} } \in \var{Source-Rel}
        \land \; \tuple{ \ell, \Veim{src2} } \in \var{Source-Rel}
    \\ \implies \var{src1} = \var{src2}
    \\ \cuzbox{derivation from
        \Eref{eq:th-leo-source-fn-10-09-32}--\Eref{eq:th-leo-source-fn-10-09-32}
        to \Eref{eq:th-leo-source-fn-34};
        universal generalization of \mylim{\ell}, \Veim{src1} and \Veim{src2}.
    }
\end{gathered}
\end{equation}
By the definition of a functional relation,
\mypareq{eq:th-leo-source-fn-38}{%
    \var{Source-Rel} is a function
    \cuz \Eref{eq:th-leo-source-fn-36}.
}
\mypareq{eq:th-leo-source-fn-40}{%
    The total function \myfnname{Source} exists, and is a surjection
    \cuz{} \Eref{eq:th-leo-source-fn-10-01},
        \Eref{eq:th-leo-source-fn-10-09-10},
        \Eref{eq:th-leo-source-fn-10-09-20},
        \Eref{eq:th-leo-source-fn-10-09-30},
        \Eref{eq:th-leo-source-fn-38}.
        \myqed
}
\end{proof}

\begin{theorem}[LIM direction]
\label{th-lim-direction}
The current location of a \type{LIM} is
the current location of its source.
That is,
\[
\forall \; \var{src} \in \type{EIM} :
\forall \; \ell \in \myfn{lims}{\var{src}} :
    \Current{\ell} = \Current{\var{src}}.
\]
\end{theorem}

\begin{proof}
The theorem follows immediately from the
definition of \type{LIM} validity \Dfref{def:lim-validity}.
\myqed
\end{proof}

\chapter{Leo source sequences}
\label{chap:lss}

\begin{definition}[Leo source sequence {[}LSS{]}]
\label{def:lss}
A \dfn{Leo source \type{EIM} sequence},
or \dfn{Leo source sequence} is an \type{LES},
every element of which is a Leo source \type{EIM}.
That is,
\begin{equation}
\label{eq:def-leo-source-sequence-10}
\type{LSS} = \set{
    \begin{gathered}
        \var{les} \in \type{LES} :
        \\ \forall \; \Vnat{i} \in \Dom{\var{les}} : \myfn{Is-source}{\VVelement{les}{i}}
    \end{gathered}
}.  \quad \dispEnd
\end{equation}
\end{definition}

\begin{theorem}[LSS--LES identity]
\label{th:lss-les-identity}
Every \type{LSS} is identical to exactly one \type{LES}.
That is,
\[
\begin{gathered}
    \forall \; \Vlss{lss} : \existsUniq \; \Vles{les} : \var{lss} = \var{les}.
\end{gathered}
\]
\end{theorem}

\begin{proof}
By the definition of \type{LSS} \Dfref{def:lss},
the type \type{LSS} is a subset of \type{LES}.
This means that every member \type{LSS} is a member of
the set \type{LES}, in other words, identical to
a member of \type{LES}, so that
\mypareq{eq:th-lss-les-identity-10}{%
    $\forall \; \Vlss{lss} : \exists \; \Vles{les} : \var{lss} = \var{les}$.
}

And obviously,
\mypareq{eq:th-lss-les-identity-20}{%
    $\forall \; \Vlss{lss} :
    \forall \; \Vles{les1} :
    \forall \; \Vles{les2} :$
    \linebreak $\var{lss} = \var{les1} \land
    \var{lss} = \var{les2} \implies
    \var{les1} = \var{les2}$.
}
The theorem follows from
\Eref{eq:th-lss-les-identity-10} and
\Eref{eq:th-lss-les-identity-20}.
\myqed
\end{proof}

\begin{definition}[Underlying LES]
\label{def:underlying-les}
If $\Vles{les} = \Vlss{lss}$ we say that
\var{les} is the \dfn{underlying LES} of \Vlss{lss}.
\dispEnd
\end{definition}

\begin{theorem}[Source \type{EIM} containment]
\label{th:source-eim-containment}
Every source \type{EIM} is the base element of at least one \type{LSS}.
That is,
\[
\forall \; \Veim{eim} : \myfn{Is-source}{\var{eim}} \implies
    \exists \; \Vlss{lss} : \var{eim} = \Velement{lss}{0}.
\]
\end{theorem}

\begin{proof}
Our proof strategy is first,
given an arbitrary source \type{EIM},
call it \Veim{eim},
to construct an example
of an \type{LSS} which has \var{eim}
as its base element.
The theorem will follow immediately by existential generalization.

\mypareq{eq:th-source-eim-containment-10-10}{%
    $\myfn{Is-source}{\var{eim}}$
    \cuz{} \longDfref{Leo source \type{EIM}}{def:leo-source-eim},
    AF derivation.
}
\mypareq{eq:th-source-eim-containment-10-20}{%
    $\Valid{\var{eim}}$
    \cuz{} \Dfref{def:leo-source-eim},
        \longDfref{LIM validity}{def:lim-validity},
        \Eref{eq:th-source-eim-containment-10-10}.
}
\mypareq{eq:th-source-eim-containment-10-30}{%
    $\myfn{Is-Leo-Eligible}{\var{eim}}$
    \cuz{} \Dfref{def:leo-source-eim},
        \Dfref{def:lim-validity},
        \Eref{eq:th-source-eim-containment-10-10}.
}
Our example of an \type{LSS} containing \var{eim} as its base element is
\mypareq{eq:th-source-eim-containment-12}{%
   $\var{example} = \tuple{\var{eim}}$ \cuz{} AF construction.
}
\mypareq{eq:th-source-eim-containment-13}{%
    $\Velement{example}{0} = \var{eim}$ \cuz{}
        \Eref{eq:th-source-eim-containment-12}.
}
\mypareq{eq:th-source-eim-containment-14}{%
   $\Dom{\var{example}} = \set{0}$ \cuz{} \Eref{eq:th-source-eim-containment-12}.
}
\begin{equation}
\label{eq:th-source-eim-containment-16}
\begin{gathered}
       \forall \; \Vnat{i} < \Vlastix{example} :
   \\ \myfn{Next-Eligible}{\Velement{example}{\var{i}+1}, \VVelement{example}{i}}.
   \\ \because \text{vacuously}, \Eref{eq:th-source-eim-containment-13}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-source-eim-containment-17}
   \begin{gathered}
        \forall \; \Vnat{i} \in \Dom{\var{example}} :
       \\ \Valid{\VVelement{example}{i}} \land \ \myfn{Is-Leo-Eligible}{\VVelement{example}{i}}
       \\ \land \; \left(
           \begin{gathered}
           \var{i} < \Vlastix{example}
           \\ \implies \myfn{Next-Eligible}{\Velement{example}{\var{i}+1}, \VVelement{example}{i}}
           \end{gathered}
       \right)
       \\ \because
            \Eref{eq:th-source-eim-containment-10-20},
            \Eref{eq:th-source-eim-containment-10-30},
            \Eref{eq:th-source-eim-containment-13},
            \Eref{eq:th-source-eim-containment-14},
            \Eref{eq:th-source-eim-containment-16}.
   \end{gathered}
\end{equation}
\mypareq{eq:th-source-eim-containment-18}{%
   $\var{example} \in \type{LES}$ \cuz{}
        \longDfref{LES}{def:next-eligible-sequence},
       \Eref{eq:th-source-eim-containment-17}.
}
\mypareq{eq:th-source-eim-containment-20}{%
   $\var{example} \in \type{LSS}$ \cuz{}
       \longDfref{LSS}{def:lss},
       \Eref{eq:th-source-eim-containment-10-10},
        \Eref{eq:th-source-eim-containment-13},
        \Eref{eq:th-source-eim-containment-14},
       \Eref{eq:th-source-eim-containment-18}.
}
\mypareq{eq:th-source-eim-containment-22}{%
   $\var{example} \in \type{LSS} \land \Velement{example}{0} = \Veim{eim}$
  \cuz{} \Eref{eq:th-source-eim-containment-13},
    \Eref{eq:th-source-eim-containment-20}.
}
\mypareq{eq:th-source-eim-containment-24}{%
   $\exists \; \Vlss{lss} : \Velement{lss}{0} = \Veim{eim}$
  \cuz{} \Eref{eq:th-source-eim-containment-22},
     existential generalization of \Vlss{example}.
}
\mypareq{eq:th-source-eim-containment-30}{%
    $\myfn{Is-source}{\var{eim}} \implies
   \exists \; \Vlss{lss} : \Velement{lss}{0} = \Veim{eim}$
    \cuz{}
   derivation \Eref{eq:th-source-eim-containment-10-10}--%
    \Eref{eq:th-source-eim-containment-24}.
}
The theorem is the existential generalization of \Veim{eim}
in \Eref{eq:th-source-eim-containment-30}.
\myqed
\end{proof}

\begin{definition}[Maximal LSS]
\label{def:maximal-lss}
A \dfn{maximal LSS} is an \type{LSS} which is not the proper prefix
of another \type{LSS}.
We use the notation \Maximal{\Vlss{lss}}
to say that \var{lss} is maximal.
\dispEnd
\end{definition}

\begin{theorem}[LSS uniqueness]
\label{th:lss-uniqueness}
If two \type{LSS}'s of the same length have the same base element,
then they are identical.
\end{theorem}

\begin{proof}
The proof is direct,
assuming the hypothesis of the theorem to
show its conclusion.
Let \Vlss{lss1} and \Vlss{lss2} be two \type{LSS}'s,
and let their underlying \type{LES}'s be \Vles{les1}
and \Vles{les2} so that
\begin{gather}
\label{eq:th-lss-uniqueness-10}
    \var{les1} \in \type{LES} \land \var{les2} \in \type{LES}
\\ \label{eq:th-lss-uniqueness-12}
    \land \; \var{les1} = \var{lss1}
\\ \label{eq:th-lss-uniqueness-14}
    \land \; \var{les2} = \var{lss2}
\\ \label{eq:th-lss-uniqueness-16}
    \land \; \Vsize{lss1} = \Vsize{lss2}
\\ \label{eq:th-lss-uniqueness-18}
    \land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\\ \nonumber \because \text{hypothesis of theorem.}
\end{gather}
\mypareq{eq:th-lss-uniqueness-26}{%
    $\Vsize{les1} = \Vsize{les2}$ \cuz{}
        \Eref{eq:th-lss-uniqueness-12},
        \Eref{eq:th-lss-uniqueness-14},
        \Eref{eq:th-lss-uniqueness-16}.
}
\mypareq{eq:th-lss-uniqueness-28}{%
    $\Velement{les1}{0} = \Velement{les2}{0}$ \cuz{}
        \Eref{eq:th-lss-uniqueness-12},
        \Eref{eq:th-lss-uniqueness-14},
        \Eref{eq:th-lss-uniqueness-18}.
}
\mypareq{eq:th-lss-uniqueness-30}{%
     $\var{les1} = \var{les2}$ \cuz{} \longThref{LES uniqueness}{th:les-uniqueness},
        \Eref{eq:th-lss-uniqueness-26}, \Eref{eq:th-lss-uniqueness-28}.
}
\mypareq{eq:th-lss-uniqueness-32}{%
     $\var{lss1} = \var{lss2}$ \cuz{}
        \Eref{eq:th-lss-uniqueness-10}, \Eref{eq:th-lss-uniqueness-30}.
}
Equation \Eref{eq:th-lss-uniqueness-32} is the conclusion
of the the theorem.
\myqed
\end{proof}

\begin{theorem}[LSS subsequence]
\label{th:lss-subsequence}
Every subsequence of an \type{LSS} is an \type{LSS}.
That is, if
\begin{gather}
\label{eq:th-lss-subsequence-02}
\var{seq} \in \type{LSS}
\\ \label{eq:th-lss-subsequence-04}
\land \; \Vnat{a} \le \Vnat{b} \le \Vlastix{seq}
\\ \label{eq:th-lss-subsequence-06}
\land \; \var{subseq} = \tuple{\VVelement{seq}{a} \ldots \VVelement{seq}{b}}
\end{gather}
then
\begin{gather}
\label{eq:th-lss-subsequence-08}
\var{subseq} \in \type{LSS}.
\end{gather}
\end{theorem}

\begin{proof}
\mypareq{eq:th-lss-subsequence-10}{%
   $\var{seq} \in \type{LES}$
        \cuz{} \longDfref{LSS}{def:lss},
        \Eref{eq:th-lss-subsequence-02}.%
}
\mypareq{eq:th-lss-subsequence-12}{%
   $\var{subseq} \in \type{LES}$
        \cuz{} \longThref{LES subsequence}{th:les-subsequence},
        \Eref{eq:th-lss-subsequence-04},
        \Eref{eq:th-lss-subsequence-06},
        \Eref{eq:th-lss-subsequence-10}.%
}
\begin{equation}
\label{eq:th-lss-subsequence-20}
\begin{gathered}
    \forall \; \Vnat{i} \in \Dom{\var{seq}} :
        \size{\myfn{lims}{\VVelement{seq}{i}}} > 0
    \\ \because \longDfref{LSS}{def:lss},
        \Eref{eq:th-lss-subsequence-02}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-subsequence-22}
\begin{gathered}
    \forall \; \Vnat{i} \in \Dom{\var{subseq}} :
        \size{\myfn{lims}{\VVelement{subseq}{i}}} > 0
    \\ \because \Eref{eq:th-lss-subsequence-06},
        \Eref{eq:th-lss-subsequence-20}.
\end{gathered}
\end{equation}
\mypareq{eq:th-lss-subsequence-24}{%
   $\var{subseq} \in \type{LSS}$
        \cuz{} \Dfref{def:lss},
        \Eref{eq:th-lss-subsequence-12},
        \Eref{eq:th-lss-subsequence-22}. \quad \myqed
}
\end{proof}

\begin{theorem}[LSS extension]
\label{th:lss-extension}
Let \var{lss} be an \type{LSS},
and let \var{next} be an \type{EIM}.
Then
\mypareq{eq:th-lss-extension-02}{%
   $\tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \type{LSS}$
}
iff
\mypareq{eq:th-lss-extension-07}{%
        $\myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
}
\end{theorem}

\begin{proof}
The forward direction follows immediately from the definition
of an \type{LSS} \Dfref{def:lss},
and definition of an \type{LES} \Dfref{def:next-eligible-sequence}.
We prove the reverse direction, by assuming
the antecedant to show the consequent.
\mypareq{eq:th-lss-extension-10}{%
        $\myfn{Is-source}{\var{next}}$
        \cuz{} antecedant \Eref{eq:th-lss-extension-07} of
        reverse direction.
}
\mypareq{eq:th-lss-extension-12}{%
        $\myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
        \cuz{} antecedant \Eref{eq:th-lss-extension-07} of
        reverse direction.
}
\mypareq{eq:th-lss-extension-14}{%
    $\var{lss} \in \type{LSS}$ \cuz{} AF theorem.
}
\mypareq{eq:th-lss-extension-16}{%
    $\forall \; \Vnat{i} \in \Dom{\var{lss}} : \myfn{Is-source}{\VVelement{lss}{i}}$
        \cuz{} \longDfref{LSS}{def:lss},
        \Eref{eq:th-lss-extension-14}.
}
\mypareq{eq:th-lss-extension-18}{%
   $\var{extended} = \tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}}$,
       new \var{extended} for convenience.
}
\mypareq{eq:th-lss-extension-20}{%
   $\var{extended} \in \type{LES}$
   \cuz{} \longThref{LES extension}{th:les-extension},
        \Eref{eq:th-lss-extension-12},
        \Eref{eq:th-lss-extension-18}.
}
\mypareq{eq:th-lss-extension-22}{%
    $\forall \; \Vnat{i} \in \Dom{\var{extended}} : \myfn{Is-source}{\VVelement{extended}{i}}$
    \cuz{} \Eref{eq:th-lss-extension-10},
    \Eref{eq:th-lss-extension-16},
    \Eref{eq:th-lss-extension-18}.
}
\mypareq{eq:th-lss-extension-24}{%
   $\var{extended} \in \type{LSS}$
    \cuz{} \longDfref{LSS}{def:lss},
        \Eref{eq:th-lss-extension-20},
        \Eref{eq:th-lss-extension-22}.
}
The equation \Eref{eq:th-lss-extension-24}
is the consequent \Eref{eq:th-lss-extension-02}.
With the consequent we have the reverse direction,
and with both directions we have the theorem.
\myqed
\end{proof}

\begin{theorem}[LSS generalized extension]
\label{th:lss-generalized-extension}
Let \var{lss} be an \type{LSS}.
Then
\mypareq{eq:th-lss-generalized-extension-02}{%
   $\exists \; \Veim{next} : \tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \type{LSS}$
}
iff
\mypareq{eq:th-lss-generalized-extension-07}{%
        $\exists \; \Veim{next} : \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
}
\end{theorem}

\begin{proof}
We first prove the forward direction, assuming the antecedant
to show the consequent.
\begin{equation}
\label{eq:th-lss-generalized-extension-10}
\begin{gathered}
   \tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \Veim{next}} \in \type{LSS}
   \\ \myparbox{\cuz{}
       antecedant \Eref{eq:th-lss-generalized-extension-02} of forward
       direction, existential instantiation of new free \Veim{next}.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-generalized-extension-12}
\begin{gathered}
    \myfn{Is-source}{\var{next}}
    \land
    \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
    \\ \because \longThref{LSS extension}{th:lss-generalized-extension},
    \Eref{eq:th-lss-generalized-extension-10}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lss-generalized-extension-14}
\begin{gathered}
    \exists \; \Veim{next} :
    \\ \myfn{Is-source}{\var{next}}
        \land \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
    \\ \because \Eref{eq:th-lss-generalized-extension-12},
    \text{existential generalization of \Veim{next}.}
\end{gathered}
\end{equation}

The equation \Eref{eq:th-lss-generalized-extension-14}
is the consequent \Eref{eq:th-lss-generalized-extension-07},
and with it we have the forward direction.
The reverse direction follows from the same line of reasoning:
first, existential instantiation;
second, invocation of the \type{LSS} extension theorem \Thref{th:lss-generalized-extension};
third, existential generalization.
With both directions, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS termination]
\label{th:maximal-lss-termination}
Let \var{lss} be an \type{LSS}.
Then
\mypareq{eq:th-maximal-lss-termination-02}{%
   \Maximal{\var{lss}}
}
iff
\mypareq{eq:th-maximal-lss-termination-07}{%
        $\nexists \; \Veim{next} : \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$.
}
\end{theorem}

\begin{proof}
We prove the mutual implication directly.
\begin{equation}
\label{eq:th-maximal-lss-termination-10}
\begin{gathered}
   \Maximal{\var{lss}} \iff
   \\
   \nexists \; \Veim{next} : \tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \type{LSS}
       \\ \because \longDfref{Maximal LSS}{def:maximal-lss}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-termination-12}
\begin{gathered}
   \left( \nexists \; \Veim{next} : \tuple{\Velement{\var{lss}}{0}\ldots
       \Velement{lss}{\Vlastix{lss}}, \var{next}} \in \type{LSS} \right)
        \\ \iff \nexists \; \Veim{next} :
        \\ \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
       \\ \because
            \longThref{LSS generalized extension}{th:lss-generalized-extension}.
\end{gathered}
\end{equation}
The theorem follows immediately from
\Eref{eq:th-maximal-lss-termination-10}
and \Eref{eq:th-maximal-lss-termination-12}.
\myqed
\end{proof}

\begin{theorem}[LSS prefix]
\label{th:lss-prefix}
Consider two \type{LSS}'s.
They have different lengths but the same base element
iff the shorter one is a proper prefix of the longer one.
That is,
\begin{gather}
\label{eq:th-lss-prefix-02}
   \forall \; \var{lss1} \in \type{LSS} :
       \forall \; \var{lss2} \in \type{LSS} :
\\ \label{eq:th-lss-prefix-04}
   \Vsize{lss1} < \Vsize{lss2}
\\ \label{eq:th-lss-prefix-06}
   \land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\\ \nonumber \iff
\\ \label{eq:th-lss-prefix-08}
   \PrPfx{\var{lss1},\var{lss2}}.
\end{gather}
\end{theorem}

\begin{proof}
This proof is similar to the one for ``\type{LES} prefix''
\Thref{th:les-prefix}.
For the reverse direction,
\Eref{eq:th-lss-prefix-04} and
\Eref{eq:th-lss-prefix-06}
follow immediately from
\Eref{eq:th-lss-prefix-08},
via the definition of proper prefix \Dfref{def:prefix}.

It remains to show the forward direction.
Let
\mypareq{eq:th-lss-prefix-20}{%
    $\Vlss{subLss2} = \Velement{lss2}{0\ldots \Vlastix{lss1}}$
    \linebreak \cuz{} \longThref{LSS subsequence}{th:lss-subsequence},
       antecedant of forward direction
           \Eref{eq:th-lss-prefix-02}--\Eref{eq:th-lss-prefix-06},
           new \Vlss{subLss2} for convenience.
}
\mypareq{eq:th-lss-prefix-21}{%
    \PrPfx{\var{subLss2},\var{lss2}} \cuz{}
        \longDfref{Proper prefix}{def:prefix},
        \Eref{eq:th-lss-prefix-04}, \Eref{eq:th-lss-prefix-20}.
}
\begin{equation}
\label{eq:th-lss-prefix-22}
\Vsize{subLss2} = \Vsize{lss1} \because \Eref{eq:th-lss-prefix-20}
\end{equation}
\mypareq{eq:th-lss-prefix-24}{%
    $\Velement{subLss2}{0} = \Velement{lss2}{0}$ \cuz{} \Eref{eq:th-lss-prefix-20}.
}
\mypareq{eq:th-lss-prefix-26}{%
    $\Velement{subLss2}{0} = \Velement{lss1}{0}$
    \linebreak \cuz{}
       antecedant of forward direction \Eref{eq:th-lss-prefix-06},
       \Eref{eq:th-lss-prefix-24}.
}
\mypareq{eq:th-lss-prefix-27}{%
    $\var{subLss2} = \var{lss1}$
    \linebreak \cuz{} \longThref{LSS uniqueness}{th:lss-prefix},
        \Eref{eq:th-lss-prefix-22},
        \Eref{eq:th-lss-prefix-26}.
}
\mypareq{eq:th-lss-prefix-28}{%
    $\PrPfx{\Vlss{lss1}, \Vlss{lss2}}$
    \cuz{} \Eref{eq:th-lss-prefix-21},
    \Eref{eq:th-lss-prefix-27}.
}
Equation \Eref{eq:th-lss-prefix-28}
is the consequent of the forward direction,
which shows the forward direction.
With both directions, we have the proof.
\myqed
\end{proof}

\begin{theorem}[LSS trichotomy]
\label{th:lss-trichotomy}
If two \type{LSS}'s of the same length have the same base element,
then there are three mutually exclusive possibilites:
\begin{itemize}
\item They are identical.
\item The first \type{LSS} is a proper prefix of the second.
\item The second \type{LSS} is a proper prefix of the first.
\end{itemize}
That is, if
\begin{gather}
\label{eq:th-lss-trichotomy-02}
\var{lss1} \in \type{LSS}
\\ \label{eq:th-lss-trichotomy-04}
\; \land \; \var{lss2} \in \type{LSS}
\\ \label{eq:th-lss-trichotomy-06}
\land \; \Velement{lss1}{0} = \Velement{lss2}{0}
\end{gather}
then
\mypareq{eq:th-lss-trichotomy-08}{%
   \size{\var{fn} \in \set{\var{Eq},\var{Lt},\var{Gt}} :
       \myfn{fn}{\var{lss1},\var{lss2}}} = 1
}
where \var{Eq}, \var{Lt} and \var{Gt} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-lss-trichotomy-09a}
    \myfn{Eq}{\Vlss{x},\Vlss{y}} \iff \var{x} = \var{y},
    \\ \label{eq:th-lss-trichotomy-09b}
    \myfn{Lt}{\Vlss{x},\Vlss{y}} \iff \PrPfx{\var{lss1},\var{lss2}}\text{, and}
    \\ \label{eq:th-lss-trichotomy-09c}
    \myfn{Gt}{\Vlss{x},\Vlss{y}} \iff \PrPfx{\var{lss2},\var{lss1}}.
\end{gather}
\end{theorem}

\begin{proof}
This proof is similar to that for
the ``\type{LES} trichotomy'' theorem \Thref{th:les-trichotomy}.
The proof strategy is direct, assuming
the hypothesis of the theorem to show
its conclusion.

We first note that,
by the Trichotomy Law for the ordering of the natural numbers,
\mypareq{eq:th-lss-trichotomy-10}{%
   \size{\var{fn} \in \set{\var{EqSz},\var{LtSz},\var{GtSz}} :
       \myfn{fn}{\var{lss1},\var{lss2}}} = 1
}
where \var{EqSz}, \var{LtSz} and \var{GtSz} are boolean functions,
defined within the context of this theorem and its proof,
such that
\begin{gather}
\label{eq:th-lss-trichotomy-12}
   \myfn{EqSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{x} = \Vsize{y},
\\ \label{eq:th-lss-trichotomy-14}
    \myfn{LtSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{lss1} < \Vsize{lss2}\text{, and}
\\ \label{eq:th-lss-trichotomy-16}
    \myfn{GtSz}{\Vlss{x},\Vlss{y}} \iff \Vsize{lss1} > \Vsize{lss2}.
\end{gather}

We now show three derivations, one for each of the functions
\var{EqSz}, \var{LtSz} and \var{GtSz}.
\mypareq{eq:th-lss-trichotomy-17-18}{
   \myfn{EqSz}{\Vlss{x},\Vlss{y}} \cuz{} AF derivation;
   new arbitrary \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-20}{
   $\Vsize{x} = \Vsize{y}$ \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-18}
   using \Eref{eq:th-lss-trichotomy-12}.
}
\mypareq{eq:th-lss-trichotomy-17-22}{
   $\Vlss{x} = \Vlss{y}$ \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-20}
   using \longThref{LSS uniqueness}{th:lss-uniqueness},
    \Eref{eq:th-lss-trichotomy-06}.
}
\mypareq{eq:th-lss-trichotomy-17-24}{
   \myfn{Eq}{\Vlss{x},\Vlss{y}} \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-22}
   using \Eref{eq:th-lss-trichotomy-09a}.
}
\mypareq{eq:th-lss-trichotomy-17-26}{
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{EqSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Eq}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} equivalence \Eref{eq:th-lss-trichotomy-17-18}--\Eref{eq:th-lss-trichotomy-17-24},
   \linebreak universal generalization of \Vlss{x}, \Vlss{y}.
}

\mypareq{eq:th-lss-trichotomy-17-28}{
   \myfn{LtSz}{\Vlss{x},\Vlss{y}} \cuz{} AF derivation;
   new arbitrary \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-30}{
   $\Vsize{x} < \Vsize{y}$ \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-28}
   using \Eref{eq:th-lss-trichotomy-14}.
}
\mypareq{eq:th-lss-trichotomy-17-32}{
   $\Vlss{x} = \Vlss{y}$ \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-30}
   using \longThref{LSS prefix}{th:lss-prefix},
    \Eref{eq:th-lss-trichotomy-06}.
}
\mypareq{eq:th-lss-trichotomy-17-34}{
   \myfn{Lt}{\Vlss{x},\Vlss{y}} \cuz{}
   equivalence with \Eref{eq:th-lss-trichotomy-17-32}
   using \Eref{eq:th-lss-trichotomy-09b}.
}
\mypareq{eq:th-lss-trichotomy-17-36}{
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{LtSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Lt}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} equivalence \Eref{eq:th-lss-trichotomy-17-28}--\Eref{eq:th-lss-trichotomy-17-34},
   \linebreak universal generalization of \Vlss{x}, \Vlss{y}.
}
\mypareq{eq:th-lss-trichotomy-17-40}{
   $\forall \; \Vlss{x} : \forall \; \Vlss{y} :$
   $\myfn{GtSz}{\Vlss{x},\Vlss{y}}
   \iff \myfn{Gt}{\Vlss{x},\Vlss{y}}$
   \linebreak \cuz{} symmetry with
   \Eref{eq:th-lss-trichotomy-17-28}--\Eref{eq:th-lss-trichotomy-17-36}.
}

The theorem follows from
\Eref{eq:th-lss-trichotomy-10}--\Eref{eq:th-lss-trichotomy-14},
\Eref{eq:th-lss-trichotomy-17-26},
\Eref{eq:th-lss-trichotomy-17-36} and \Eref{eq:th-lss-trichotomy-17-40}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS non-duplication]
\label{th:maximal-lss-non-duplication}
An \type{EIM} is the base of at most one maximal \type{LSS}.
\end{theorem}

\begin{proof}
Call the base \type{EIM}, \Veim{base}.
If there is not a maximal \type{LSS} with base \Veim{base}, we
have the theorem trivially.
(This will be the case if \var{base} was not a source \type{EIM}.)

For the rest of this theorem, then,
we may assume the existence of these
two arbitrary, but not necessarily distinct, maximal \type{LSS}'s:
\mypareq{eq:th-maximal-lss-non-duplication-10}{%
    $\Velement{max1}{0} = \Veim{base}$ \cuz{} AF proof,
       arbitrary \var{max1}.
}
\mypareq{eq:th-maximal-lss-non-duplication-12}{%
    $\Velement{max2}{0} = \Veim{base}$ \cuz{} AF proof,
       arbitrary \var{max2}.
}
\mypareq{eq:th-maximal-lss-non-duplication-14}{%
    $\Maximal{\var{max1}} \land \Maximal{\var{max2}}$
    \cuz{} AF proof.
}
We now proceed by reductio.
\mypareq{eq:th-maximal-lss-non-duplication-18}{%
    $\var{max1} \neq \var{max2}$ \cuz{} AF reductio.
}
\mypareq{eq:th-maximal-lss-non-duplication-20}{%
    $\PrPfx{\var{max1},\var{max2}}
    \lor \PrPfx{\var{max2},\var{max1}}$
    \cuz{} \longThref{LSS trichotomy}{th:lss-trichotomy},
    \Eref{eq:th-maximal-lss-non-duplication-10},
    \Eref{eq:th-maximal-lss-non-duplication-12}, and
    \Eref{eq:th-maximal-lss-non-duplication-18}.
}
\mypareq{eq:th-maximal-lss-non-duplication-22}{%
    $\neg \Maximal{\var{max1}} \lor \neg \Maximal{\var{max2}}$
    \cuz{} \longDfref{Maximal LSS}{def:maximal-lss}.
    This contradicts \Eref{eq:th-maximal-lss-non-duplication-14}.
}
\mypareq{eq:th-maximal-lss-non-duplication-24}{%
    $\var{max1} = \var{max2}$ \cuz{} reductio
    \Eref{eq:th-maximal-lss-non-duplication-18}--%
    \Eref{eq:th-maximal-lss-non-duplication-22}.
}
Since the choice of \var{max1} and \var{max2} was arbitrary,
\Eref{eq:th-maximal-lss-non-duplication-24} shows the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS existence]
\label{th:maximal-lss-existence}
Every Leo source \type{EIM} is the base of at least one
maximal \type{LSS}.  That is,
\mypareq{eq:th-maximal-lss-existence-02}{
    $\myfn{Is-source}{\var{eim}} \implies
    \exists \; \var{max} : \Velement{max}{0} = \var{eim} \land \Maximal{\var{max}}$.
}
\end{theorem}

\begin{proof}
The proof strategy is direct.
We assume the antecedant of the theorem to show its
consequent.
Consider \var{based}, the set of all \type{LSS}'s
with \var{eim} as their base:
\mypareq{eq:th-maximal-lss-existence-10-10}{%
$\Vlssset{based} = \set{
   \Vlss{lss} : \Velement{lss}{0} = \var{eim}}$
    \linebreak \cuz{}
        \longDfref{LSS}{def:lss},
        antecedant of theorem \Eref{eq:th-maximal-lss-existence-02},
       new \var{based} for convenience.
}
\begin{equation}
\label{eq:th-maximal-lss-existence-10-20}
\forall \; \Vlss{lss} : \var{lss} \in \var{based} \iff \Velement{lss}{0} = \var{eim}
   \because \Eref{eq:th-maximal-lss-existence-10-10}.
\end{equation}
\var{based} is never empty, because
any Leo source \type{EIM} is the base of
at least one \type{LSS}
of length~1:
\mypareq{eq:th-maximal-lss-existence-14}{%
    $\Vsize{based} \ge 1$
       \cuz{} \longThref{Leo source containment}{th:source-eim-containment},
            \Eref{eq:th-maximal-lss-existence-02}.
}
For any length,
\var{based} contains at most one \type{LSS} of that length:
\begin{equation}
\label{eq:th-maximal-lss-existence-16}
\begin{gathered}
\forall \; \Vlss{lss1} \in \var{based} :
\forall \; \Vlss{lss2} \in \var{based} :
\\ \Vsize{lss1} = \Vsize{lss2} \implies
\var{lss1} = \var{lss2}
\\ \because \longThref{LSS uniqueness}{th:lss-uniqueness},
    \Eref{eq:th-maximal-lss-existence-10-20}.
\end{gathered}
\end{equation}
Among the \type{LES}'s with \var{eim} as their base,
there is a maximum length,
call it \Vnat{possible}:
\begin{equation}
\label{eq:th-maximal-lss-existence-18}
\begin{gathered}
\forall \; \Vles{les} : \Velement{les}{0} = \var{eim}
    \implies \Vsize{les} \le \Vnat{possible}
     \\ \text{where } \var{possible} = \size{\Vocab{\Vint{g}}}+1 \times (\Current{\Velement{les}{0}}+1)
    \\ \because \longThref{LES finiteness}{th:les-finiteness}.
\end{gathered}
\end{equation}
Therefore, there is a maximum possible \type{LSS} length for
\type{LSS}'s with \var{eim} as their base.
\mypareq{eq:th-maximal-lss-existence-20}{%
    The length of an \type{LSS} is the same as that of its underlying \type{LES}
    \cuz{} \longThref{LSS--LES identity}{th:lss-les-identity}.
}
\begin{equation}
\label{eq:th-maximal-lss-existence-22}
\begin{gathered}
\forall \; \Vles{lss} \in \var{based} : \Vsize{lss} \le \var{possible}
    \\ \because \Eref{eq:th-maximal-lss-existence-10-20},
    \Eref{eq:th-maximal-lss-existence-18},
        \Eref{eq:th-maximal-lss-existence-20}.
\end{gathered}
\end{equation}
Since there is a maximum \type{LSS} size for the elements of \var{based},
and since, for each \type{LSS} size, there is at most one \type{LSS} in \var{based},
we know that \var{based} is a finite set:
\mypareq{eq:th-maximal-lss-existence-24}{%
    $\Vsize{based} \in \naturals$
    \cuz{}
        \Eref{eq:th-maximal-lss-existence-16},
        \Eref{eq:th-maximal-lss-existence-22}.
}
Since \var{based} is finite and contains at least one element,
there is a maximum size of \type{LSS}'s actually in \var{based},
and at least one \type{LSS} is of that size:
\begin{equation}
\label{eq:th-maximal-lss-existence-26}
\begin{gathered}
\exists \; \Vlss{max} \in \var{based} :
\forall \; \Vlss{lss} \in \var{based} :
    \Vsize{max} \ge \Vsize{lss}
\\ \because \Eref{eq:th-maximal-lss-existence-14},
    \Eref{eq:th-maximal-lss-existence-24}.%
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-existence-28}
\begin{gathered}
    \Vlss{max} \in \var{based} \land
\forall \; \Vlss{lss} \in \var{based} :
    \Vsize{\var{max}} \ge \Vsize{lss}
\\ \because \Eref{eq:th-maximal-lss-existence-26},
   \text{existential instantiation of \var{max}}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-existence-29-30}{%
    $\Vlss{max} \in \var{based}$
    \cuz{} \Eref{eq:th-maximal-lss-existence-28}.
}
\mypareq{eq:th-maximal-lss-existence-29-40}{%
    $\forall \; \Vlss{lss} \in \var{based} : \Vsize{\var{max}} \ge \Vsize{lss}$
    \cuz{} \Eref{eq:th-maximal-lss-existence-28}.
}
\mypareq{eq:th-maximal-lss-existence-29-50}{%
    $\Velement{max}{0} = \var{eim}$
        \cuz{} \Eref{eq:th-maximal-lss-existence-10-20},
            \Eref{eq:th-maximal-lss-existence-29-30}.
}
We now consider, for a reductio,
the possibility that \Vlss{max} is the proper
prefix of another \type{LSS}.
\mypareq{eq:th-maximal-lss-existence-30}{%
   $\exists \; \Vlss{maxier} : \PrPfx{\var{max}, \var{maxier}}$
   \cuz{} AF reductio.%
}
\mypareq{eq:th-maximal-lss-existence-32}{%
    $\var{maxier} \in \type{LSS} \land \PrPfx{\var{max}, \var{maxier}}$
    \cuz{} \Eref{eq:th-maximal-lss-existence-30},
    existental instantiation.%
}
\mypareq{eq:th-maximal-lss-existence-33}{%
    $\PrPfx{\var{max}, \var{maxier}}$
    \cuz{} \Eref{eq:th-maximal-lss-existence-32}.
}
\mypareq{eq:th-maximal-lss-existence-34}{%
    $\Vsize{maxier} > \Vsize{max}$
    \cuz{} \longDfref{Proper prefix}{def:prefix},
    \Eref{eq:th-maximal-lss-existence-30}.%
}
\mypareq{eq:th-maximal-lss-existence-36}{%
    $\Velement{maxier}{0} = \Velement{max}{0}$
    \cuz{} \longDfref{Proper prefix}{def:prefix},
    \Eref{eq:th-maximal-lss-existence-30}.%
}
\mypareq{eq:th-maximal-lss-existence-40}{%
    $\var{maxier} \in \Vlssset{based}$
        \cuz{} \Eref{eq:th-maximal-lss-existence-10-20},
        \Eref{eq:th-maximal-lss-existence-29-50},
        \Eref{eq:th-maximal-lss-existence-36}.
}
\mypareq{eq:th-maximal-lss-existence-42}{%
    $\var{maxier} \notin \Vlssset{based}$
        \cuz{} \Eref{eq:th-maximal-lss-existence-29-40},
        \Eref{eq:th-maximal-lss-existence-34}.%
}
The equation \Eref{eq:th-maximal-lss-existence-42}
directly contradicts \Eref{eq:th-maximal-lss-existence-40}
and gives us the reductio:
\mypareq{eq:th-maximal-lss-existence-44}{%
   $\nexists \; \Vlss{maxier} : \PrPfx{\var{max}, \var{maxier}}$
   \linebreak \cuz{} reductio
    \Eref{eq:th-maximal-lss-existence-30}--%
    \Eref{eq:th-maximal-lss-existence-42}.%
}
\mypareq{eq:th-maximal-lss-existence-46}{%
   \Maximal{\Vlss{max}} \cuz{}
    \longDfref{Maximal LSS}{def:maximal-lss},
    \Eref{eq:th-maximal-lss-existence-44}.%
}
\begin{equation}
\label{eq:th-maximal-lss-existence-48}
\begin{gathered}
    \exists \; \var{max} : \Velement{max}{0} = \var{eim} \land \Maximal{\var{max}}
\\  \because \Eref{eq:th-maximal-lss-existence-29-50},
    \Eref{eq:th-maximal-lss-existence-46},
    \text{existential generalization}.
\end{gathered}
\end{equation}
Equation \Eref{eq:th-maximal-lss-existence-48}
is the consequent of
\Eref{eq:th-maximal-lss-existence-02},
which is what we needed for the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS uniqueness]
\label{th:maximal-lss-uniqueness}
For every Leo source \type{EIM}, there is exactly one maximal \type{LSS}.
\end{theorem}

\begin{proof}
This theorem follows immediately from the \type{LSS} Non-duplication Theorem
\Thref{th:maximal-lss-non-duplication},
and the \type{LSS} Existence Theorem \Thref{th:maximal-lss-existence}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS dichotomy]
\label{th:maximal-lss-dichotomy}
Every \type{LSS} is either
a maximal \type{LSS}, or a proper prefix of a maximal \type{LSS}.
Also that maximal \type{LSS} shares the same base \type{EIM}.
That is,
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-02}
\begin{gathered}
\forall \; \Vlss{lss} : \exists \; \Vlss{max} :
\\ \Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \;
    (\var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}}).
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is direct.
\mypareq{eq:th-maximal-lss-dichotomy-10}{%
    $\var{lss} \in \type{LSS}$
    \cuz{} AF theorem, new free \Vlss{lss}.
}
\mypareq{eq:th-maximal-lss-dichotomy-14}{%
    $\exists \; \var{max} : \Velement{max}{0} = \Velement{lss}{0} \land \Maximal{\var{max}}$.
        \cuz{} \longThref{Maximal LSS existence}{th:maximal-lss-existence},
        \Eref{eq:th-maximal-lss-dichotomy-10}.%
}
\mypareq{eq:th-maximal-lss-dichotomy-16}{%
    $\Velement{max}{0} = \Velement{lss}{0} \land \Maximal{\var{max}}$ \cuz{}
        \Eref{eq:th-maximal-lss-dichotomy-16}, existential instantiation.
}
\mypareq{eq:th-maximal-lss-dichotomy-17-20}{%
    $\Velement{max}{0} = \Velement{lss}{0}$ \cuz{}
        \Eref{eq:th-maximal-lss-dichotomy-16}.
}
\mypareq{eq:th-maximal-lss-dichotomy-17-40}{%
    \Maximal{\var{max}} \cuz{} \Eref{eq:th-maximal-lss-dichotomy-16}.
}
%
We can restate the ``\type{LSS} trichotomy'' in the following form:
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-18}
\begin{gathered}
\forall \; \Vlss{lss1} : \forall \; \Vlss{lss2} :
\\ \Velement{lss1}{0} = \Velement{lss2}{0} \land \var{lss1} \neq \var{lss2}
\\ \implies
        \PrPfx{\var{lss1}, \var{lss2}} \lor \PrPfx{\var{lss2}, \var{lss1}}
\\ \because \Thref{th:lss-trichotomy}.
\end{gathered}
\end{equation}
If we instantiate \Vlss{lss1} as \var{lss}
and \Vlss{lss2} as \var{max}, we have
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-20}
\begin{gathered}
\Velement{lss}{0} = \Velement{max}{0} \land \var{lss} \neq \var{max}
\\ \implies \PrPfx{\var{lss}, \var{max}} \lor \PrPfx{\var{max}, \var{lss}}
\\ \because \Eref{eq:th-maximal-lss-dichotomy-18},
    \text{universal instantiation}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-24}
\begin{gathered}
\var{lss} \neq \var{max} \implies \PrPfx{\var{lss}, \var{max}} \lor \PrPfx{\var{max}, \var{lss}}
\\ \because \Eref{eq:th-maximal-lss-dichotomy-17-20},
    \Eref{eq:th-maximal-lss-dichotomy-20}.
\end{gathered}
\end{equation}
We know that a maximal \type{LSS} cannot be a proper prefix of any other \type{LSS}:
\mypareq{eq:th-maximal-lss-dichotomy-26}{%
    $\neg \PrPfx{\var{max}, \var{lss}}$ \cuz{}
        \longDfref{Maximal LSS}{def:maximal-lss},
        \Eref{eq:th-maximal-lss-dichotomy-17-40}.
}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-28}
\begin{gathered}
    \var{lss} \neq \var{max} \implies \PrPfx{\var{lss}, \var{max}}
        \because \Eref{eq:th-maximal-lss-dichotomy-24},
            \Eref{eq:th-maximal-lss-dichotomy-26}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-30}
\begin{gathered}
    \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}}
        \because \Eref{eq:th-maximal-lss-dichotomy-28}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-32}
\begin{gathered}
\Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \; ( \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}} ).
\\ \because \Eref{eq:th-maximal-lss-dichotomy-16},
    \Eref{eq:th-maximal-lss-dichotomy-30}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-dichotomy-34}
\begin{gathered}
\forall \; \Vlss{lss} : \exists \; \Vlss{max} :
\\ \Maximal{\var{max}} \land \Velement{max}{0} = \Velement{lss}{0}
\\ \land \; ( \var{lss} = \var{max} \lor \PrPfx{\var{lss}, \var{max}} ).
\\ \myparbox{\cuz{}
    \Eref{eq:th-maximal-lss-dichotomy-32},
    universal generalization of \Vlss{lss},
    existential generalization of \Vlss{max}.%
    }
\end{gathered}
\end{equation}
Equation \Eref{eq:th-maximal-lss-dichotomy-34} is the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS top]
\label{th:maximal-lss-top}
Every \type{LSS} with the same top as a maximal \type{LSS} is also maximal.
That is, if
\mypareq{eq:th-maximal-lss-top-02}{%
    \Maximal{\Vlss{max}}
}
then
\mypareq{eq:th-maximal-lss-top-07}{%
   $\forall \; \Vlss{lss} :
       \Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}.$
}
\end{theorem}

\begin{proof}
The proof is direct assuming the antecedant
\Eref{eq:th-maximal-lss-top-02}
to show the consequent
\Eref{eq:th-maximal-lss-top-07}.
\mypareq{eq:th-maximal-lss-top-10}{%
    $\nexists \; \Veim{next} :
        \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{max}{\Vlastix{max}}}$
        \linebreak \cuz{}
        \longThref{Maximal LSS termination}{th:maximal-lss-termination},
        antecedant \Eref{eq:th-maximal-lss-top-02}
        of theorem.
}
To show the consequent \Eref{eq:th-maximal-lss-top-07},
we first instantiate the universal generalization,
then we assume the antecedant of its predicate to show
its consequent.
\mypareq{eq:th-maximal-lss-top-12}{%
    $\var{lss} \in \type{LSS}$ \cuz{} new free \Vlss{lss}.
}
\mypareq{eq:th-maximal-lss-top-14}{%
   $\Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}$
     AF derivation.
}
\mypareq{eq:th-maximal-lss-top-16}{%
    $\nexists \; \Veim{next} :
        \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
        \linebreak \cuz{}
        \Eref{eq:th-maximal-lss-top-10},
        \Eref{eq:th-maximal-lss-top-14}.
}
\mypareq{eq:th-maximal-lss-top-18}{%
    \Maximal{\var{lss}}
    \cuz{} \longThref{Maximal LSS termination}{th:maximal-lss-termination},
    \Eref{eq:th-maximal-lss-top-16}.
}
\mypareq{eq:th-maximal-lss-top-20}{%
   $\Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}$
       \cuz{} derivation \Eref{eq:th-maximal-lss-top-14}--%
           \Eref{eq:th-maximal-lss-top-18}.
}
\mypareq{eq:th-maximal-lss-top-22}{%
   $\forall \; \var{lss} : \Velement{lss}{\Vlastix{lss}} = \Velement{max}{\Vlastix{max}}
       \implies \Maximal{\var{lss}}$
       \linebreak \cuz{} \Eref{eq:th-maximal-lss-top-20}, universal instantiation.
}
Equation \Eref{eq:th-maximal-lss-top-22} is the consequent of theorem,
and with it, we have the theorem.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS \type{LIM}s termination]
\label{th:maximal-lss-lims-termination}
\mypareq{eq:th-maximal-lss-lims-termination-02}{%
    $\Maximal{\Vlss{max}} \iff \size{ \myfn{next-lims}{\Vlastix{max}} } = 0$.
}
\end{theorem}

\begin{proof}
The proof is by mutual implication.
\mypareq{eq:th-maximal-lss-lims-termination-10}{%
    $\Maximal{\Vlss{max}}$
    \cuz{} left term of \Eref{eq:th-maximal-lss-lims-termination-02},
    AF proof.
}
\mypareq{eq:th-maximal-lss-lims-termination-12}{%
        $\nexists \; \Veim{next} : \myfn{Is-source}{\var{next}}
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
    \linebreak \cuz{}
    equivalence with
    \Eref{eq:th-maximal-lss-lims-termination-10},
    using \longThref{Maximal LSS termination}{th:maximal-lss-termination}.
}
\mypareq{eq:th-maximal-lss-lims-termination-14}{%
        $\nexists \; \Veim{next} :
    \linebreak \size{\myfn{lims}{\var{next}}} \neq 0
        \land
        \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}$
    \linebreak \cuz{}
    equivalence with
    \Eref{eq:th-maximal-lss-lims-termination-12},
    using \longDfref{Leo source \type{EIM}}{def:leo-source-eim}.
}
\mypareq{eq:th-maximal-lss-lims-termination-16}{%
        $\forall \; \Veim{next} :
    \linebreak \myfn{Next-Eligible}{\var{next},\Velement{lss}{\Vlastix{lss}}}
        \implies \size{\myfn{lims}{\var{next}}} = 0$
    \linebreak \cuz{}
        equivalence with
        \Eref{eq:th-maximal-lss-lims-termination-14}.
}
\begin{equation}
\label{eq:th-maximal-lss-lims-termination-80}
\begin{gathered}
\size{ \set{
    \begin{gathered}
       \ell \in \type{LIM} :
        \\ \myfn{Next-Eligible}{\Veim{next},\Veim{src}}
        \land \; \ell \in \myfn{lims}{\var{next}}
    \end{gathered}
    } } = 0
    \\ \because \text{
        equivalence with
        \Eref{eq:th-maximal-lss-lims-termination-16}}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lims-termination-82}{%
    $\size{ \myfn{next-lims}{\Vlastix{max}} } = 0$
    \cuz{} equivalence with
    \Eref{eq:th-maximal-lss-lims-termination-80},
    using \longDfref{LIM validity}{def:lim-validity}.
}
The theorem
\Eref{eq:th-maximal-lss-lims-termination-02}
follows from the chain of equivalences
\Eref{eq:th-maximal-lss-lims-termination-10}--\Eref{eq:th-maximal-lss-lims-termination-82}.
\myqed
\end{proof}

\begin{theorem}[Maximal LSS \type{LIM} invariant]
\label{th:eq-maximal-lss-lim-invariant}
The dotted rule and origin of
every \type{LIM} of a maximal \type{LSS} is the same
as the
dotted rule and the origin of the top \type{EIM}
of that maximal \type{LSS}.
That is,
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-02}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
The proof is by induction on the last index of the \type{LSS},
so that the induction hypothesis is
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-12}
\begin{gathered}
\myfn{INDH}{\Vnat{indx}} \defined
\\ \forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{indx} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\end{gathered}
\end{equation}

For the basis, we seek
to show \myfn{INDH}{0}.
\begin{gather}
\label{eq:th-maximal-lss-lim-invariant-14}
    \Maximal{\Vlss{max}} = 0
\\ \label{eq:th-maximal-lss-lim-invariant-16}
    \land \; \Vlastix{max} = 0
\\ \nonumber
    \myparbox{\cuz{} AF derivation, new free \Vlss{max}.}
\end{gather}
\mypareq{eq:th-maximal-lss-lim-invariant-18}{%
    $\size{ \myfn{next-lims}{\Velement{max}{\Vlastix{max}}} } = 0$
    \cuz{} \longThref{Maximal LSS \type{LIM}s termination}{th:maximal-lss-lims-termination},
    \Eref{eq:th-maximal-lss-lim-invariant-14}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-20}{%
    $\Dom{\var{max}} = \set{0}$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-16}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-22}{%
     $\exists \; \Vnat{i} \in \Dom{\var{max}} :
    \mylim{\ell} \in \myfn{lims}{\VVelement{max}{i}}$
    \cuz{} AF derivation, new free \mylim{\ell}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-24}{%
    $\mylim{\ell} \in \myfn{lims}{\VVelement{max}{0}}$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-20},
        \Eref{eq:th-maximal-lss-lim-invariant-22},
        existential instantiation of \Vnat{i} with 0.
}
\mypareq{eq:th-maximal-lss-lim-invariant-26}{%
    $\size{ \myfn{next-lims}{\Velement{max}{0}} } = 0$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-16},
        \Eref{eq:th-maximal-lss-lim-invariant-18}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-28}{%
    $\DR{\ell} = \DR{\Velement{max}{0}}
    \land \Origin{\ell} = \Origin{\Velement{max}{0}}$
    \cuz{} \longDfref{LIM validity}{def:lim-validity},
        \Eref{eq:th-maximal-lss-lim-invariant-24},
        \Eref{eq:th-maximal-lss-lim-invariant-26}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-30}{%
    $\DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-16},
        \Eref{eq:th-maximal-lss-lim-invariant-28}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-32}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \myparbox{\cuz{}
    derivation \Eref{eq:th-maximal-lss-lim-invariant-22}--%
    \Eref{eq:th-maximal-lss-lim-invariant-30},
    universal generalization of \mylim{\ell}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-34}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = 0 } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    derivation \Eref{eq:th-maximal-lss-lim-invariant-14}--%
    \Eref{eq:th-maximal-lss-lim-invariant-32},
    universal generalization of \Vlss{max}.
    This is \myfn{INDH}{0}, the basis of the induction.
}
\end{gathered}
\end{equation}

For the step we assume \myfn{INDH}{\var{n}}
to show \myfn{INDH}{\var{n}+1}.
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-40}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{n} } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because \myfn{INDH}{\var{n}}, \text{AF step}.
\end{gathered}
\end{equation}
\begin{gather}
\label{eq:th-maximal-lss-lim-invariant-42}
    \var{max} \in \type{LSS}
\\ \label{eq:th-maximal-lss-lim-invariant-43}
    \land \; \Maximal{\var{max}}
\\ \label{eq:th-maximal-lss-lim-invariant-44}
    \land \; \Vlastix{max} = \var{n}+1
\\ \nonumber
    \myparbox{\cuz{} AF derivation, new free \Vlss{max}.}
\end{gather}
\mypareq{eq:th-maximal-lss-lim-invariant-46}{%
    $1 \in \Dom{\var{max}}$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-44}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-48}{%
    $\Vlss{suffix} = \Velement{max}{1\ldots \Vlastix{max}}$
    \cuz{} \longThref{LSS subsequence}{th:lss-subsequence},
        \Eref{eq:th-maximal-lss-lim-invariant-42},
        \Eref{eq:th-maximal-lss-lim-invariant-46},
        new \Vlss{suffix} for convenience.
}
\mypareq{eq:th-maximal-lss-lim-invariant-49}{%
    $\var{suffix} \in \type{LSS}$ \cuz{}
        \Eref{eq:th-maximal-lss-lim-invariant-48}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-50}{%
    $\Velement{suffix}{\Vlastix{suffix}} = \Velement{max}{\Vlastix{max}}$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-48}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-52}{%
    $\Maximal{\var{suffix}}$
    \cuz{} \longThref{Maximal LSS top}{th:maximal-lss-top},
        \Eref{eq:th-maximal-lss-lim-invariant-43},
        \Eref{eq:th-maximal-lss-lim-invariant-49},
        \Eref{eq:th-maximal-lss-lim-invariant-50}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-54}{%
    $\Vlastix{suffix} = \var{n}$
    \cuz{} \Eref{eq:th-maximal-lss-lim-invariant-44},
        \Eref{eq:th-maximal-lss-lim-invariant-48}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-56}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{suffix}} :
    \ell1 \in \myfn{lims}{\VVelement{suffix}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{suffix}{\Vlastix{suffix}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{suffix}{\Vlastix{suffix}}}.
\\ \myparbox{\cuz{}
    instantiation of \Vlss{max} with \Vlss{suffix},
    \Eref{eq:th-maximal-lss-lim-invariant-40},
    \Eref{eq:th-maximal-lss-lim-invariant-52},
    \Eref{eq:th-maximal-lss-lim-invariant-54}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-58}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{suffix}} :
    \ell1 \in \myfn{lims}{\VVelement{suffix}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{eq:th-maximal-lss-lim-invariant-50},
    \Eref{eq:th-maximal-lss-lim-invariant-56}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-60}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \set{1\ldots \Vlastix{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{eq:th-maximal-lss-lim-invariant-48},
    \Eref{eq:th-maximal-lss-lim-invariant-58}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lim-invariant-62}{%
   \myfn{Is-source}{\Velement{max}{1}}
   \cuz{} \longDfref{LSS}{def:lss},
        \Eref{eq:th-maximal-lss-lim-invariant-42},
        \Eref{eq:th-maximal-lss-lim-invariant-46}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-64}{%
    $\size{\myfn{lims}{\Velement{max}{1}}} > 0$
    \cuz{} \longDfref{Leo source \type{EIM}}{def:leo-source-eim},
    \Eref{eq:th-maximal-lss-lim-invariant-62}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-66}{%
    $\var{max} \in \type{LES}$
    \cuz{} \longDfref{LSS}{def:lss},
        \Eref{eq:th-maximal-lss-lim-invariant-42}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-68}{%
    $\myfn{Next-Eligible}{\Velement{max}{1},\Velement{max}{0}}$
        \cuz{} \longDfref{Leo eligible sequence}{def:next-eligible-sequence},
            \Eref{eq:th-maximal-lss-lim-invariant-46},
            \Eref{eq:th-maximal-lss-lim-invariant-66}.
}
\mypareq{eq:th-maximal-lss-lim-invariant-70}{%
    $\size{\myfn{next-lims}{\Velement{max}{0}}} > 0$
    \cuz{} \longDfref{LIM validity}{def:lim-validity},
        \Eref{eq:th-maximal-lss-lim-invariant-64},
        \Eref{eq:th-maximal-lss-lim-invariant-68}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-72}
\begin{gathered}
    \myfn{lims}{\Velement{max}{0}} =
    \\ \left\lbrace \begin{gathered}
        \tuple{ \Vdr{dr}, \Postdot{\Velement{max}{0}}, \Vorig{orig}, \Current{\Velement{max}{0}}} :
        \\ \Valid{\Velement{max}{0}} \land \myfn{Is-Leo-Eligible}{\Velement{max}{0}}
        \\ \land \; \mylim{\ell} \in \myfn{next-lims}{\Velement{max}{0}}
        \\ \land \; \var{dr} = \DR{\ell} \land \var{orig} = \Origin{\ell}
    \end{gathered} \right\rbrace
    \\ \because \Dfref{def:lim-validity},
        \Eref{eq:th-maximal-lss-lim-invariant-70}.
\end{gathered}
\end{equation}
\mypareq{eq:th-maximal-lss-lim-invariant-74}{%
    $\Valid{\Velement{max}{0}} \land \myfn{Is-Leo-Eligible}{\Velement{max}{0}}$
    \cuz{} \longDfref{LES}{def:next-eligible-sequence},
        \Eref{eq:th-maximal-lss-lim-invariant-66}.
}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-75}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{lims}{\Velement{max}{1}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \myparbox{\cuz{}
       \Eref{eq:th-maximal-lss-lim-invariant-60},
       instantiation of \Vnat{i} with 1.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-76}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{next-lims}{\Velement{max}{0}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because \longDfref{LIM validity}{def:lim-validity},
    \Eref{eq:th-maximal-lss-lim-invariant-68},
    \Eref{eq:th-maximal-lss-lim-invariant-75}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-78}
\begin{gathered}
    \myfn{lims}{\Velement{max}{0}} =
    \\ \left\lbrace \begin{gathered}
        \tuple{ \Vdr{dr}, \Postdot{\Velement{max}{0}}, \Vorig{orig}, \Current{\Velement{max}{0}}} :
        \\ \var{dr} = \DR{\Velement{max}{\Vlastix{max}}} \land \var{orig} = \Origin{\Velement{max}{\Vlastix{max}}}
    \end{gathered} \right\rbrace
    \\ \because \Eref{eq:th-maximal-lss-lim-invariant-72},
        \Eref{eq:th-maximal-lss-lim-invariant-74},
        \Eref{eq:th-maximal-lss-lim-invariant-76}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-80}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \set{0\ldots \Vlastix{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\\ \land \; \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \because
    \Eref{eq:th-maximal-lss-lim-invariant-60},
    \Eref{eq:th-maximal-lss-lim-invariant-78}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-84}
\begin{gathered}
\var{max} \in \type{LSS} \land \Maximal{\var{max}} \land \Vlastix{max} = \var{n}+1
\\ \implies \forall \; \mylim{\ell} \in
    \set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
        \ell1 \in \myfn{lims}{\VVelement{max}{i}}
    } :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    derivation from \Eref{eq:th-maximal-lss-lim-invariant-42}--%
    \Eref{eq:th-maximal-lss-lim-invariant-44} to
    \Eref{eq:th-maximal-lss-lim-invariant-80}.
    }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-maximal-lss-lim-invariant-86}
\begin{gathered}
\forall \; \var{max} \in \set{ \Vlss{lss} : \Maximal{\var{lss}} \land \Vlastix{lss} = \var{n}+1 } :
\\ \forall \; \mylim{\ell} \in
\set{ \ell1 : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell1 \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}.
\\ \myparbox{\cuz{}
    \Eref{eq:th-maximal-lss-lim-invariant-84},
    universal generalization of free \Vlss{max}.
    }
\end{gathered}
\end{equation}
\Eref{eq:th-maximal-lss-lim-invariant-86} is \myfn{INDH}{\var{n}+1}, which is what we
sought to show for the step of the induction.
With the step
and the basis \Eref{eq:th-maximal-lss-lim-invariant-34}
of the induction,
we have the induction and the theorem.
\myqed
\end{proof}

\begin{theorem}[LIM Non-duplication]
\label{th:lim-non-duplication}
Every \type{EIM} has at most one valid \type{LIM}.
That is,
\[
\forall \; \Veim{eim} : \Valid{\var{eim}} \implies \size{\myfn{lims}{\var{eim}}} \le 1.
\]
\end{theorem}

\begin{proof}
Recall that a \type{LIM} is valid iff it is an element of
\myfn{lims}{\var{x}} for some \type{EIM} \var{x}.
The proof is by reductio.
\begin{gather}
\label{eq:th-lim-non-duplication-10}
\var{src} \in \type{EIM}
\\ \label{eq:th-lim-non-duplication-12}
\land \; \size{\myfn{lims}{\var{src}}} \ge 2.
\\ \nonumber
\myparbox{\cuz{} new arbitrary \Veim{src}, AF reductio.}
\end{gather}
\begin{gather}
\label{eq:th-lim-non-duplication-13-10}
\ell1 \in \myfn{lims}{\var{src}}
\\ \label{eq:th-lim-non-duplication-13-20}
\land \; \ell2 \in \myfn{lims}{\var{src}}
\\ \label{eq:th-lim-non-duplication-13-30}
\land \; \ell1 \ne \ell2.
\\ \nonumber
\cuzbox{
    \Eref{eq:th-lim-non-duplication-12},
    new \mylim{\ell1}, new \mylim{\ell2}.}
\end{gather}
\mypareq{eq:th-lim-non-duplication-14}{%
        $\mylim{\ell1} = \tuple{
            \Vdr{dr1}, \Postdot{\var{src}}, \Vorig{orig1}, \Current{\var{src}} }$
        \linebreak \cuz{}
            \longDfref{LIM validity}{def:lim-validity},
            \Eref{eq:th-lim-non-duplication-13-10}, WLOG,
            new \Vdr{dr1}, new \Vdr{orig1}.
}
\mypareq{eq:th-lim-non-duplication-16}{%
        $\mylim{\ell2} = \tuple{
            \Vdr{dr2}, \Postdot{\var{src}}, \Vorig{orig2}, \Current{\var{src}} }$
        \linebreak \cuz{}
            \Dfref{def:lim-validity},
            \Eref{eq:th-lim-non-duplication-13-20}, WLOG,
            new \Vdr{dr2}, new \Vdr{orig2}.
}
\mypareq{eq:th-lim-non-duplication-18}{%
    $\exists \; \var{max} : \Velement{max}{0} = \var{src} \land \Maximal{\var{max}}$
    \cuz{} \longThref{Maximal LSS existence}{th:maximal-lss-existence},
        \Eref{eq:th-lim-non-duplication-10}.
}
\begin{gather}
\label{eq:th-lim-non-duplication-19}
    \var{max} \in \type{LSS}
\\ \label{eq:th-lim-non-duplication-20}
    \Velement{max}{0} = \var{src}
\\ \label{eq:th-lim-non-duplication-22}
    \land \; \Maximal{\var{max}}
\\ \nonumber
    \cuzbox{\Eref{eq:th-lim-non-duplication-18}, existential instantiation.}
\end{gather}
\begin{equation}
\label{eq:th-lim-non-duplication-24}
\begin{gathered}
\forall \; \mylim{\ell} \in
\set{ \ell\ell : \exists \; \Vnat{i} \in \Dom{\var{max}} :
    \ell\ell \in \myfn{lims}{\VVelement{max}{i}}
} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \cuzbox{%
    \longThref{Maximal-LSS--LIM invariant}{th:eq-maximal-lss-lim-invariant},
    \Eref{eq:th-lim-non-duplication-22},
    universal instantiation with \Vlss{max}.
   }
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lim-non-duplication-25}
\begin{gathered}
\forall \; \mylim{\ell} \in \myfn{lims}{\VVelement{max}{0}} :
\\ \DR{\ell} = \DR{\Velement{max}{\Vlastix{max}}}
\land \Origin{\ell} = \Origin{\Velement{max}{\Vlastix{max}}}
\\ \cuzbox{%
       \Eref{eq:th-lim-non-duplication-19},
       \Eref{eq:th-lim-non-duplication-20},
       \Eref{eq:th-lim-non-duplication-24}.
   }
\end{gathered}
\end{equation}
\mypareq{eq:th-lim-non-duplication-26}{%
    $\ell1 \in \myfn{lims}{\VVelement{max}{0}}$
    \linebreak \cuz{} \Eref{eq:th-lim-non-duplication-13-10},
        \Eref{eq:th-lim-non-duplication-19},
        \Eref{eq:th-lim-non-duplication-20}.
}
\mypareq{eq:th-lim-non-duplication-28}{%
    $\ell2 \in \myfn{lims}{\VVelement{max}{0}}$
    \linebreak \cuz{} \Eref{eq:th-lim-non-duplication-13-20},
        \Eref{eq:th-lim-non-duplication-19},
        \Eref{eq:th-lim-non-duplication-20}.
}
\mypareq{eq:th-lim-non-duplication-30}{%
    $\DR{\ell1} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell1} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{eq:th-lim-non-duplication-25},
    universal instantiation of $\ell$ as \mylim{\ell1},
    \Eref{eq:th-lim-non-duplication-26}.
}
\mypareq{eq:th-lim-non-duplication-32}{%
    $\DR{\ell2} = \DR{\Velement{max}{\Vlastix{max}}}
    \land \Origin{\ell2} = \Origin{\Velement{max}{\Vlastix{max}}}$
    \linebreak \cuz{} \Eref{eq:th-lim-non-duplication-25},
    universal instantiation of $\ell$ as \mylim{\ell2},
    \Eref{eq:th-lim-non-duplication-28}.
}
\begin{equation}
\label{eq:th-lim-non-duplication-34}
\begin{gathered}
    \mylim{\ell1} = \tuple{
         \begin{gathered}
         \DR{\Velement{max}{\Vlastix{max}}}, \Postdot{\var{src}},
         \\ \Current{\var{src}}, \Origin{\Velement{max}{\Vlastix{max}}}
         \end{gathered}
     }
    \\ \because \Eref{eq:th-lim-non-duplication-14},
            \Eref{eq:th-lim-non-duplication-30}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-lim-non-duplication-36}
\begin{gathered}
    \mylim{\ell2} = \tuple{
         \begin{gathered}
         \DR{\Velement{max}{\Vlastix{max}}}, \Postdot{\var{src}},
         \\ \Current{\var{src}}, \Origin{\Velement{max}{\Vlastix{max}}}
         \end{gathered}
     }
    \\ \because \Eref{eq:th-lim-non-duplication-16},
            \Eref{eq:th-lim-non-duplication-32}.
\end{gathered}
\end{equation}
\mypareq{eq:th-lim-non-duplication-38}{
    $\mylim{\ell1} = \mylim{\ell2}$
    \cuz{} \Eref{eq:th-lim-non-duplication-34},
        \Eref{eq:th-lim-non-duplication-36}.
}
Equation \Eref{eq:th-lim-non-duplication-38}
contradicts \Eref{eq:th-lim-non-duplication-13-30},
which shows the reductio
and the theorem.
\myqed
\end{proof}

\begin{theorem}[Leo source bijection]
\label{th:source-fn-injection}
The \myfnname{Source} function exists,
and is a total bijection.
\end{theorem}

\begin{proof}
The strategy is to show that
\myfnname{Source} is an injection.
From this the theorem will follow immediately via
the \myfnname{Source} function theorem \Thref{th:leo-source-fn}.

To prove that \myfnname{Source}
has the injection property,
we treat \myfnname{Source} as a relation
and consider two tuples with distinct first elements (arguments),
but otherwise arbitrary.
We seek to show that this pair of tuples have different second elements (values).

Initially, we consider only the subcase where the size
of the domain of \myfnname{Source} is greater than 1:
\mypareq{eq:th-source-fn-injection-98}{
    $\Dom{\myfnname{Source}} > 1$ \cuz{} AF subcase
}
\begin{gather}
\label{eq:th-source-fn-injection-100}
\land\; \tuple{\mylim{\ell1}, \Veim{src1}} \in \myfnname{Source}
\\ \label{eq:th-source-fn-injection-120}
\land\; \tuple{\mylim{\ell2}, \Veim{src2}} \in \myfnname{Source}
\\ \label{eq:th-source-fn-injection-130}
\land\; \ell1 \ne \ell2
\\ \nonumber \cuzbox{AF derivation;
        new arbitrary \mylim{\ell1}, \mylim{\ell2}, \Veim{src1}, \Veim{src2}.
        }
\end{gather}
\mypareq{eq:th-source-fn-injection-140}{%
    $\var{src1} = \var{src2}$ \cuz{} AF reductio.
}
\mypareq{eq:th-source-fn-injection-150}{%
    \var{src1} has two distinct valid \type{LIM}s,
    $\ell1$ and $\ell2$ \cuz{} \Eref{eq:th-source-fn-injection-100},
        \Eref{eq:th-source-fn-injection-120},
        \Eref{eq:th-source-fn-injection-140}.
}
\mypareq{eq:th-source-fn-injection-160}{%
    \var{src1} has at most one distinct valid \type{LIM}
    \cuz{} \longThref{LIM Non-duplication}{th:lim-non-duplication}.
    This contradicts \Eref{eq:th-source-fn-injection-150}.
}
\mypareq{eq:th-source-fn-injection-170}{%
    $\var{src1} \ne \var{src2}$ \cuz{} reductio
        \Eref{eq:th-source-fn-injection-140}--%
        \Eref{eq:th-source-fn-injection-160}.
}
\begin{gather}
\label{eq:th-source-fn-injection-190}
        \Dom{\myfnname{Source}} > 2
\\ \label{eq:th-source-fn-injection-200}
        \implies \forall\; \mylim{\ell1} : \forall\; \mylim{\ell2} : \forall\; \Veim{src1} : \forall\; \Veim{src2} :
\\ \label{eq:th-source-fn-injection-210}
        \tuple{\mylim{\ell1}, \Veim{src1}} \in \myfnname{Source}
            \land \; \tuple{\mylim{\ell2}, \Veim{src2}} \in \myfnname{Source}
\\ \label{eq:th-source-fn-injection-220}
            \land \; \ell1 \ne \ell2
\\ \label{eq:th-source-fn-injection-230}
        \implies \var{src1} \ne \var{src2}
\\ \nonumber
        \cuzbox{derivation \Eref{eq:th-source-fn-injection-98}--%
            \Eref{eq:th-source-fn-injection-170};
            universal generalization of \mylim{\ell1}, \mylim{\ell2}, \Veim{src1}, \Veim{src2}.
        }
\end{gather}
The subequation \Eref{eq:th-source-fn-injection-200}--\Eref{eq:th-source-fn-injection-230}
instantiates the definition
of an injection,
and every function with an empty or singleton domain is an injection,
so that
\mypareq{eq:th-source-fn-injection-250}{%
    \myfnname{Source} is an injection
        \cuz{} \Eref{eq:th-source-fn-injection-190}--\Eref{eq:th-source-fn-injection-230};
        vacuously; trivially.
}
\mypareq{eq:th-source-fn-injection-260}{%
    The total function \myfnname{Source} exists and is a bijection
    \cuz{} \longThref{\myfnname{Source} function theorem}{th:leo-source-fn}.
    \myqed
}
\end{proof}

\begin{theorem}[LIM function]
\label{th:lim-fn}
The total function
\mypareq{}{%
    $\myfnname{Lim} \in \fnset{
        \set{\Veim{src} : \myfn{Is-source}{\var{src}}}
    }{
        \set{\mylim{\ell} : \Valid{\ell}}
    }$,
}
\mypareq{}{%
    such that $\Lim{\Veim{src}} \defined \riota \mylim{\ell} : \ell \in \myfn{lims}{\var{src}}$,
}
exists and is a bijection.
\end{theorem}

\begin{proof}
First, we observe that \myfnname{LIM} is the inverse of \myfnname{Source} \Thref{th:leo-source-fn}.
Second, with the \myfnname{Source} bijection theorem
\Thref{th:source-fn-injection}
we has shown
that \myfnname{Source} exists and is total and a bijection.
Third, if a total bijective function exists,
then we know that its inverse exists and is also a
total bijective function.
From these three facts, the theorem follows immediately.
\myqed
\end{proof}

\begin{definition}[LIM matching]
To deal with \type{LIM}s,
we overload \var{Matches} as follows:
\begin{equation*}
\begin{gathered}
\Matches{\mylim{\ell},\Veim{cuz}} \defined \\
         \Postdot{\var{cuz}} = \Lambda \\
\land \; \Transition{\ell} = \Symbol{\var{cuz}} \\
\land \; \Current{\ell} = \Origin{\var{cuz}}.
  \quad \dispEnd
\end{gathered}
\end{equation*}
\end{definition}

\begin{theorem}[ES \type{LIM} Count]
\label{th:es-lim-count}
Every ES has at most
one \type{LIM} for every symbol in the grammar.
That is,
\[
\forall \; \Vloc{j} \in \Dom{\Vet{S}} :
\size{\set{\mylim{\ell} : \Current{\ell} = \var{j}}} \le
\size{\Vocab{\var{g}}}.
\]
\end{theorem}

\begin{proof}
\mypareq{eq:th-es-lim-count-10}{%
$\var{j} \in \naturals$
    \cuz{} new free \Vnat{j}
}
\mypareq{eq:th-es-lim-count-12}{%
$\var{j} \in \Dom{\Vet{S}}$
    \cuz{} AF derivation.
}
\mypareq{eq:th-es-lim-count-14}{%
$\size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{Is-Leo-Eligible}{\var{x}}
       }
    }
    \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{eq:th-es-lim-count-12},
        \longThref{Leo Eligible \type{EIM} count}{th:leo-eligible-eim-count}.
}
\mypareq{eq:th-es-lim-count-16}{%
$\forall \; \Veim{x} : \myfn{Is-source}{\var{x}}
    \implies \myfn{Is-Leo-Eligible}{\var{x}}$
\\ \cuz{} \longDfref{LIM validity}{def:lim-validity},
            \longDfref{Leo source}{def:leo-source-eim}.
}
\mypareq{eq:th-es-lim-count-18}{%
$\size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{Is-Source}{\var{x}}
       }
    }
    \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{eq:th-es-lim-count-14},
        \Eref{eq:th-es-lim-count-16}.
}
By the \type{LIM} Non-duplication Theorem,
there is, for each source \type{EIM},
at most one \type{LIM} with that \type{EIM} as its source,
so that
\mypareq{eq:th-es-lim-count-20}{%
$\size{
       \set{ \mylim{\ell} :
                \exists\; \Veim{x} \in \Velement{S}{\var{j}} :
        \ell = \Lim{\var{x}} \land \myfn{Is-Source}{\var{x}}
       }
    }$
    \linebreak
    $\le \size{
       \set{
                \Veim{x} \in \Velement{S}{\var{j}} :
        \myfn{Is-Source}{\var{x}}
       }
    }$
    \linebreak \cuz{} \Thref{th:lim-non-duplication},
        \Eref{eq:th-es-lim-count-18}.
}
\mypareq{eq:th-es-lim-count-24}{%
$\size{
       \set{ \mylim{\ell} :
                \exists\; \Veim{x} \in \Velement{S}{\var{j}} :
        \ell = \Lim{\var{x}} \land \myfn{Is-Source}{\var{x}}
       }
    }$
    $\le \size{\Vocab{\Vint{g}}}$
        \cuz{\Eref{eq:th-es-lim-count-18},
            \Eref{eq:th-es-lim-count-20}}.
}
\mypareq{eq:th-es-lim-count-26}{%
    $\forall\; \Veim{x} : \left( \exists\; \mylim{\ell} : \ell = \Lim{\var{x}} \right)
        \implies \myfn{Is-Source}{\var{x}}$
    \cuz{} \longThref{\myfnname{Lim} function}{th:lim-fn},
            \longDfref{Leo source \type{EIM}}{def:leo-source-eim}.
}
\mypareq{eq:th-es-lim-count-28}{%
    $\size{
       \set{ \mylim{\ell} :
            \exists\; \Veim{x} \in \Velement{S}{\var{j}} : \ell = \Lim{\var{x}}
       }
    } \le \size{\Vocab{\Vint{g}}}$
        \linebreak \cuz{\Eref{eq:th-es-lim-count-24},
            \Eref{eq:th-es-lim-count-26}}.
}
\mypareq{eq:th-es-lim-count-30}{%
    $\size{
       \set{ \mylim{\ell} :
            \exists\; \Veim{x} : \Current{\var{x}} = \var{j} \land \ell = \Lim{\var{x}}
       }
    }$
    \linebreak $\le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \longDfref{ET validity}{def:et-validity},
        \Eref{eq:th-es-lim-count-30}.
}
\mypareq{eq:th-es-lim-count-32}{%
    $\forall\; \Veim{x} : \Current{\var{x}} = \Current{\Lim{\var{x}}}$
    \cuz{} \longDfref{LIM validity}{def:lim-validity},
        \longThref{\myfnname{Lim} function}{th:lim-fn}.
}
\mypareq{eq:th-es-lim-count-34}{%
    $\size{
       \set{ \mylim{\ell} :
            \Current{\ell} = \var{j} \land \ell = \Lim{\var{x}}
       }
    } \le \size{\Vocab{\Vint{g}}}$
    \linebreak \cuz{} \Eref{eq:th-es-lim-count-30}, \Eref{eq:th-es-lim-count-32}.
}
\begin{equation}
\label{eq:th-es-lim-count-36}
\begin{gathered}
    \forall \; \Vloc{j} \in \Dom{\Vet{S}} :
    \size{\set{\mylim{\ell} : \Current{\ell} = \var{j}}} \le
    \size{\Vocab{\var{g}}}
    \\ \cuzbox{derivation
        \Eref{eq:th-es-lim-count-12}--\Eref{eq:th-es-lim-count-34};
        universal generalization of \Vloc{j}.
        \myqed
    }
\end{gathered}
\end{equation}
\end{proof}

\begin{definition}[LIM predecessor function]
\label{def:lim-predecessor-fn}
\begin{equation}
\begin{gathered}
\LIMPredecessor{\Veim{src}} \defined
    \\ \begin{cases}
    \riota \mylim{\ell} : \ell \in \var{nxtlims}
    \\ \qquad \text{ if } \Vsize{nxtlims} = 1;
    \\ \Lambda\text{, otherwise};
    \end{cases}
\end{gathered}
\end{equation}
where $\var{nxtlims} = \myfn{next-lims}{\Veim{src}}$.
\dispEnd
\end{definition}

\begin{theorem}[LIM predecessor direction]
\label{th:lim-predecessor-direction}
\begin{gather*}
    \LIMPredecessor{\Veim{prev}} \ne \Lambda
    \\ \implies \Current{\LIMPredecessor{\var{prev}}} \le
        \Current{\var{prev}}.
\end{gather*}
\end{theorem}

\begin{proof}
This theorem follows from
the Next Eligible \type{EIM} Direction Theorem \Thref{th:next-eligible-direction},
the definition of \type{LIM} validity \Dfref{def:lim-validity},
the \type{LIM} Direction Theorem \Thref{th-lim-direction},
and the definition of the \type{LIM} predecessor function \Dfref{def:lim-predecessor-fn}.
\myqed
\end{proof}

\chapter{Ancestry}
\label{chap:ancestry}

\Marpa{}, like other variants of \Earley{}, performs
``operations'' on the Earley sets to produce new
Earley items and, in \Marpa{}'s case,
new Leo items.
A operation may require that one or two other
\type{PIM}s already exist in the Earley sets before
it becomes applicable.
If an operation requires that \Vpim{anc}
exist in the Earley sets
before adjoining \Vpim{desc} with an Earley set \VVelement{S}{j},
then \Vpim{anc} is said to be the \dfn{direct ancestor}
of \Vpim{desc},
and \Vpim{desc} is said to be the \dfn{direct descendant}
of \Vpim{anc}.
If \Vpim{desc} is already a member of \VVelement{S}{j},
\Vpim{anc}
becomes a direct ancestor of \Vpim{desc},
and \Vpim{desc} becomes a direct descendant of \Vpim{anc}.

An \dfn{ancestor} of \Vpim{p} is recursively defined as
\begin{itemize}
\item \var{p} itself, as its own \dfn{trivial ancestor};
\item any direct ancestor of \var{p}; and
\item any direct ancestor of an ancestor of \var{p}.
\end{itemize}
Similarly, a \dfn{descendant} of \Vpim{p} is defined to be
\var{p} itself, as its own \dfn{trivial descendant};
any direct descendant of \var{p}; and
any direct descendant of a descendant of \var{p}.

For each operation, there are at most two direct ancestors.
We follow \cite{AH2002} in calling one the ``predecessor''
and the other the ``cause''.
This terminology makes sense for some operations and,
for consistency, we stretch it to cover all cases.
If the predecessor or the cause is not applicable
to a particular operation, we represent it as a
$\Lambda$ value.

The \Marpa{} implementation tracks the ancestry of
\type{PIM}s.
The reasons are tracked in \dfn{link pairs},%
\footnote{The concept of link pairs was present
in
\cite{Earley1968} and
\cite{Earley1970},
but \Marpa{} owes much to the
careful elaboration of the idea in
\cite{AH2002}.
}
which are essential for efficient evaluation.

As we will see below,
some \type{PIM}s do not track link pairs.
For those \type{PIM}s which do track link pairs,
there is at exactly one link pair for each adjunction of a
\type{PIM} with an Earley set.

A link pair
is a duple of predecessor and cause:
\[
[ \var{predecessor}, \var{cause} ]
\]
In the \Marpa{} implementation,
the elements of link pairs are pointers
which are used as ``links'' to other data.
The \Marpa{} implementation
represents
$\Lambda$ elements
as C language null pointers.

Every link pair is either an \dfn{initial link pair}
or an \dfn{ambiguity link pair}.
An \dfn{initial link pair} is the link pair added when a new \type{PIM} is
added to an Earley set.
An \dfn{ambiguous link pair} is a link pair
that is added to a \type{PIM} that
is already an element of an Earley set.

A link pair has type \type{LP}.
The set of link pairs of a \type{PIM}
can be written
\LinkPairs{\Vpim{pim}},
or as
\LinkPairs{\Veim{eim}}, or
\LinkPairs{\Vlim{lim}},
for the \type{PIM} subtypes.

\chapter{Complexity preliminaries}
\label{chap:complexity-preliminaries}
\todo{Rewrite ``Complexity preliminaries'' section.}

Our approach to proving our complexity claims is based
on the traditional one for \Earley{} variants.
In the complexity context,
\dfn{resource} means time or space.
We divide space into two kinds:
\dfn{trimmed space} and \dfn{link space}.

\todo{Ensure our treatment of link vs. trimmed space is consistent}
All space, except for links, is charged to trimmed space.
Link space may be charged either
to trimmed space or to link space.
This is a free choice
and therefore is not justified in proofs.
However, while free, the choice between link and trimmed
space has consequences
for later theorems,
and cannot be made injudiciously.

Sometimes it will be convenient to defer the decision
to charge a link pair to trimmed or link space.
Space which is yet to be assigned to trimmed or link space
is called \dfn{unassigned} space.

We use the distinction between link
and trimmed space to allow us to report
two kinds of space complexity results.
We want to report space complexity results
which apply to practical applications,
but we also want to report results which
allow ``apples to apples'' comparisons
with the space complexity claims
in prior literature.

It is a rare practical parsing application
that does not require an evaluation phase of
some kind, and
efficient evaluation is not possible without link
pairs.
Therefore, to be of practical significance, we
need to report space complexity results which
include all link space.
On the other hand, most of the Earley literature
reports results in terms of trimmed space,
and to allow accurate comparisons,
we need to be able to ignore at least some
of the space consumed by link pairs.
This is why we track some of the space consumed
by link pairs separately.

\begin{definition}[Try, dup and new resource]
\label{def:try-dup-new}
We ``charge'' all resource to \type{EIM}s.
Recall that when an attempt is made to add a duplicate \type{EIM} to an Earley set,
that it does not change the Earley set.
\dfn{Try-charged resource}, or \dfn{try resource},
is consumed by either
\begin{itemize}
\item an attempt to add a duplicate to the Earley set,
  in which case the try resource is also called \dfn{dup-charged resource}, or
  \dfn{dup resource};
\item a successful attempt to add a new \type{EIM} to the Earley set,
  in which case the try resource is also \dfn{new-charged resource}, or
  \dfn{new resource}.
\end{itemize}
Execution of logic which consumes a type of resource is
named after that resource,
so that execution of logic which successfully adds an \type{EIM} to an Earley set is called
\dfn{new-charged execution}, a \dfn{new-charged}, or a \dfn{new}.
Similarly,
an execution which unsuccessfully attempts to add an \type{EIM} is called \dfn{dup-charged execution},
or a \dfn{dup}.
A dup or a new can also be called
a \dfn{try-charged execution}, a \dfn{try-charged} or a \dfn{try}.
\end{definition}

\begin{definition}[Old resource]
\label{def:old-resource}
Resource may also be charged to an \type{EIM} outside of an
attempt to add it to the Earley set.
Resource charged to an existing \type{EIM},
when it is not dup resource,
is called \dfn{old resource}.
Execution of logic which charges resource to an \type{EIM},
but which is not a try,
is called an \dfn{old charge execution}, or an \dfn{old}.
\end{definition}

We have yet to show this,
but we will find that,
for each \type{EIM},
\begin{itemize}
\item new and dup resource is always \Oc{};
\item which means that try resource is always \Oc{}; and
\item total resource for an \type{EIM} (old, new and dup)
    is always \Oc{}.
\end{itemize}

In a pseudocode procedure,
charges are made either to a ``destination'',
or ``to the caller''.
A ``destination'' is either an ``ultimate destination''
or an ``intermediate desination''.

The ``ultimate destination'' of a charge is always an \type{EIM}.
An ``intermediate destination'' can be either an Earley set,
or the parse itself.
All charges not made to an ultimate destination are
re-charged at some point,
and eventually every charge is re-charged to an
ultimate destination.

\begin{definition}[Charging to the parse]
\label{def:charging-to-parse}
A resource ``charged to the parse'' is re-charged to the first \type{EIM}
of Earley set 0.
There is always a start \type{EIM} in Earley set 0,
so that the first \type{EIM} of Earley set 0
exists in every parse.
\dispEnd
\end{definition}

\begin{definition}[Charging to the Earley set]
\label{def:charging-to-es}
A resource ``charged to an Earley set'' is re-charged to the first \type{EIM}
of that Earley set.
In a successful parse, every Earley set will have at least one \type{EIM},
so that the first \type{EIM} of an Earley set will always exist.
An Earley set without a \type{EIM} causes a parse failure,
so that in a failed parse, every Earley set except the last will have at least one \type{EIM},
and resource charged to those Earley sets can be charged to their first \type{EIM}.
In the last Earley set of a failed parse,
resource charged to the Earley set is charged to the parse.
\dispEnd
\end{definition}

\begin{definition}[Charging to the caller]
\label{def:charging-to-caller}
If a resource is ``charged to the caller'',
the decision of how
to re-charge the resource is made by
the pseudocode procedure that called the current pseudocode procedure.
\dispEnd
\end{definition}

Reads and writes do not consume space -- only allocations of space
consume space.
We will not examine allocations of non-global space in detail.
Instead, we assume
that space local to the pseudocode procedures
resides in a data structure
(in common implementations, a stack)
whose maximum space consumed is \Oc{}.

\begin{definition}[Housekeeping and body execution resource]
\label{obs:housekeeping-vs-body-execution}
For the purposes of complexity analysis,
we find it helpful to divide the resource consumption
of a called pseudocode procedure into \dfn{housekeeping}
and \dfn{body execution}.
Resource not consumed by housekeeping
is considered to be consumed by body execution.

Housekeeping always consumes \Oc{} time and zero space.
Body execution may consume arbitrary resource.
\dispEnd
\end{definition}

In a common implementation, the housekeeping will be execution of
one or more prologues, which set up registers and stack for
the procedure,
and the execution of one more more epilogues,
which restore the registers and stack.

\begin{observation}[Direct and indirect resource]
\label{obs:direct-vs-indirect}
From the point of view of a current pseudocode procedure,
\begin{itemize}
\item the resource consumed by the execution of logic in the pseudocode
which is not a call of another pseudocode procedure,
is consumed ``directly''; and
\item the housekeeping resource consumed by the execution of
calls of another pseudocode procedure,
is consumed ``directly''; and
\item resource charged by other pseudocode procedures
``to the caller'', which is passed up to the current pseudocode
procedure, and which the current pseudocode procedure charges to
a destination,
is consumed ``directly''; and
\item all other resource is consumed ``indirectly''.
\end{itemize}
\end{observation}

``Exclusive'' resource means resource consumed directly.
``Included'' resource means resource consumed indirectly.
``Inclusive'' resource means resource consumed both
directly and indirectly.
Resource consumed is exclusive unless otherwise stated.

\todo{Delete the "resource functions"?}
For use in the complexity analysis,
we define the following ``resource functions'',
where \var{x} may be a code snippet, a line,
or an algorithm:
\begin{itemize}
\item \Time{\var{x}} is the exclusive time consumed by one invocation of \var{x}.
\item \Space{\var{x}} is the exclusive space consumed by one invocation of \var{x}.
\item \Invocs{\var{x}} is the number of invocations during one call of the current pseudocode procedure of \var{x}.
\item \Passes{\ell} is the number of passes during one call of the current pseudocode procedure of the loop $\ell$.
\item \Invocs{\var{x}}/\var{unit} is the number of invocations modulo one \var{unit} of \var{x}.
\item \Passes{\ell/\var{unit}} is the number of passes modulo one \var{unit} of the loop $\ell$.
\end{itemize}
The resource functions
defined as ``modulo a unit'' are for counting invocations
and passes at a level finer than the total count for the pseudocode procedure.
Their precise meaning will defined in the context in which they are used.

\section{Earley set indexing}
\label{sec:es-indexing}

\begin{theorem}[Location access time complexity]
The function calls \Current{\Vpim{pim}} and
\Current{\Ves{es}},
and accesses of the form
\Velement{S}{\Vloc{j}},
incur \Oc{} time and zero space.
\end{theorem}

\begin{proof}
Our computation model, conceptually, is a random access machine,
but it will be more useful to justify
the theorem in terms of the \Marpa{} implementation.
The implementation description is equivalent to,
or straightforwardly translates to, a description in
terms of a RAM.
The accesses obviously require no new space.
The constant times are accomplished by
storing the integer location of that ES
in each ES;
storing ES's in a dynamic array
indexed by location;
and storing a pointer to its ES in
every \type{PIM}.
\myqed
\end{proof}

In practice, for efficiency,
\Velement{S}{\Vloc{j}} accesses are not actually used
by the \Marpa{} implementation during parsing.
During parsing
\Rtwo\cite{Marpa-R2}
builds Earley sets as a linked list of structures,
and a dynamic array indexing them is only created
when needed.
The dynamic array is not needed when
running the recognizer in normal production,
because links to Earley sets are kept in the \type{PIM}s,
a link to the current Earley set is kept,
and the event mechanism works using links.

On the other hand,
accesses of the form
\Velement{S}{\Vloc{j}}
are made use of
for tracing and debugging in \Rtwo{}.
The trace and
debug facilities must allow the developer to
specify arbitrary Earley sets,
and the developer will often want to identify a location by
its numerical ID.

An array is also needed for evaluation,
but at evaluation time the dynamic array implementing
\Velement{S}{\Vloc{j}} accesses
usually is dynamic in theory only.
For most applications, evaluation only
occurs after the parse is complete,
at which point
the array of Earley sets is static,
so that the overhead of using a dynamic array
is minimal.

\section{Accounts}
\label{sec:accounts}

In the pseudocode,
resource is \dfn{incurred},
or \dfn{consumed}.
Once incurred, resource must be \dfn{charged} to
one of the following accounts:
\begin{itemize}
\item \var{parse}: An account for resource charged to the parse itself.
\item \var{es}: One account per ES,
    for resource charged to the current working Earley set.
\item \var{eim}: One account per \type{EIM},
  for resource charged to an eim which already exists.
\item \var{new}: One account per \type{EIM}, for resource charged to a newly created \type{EIM}.
\item \var{dup}: One account per ambiguous link pair,
    for resource charged to ambiguous link pairs.
\item \var{try}:
\todo{Figure "try" accounts out}
This is not a real account, but
    a useful shorthand for the total
    of the \var{new} and \var{dup} accounts
    for an \type{EIM}.
    It behaves like a per-\type{EIM} account.
\item \var{lim}: A per-\type{LIM} account.
\item \var{pred}: A per-prediction-\type{EIM} account.
\item \var{proc}: A per-pseudocode-procedure account,
  for resource charged to the current pseudocode procedure.
  Resource charged to \var{proc} must be constant or less.
\item \var{caller}: A per-pseudocode-procedure account,
  for resource charged to the current pseudocode procedure.
  Resource charged to \var{caller} may be greater than constant.
\end{itemize}

In the statement of theorems, we will sometimes state the
account to which a resource is charged.
This charging decision is a matter of choice,
and so is not subject to proof.
It is, however, a choice with consequences
for those later theorems
which add up the charged resources.

The following theorem allows some useful simplifications.
It states a condition under which we
may safely ignore \var{proc} time.

\begin{theorem}[Procedure overhead discard]
\label{th:procedure-overhead-discard}
When a pseudocode procedure (``caller'') calls another
pseudocode procedure (``callee''), caller may treat \Oc{} time
as part of the ``housekeeping''
for callee.
\myqed
\end{theorem}

\begin{proof}
By \Obref{obs:housekeeping-vs-body-execution},
a caller always incurs \Oc{} ``housekeeping''
time when it calls a callee.
If we add \Oc{} of time from the callee's
\var{proc} account to the housekeeping time
of the callee, the total time is
\[
    \Oc + \Oc = \Oc,
\]
so that, in Landau notation terms,
the housekeeping time of the callee is unchanged.
\myqed
\end{proof}

\section{Counting invocations}

We first define our conventions for counting line
invocations,
and will then follow with a toy example.

\begin{definition}[Line invocation]
\label{def:line-invocation}
The number of \dfn{invocations} of a line
is the number of executions of
that line's logic.
\dispEnd
\end{definition}

\begin{definition}[Loop invocation]
\label{def:loop-invocation}
The number of \dfn{invocations} of a loop is the number of times
the loop is initialized.
Every line in a loop is either its \dfn{first line},
or an \dfn{interior} line.
The interior lines of loop are invoked zero or more times,
once for every \dfn{pass} of the loop.

The first line of a loop is invoked
\begin{itemize}
\item once for every successful test of the test condition of the loop; and
\item exactly once for an unsucessful, and therefore final, test
of the test condition of the loop.
\dispEnd
\end{itemize}
\end{definition}

The need for a final, unsuccessful, test implies
that the first line of the loop is always invoked at last once.
In addition,
for every pass of the loop.
there is exactly
one successful test of the loop condition.

Definitions \Dfref{def:line-invocation} and
\Dfref{def:loop-invocation}
also imply that the number of
invocations of the first line of a loop
is only the same as the number of invocations
of the loop in a trivial case.
The two invocation counts will be identical if and only if the loop has zero passes ---
in other words, if the loop test fails the first time.

In the comments in the following fragment
``inv'' abbreviates invocation(s) and
``init'' abbreviates loop initializations.

\begin{algorithmic}[1]
\State do something
  \Comment inv=1
\label{line:toy-10}
\For{$\var{j} \in \set{1,2,3}$}
  \Comment init=1; inv=4; passes=3
\label{line:toy-20}
\State do an outer loop thing
  \Comment inv=3
\label{line:toy-30}
\For{$\var{i} \in \set{1,2}$}
  \Comment init=3; inv=9; passes=6
\label{line:toy-40}
\State do an inner loop thing
  \Comment inv=6
\label{line:toy-50}
\EndFor
\EndFor
\end{algorithmic}

According to our conventions, the following are true:
\begin{itemize}
\item Line \ref{line:toy-10} is invoked once.
\item Loop \enRefs{line:toy-20}{line:toy-50},
the outer loop,
is invoked once and therefore initialized once.
It has 3 passes.
\item Line \ref{line:toy-20},
the first line of the outer loop,
is invoked 4 times,
once for each pass of the outer loop,
and once to execute the test which terminates the outer loop.
\item Line \ref{line:toy-30} is the top-level interior line
of the outer loop.
It is invoked once for every pass of the outer loop,
or 3 times.
\item Loop \enRefs{line:toy-40}{line:toy-50},
the inner loop,
is initialized 3 times, once for every pass of the outer loop.
Loop \enRefs{line:toy-40}{line:toy-50}
is therefore said to have been
invoked 3 times.
The inner loop has 2 passes per intialization,
for a total of 6 passes.
\item Line \ref{line:toy-40},
the first line of the inner loop,
is invoked 3 times
for every initialization of the inner loop.
The 3 invocations per initialization consist of one
invocation for each of the 2 passes of the inner loop,
plus one invocation for the final test.
The total number of invocations of
line \ref{line:toy-40}
is therefore $3\times3=9$ times:
3 initializations multiplied by
3 invocations per initialization.
\item Line \ref{line:toy-50},
the interior line of the inner loop,
is a 2nd level line of the outer loop,
and a top level line of the inner loop.
Line \ref{line:toy-50}
is invoked once per pass of the inner loop, or 6 times.
\end{itemize}

\section{Complexity tables}
\label{sec:complexity-tables}

Let \var{inv} be the number of invocations,
let \var{dir} the direct resource consumed per invocation,
let \var{ind} the indirect resource consumed per invocation, and
let \var{total} the total resource consumed for that line.
The tables contain columns for \var{inv}, \var{dir}, \var{ind} and \var{total},
and
\[
  \var{total} = \var{inv} \times ( \var{dir} + \var{ind} ).
\]

In the presence of conditional logic, the number of invocations is sometimes
given as a range, for example, \var{lo}--\var{hi}.
In this case only the high value, \var{hi} is relevant
and we may write it \order{\le\var{hi}}, so that
\[
\order{\le\var{hi}} = \order{\var{hi}}.
\]
If, for example, $\var{lo}=0$ and $\var{hi}=42$,
the complexity of the range \var{lo}--\var{hi}
would be
$\order{\le42} = \Oc$.

The tables also contain columns indicating the pseudocode line number of each row,
and any per-line justifications used for the line's statements.
Later rows in the table give the total amount of resource accounted for in
that table,
and the per-table justifications.

The pseudocode uses a few basic control structures,
with which the reader is expected to be familiar,
and our justifications of the invocation counts
refer only to a few generalized behaviors.

\begin{definition}[Sequential and conditional execution]
\todo{Define Sequential and conditional execution}
If an execution of line \var{L2} always follows
an execution of line \var{L1} in an execution sequence,
then clearly line \var{L2} is executed
the same number of times as line \var{L1}.
In justification for line \var{L2},
we will point this out with
the notation ``line L1 Seq''.
Just as clearly in the above case,
line \var{L1} is executed as many times
as line \var{L2},
which observation our justifications notate as
``line L2 RSeq''.

If an execution of line \var{L2} sometimes follows
line an execution of \var{L1} in an execution sequence,
then line \var{L2} is executed
at most the same number of times as line \var{L1}.
We notate this observation in the form
``line L1 Cond''.
\dispEnd
\end{definition}

\chapter{The \Marpa{} algorithm}
\label{chap:marpa}

\section{Parse-setup op}
\label{sec:parse-setup}

\begin{theorem}[Grammar complexity]
Setup of grammar incurs \Oc{} time,
\Oc{} trimmed space
and zero link space,
all of which is charged to the parse.
\end{theorem}

\begin{proof}
The grammar is treated as constant,
and therefore its space and time demands are constant.
\myqed
\end{proof}

\todo{Write parse-setup op section}

\begin{theorem}[Symbol ID Complexity]
\label{th:symbol-id-complexity}
The function \ID{\Vsym{x}} maps each symbol to an
unique integer ID in \Oc{} time and zero space.
A symbol may be looked up by ID
in \Oc{} time and zero space.
Also, the entire list of symbols may be traversed
in \Oc{} time and zero space.
\end{theorem}

\begin{proof}
The data structure for \Vint{g}
contains a ``symbol table'',
an array for looking up symbols by ID,
so that symbol lookup by ID is \Oc{}.
The symbol structure stores its ID,
so that the time incurred by a call of
\ID{\var{x}} is \Oc{}.
In fact,
since \Vocab{\var{g}} is a constant depending on \var{g},
the symbol list is static,
so traversing the entire symbol list requires
\Oc{} time.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\begin{theorem}[Rule ID Complexity]
\label{th:rule-id-complexity}
The function \ID{\Vrule{r}} maps each rule to an
unique integer ID in \Oc{} time and zero space.
A rule may be looked up by ID
in \Oc{} time and zero space.
Also, the entire list of rules may be traversed
in \Oc{} time and zero space.
\end{theorem}

\begin{proof}
The data structure for \Vrule{r}
contains its ID, so that the time to
find \ID{\var{r}} is \Oc{}.
The data structure for \Vint{g}
contains an array for looking up rules by ID,
so that rule lookup by ID is \Oc{}.
In fact,
since \Rules{\var{g}} is a constant depending on \var{g},
the rule list is static,
so that traversing the entire rule list requires
\Oc{} time.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\begin{theorem}[Dotted Rule ID Complexity]
\label{th:dotted-rule-id-complexity}
The function \ID{\Vdr{x}} maps each dotted rule to
an ID in \Oc{} time and zero space.
\end{theorem}

\begin{proof}
One way to assign the
dotted rule ID's is to interate through the array
of rules by rule ID,
and within each rule iterate through its possible dot
positions,
assigning an integer to each dot position.
The data structure for \Vrule{r},
allows, given a dot position \var{pos},
the ID of $\tuple{\var{r}, \var{pos}}$ to be found in \Oc{}
time.
Also, the data structure for \Vint{g}
contains an array for looking up a dotted rule in \Oc{} time,
given its ID.
All of these operations are accesses and none consume space.
\myqed
\end{proof}

\section{ES-setup op}
\label{sec:es-setup}

\subsection{Earley set}
\label{sec:es-complexity}

When \Marpa{} reaches a new current location,
it creates an empty Earley set.
For complexity analysis purposes,
an Earley set can be regarded as a linked list
of \type{LIM}s and \type{PIM}s.

Link pairs are kept in a finite set of linked lists.
The heads of the lists stored with the \type{EIM} and
occupy trimmed space.
The rest of the lists of links pairs occupies
unassigned space.

\begin{theorem}[Earley set allocation]
\label{th:es-allocation}
The resource incurred
by allocating an empty Earley set is
charged as \Oc{} time,
\Oc{} trimmed space and zero link space.
\end{theorem}

\begin{proof}
Allocation of an empty linked list
incurs \Oc{} time and \Oc{} space.
\myqed
\end{proof}

When examining the resource consumed in adding
a new \type{PIM}, it is important to note that
it includes an update of the \var{transitions} array
\Sref{sec:transition-complexity}.

\begin{theorem}[Earley set add]
\label{th:es-add}
The resource incurred by adding a new
\type{PIM} to an Earley set incurs
\Oc{} time, \Oc{} trimmed space and
zero link space.
\end{theorem}

\begin{proof}
\todo{Redo proof}
To add a new \type{PIM}, we must add it to the
linked list of \type{PIM}s,
as well as to the transition array.
\todo{Finish this}
\myqed
\end{proof}

\begin{theorem}[Earley set adjoin]
\label{th:es-adjoin}
\todo{Replace es-complexity with broken down theorems}
The adjoin of an existing \type{EIM} (a dup)
incurs \Oc{} time, zero trimmed space and \Oc{} link space.
\end{theorem}

\begin{proof}
\todo{Redo proof}
When \Marpa{} reaches a new current location,
it creates an empty Earley set.
which
requires \Oc{} time,
\Oc{} trimmed space
and \Oc{} link space.
Each \type{EIM} is added at the end of a linked list,
so that each add of a \type{EIM} takes \Oc{} time
and \Oc{} trimmed space.
Adding an \type{EIM} requires
zero link space because the initial link pair
is kept in trimmed space.
\myqed
\end{proof}

\begin{theorem}[Earley set traversal]
\label{th:es-traversal}
\todo{Account for PIM's}
Let \Ves{eset} be an Earley set.
Traversal of \var{eset} requires \Oc{} time for each \type{PIM}
in \var{eset}.
Also,
traversal of \var{eset} requires \Oc{} time for each \type{EIM}
in \var{eset}.
\end{theorem}

\begin{proof}
\end{proof}

\subsection{\var{dotZeroUsed}}
\label{sec:dotzeroused-complexity}

\var{dotZeroUsed} is a bitmap with
one bit for every rule in the grammar.
It is used when adding predictions to an Earley set.

\begin{theorem}[\var{dotZeroUsed} bitmap]
\label{th:dotzeroused-bitmap}
Clearing \var{dotZeroUsed} requires \Oc{} time
and zero space.
Bit accesses and writes take \Oc{} time and zero space.
\end{theorem}

\begin{proof}
The size of the bitmap is \size{\Rules{\var{g}}}.
\Rules{\var{g}} is a constant
that depends on the grammar \Dfref{def:context-free-grammar},
which justifies the claim of constant time for bit reads
bit writes, and clearing of the bitmap.
Space for \var{dotZeroUsed} is accounted for
a part of the space for the grammar.
\myqed
\end{proof}

\subsection{\var{seen} PSL}
\label{sec:seen-psl-complexity}

For each Earley set,
\var{seen} is a PSL with
one entry for every dotted rule in the grammar.
Earley sets must not contain duplicated \type{EIM}s and
\var{seen} is used to efficiently enforce this.
\myfn{Seen}{\Veim{x}} is a function which returns
the EIM equal to \var{x} if it already exists,
and $\Lambda$ otherwise.
\myfn{SetSeen}{\Veim{x}} is a function which adds
\var{x} to the PSL.

\begin{theorem}[\var{seen} PSL]
\label{th:seen-psl}
In each Earley set,
set up of \var{seen} consumes \Oc{} time,
\Oc{} trimmed space
and zero link space.
Each call of \Seen{} and \SetSeen{} consumes
consumes \Oc{} time
and zero space.
\end{theorem}

\begin{proof}
The claims of this theorem are justified
in section \Chref{chap:per-set-lists},
where PSL's are described in detail.
\myqed
\end{proof}

\FloatBarrier

\section{Bookkeeper op}
\label{sec:bookkeeper}

\begin{algorithm}[H]
\caption{
    \label{alg:bookkeeper-op}
    \var{Bookkeeper}}
\begin{algorithmic}[1]
\Procedure{Bookkeeper}{\VVelement{S}{j}}
\State Create \var{work}, an temporary working array of length \size{\Vocab{\var{g}}},
indexed by symbol ID
\label{line:bookkeeper-op-2}
\State $\Vnat{cnt} \gets 0$
\label{line:bookkeeper-op-3}
\For{$\Vpim{x} \in \VVelement{S}{j}$}
\label{line:bookkeeper-op-4}
\State $\Vnat{symID} \gets \ID{\Transition{\Vpim{x}}}$
\label{line:bookkeeper-op-5}
\If { \Velement{work}{\var{symID}} is unpopulated }
\label{line:bookkeeper-op-6}
\State Create an empty linked list at \Velement{work}{\var{symID}}
\label{line:bookkeeper-op-7}
\State $\var{cnt} \gets \var{cnt}+1$
\label{line:bookkeeper-op-8}
\EndIf
\State Add \Vpim{x} at the head of the list at \Velement{work}{\var{symID}}
\label{line:bookkeeper-op-9}
\EndFor
\State Create \var{transition} for \VVelement{S}{j}, $\Vsize{transition} = \var{cnt}$
\label{line:bookkeeper-op-10}
\State Copy the populated entries of \var{work} to \var{transition},
   in order
\label{line:bookkeeper-op-11}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

The \op{Bookkeeper} op is run for each \type{ES}
after that \type{ES} is fully populated with \type{PIM}s.
The \op{Bookkeeper} op and creates
the \var{transition} array for that \type{ES}.

Each entry in the \var{transition} array
contains a pointer
to the head of a linked list of
\type{PIM}s which have the same
transition symbol.
The entries are in symbol ID order,
to allow the array to be binary searched.
The \var{transition} array is for efficient lookup
of \type{EIM} and \type{LIM} matches.

\begin{theorem}[\var{cnt} size]
In \Aref{alg:bookkeeper-op}, $\var{cnt} = \Oc{}$.
\end{theorem}

\begin{proof}
\var{cnt} is incremented at line
\ref{line:bookkeeper-op-8} only if
\Velement{work}{\var{symID}} is unpopulated
at line \ref{line:bookkeeper-op-6}.
But whenever \Velement{work}{\var{symID}} is unpopulated
at line \ref{line:bookkeeper-op-6},
it is populated
at line \ref{line:bookkeeper-op-7}.
Therefore,
\mypareq{eq:th-transition-creation-100}{%
    for every value of \var{symID},
    line \ref{line:bookkeeper-op-8} is executed at most once.
}
\mypareq{eq:th-transition-creation-110}{%
   \var{symID}
   at line \ref{line:bookkeeper-op-6}
   is the ID of a symbol in \Vint{g}
   \cuz{}
   line \ref{line:bookkeeper-op-5}.
}
\mypareq{eq:th-transition-creation-120}{%
    Line \ref{line:bookkeeper-op-8} is executed at most
    \size{\Vocab{\var{g}}} times
    \cuz{}
        \Eref{eq:th-transition-creation-100},
        \Eref{eq:th-transition-creation-110}.
}
\mypareq{eq:th-transition-creation-130}{%
    In \Aref{alg:bookkeeper-op}, $\var{cnt} \le
    \size{\Vocab{\var{g}}}$
    \cuz{} lines \ref{line:bookkeeper-op-3},
        \ref{line:bookkeeper-op-8};
        \Eref{eq:th-transition-creation-120}.
}
The theorem follows from
\Eref{eq:th-transition-creation-130}
and that fact that
\Vocab{\var{g}} is a constant which depends on \Vint{g}.
\myqed
\end{proof}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:bookkeeper-invocations}
    Line invocations for \op{Bookkeeper}}
\vspace{1ex}
\begin{tabular}{|r|r|p{2.75in}|}
\hline
\multicolumn{1}{|c|}{Line}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:bookkeeper-op-2}&1/\var{proc}&Immediate.
\\ \hline
    \ref{line:bookkeeper-op-3}&1/\var{proc}&
       Line \ref{line:bookkeeper-op-2} Seq.
\\ \hline
    \multirow{2}{*}{\ref{line:bookkeeper-op-4}}
    &1/\var{proc}&
    \multirow{2}{*}{
        Line \ref{line:bookkeeper-op-3} Seq;
        \Obref{def:loop-invocation}.}
\\
    &+\;1/\var{pim}&
\\ \hline
    \ref{line:bookkeeper-op-5}&1/\var{pim}&
       Line \ref{line:bookkeeper-op-4} Seq.
\\ \hline
    \ref{line:bookkeeper-op-6}&1/\var{pim}&
       Line \ref{line:bookkeeper-op-5} Seq.
\\ \hline
    \ref{line:bookkeeper-op-7}&{$\le$}1/\var{pim}&
       Line \ref{line:bookkeeper-op-6} Cond.
\\ \hline
    \ref{line:bookkeeper-op-8}&{$\le$}1/\var{pim}&
       Line \ref{line:bookkeeper-op-7} Seq.
\\ \hline
    \ref{line:bookkeeper-op-9}&1/\var{pim}&
       Line \ref{line:bookkeeper-op-6} Seq.
\\ \hline
    \ref{line:bookkeeper-op-10}&1/\var{proc}&
        Line \ref{line:bookkeeper-op-4} Seq.
\\ \hline
    \ref{line:bookkeeper-op-11}&1/\var{proc}&
        Line \ref{line:bookkeeper-op-10} Seq.
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
Lines are listed in order of deduction.
1/\var{proc}: ``once per procedure call''.
1/\var{pim}: ``once per
$\type{PIM} \in \VVelement{S}{j}$''.
{$\le$}1/\var{pim}: ``at most once per
$\type{PIM} \in \VVelement{S}{j}$''.
    }
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:bookkeeper-time}
    \op{Bookkeeper} time per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|p{3.5in}|}
\hline
Line&Time&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:bookkeeper-op-2}&\Oc{}&
        \Vocab{\var{g}} is a constant which depends on \Vint{g}.
\\ \hline
    \ref{line:bookkeeper-op-3}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-4}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-5}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-6}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-7}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-8}&\Oc{}&\Thref{th:bookkeeper-time},
        \Thref{th:procedure-overhead-discard}.
        Zero indirect time.
\\ \hline
    \ref{line:bookkeeper-op-9}&\Oc{}&
\\ \hline
    \ref{line:bookkeeper-op-10}&\Oc{}&\Tbref{tab:bookkeeper-time},
        \Thref{th:procedure-overhead-discard}.
        Zero indirect time.
\\ \hline
    \ref{line:bookkeeper-op-11}&\Oc{}&
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
    Empty justifications are considered immediately obvious.
    See \AFref{alg:bookkeeper-op}.
}
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{theorem}[Transition Array Creation]
\label{th:transition-creation}
The resource incurred by
the \op{Bookkeeper} op
may be charged as to
the \var{eim} account of each \type{EIM}
in \VVelement{S}{j}
as \Oc{} time,
\Oc{} trimmed space,
and zero link space.
\end{theorem}

\begin{proof}
Since there is at most one entry for every symbol
in the grammar,
the number of array entries in \var{transition} is,
\order{\size{\Vocab{\var{g}}}}.
\Vocab{\var{g}} is a constant which depends on \Vint{g},
and the \var{transition} array itself,
excluding the linked lists,
is therefore of size \Oc{}.
From this it follows immediately that the
Per-\type{ES} creation of \var{transition}
consumes \Oc{} time and \Oc{} space.
when the \type{PIM}s are created.
The lists grow from the head, so that
creation time for a new link in one of the
\var{transitions}'s linked lists is \Oc{}.
\myqed
\end{proof}

\todo{TO HERE Sun 08 Nov 2020 04:36:21 PM EST}

Writes to the \var{transition} array
do not occur once it has been created.

Reads of the \var{transition} array
use the \Transition{} function.
A call of the form
$\var{Transitions}{\Vsym{t}}{\VVelement{S}{j}}$
returns the set of \type{PIM}s in \VVelement{S}{j} whose transition symbol
is \Vsym{t}.

\begin{theorem}[\myfn{Transitions}{} Complexity]
\label{th:transitions-read}
A call of the form
\Transitions{\Vsym{t}}{\VVelement{S}{j}}
consumes \Oc{} time, and zero space.
It returns a pointer to
a linked list of \order{\Vloc{j}} size,
that is,
\begin{gather*}
\forall\; \Vsym{t} : \forall\; \var{j} \in \Dom{\var{S}} :
\\ \size{\Transitions{\Vsym{t}}{\VVelement{S}{j}}} = \order{\Vloc{j}}
\end{gather*}
\end{theorem}

\begin{proof}
A call of the function \myfn{Transitions}{}
searches \var{transition} for
\VVelement{S}{j}, a binary array of \Oc{} size, and returns a pointer
to the head of a linked list:
this can be done in \Oc{} time.
\Transitions{\Vsym{t}}{\VVelement{S}{j}} returns a linked link of
the \type{PIM}s at
\VVelement{S}{j}, so that
the claim for linked list size follows from
the ES \type{EIM} Count Theorem \Thref{th:es-eim-count},
and the ES \type{LIM} Count Theorem \Thref{th:es-lim-count}.
A \var{transitions} array is not altered after its creation,
and therefore calls of \myfn{Transitions}{}
do not consume additional space.
\myqed
\end{proof}

\todo{Write ES-setup op section}

\section{Initializer op}
\label{sec:initializer}

\begin{algorithm}[t]
\caption{\var{Predict1}}
\label{alg:predict1}
\begin{algorithmic}[1]
\Procedure{Predict1}{\Veim{prediction}}
\If{$\VVelement{dotZeroUsed}{\ID{\Rule{\var{prediction}}}} = \var{true}$}
\label{line:marpa-predict-65}
\Statex \Comment \tightMath{\var{x}=1\times\var{try}}
  \State return \Comment \tightMath{\var{x}=1\times\var{dup}}
\label{line:marpa-predict-70}
\EndIf
\label{line:marpa-predict-75}
\State $\LinkPairs{\var{prediction}} = \set{}$ \Comment \tightMath{\var{x}=1\times\var{new}}
\label{line:marpa-predict-80}
\State Add \var{prediction} to \Velement{S}{\Current{\var{prediction}}}
\Statex  \Comment \tightMath{\var{s}=\Oc;\var{x}=1\times\var{new}}
\label{line:marpa-predict-85}
\State $\VVelement{dotZeroUsed}{\ID{\var{\Rule{\var{prediction}}}}} \gets \var{true}$
   \Comment \tightMath{\var{x}=1\times\var{new}}
\label{line:marpa-predict-90}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The \op{Initializer} op adds the start \type{EIM} and its predictions.
We first look at
Algorithms \Aref{alg:predict1} and \Aref{alg:predict-es},
which contain the pseudocode for adding predictions.

\begin{theorem}[\var{Predict1} Invocations]
\label{th:predict1-invocs}
\Aref{alg:predict1},
\begin{enumerate}
\item \label{enum:th-predict1-invocs-04}
Lines \enRange{\ref{line:marpa-predict-65}}{\ref{line:marpa-predict-70}}
of \var{Predict1}
are invoked once.
\item \label{enum:th-predict1-invocs-05}
Lines
\enRange{\ref{line:marpa-predict-75}}{\ref{line:marpa-predict-90}}
of \var{Predict1}
are invoked once per add, and at most once.
\item \label{enum:th-predict1-invocs-06}
For every line of \var{Predict1},
the number of invocations per call is \Oc{}.
\end{enumerate}
\end{theorem}

\begin{proof}
We have
\ref{enum:th-predict1-invocs-04}
and
\ref{enum:th-predict1-invocs-05}
of \Thref{th:predict1-invocs}
directly from \Aref{alg:predict1}.
\ref{enum:th-predict1-invocs-06}
of \Thref{th:predict1-invocs}
follows from
\ref{enum:th-predict1-invocs-04}
and
\ref{enum:th-predict1-invocs-05}
of \Thref{th:predict1-invocs}.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-predict1-time-charge}
\myparbox{%
  Time in \proc{Predict1} is charged to the
  caller
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\proc{Predict1} Time]
\label{th:predict1-time}
The time of \var{Predict1}
is \Oc{}, charged to the caller.
\end{theorem}

\begin{proof}
All lines of \proc{Predict1} take \Oc{} time.
For the justification in the case of lines
\ref{line:marpa-predict-65} and
\ref{line:marpa-predict-90},
see
\Thref{th:rule-id-complexity} and
\Thref{th:dotzeroused-bitmap}.
For the justification of line
\ref{line:marpa-predict-85},
see \Thref{th:es-complexity}.
For other lines, the time complexity follows
directly from \Aref{alg:predict1}.

Total time complexity is the sum over all lines of \proc{Predict1} of
time complexity times the number of invocations, or
\begin{equation}
\label{eq:th-predict1-time}
\sum_{\var{line}=\ref{line:marpa-predict-65}}
^{\ref{line:marpa-predict-90}} \Oc{} \times \Oc{}
= \Oc{}.
\end{equation}
Per \Eref{eq:def-predict1-time-charge},
the \Oc{} time of \Eref{eq:th-predict1-time}
is charged
to the caller.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-predict1-space-charge}
\myparbox{%
  Space in \var{Predict1} is charged to the
  prediction it adds
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\var{Predict1} Space]
\label{th:predict1-space}
Space for \var{Predict1} is
\Oc{} charged to the prediction it adds.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-predict1-space-10}
\myparbox{%
  All lines of \proc{Predict1},
  other than
  \ref{line:marpa-predict-85},
  consume zero space
  \cuz{}
  \Thref{th:rule-id-complexity},
  \Thref{th:dotzeroused-bitmap},
  \Aref{alg:predict1}.}
\end{equation}
\begin{equation}
\label{eq:th-predict1-space-15}
\myparbox{%
  Line \ref{line:marpa-predict-85}
  consumes \Oc{} space
  \cuz{} \Thref{th:es-complexity}.}
\end{equation}
\begin{equation}
\label{eq:th-predict1-space-20}
\myparbox{%
  Line \ref{line:marpa-predict-85}
  is invoked only when a prediction is added \cuz{}
  \Eref{enum:th-predict1-invocs-05} of \Thref{th:predict1-invocs}.}
\end{equation}
\begin{equation}
\label{eq:th-predict1-space-25}
\myparbox{%
  \proc{Predict1} consumes \Oc{} space for each prediction added
  \cuz{}
  \Eref{eq:th-predict1-space-10},
  \Eref{eq:th-predict1-space-15},
  \Eref{eq:th-predict1-space-20}.}
\end{equation}
\begin{equation}
\myparbox{%
  Space for \proc{Predict1} is \Oc{} space
  per prediction added,
  which is charged to the prediction that it adds
  \cuz{} \Eref{eq:def-predict1-space-charge},
    \Eref{eq:th-predict1-space-25}.
  \quad \myqed}
\end{equation}
\end{proof}

\begin{algorithm}[t]
\caption{\var{Predict-ES}}
\label{alg:predict-es}
\begin{algorithmic}[1]
\Procedure{Predict-ES}{\Ves{es}}
\label{line:marpa-predict-es-00}
\State Allocate \var{dotZeroUsed}
   \Comment \tightMath{\var{s}=\Oc}
\label{line:marpa-predict-es-05}
\State $\forall \; \Vrule{r}, \VVelement{dotZeroUsed}{\ID{\var{r}}} \gets \var{true}$
\label{line:marpa-predict-es-10}
\Statex \hspace\algorithmicindent
    $\triangleright$ \rawparcomment[4.3in]{Loop over every \Veim{x} in \Ves{es},
    including those that \var{Predict1} adds while the loop
    is running.
    }
\For{$\Veim{x} \in \Ves{es}$}
       \Comment \tightMath{\var{x}=\order{\Current{\var{es}}}}
\label{line:marpa-predict-es-12}
  \For{$\Vrule{r} \in \set{\Vrule{r} : \Postdot{\var{x}} = \LHS{\var{r}}}$}
  \label{line:marpa-predict-es-14}
  \Statex \Comment \tightMath{\var{x}=\order{\Current{\var{es}}}}
    \State $\Veim{pred} \gets \tuple{ \dr{\tuple{\Vrule{r}, 0}}, \Current{\var{es}} } @ \Current{\var{es}}$
      \label{line:marpa-predict-es-16}
       \Statex \Comment \tightMath{\var{x}=\order{\Current{\var{es}}}}
    \State \Call{Predict1}{\var{pred}}
       \Comment \tightMath{\text{Ind};\var{x}=\order{\Current{\var{es}}}}
    \label{line:marpa-predict-es-18}
  \EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\proc{Predict-ES} Invocations]
\label{th:predict-es-invocations}
Consider \proc[\Ves{es}]{Predict-ES}.
\begin{enumerate}
\item
  \label{enum:th-predict-es-invocations-10}
  There are \Oc{} invocations per Earley set
  \begin{itemize}
    \item for each of lines
      \enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}; and
    \item for the final, test-failure, invocation of
      line \ref{line:marpa-predict-es-12}.
  \end{itemize}
\item
  \label{enum:th-predict-es-invocations-20}
  There are \Oc{} invocations for each $\Veim{x} \in \Ves{es}$
  \begin{itemize}
    \item for the non-final, successful, invocations of
      line \ref{line:marpa-predict-es-12}; and
    \item for each of lines
      \enRange{\ref{line:marpa-predict-es-14}}{\ref{line:marpa-predict-es-18}}.
  \end{itemize}
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-predict-es-invocations-10}
\Invocs{\ref{line:marpa-predict-es-05}} = \Oc{} \because
   \Aref{alg:predict-es}, \text{sequential execution}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-12}
\Invocs{\ref{line:marpa-predict-es-10}} = \Oc{} \because
    \Eref{eq:th-predict-es-invocations-10},
    \text{sequential execution}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-16}
  \Invocs{\text{loop } \enRefs{line:marpa-predict-es-12}{line:marpa-predict-es-18}} = \Oc{}
    \because
    \Eref{eq:th-predict-es-invocations-12},
    \text{sequential execution}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-16-10}
\myparbox{%
   There is exactly one final, test-failure invocation of
    line \ref{line:marpa-predict-es-12} \cuz{}
    \Obref{def:loop-invocation}.
    \Eref{eq:th-predict-es-invocations-16},
    \text{sequential execution}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-16-20}
\myparbox{%
   There is exactly one successful invocation of
    line \ref{line:marpa-predict-es-12} for each
    $\Veim{x} \in \Ves{es}$
    \cuz{} \Obref{def:loop-invocation}.}
\end{equation}
In the rest of this proof,
we will write $\Passes{\var{lp}}/\Veim{x}$
to say ``passes of loop \var{lp} per \Veim{x}'';
and $\Invocs{\var{ln}}/\Veim{x}$
to say ``invocations of line \var{ln} per \Veim{x}''.
\begin{equation}
\label{eq:th-predict-es-invocations-18}
  \Passes{\text{loop } \enRefs{line:marpa-predict-es-12}{line:marpa-predict-es-18}} / \Veim{x} = \Oc
    \because \Eref{eq:th-predict-es-invocations-16-20}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-24}
  \Invocs{\text{loop } \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}} / \Veim{x}
    = \Oc
    \because
    \Eref{eq:th-predict-es-invocations-18},
    \text{sequential execution}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-26}
\begin{gathered}
  \size{\set{\Vrule{r} : \Postdot{\var{x}} = \LHS{\var{r}}}} = \Oc
  \\
  \because \text{ \Rules{\var{g}} is a constant depending on \Vint{g}.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-28}
  \begin{gathered}
      \Passes{\text{loop } \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}}
          / \Veim{x}
    \\
      = \Invocs{\text{loop } \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}}
          / \Veim{x}
      \times
      \size{\set{\Vrule{r} : \Postdot{\var{x}}}}
    \\
      \because \Obref{def:loop-invocation}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-29}
  \begin{gathered}
  \Passes{\text{loop } \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}} / \Veim{x}
      = \Oc{} \times \Oc = \Oc{} \\
    \because
    \Eref{eq:th-predict-es-invocations-24},
    \Eref{eq:th-predict-es-invocations-26},
    \Eref{eq:th-predict-es-invocations-28}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-30}
  \begin{gathered}
    \Invocs{\ref{line:marpa-predict-es-14}}/\Veim{x} =
      \Passes{\text{loop } \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}}/\Veim{x}
      +1
    \\
    \because
    \Obref{def:loop-invocation}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-32}
  \begin{gathered}
    \Invocs{\ref{line:marpa-predict-es-14}}/\Veim{x} =
      \Oc+1 = \Oc{}
    \\
    \because
    \Eref{eq:th-predict-es-invocations-29},
    \Eref{eq:th-predict-es-invocations-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-34}
    \Invocs{\ref{line:marpa-predict-es-16}}/\Veim{x} = \Oc{}
    \because \Eref{eq:th-predict-es-invocations-28},
    \text{ sequential execution.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-invocations-36}
    \Invocs{\ref{line:marpa-predict-es-18}}/\Veim{x} = \Oc{}
    \because \Eref{eq:th-predict-es-invocations-34},
    \text{ sequential execution.}
\end{equation}
We have \ref{enum:th-predict-es-invocations-10}
of \Thref{th:predict-es-invocations}
from \Eref{eq:th-predict-es-invocations-10},
  \Eref{eq:th-predict-es-invocations-12},
  \Eref{eq:th-predict-es-invocations-16-10}.
We have \ref{enum:th-predict-es-invocations-20}
of \Thref{th:predict-es-invocations}
from \Eref{eq:th-predict-es-invocations-16-20},
  \Eref{eq:th-predict-es-invocations-32},
  \Eref{eq:th-predict-es-invocations-34},
  \Eref{eq:th-predict-es-invocations-36}.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-predict-es-time-charge}
\myparbox{%
  Time in \var{Predict-ES} is charged to the
  Earley item which is the loop variable of
  the outer loop, when it is in scope,
  and to the
  Earley set, otherwise
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\var{Predict-ES} time complexity]
\label{th:predict-es-time}
The inclusive time of $\var{Predict-ES}(\VVelement{S}{j})$
\Aref{alg:predict-es}
is
\Oc{} charged to each \type{EIM} of \VVelement{S}{j},
and \Oc{} charged to \VVelement{S}{j} itself.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-predict-es-time-10}
\myparbox{%
  \Rules{\Vcfg{g}} is a constant depending on \var{g}
  \cuz{}~\longDfref{CFG}{def:context-free-grammar}.%
}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-12}
\myparbox{%
  The loop variable for
  loop \enRefs{line:marpa-predict-es-12}{line:marpa-predict-es-18} is in scope
  for all invocations of
  lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18};
  for all but the final test invocation of
  line \ref{line:marpa-predict-es-12};
  and only for those invocations.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-14}
\Time{\ref{line:marpa-predict-es-05}} = \Oc{}
  \because \Thref{th:dotzeroused-bitmap}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-16}
\Time{\ref{line:marpa-predict-es-10}} = \Oc{}
  \because
  \Thref{th:rule-id-complexity},
  \Thref{th:dotzeroused-bitmap},
  \Eref{eq:th-predict-es-time-10}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-18}
\Time{\ref{line:marpa-predict-es-12}} = \Oc{}
   \because \Thref{th:es-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-20}
\Time{\ref{line:marpa-predict-es-14}} = \Oc{}
  \because \Thref{th:rule-id-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-22}
\Time{\ref{line:marpa-predict-es-16}} = \Oc{}
  \because \text{ direct from line \ref{line:marpa-predict-es-16}}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-24}
\Time{\ref{line:marpa-predict-es-18}} = \Oc{}
  \because \Thref{th:predict-es-time}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-26}
\myparbox{%
   Total exclusive time for lines \enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}
   is the number of invocations of each multiplied by the time for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-28}
\begin{gathered}
   \myparbox{%
     Total exclusive time for lines \enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}
   is}
   \\
     \smashoperator[r]{\sum_{\textstyle{
       \var{line}\in\set{\enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}}
     }}}
     \quad \Oc{} \times \Oc{} = \Oc{}
   \\
     \because
     \ref{enum:th-predict-es-invocations-10}\text{ of }\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-time-14},
     \Eref{eq:th-predict-es-time-16},
     \Eref{eq:th-predict-es-time-26}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-time-30}
\myparbox{%
   Total exclusive time for the final invocation of
   line \ref{line:marpa-predict-es-12} is \Oc{}
   \cuz{} \Eref{eq:th-predict-es-time-18}.
}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-time-32}
\myparbox{%
   Total exclusive time for \proc{Predict-ES}
   charged to the Earley set is
   $\Oc{} + \Oc{} = \Oc{}$
   \cuz{}
  \Eref{eq:def-predict-es-time-charge},
  \Eref{eq:th-predict-es-time-12},
  \Eref{eq:th-predict-es-time-28},
  \Eref{eq:th-predict-es-time-30}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-34}
\myparbox{%
   Total exclusive time
   per \Veim{x}
   for lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}
   is the number of invocations
   multiplied by the time for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-36}
\begin{gathered}
   \myparbox{%
     Total exclusive time
     per \Veim{x}
     for lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18} is
     }
   \\
     \smashoperator[r]{\sum_{\textstyle{
       \var{line}\in\set{\enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}}
     }}}
     \quad \Oc{} \times \Oc{} = \Oc{}
   \\
     \because
     \ref{enum:th-predict-es-invocations-20}\text{ of }\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-time-20},
     \Eref{eq:th-predict-es-time-22},
     \Eref{eq:th-predict-es-time-24},
     \Eref{eq:th-predict-es-time-34}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-time-38}
\myparbox{%
   Total exclusive time for
   line \ref{line:marpa-predict-es-12}
   for each \Veim{x}
   is \Oc{}
   \cuz{} \ref{enum:th-predict-es-invocations-20}~of~\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-time-18}.
}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-40}
\myparbox{%
   Total exclusive time for \proc{Predict-ES}
   charged to each \Veim{x} is
   $\Oc{} + \Oc{} = \Oc{}$
   \cuz{}
  \Eref{eq:def-predict-es-time-charge},
  \Eref{eq:th-predict-es-time-12},
  \Eref{eq:th-predict-es-time-36},
  \Eref{eq:th-predict-es-time-38}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-42}
\myparbox{%
  The only indirect time is \Oc{} at
       line \ref{line:marpa-predict-es-18}
      \cuz{}
      \Obref{obs:direct-vs-indirect},
      \Aref{alg:predict-es}, \Thref{th:predict1-time}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-44}
\myparbox{%
  The indirect time of \proc{Predict-ES} is \Oc{} per \Veim{x},
       which is re-charged directly to each \Veim{x}
       \cuz{} \Eref{eq:def-predict-es-time-charge}, \Eref{eq:th-predict-es-time-42}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-time-48}
\myparbox{%
  The time charged by \proc{Predict-ES} to each \Veim{x} is
  $\Oc + \Oc = \Oc$
  \cuz
  \Eref{eq:th-predict-es-time-12},
  \Eref{eq:th-predict-es-time-40},
  \Eref{eq:th-predict-es-time-44}.
}
\end{equation}
The theorem follows directly from
\Eref{eq:th-predict-es-time-32}
and \Eref{eq:th-predict-es-time-48}.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-predict-es-space-charge}
\myparbox{%
  Direct space in \var{Predict-ES} is charged to the
  Earley item which is the loop variable of
  the outer loop, when it is in scope,
  and to the
  Earley set, otherwise
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\var{Predict-ES} Space Complexity]
\label{th:predict-es-space}
The space consumed by
\proc[\VVelement{S}{j}]{Predict-ES}
is \Oc{} charged to \VVelement{S}{j}
and \Oc{} charged to each \type{EIM} added.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-predict-es-space-12}
\myparbox{%
  The loop variable for
  loop \enRefs{line:marpa-predict-es-12}{line:marpa-predict-es-18} is in scope
  for all invocations of
  lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18};
  for all but the final test invocation of
  line \ref{line:marpa-predict-es-12};
  and only for those invocations.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-14}
\Space{\ref{line:marpa-predict-es-05}} = \Oc
  \because \Thref{th:dotzeroused-bitmap}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-16}
\Space{\ref{line:marpa-predict-es-10}} = 0
  \because
  \Thref{th:rule-id-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-18}
\Space{\ref{line:marpa-predict-es-12}} = 0
   \because \Thref{th:es-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-20}
\Space{\ref{line:marpa-predict-es-14}} = 0
  \because \Thref{th:rule-id-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-22}
\Space{\ref{line:marpa-predict-es-16}} = 0
  \because \text{ direct from line \ref{line:marpa-predict-es-16}}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-24}
\Space{\ref{line:marpa-predict-es-18}} = 0
  \because \Thref{th:predict1-space}.
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-26}
\myparbox{%
   Total exclusive space for lines \enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}
   is the number of invocations of each multiplied by the space for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-28}
\begin{gathered}
   \myparbox{%
     Total exclusive space for lines \enRefs{line:marpa-predict-es-05}{line:marpa-predict-es-10}
   is}
   \\
     \left(
         \smashoperator[r]{\sum_{\textstyle{
           \var{line}\in\set{\ref{line:marpa-predict-es-05}}
         }}}
         \;\; \Oc \times \Oc
     \right)
     + \left(
     \smashoperator[r]{\sum_{\textstyle{
       \var{line}\in\set{\enRefs{line:marpa-predict-es-10}{line:marpa-predict-es-18}}
     }}}
     \;\; 0 \times \Oc
     \right)
     = \Oc
   \\
     \because
     \ref{enum:th-predict-es-invocations-10}\text{ of }\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-space-14},
     \Eref{eq:th-predict-es-space-16},
     \Eref{eq:th-predict-es-space-26}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-space-30}
\myparbox{%
   Total exclusive space for the final invocation of
   line \ref{line:marpa-predict-es-12} is 0
   \cuz{} \Eref{eq:th-predict-es-space-18}.
}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-space-32}
\myparbox{%
   Total exclusive space for \proc{Predict-ES}
   charged to the Earley set is
   $\Oc{} + 0 = \Oc{}$
   \cuz{}
  \Eref{eq:def-predict-es-space-charge},
  \Eref{eq:th-predict-es-space-12},
  \Eref{eq:th-predict-es-space-28},
  \Eref{eq:th-predict-es-space-30}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-34}
\myparbox{%
   Total exclusive space
   per \Veim{x}
   for lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}
   is the number of invocations
   multiplied by the space for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-36}
\begin{gathered}
   \myparbox{%
     Total exclusive space
     per \Veim{x}
     for lines \enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18} is
     }
   \\
     \smashoperator[r]{\sum_{\textstyle{
       \var{line}\in\set{\enRefs{line:marpa-predict-es-14}{line:marpa-predict-es-18}}
     }}}
     \;\; \Oc \times 0 = 0
   \\
     \because
     \ref{enum:th-predict-es-invocations-20}\text{ of }\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-space-20},
     \Eref{eq:th-predict-es-space-22},
     \Eref{eq:th-predict-es-space-24},
     \Eref{eq:th-predict-es-space-34}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-predict-es-space-38}
\myparbox{%
   Total exclusive space for
   line \ref{line:marpa-predict-es-12}
   for each \Veim{x}
   is 0
   \cuz{} \ref{enum:th-predict-es-invocations-20}~of~\Thref{th:predict-es-invocations},
     \Eref{eq:th-predict-es-space-18}.
}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-40}
\myparbox{%
   Total exclusive space for \proc{Predict-ES}
   charged to each \Veim{x} is
   $0 + 0 = 0$
   \cuz{}
  \Eref{eq:def-predict-es-space-charge},
  \Eref{eq:th-predict-es-space-12},
  \Eref{eq:th-predict-es-space-36},
  \Eref{eq:th-predict-es-space-38}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-42}
\myparbox{%
  The only indirect space is \Oc{} at
       line \ref{line:marpa-predict-es-18}
      \cuz{}
      \Obref{obs:direct-vs-indirect},
      \Aref{alg:predict-es}, \Thref{th:predict1-space}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-44}
\myparbox{%
  The indirect space of \proc{Predict-ES} is \Oc{} per prediction added,
       which is charged to the prediction it adds.
       \cuz{} \Aref{alg:predict-es}, \Thref{th:predict1-space},
           \Eref{eq:th-predict-es-space-42}.}
\end{equation}
\begin{equation}
\label{eq:th-predict-es-space-48}
\myparbox{%
  The space charged by \proc{Predict-ES} to each \Veim{x} is
  $0 + 0 = 0$
  \cuz{}
  \Eref{eq:th-predict-es-space-12},
  \Eref{eq:th-predict-es-space-40},
  \Eref{eq:th-predict-es-space-44}.
}
\end{equation}
The theorem follows directly from
\Eref{eq:th-predict-es-space-32},
\Eref{eq:th-predict-es-space-44},
and \Eref{eq:th-predict-es-space-48}.
\myqed
\end{proof}

If precomputation is used
\Aref{alg:predict-es} can be sped up using Warshall's algorithm.
A version of \proc{Predict-ES} using precomputation
will clearly run no worse than
\proc{Predict-ES}.

\begin{observation}[Ancestry]
\label{obs:common-predict-ancestry}
The \Marpa{} implementation
does not record link pairs for predictions.
Tracking link pairs for a prediction would be cumbersome,
because, even in an unambiguous grammar,
each prediction can have many reasons to be in an Earley
set.

Conceptually,
predictions have predecessors, but no causes,
and the link pair generated for
\Veim{prediction} in
line \ref{line:marpa-predict-80} of \proc{Predict1}~\Aref{alg:predict1}
would be
$\tuple{\Veim{x}, \Lambda}$,
where \Veim{x}
is the variable of line \ref{line:marpa-predict-es-16}
of \proc{Predict-ES}~\Aref{alg:predict-es},
which is not actually in scope at
line \ref{line:marpa-predict-80} of \proc{Predict1}.

If link pairs were generated,
for the arbitrary prediction
\begin{equation}
\left[
  \left[\Vsym{lhs} \de \mydot \Vstr{rhs}\right],
  \Vorig{j}
\right]
\end{equation}
they would be
\begin{equation}
\label{eq:common-predict-ancestry-10}
\left\lbrace
  \begin{gathered}
  \tuple{\Veim{pred}, \Lambda}_\type{LP} : \\
  \Current{\var{pred}} = \Vloc{j}
  \; \land \; \Postdot{\var{pred}} = \Vsym{lhs}.
  \end{gathered}
\right\rbrace
\end{equation}
The computation of
\Eref{eq:common-predict-ancestry-10}
could be sped up using
the transition array
\Thref{th:transition-array} for \VVelement{S}{j}.
\dispEnd{}
\end{observation}

\begin{algorithm}[t]
\caption{\op{Initializer}}
\label{alg:initializer-op}
\begin{algorithmic}[1]
\Procedure{Initializer}{}
\State $\Veim{start} \gets \tuple{\dr{\tuple{\Vrule{accept}, 0}}, 0}$
\label{line:initializer-op-10}
\Statex \Comment \Vrule{accept} is the accept rule of \Vint{g} \Eref{eq:def-accept-rule}.
\State $\LinkPairs{\Veim{start}} = \set{}$
\label{line:initializer-op-20}
\State Add \Veim{start} to \es{(0)}
  \Comment \tightMath{\var{s}=\Oc}
\label{line:initializer-op-30}
\State $\VVelement{dotZeroUsed}{\ID{\Vrule{accept}}} \gets \var{true}$
\label{line:initializer-op-60}
\State \Call{Predict-ES}{\Ves{(0)}} \Comment Ind
\label{line:initializer-op-70}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\proc{Initializer} invocations]
\label{th:initializer-invocations}
Every line of \proc{Ini\-tial\-izer} is invoked exactly once.
\end{theorem}

\begin{proof}
The flow of control in
\proc{Initializer}~\Aref{alg:initializer-op}
consists entirely of sequential executions.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-initializer-time-charge}
\myparbox{%
  Direct time in \proc{Ini\-tial\-izer} is charged to the
  start \type{EIM}
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\op{Initializer} time complexity]
\label{th:initializer-time}
The inclusive time of $\op{Initializer}$
is
\Oc{} charged to each \type{EIM} of Earley set 0.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-initializer-time-14}
\Time{\ref{line:initializer-op-10}} = \Oc{}
  \because \text{ direct from \Aref{alg:initializer-op}.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-16}
\Time{\ref{line:initializer-op-20}} = \Oc{}
  \because \text{ direct from \Aref{alg:initializer-op}.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-18}
\Time{\ref{line:initializer-op-30}} = \Oc{}
   \because \Thref{th:es-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-20}
\Time{\ref{line:initializer-op-60}} = \Oc{}
  \because
  \Thref{th:rule-id-complexity},
  \Thref{th:dotzeroused-bitmap}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-22}
\Time{\ref{line:initializer-op-70}} = \Oc{}
  \because \Obref{obs:housekeeping-vs-body-execution},
    \Thref{th:predict-es-time}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-24}
\myparbox{%
   Total exclusive time
   for \op{Initializer}
   is the number of invocations of each line
   multiplied by the exclusive time for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-26}
\myparbox{%
     Total exclusive time
     by \op{Initializer} is
   \[
     \smashoperator[r]{\sum_{\textstyle{
       \var{line}\in\set{\enRefs{line:initializer-op-10}{line:initializer-op-70}}
     }}}
     \;\; \Oc \times \Oc
   \]
     \cuz{}
     \Thref{th:initializer-invocations},
     \Eref{eq:th-initializer-time-14},
     \Eref{eq:th-initializer-time-16},
     \Eref{eq:th-initializer-time-18},
     \Eref{eq:th-initializer-time-20},
     \Eref{eq:th-initializer-time-22},
     \Eref{eq:th-initializer-time-24}.
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-26-50}
\myparbox{%
     Total exclusive time
     by \op{Initializer} is \Oc{},
     which is charged to \Veim{start}
     \cuz{}
        \Eref{eq:def-initializer-time-charge},
        \Eref{eq:th-initializer-time-26}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-27}
\myparbox{%
   Indirect time is charged by \op{Initializer}
   only at line \ref{line:initializer-op-70}.
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-28}
\myparbox{%
    Total indirect time charged to \Veim{start}
    by \op{Initializer} is \Oc{}
    \cuz{}
       \Thref{th:predict-es-time},
       \longDfref{Charging to the ES}{def:charging-to-es},
       \Eref{eq:th-initializer-time-27}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-30}
\myparbox{%
    Total indirect time charged
    to every \type{EIM} of Earley set 0
    by \op{Initializer}
    is \Oc{}
    \cuz{} \Thref{th:predict-es-time},
    \Eref{eq:th-initializer-time-27}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-32}
\myparbox{%
    Total time charged to every \type{EIM} of Earley set 0
    by \op{Initializer} is \Oc{}
    \cuz{}
    \Eref{eq:def-initializer-time-charge},
    \Eref{eq:th-initializer-time-30}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-time-34}
\myparbox{%
    Total time charged to \Veim{start}
     by \op{Initializer} is $\Oc+\Oc+\Oc=\Oc$
   \cuz{}
       \Eref{eq:def-initializer-time-charge},
       \Eref{eq:th-initializer-time-26-50},
       \Eref{eq:th-initializer-time-28},
       \Eref{eq:th-initializer-time-30}.%
}
\end{equation}
The theorem follows from
\Eref{eq:th-initializer-time-32} and
\Eref{eq:th-initializer-time-34}.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-initializer-space-charge}
\myparbox{%
  Direct space in \proc{Ini\-tial\-izer} is charged to the
  start \type{EIM}.
  \cuz{} AF complexity analysis.}
\end{equation}

\begin{theorem}[\op{Initializer} space complexity]
\label{th:initializer-space}
The inclusive space of $\op{Initializer}$
is
\Oc{} charged to each \type{EIM} of Earley set 0.
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-initializer-space-14}
\Space{\ref{line:initializer-op-10}} = 0
  \because \text{ direct from \Aref{alg:initializer-op}.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-16}
\Space{\ref{line:initializer-op-20}} = 0
  \because \text{ direct from \Aref{alg:initializer-op}.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-18}
\Space{\ref{line:initializer-op-30}} = \Oc{}
   \because \Thref{th:es-complexity}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-20}
\Space{\ref{line:initializer-op-60}} = 0
  \because
  \Thref{th:rule-id-complexity},
  \Thref{th:dotzeroused-bitmap}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-22}
\Space{\ref{line:initializer-op-70}} = 0
  \because \Obref{obs:housekeeping-vs-body-execution},
    \Thref{th:predict-es-space}.
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-24}
\myparbox{%
   Total exclusive space
   for \op{Initializer}
   is the number of invocations of each line
   multiplied by the exclusive space for each invocation.}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-26}
\myparbox{%
     Total exclusive space
     by \op{Initializer} is
   \[
     \left(
         \smashoperator[r]{\sum_{\textstyle{
           \var{line}\in\set{\ref{line:initializer-op-30}
           }
         }}}
         \;\; \Oc \times \Oc
     \right)
     + \left(
         \smashoperator[r]{\sum_{\textstyle{
           \var{line}\in\set{
               \enRefs{line:initializer-op-10}{line:initializer-op-20},
               \enRefs{line:initializer-op-60}{line:initializer-op-70}
           }
         }}}
         \;\; \Oc \times 0
     \right)
   \]
     \cuz{}
     \Thref{th:initializer-invocations},
     \Eref{eq:th-initializer-space-14},
     \Eref{eq:th-initializer-space-16},
     \Eref{eq:th-initializer-space-18},
     \Eref{eq:th-initializer-space-20},
     \Eref{eq:th-initializer-space-22},
     \Eref{eq:th-initializer-space-24}.
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-26-50}
\myparbox{%
     Total exclusive space
     by \op{Initializer} is \Oc{},
     which is charged to \Veim{start}
     \cuz{}
        \Eref{eq:def-initializer-space-charge},
        \Eref{eq:th-initializer-space-26}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-27}
\myparbox{%
   Indirect space is charged by \op{Initializer}
   only at line \ref{line:initializer-op-70}.
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-28}
\myparbox{%
    Total indirect space
    consumed by \op{Initializer} is
    \Oc{} charged to \Veim{start},
    and \Oc{} charged to every prediction added by \proc{Predict-ES}
    \cuz{}
       \Thref{th:predict-es-space},
       \Eref{eq:th-initializer-space-27}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-32}
\myparbox{%
    Total space charged by \op{Initializer}
    is
    $\Oc+\Oc=\Oc$ charged to \Veim{start},
    and \Oc{} charged to every prediction added by \proc{Predict-ES}
    \cuz{}
    \Eref{eq:def-initializer-space-charge},
    \Eref{eq:th-initializer-space-26-50},
    \Eref{eq:th-initializer-space-28}.%
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-36}
\myparbox{%
    Every \type{EIM} added by
    \op{Initializer} is either \Veim{start}
    or a prediction \cuz
    \Aref{alg:predict1},
    \Aref{alg:predict-es},
    \Aref{alg:initializer-op}.
}
\end{equation}
\begin{equation}
\label{eq:th-initializer-space-38}
\myparbox{%
    The space consumed by \op{Initializer} is \Oc{}
    for each new,
    which is charged to that \type{EIM} \cuz{}
    \Eref{eq:th-initializer-space-32},
    \Eref{eq:th-initializer-space-36}.
}
\end{equation}
The theorem is
\Eref{eq:th-initializer-space-38}.
\myqed
\end{proof}

\begin{observation}[Ancestry]
\label{obs:initializer-ancestry}
The \Marpa{} implementation does not keep
any link pairs for any \type{PIM} in Earley set 0.
\Veim{start} does not have any non-trivial ancestor,
so that a link pair does not exist for it,
even conceptually.
And \Marpa{} does not track link pairs for
predictions
\Obref{obs:common-predict-ancestry}.
\dispEnd{}
\end{observation}

\begin{observation}[\Earley{}'s Earley set 0]
\label{obs:earley-set-0}
\Marpa{}'s % \linebreak
\op{Ini\-tial\-iza\-tion} op has the same effect as the construction
of Earley set 0 by \Earley{}.
\dispEnd{}
\end{observation}

\section{Predictor op}
\label{sec:predictor-op}

The \Marpa{} Predictor op is equivalent to the
\proc{Predict-ES} pseudocode procedure \Aref{alg:predict-es}.

\begin{equation}
\label{eq:def-predictor-op}
\op[\VVelement{S}{j}]{Predictor} \;\; \defined \;\; \proc[\VVelement{S}{j}]{Predict-ES}
\end{equation}

\begin{theorem}[\op{Predictor} time complexity]
\label{th:predictor-time}
The time of $\op{Predictor}(\VVelement{S}{j})$
is
\Oc{} charged to each \type{EIM} of \VVelement{S}{j},
and \Oc{} charged to \VVelement{S}{j} itself.
\end{theorem}

\begin{proof}
The theorem follows from
\Thref{th:predict-es-time} and \Eref{eq:def-predictor-op}.
\end{proof}

\begin{theorem}[\op{Predictor} space complexity]
\label{th:predictor-space}
The space of $\op{Predictor}(\VVelement{S}{j})$
\Aref{alg:predict-es}
is
\Oc{} charged to each \type{EIM} of \VVelement{S}{j},
and \Oc{} charged to each \type{PIM} added.
\end{theorem}

\begin{proof}
The theorem follows from
\Thref{th:predict-es-space} and \Eref{eq:def-predictor-op}.
\end{proof}

\begin{observation}[Ancestry]
\label{obs:predictor-ancestry}
The \Marpa{} implementation does not track
link pairs
for predictions.
For details, see \Obref{obs:common-predict-ancestry}.
\dispEnd{}
\end{observation}

\begin{observation}[Original Earley algorithm: Predictor Operation]
\label{obs:earley-predictor-op}
\Marpa{}'s predictor op has the same effect as the ``Predictor''
operation of \Earley{}\cite{Earley1970}.
\dispEnd{}
\end{observation}

\section{Scanner op}
\label{def:scanner}

\begin{algorithm}[t]
\caption{\var{Scanner}}
\label{alg:scanner-op}
\begin{algorithmic}[1]
\Procedure{Scanner}{\VVelement{S}{j}}
\Statex \lcomment{$\Vtoken{token} = \tuple{\Velement{inp}{\var{j}\subtract 1}, \var{val}}$.
  See \Dfref{def:token}.
}
\State $\var{matches} = \Transitions{\Symbol{\var{token}}}{\var{j}\subtract 1}$
\label{line:scanner-op-2}
\For{every $\Veim{match} \in \Veimset{matches}$}
   \Comment \tightMath{\var{x}=1+1\times\var{try}}
\label{line:scanner-op-3}
\State $\Veim{trial} = \tuple{\Next{\DR{\var{match}}},\Origin{\var{match}}}@\VVelement{S}{j}$
\Statex \Comment \tightMath{\var{x}= 1\times\var{try}}
\label{line:scanner-op-4}
\State $\var{lp} = \tuple{\Veim{match},\var{token}}_\type{LP}$
   \Comment \tightMath{\var{x}= 1\times\var{try}}
\label{line:scanner-op-5}
  \If{\Seen{\var{trial}}}
   \Comment \tightMath{\var{x}= 1\times\var{try}}
\label{line:scanner-op-6}
    \State Add \var{lp} to \LinkPairs{\var{trial}}
        \Comment \tightMath{\var{ln}=\Oc;\var{x}= 1\times\var{dup}}
\label{line:scanner-op-7}
  \Else \Comment \tightMath{\var{x}= 1\times\var{new}}
\label{line:scanner-op-8}
    \State $\Seen{\var{trial}} = \var{true}$
      \Comment \tightMath{\var{x}= 1\times\var{new}}
\label{line:scanner-op-9}
    \State $\LinkPairs{\var{trial}} = \set{\var{lp}}$
      \Comment \tightMath{\var{x}= 1\times\var{new}}
\label{line:scanner-op-10}
    \State Add \var{trial} to \VVelement{S}{j}
      \Comment \tightMath{\var{ln}=\var{s}=\Oc;\var{x}=1\times\var{new}}
\label{line:scanner-op-11}
  \EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\proc{Scanner} invocations]
\label{th:scanner-invocations}
\op{Scanner} invokes
\begin{align*}
 \text{line }\ref{line:scanner-op-2} & \;\;\text{once;} \\
 \text{line }\ref{line:scanner-op-3} & \;\;\text{once, plus once for each try;} \\
 \text{lines }\enRefs{line:scanner-op-4}{line:scanner-op-6} & \;\;\text{once for each try;} \\
 \text{line }\ref{line:scanner-op-7} & \;\;\text{once for each dup; and} \\
 \text{lines }\enRefs{line:scanner-op-8}{line:scanner-op-11} & \;\;\text{once for each add.}
\end{align*}

\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-scanner-invocations-10}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-2}} = 1$
    \cuz{} sequential execution.
    This is the theorem for
    line \ref{line:scanner-op-2}.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-12}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-7}} = 1\times\var{dup}$ \\
    \cuz{} line \ref{line:scanner-op-7}
    adds a duplicate link for \Veim{trial}.
    This is the theorem for
    line \ref{line:scanner-op-7}.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-14}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-11}} = 1\times\var{new}$ \\
    \cuz{} line \ref{line:scanner-op-11}
    adds \Veim{trial} as a new \type{EIM}.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-16}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-8}}
        = \Invocs{\ref{line:scanner-op-9}}
        = \Invocs{\ref{line:scanner-op-10}}$ \\
    $= \Invocs{\ref{line:scanner-op-11}}
        = 1\times\var{new}$ \\
    \cuz{} \
    \Eref{eq:th-scanner-invocations-14},
    sequential execution.
    This is the theorem for
    lines \enRefs{line:scanner-op-8}{line:scanner-op-11}.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-18}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-6}}
        = 1\times(\var{new}+\var{dup})$
    \cuz{} \
    combining both branches of the if statement.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-20}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-4}}
        = \Invocs{\ref{line:scanner-op-5}}
        = \Invocs{\ref{line:scanner-op-6}}
        = 1\times\var{try}$ \\
    \cuz{}
    \Eref{eq:th-scanner-invocations-18},
    \longDfref{Try is dup or new}{def:try-dup-new},
    sequential execution.
    This is the theorem for
    lines \enRefs{line:scanner-op-4}{line:scanner-op-6}.}
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-22}
\Passes{\text{loop }\enRefs{line:scanner-op-3}{line:scanner-op-11}}
    = \Invocs{\ref{line:scanner-op-4}}
    \because \Obref{def:loop-invocation}.
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-24}
\Invocs{\ref{line:scanner-op-3}}
    = \Invocs{\ref{line:scanner-op-4}} + 1
    \because \Obref{def:loop-invocation},
        \Eref{eq:th-scanner-invocations-22}.
\end{equation}
\begin{equation}
\label{eq:th-scanner-invocations-26}
\myparbox{%
    $\Invocs{\ref{line:scanner-op-3}} = 1+1\times\var{try}
    \because \Eref{eq:th-scanner-invocations-20},
        \Eref{eq:th-scanner-invocations-24}$ \\
    This is the theorem for line \ref{line:scanner-op-3}.}
\end{equation}
We now have the theorem for lines
\enRefs{line:scanner-op-2}{line:scanner-op-11},
which is what we needed for the theorem.
\myqed
\end{proof}

\begin{equation}
\label{eq:def-scanner-charges}
\myparbox{%
  Direct resource in \op{Scanner} is charged as indicated
  in \Thref{th:scanner-invocations}.
  Specifically, resource incurred by an add is charged
  to an add of \Veim{trial};
  resource incurred by a try is charged
  to an attempt to add \Veim{trial}; and
  resource not associated with an \type{EIM} add or try is
  charged to the Earley set.
  \cuz{} AF complexity analysis.}
\end{equation}

% Headers that allow line ranges -- just in case I want
% to go back to this.
% \hline
% &&&Direct&Indirect&& \\
% &&Invoc-&time&time&& \\
% &Line&cations&per in-&per in-&& \\
% Lines&Count&per line&vocation&vocation&Total&\multicolumn{1}{c|}{Because}
% \\ \hline

\begin{table}[t]
\caption{Inclusive time charged to Earley set for \op{Scanner}}
\label{tab:scanner-es-time}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Time per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-2}&1&\Oc{}&&\Oc{}&\Thref{th:transition-array}
\\ \hline
    \ref{line:scanner-op-3}&1&\Oc{}&&\Oc{}&
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|p{4.5in}|}{\cuz{} \Sref{sec:complexity-tables},
    \AFref{alg:scanner-op},
    \ThFref{th:scanner-invocations},
    \Eref{eq:def-scanner-charges}.}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive new-charged time for \op{Scanner}}
\label{tab:scanner-add-time}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Time per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-4}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-5}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-6}&1&\Oc{}&&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-8}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-9}&1&\Oc{}&&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-10}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:scanner-op-11}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive dup-charged time for \op{Scanner}}
\label{tab:scanner-dup-time}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Time per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-4}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-5}&1&\Oc{}&&\Oc{}&
\\ \hline
    \ref{line:scanner-op-6}&1&\Oc{}&&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-7}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{theorem}[\op{Scanner} time]
\label{eq:th-scanner-times}
The time consumed by \op{Scanner}
is charged as follows:
\Oc{} time to
each add;
\Oc{} time to
each try; and
\Oc{} time to the Earley set which is
the argument of \op{Scanner}.
\end{theorem}

\begin{proof}
For the Earley set charge,
the theorem follows from
\Tbref{tab:scanner-es-time}.
For the \type{EIM} add charge,
the theorem follows from
\Tbref{tab:scanner-add-time}.
For the \type{EIM} dup charge,
the theorem follows from
\Tbref{tab:scanner-add-time},
\Tbref{tab:scanner-dup-time},
and
\longDfref{Try is dup or new}{def:try-dup-new},
\end{proof}

\begin{table}[t]
\caption{Inclusive \type{EIM} space charged to Earley set for \op{Scanner}}
\label{tab:scanner-es-eim-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-2}&1&0&&0&\Thref{th:transition-array}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{0}&
\\ \hline
\multicolumn{6}{|p{4.5in}|}{\cuz{} \Sref{sec:complexity-tables},
    \AFref{alg:scanner-op},
    \ThFref{th:scanner-invocations},
    \Eref{eq:def-scanner-charges}.}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive add-charged \type{EIM} space for \op{Scanner}}
\label{tab:scanner-add-eim-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-4}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-5}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-6}&1&0&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-8}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-9}&1&0&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-10}&1&0&&0&\Thref{th:es-complexity}
\\ \hline
    \ref{line:scanner-op-11}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive dup-charged \type{EIM} space for \op{Scanner}}
\label{tab:scanner-dup-eim-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-4}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-5}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-6}&1&0&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-7}&1&0&&0&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{0}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{theorem}[\op{Scanner} \type{EIM} space]
\label{eq:th-scanner-eim-space}
\op{Scanner}
consumes \Oc{} \type{EIM} space per add,
which is charged to each add.
\end{theorem}

\begin{proof}
For the Earley set charge,
the theorem follows from
\Tbref{tab:scanner-es-eim-space}.
For the \type{EIM} add charge,
the theorem follows from
\Tbref{tab:scanner-add-eim-space}.
For the \type{EIM} dup charge,
the theorem follows from
\Tbref{tab:scanner-add-eim-space},
\Tbref{tab:scanner-dup-eim-space},
and
\longDfref{Try is dup or new}{def:try-dup-new},
\end{proof}

\begin{table}[t]
\caption{Inclusive link space charged to Earley set for \op{Scanner}}
\label{tab:scanner-es-link-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-2}&1&0&&0&\Thref{th:transition-array}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{0}&
\\ \hline
\multicolumn{6}{|p{4.5in}|}{\cuz{} \Sref{sec:complexity-tables},
    \AFref{alg:scanner-op},
    \ThFref{th:scanner-invocations},
    \Eref{eq:def-scanner-charges}.}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive new-charged link space for \op{Scanner}}
\label{tab:scanner-add-link-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-4}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-5}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-6}&1&0&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-8}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-9}&1&0&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-10}&1&0&&0&\Thref{th:es-complexity}
\\ \hline
    \ref{line:scanner-op-11}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Inclusive dup-charged link space for \op{Scanner}}
\label{tab:scanner-dup-link-space}
\begin{tabular}{|c|c|c|c|c|p{1.5in}|}
\hline
&Invoc-&\multicolumn{2}{|c|}{Space per invocation}&Line& \\ \cline{3-4}
Line&cations&Direct&Indirect&Total&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:scanner-op-3}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-4}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-5}&1&0&&0&
\\ \hline
    \ref{line:scanner-op-6}&1&\c{}&&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:scanner-op-7}&1&\Oc{}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|l}{Total for \op{Scanner}}&\multicolumn{1}{c|}{\Oc{}}&
\\ \hline
\multicolumn{6}{|l|}{
    \myparbox[4.5in]{%
        \cuz{}
        \longDfref{Try is dup or new}{def:try-dup-new},
        \Sref{sec:complexity-tables},
        \AFref{alg:scanner-op},
        \ThFref{th:scanner-invocations},
        \Eref{eq:def-scanner-charges}.}%
}
\\ \hline
\end{tabular}
\end{table}

\begin{theorem}[\op{Scanner} link space]
\label{eq:th-scanner-link-space}
\op{Scanner}
consumes \Oc{} link space per
try,
which is charged to each try.
\end{theorem}

\begin{proof}
For the Earley set charge,
the theorem follows from
\Tbref{tab:scanner-es-link-space}.
For the \type{EIM} add charge,
the theorem follows from
\Tbref{tab:scanner-add-link-space}.
For the \type{EIM} dup charge,
the theorem follows from
\Tbref{tab:scanner-add-link-space},
\Tbref{tab:scanner-dup-link-space},
and
\longDfref{Try is dup or new}{def:try-dup-new}.
\end{proof}

\begin{observation}[Original Earley algorithm: Scanner Operation]
\label{obs:earley-scanner-op}
\Marpa{}'s scanner op has the same effect as the ``Scanner''
operation of \Earley{}.
\dispEnd
\end{observation}

\FloatBarrier

\section{Reducer op}
\label{def:reducer}

\begin{algorithm}[H]
\caption{
    \label{alg:earley-reducer-op}
    \var{Earley-reducer}}
\begin{algorithmic}[1]
\Procedure{Earley-reducer}{\VVelement{S}{j},\Veimset{matches},\Veim{cuz}}
\For{every $\Veim{match} \in \Veimset{matches}$}
      \label{line:earley-reducer-op-2}
\State $\Veim{trial} \gets \tuple{
     \begin{gathered}
        \Next{\DR{\var{match}}},
        \\ \Origin{\var{match}}, \Current{\VVelement{S}{j}}
     \end{gathered}
        }
        $
      \label{line:earley-reducer-op-3}
\State $\Vlp{lp} \gets \tuple{\Veim{match},\Veim{cuz}}$
      \label{line:earley-reducer-op-4}
\State $\var{found} \gets \Seen{\Veim{trial}}$
      \label{line:earley-reducer-op-4-50}
\If{$\var{found} \ne \Lambda$}
      \label{line:earley-reducer-op-5}
    \State Add \var{lp} to \LinkPairs{\Veim{found}}
      \label{line:earley-reducer-op-6}
\Else
      \label{line:earley-reducer-op-7}
    \State $\SetSeen{\Veim{trial}}$
      \label{line:earley-reducer-op-8}
    \State $\LinkPairs{\var{trial}} \gets \set{\var{lp}}$
      \label{line:earley-reducer-op-9}
    \State Add \var{trial} to \VVelement{S}{j}
      \label{line:earley-reducer-op-10}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{definition}[\op{Earley-reducer} passes]
\label{def:op-earley-reducer-passes}
A call of \op{Earley-reducer} is a \dfn{new pass}
if it adds a new EIM to an ES.
A call of \op{Earley-reducer} is a \dfn{dup pass}
if it adjoins a duplicate EIM to an ES.
\dispEnd
\end{definition}

\begin{theorem}[\op{Earley-reducer} passes]
\label{th:op-earley-reducer-passes}
Every call of \op{Earley-reducer} is either
a new or a dup pass, but never both.
\end{theorem}

\begin{proof}
From examination of \Aref{alg:earley-reducer-op},
we see that
\begin{itemize}
\item Line \ref{line:earley-reducer-op-6}
is the only line at which a duplicate EIM is adjoined to an
ES.
\item Line \ref{line:earley-reducer-op-10}
is the only line at which an EIM is added to an ES.
\item One of lines \ref{line:earley-reducer-op-6}
or \ref{line:earley-reducer-op-10} is always executed,
but never both.  \myqed
\end{itemize}
\end{proof}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-invocations}
    \op{Earley-reducer} line invocations}
\vspace{1ex}
\begin{tabular}{|c|c|p{2.75in}|}
\hline
Line&Invocations&Justification
\\ \hline
    \ref{line:earley-reducer-op-3}&1/\var{try}&\Obref{def:loop-invocation},
        this line creates a ``trial'' EIM for either a  dup or new pass
\\ \hline
    \ref{line:earley-reducer-op-2}&1/\var{proc}+1/\var{try}&
       \makebox{line \ref{line:earley-reducer-op-3}, sequential execution,}
       \Obref{def:loop-invocation}
\\ \hline
    \ref{line:earley-reducer-op-4}&1/\var{try}&
       line \ref{line:earley-reducer-op-3}, sequential execution
\\ \hline
    \ref{line:earley-reducer-op-4-50}&1/\var{try}&
       line \ref{line:earley-reducer-op-4}, sequential execution
\\ \hline
    \ref{line:earley-reducer-op-5}&1/\var{try}&
       line \ref{line:earley-reducer-op-4-50}, sequential execution
\\ \hline
    \ref{line:earley-reducer-op-6}&1/\var{dup}&line adjoins a duplicate
       EIM
\\ \hline
    \ref{line:earley-reducer-op-10}&1/\var{new}&line adds a new EIM
\\ \hline
    \ref{line:earley-reducer-op-7}&1/\var{new}&
       line \ref{line:earley-reducer-op-10}, sequential execution
\\ \hline
    \ref{line:earley-reducer-op-8}&1/\var{new}&
       line \ref{line:earley-reducer-op-7}, sequential execution
\\ \hline
    \ref{line:earley-reducer-op-9}&1/\var{new}&
       line \ref{line:earley-reducer-op-8}, sequential execution
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
Lines are listed in order of deduction.
See the definition
\Dfref{def:op-earley-reducer-passes} of,
and theorem
\Thref{th:op-earley-reducer-passes}
for, \op{Earley-reducer} passes.
1/\var{proc}: ``once per procedure call''.
1/\var{try}: ``once per dup or new loop pass''
1/\var{dup}: ``once per dup \type{EIM} loop pass''.
1/\var{new}: ``once per new \type{EIM} loop pass''.
    }
\\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-time}
    \op{Earley-reducer} time per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|p{3.5in}|}
\hline
Line&Time&Justification
\\ \hline
    \ref{line:earley-reducer-op-2}&\Oc{}&
        \Veimset{match} is a pointer to the head of a linked list,
        as returned by \myfn{Transition}{}
        \Thref{th:transition-array}.
\\ \hline
    \ref{line:earley-reducer-op-3}&\Oc{}&
\\ \hline
    \ref{line:earley-reducer-op-4}&\Oc{}&
\\ \hline
    \ref{line:earley-reducer-op-4-50}&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:earley-reducer-op-5}&\Oc{}&
\\ \hline
    \ref{line:earley-reducer-op-6}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:earley-reducer-op-7}&\Oc{}&
\\ \hline
    \ref{line:earley-reducer-op-8}&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:earley-reducer-op-9}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:earley-reducer-op-10}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
    See \AFref{alg:earley-reducer-op}.
}
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-time-worksheet}
    \op{Earley-reducer} time worksheet}
\vspace{1ex}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{1}{|c}{}&\multicolumn{3}{|c|}{Accounts}
\\ \cline{2-4}
Line&\var{proc}&\var{new}&\var{dup}
\\ \hline
    \ref{line:earley-reducer-op-2}&\Oc{}&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-3}&&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-4}&&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-4-50}&&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-5}&&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-6}&&&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-7}&&\Oc{}&\Oc{}
\\ \cline{1-4}
    \ref{line:earley-reducer-op-8}&&\Oc{}&
\\ \cline{1-4}
    \ref{line:earley-reducer-op-9}&&\Oc{}&
\\ \cline{1-4}
    \ref{line:earley-reducer-op-10}&&\Oc{}&
\\ \cline{1-4}
    Totals&\Oc{}&\Oc{}&\Oc{}
\\ \hline
\end{tabular}
\;\;\parbox{2.5in}{
    \raggedright
    See \Tbref{tab:earley-reducer-time}.
    Charges to \var{proc} are for every call.
    Charges to \var{new} are
    only for calls where \Veim{trial} is a new EIM.
    Charges to \var{dup} are
    only for calls where \Veim{trial} is a duplicate EIM.
}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Earley-reducer} time]
\label{th:earley-reducer-time}
The time that \op{Earley-reducer} incurs may
be accounted for as follows:
\begin{itemize}
\item \label{item:th-earley-reducer-time-1}
By charging \Oc{} time to the \var{new} account of
\Veim{trial}, when \var{trial} is added to the current ES.
\item \label{item:th-earley-reducer-time-2}
By charging \Oc{} time to the \var{dup} account of
\Veim{trial}, when \var{trial} is a duplicate of an EIM
already in the current ES.
\end{itemize}
\end{theorem}

\begin{proof}
By the Procedure Overhead Theorem
\Thref{th:procedure-overhead-discard},
we may ignore \var{proc} time.
This theorem then follows from
the definition
\Dfref{def:op-earley-reducer-passes}
and theorem
\Thref{th:op-earley-reducer-passes}
for \op{Earley-reducer} passes,
and the \op{Earley-reducer} time worksheet
\Tbref{tab:earley-reducer-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:earley-reducer-space}
    \op{Earley-reducer} space per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|c|p{1.5in}|}
\hline
Line&Trimmed&Link&Justification
\\ \hline
    \ref{line:earley-reducer-op-2}&0&0&
\\ \hline
    \ref{line:earley-reducer-op-3}&0&0&
\\ \hline
    \ref{line:earley-reducer-op-4}&0&0&
\\ \hline
    \ref{line:earley-reducer-op-4-50}&0&0&
\\ \hline
    \ref{line:earley-reducer-op-5}&0&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:earley-reducer-op-6}&0&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:earley-reducer-op-7}&0&0&
\\ \hline
    \ref{line:earley-reducer-op-8}&0&0&\Thref{th:seen-psl}
\\ \hline
    \ref{line:earley-reducer-op-9}&0&0&\Thref{th:es-complexity}
\\ \hline
    \ref{line:earley-reducer-op-10}&\Oc{}&0&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|p{4.5in}|}{
    See \AFref{alg:earley-reducer-op}.
}
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Earley-reducer} trimmed space]
\label{th:earley-reducer-trimmed-space}
The trimmed space that \op{Earley-reducer} incurs may
be accounted for
by charging \Oc{} trimmed space to the \var{new} account of
\Veim{trial}.
Trimmed space needs to be charged only
for those calls of \op{Earley-reducer} in which
\var{trial} is not already in the current ES.
\end{theorem}

\begin{proof}
This theorem follows from
the definition
\Dfref{def:op-earley-reducer-passes}
and theorem
\Thref{th:op-earley-reducer-passes}
for \op{Earley-reducer} passes,
and the \op{Earley-reducer} Time Worksheet
\Tbref{tab:earley-reducer-time-worksheet}.
\myqed
\end{proof}

\begin{theorem}[\op{Earley-reducer} link space]
\label{th:earley-reducer-link-space}
The link space that \op{Earley-reducer} incurs may
be accounted for
by charging \Oc{} link space to the \var{dup} account of
\Veim{trial}.
Link space needs to be charged only
for those calls of \op{Earley-reducer} in which
\var{trial} is a duplicate
of an EIM already in the current ES.
\end{theorem}

\begin{proof}
This theorem follows from
the definition
\Dfref{def:op-earley-reducer-passes}
and theorem
\Thref{th:op-earley-reducer-passes}
for \op{Earley-reducer} passes,
and the \op{Earley-reducer} Time Worksheet
\Tbref{tab:earley-reducer-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{Leo-reducer}}
\label{alg:leo-reducer-op}
\begin{algorithmic}[1]
\Procedure{Leo-reducer}{\VVelement{S}{j},\Vlim{blocker},\Veim{cuz}}
\State $\Veim{trial} \gets \tuple{
     \begin{gathered}
        \Next{\DR{\var{blocker}}},
        \\ \Origin{\var{blocker}}, \Current{\VVelement{S}{j}}
     \end{gathered}
        }$
  \label{line:leo-reducer-op-2}
\State $\Vlp{lp} = \tuple{\Vlim{blocker},\Veim{cuz}}$
  \label{line:leo-reducer-op-3}
\State $\var{found} \gets \Seen{\Veim{trial}}$
  \label{line:leo-reducer-op-3-50}
\If{$\var{found} \ne \Lambda$}
  \label{line:leo-reducer-op-4}
\State Add \var{lp} to \LinkPairs{\Veim{found}}
  \label{line:leo-reducer-op-5}
\Else
  \label{line:leo-reducer-op-6}
    \State $\SetSeen{\Veim{trial}}$
  \label{line:leo-reducer-op-7}
    \State $\LinkPairs{\var{trial}} = \set{\var{lp}}$
  \label{line:leo-reducer-op-8}
    \State Add \var{trial} to \VVelement{S}{j}
  \label{line:leo-reducer-op-9}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{definition}[\op{Leo-reducer} passes]
\label{def:op-leo-reducer-passes}
A call of \op{Leo-reducer} is a \dfn{new pass}
if it adds a new EIM to an ES.
A call of \op{Leo-reducer} is a \dfn{dup pass}
if it adjoins a duplicate EIM to an ES.
\dispEnd
\end{definition}

\begin{theorem}[\op{Leo-reducer} passes]
\label{th:op-leo-reducer-passes}
Every call of \op{Leo-reducer} is either
a new or a dup pass, but never both.
\end{theorem}

\begin{proof}
From examination of \Aref{alg:leo-reducer-op},
we see that
\begin{itemize}
\item Line \ref{line:leo-reducer-op-5}
is the only line at which a duplicate EIM is adjoined to an
ES.
\item Line \ref{line:leo-reducer-op-9}
is the only line at which an EIM is added to an ES.
\item One of lines \ref{line:leo-reducer-op-5}
or \ref{line:leo-reducer-op-9} is always executed,
but never both.  \myqed
\end{itemize}
\end{proof}

\FloatBarrier

\begin{table}[t]
\caption{Line invocations for \op{Leo-reducer}}
\label{tab:leo-reducer-invocations}
\begin{tabular}{|c|c|p{2.75in}|}
\hline
Line&Invocations&Justification
\\ \hline
    \ref{line:leo-reducer-op-2}&1/\var{try}&
        this line creates a ``trial'' EIM for either a  dup or new pass
\\ \hline
    \ref{line:leo-reducer-op-3}&1/\var{try}&
       line \ref{line:leo-reducer-op-2}, sequential execution
\\ \hline
    \ref{line:leo-reducer-op-3-50}&1/\var{try}&
       line \ref{line:leo-reducer-op-3}, sequential execution
\\ \hline
    \ref{line:leo-reducer-op-4}&1/\var{try}&
       line \ref{line:leo-reducer-op-3-50}, sequential execution
\\ \hline
    \ref{line:leo-reducer-op-5}&1/\var{dup}&
        line adjoins a duplicate EIM
\\ \hline
    \ref{line:leo-reducer-op-9}&1/\var{new}&
        line adds a new EIM
\\ \hline
    \ref{line:leo-reducer-op-6}&1/\var{new}&
       line \ref{line:leo-reducer-op-9}, sequential execution
\\ \hline
    \ref{line:leo-reducer-op-7}&1/\var{new}&
       line \ref{line:leo-reducer-op-9}, sequential execution
\\ \hline
    \ref{line:leo-reducer-op-8}&1/\var{new}&
       line \ref{line:leo-reducer-op-9}, sequential execution
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
Lines are listed in order of deduction.
1/\var{try}: ``once per dup or new call''
1/\var{dup}: ``once per dup \type{EIM} call''.
1/\var{new}: ``once per new \type{EIM} call''.
    }
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:leo-reducer-time}
    \op{Leo-reducer} time per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|p{3.5in}|}
\hline
Line&Time&Justification
\\ \hline
    \ref{line:leo-reducer-op-2}&\Oc{}&
\\ \hline
    \ref{line:leo-reducer-op-3}&\Oc{}&
\\ \hline
    \ref{line:leo-reducer-op-3-50}&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:leo-reducer-op-4}&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:leo-reducer-op-5}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:leo-reducer-op-6}&\Oc{}&
\\ \hline
    \ref{line:leo-reducer-op-7}&\Oc{}&\Thref{th:seen-psl}
\\ \hline
    \ref{line:leo-reducer-op-8}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:leo-reducer-op-9}&\Oc{}&\Thref{th:es-complexity}
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
    See
    \AFref{alg:leo-reducer-op},
    \Tbref{tab:leo-reducer-invocations}.
}
\\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{
    \label{tab:leo-reducer-time-worksheet}
    \op{Leo-reducer} time worksheet}
\vspace{1ex}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{1}{|c}{}&\multicolumn{2}{|c|}{Accounts}
\\ \cline{2-3}
Line&\var{new}&\var{dup}
\\ \hline
    \ref{line:leo-reducer-op-2}&\Oc{}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-3}&\Oc{}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-3-50}&\Oc{}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-4}&\Oc{}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-5}&&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-6}&\Oc{}&\Oc{}
\\ \hline
    \ref{line:leo-reducer-op-7}&\Oc{}&
\\ \hline
    \ref{line:leo-reducer-op-8}&\Oc{}&
\\ \hline
    \ref{line:leo-reducer-op-9}&\Oc{}&
\\ \hline
    Totals&\Oc{}&\Oc{}
\\ \hline
\end{tabular}
\;\;\parbox{2.5in}{
    \raggedright
    See \Tbref{tab:leo-reducer-time}.
    Charges to \var{new} are
    only for calls where \Veim{trial} is a new EIM.
    Charges to \var{dup} are
    only for calls where \Veim{trial} is a duplicate EIM.
}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Leo-reducer} time]
\label{th:leo-reducer-time}
The time that \op{Leo-reducer} incurs may
be accounted for as follows:
\begin{itemize}
\item \label{item:th-leo-reducer-time-1}
By charging \Oc{} time to the \var{new} account of
\Veim{trial}, when \var{trial} is added to the current ES.
\item \label{item:th-leo-reducer-time-2}
By charging \Oc{} time to the \var{dup} account of
\Veim{trial}, when \var{trial} is a duplicate of an EIM
already in the current ES.
\end{itemize}
\end{theorem}

\begin{proof}
This theorem then follows from
the definition
\Dfref{def:op-leo-reducer-passes}
and theorem
\Thref{th:op-leo-reducer-passes}
for \op{Leo-reducer} passes,
and the \op{Leo-reducer} time worksheet
\Tbref{tab:leo-reducer-time-worksheet}.
\myqed
\end{proof}

\FloatBarrier

\todo{TO HERE Fri 13 Nov 2020 08:59:51 PM EST}

\begin{table}[H]
\caption{
    \label{tab:leo-reducer-space}
    \op{Leo-reducer} space per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|c|p{1.5in}|}
\hline
Line&Trimmed&Link&Justification
\\ \hline
    \ref{line:leo-reducer-op-2}&&&
\\ \hline
    \ref{line:leo-reducer-op-3}&&&
\\ \hline
    \ref{line:leo-reducer-op-3-50}&&&
\\ \hline
    \ref{line:leo-reducer-op-4}&&&
\\ \hline
    \ref{line:leo-reducer-op-5}&&\Oc{}&\Thref{th:es-complexity}
\\ \hline
    \ref{line:leo-reducer-op-6}&&&
\\ \hline
    \ref{line:leo-reducer-op-7}&&&
\\ \hline
    \ref{line:leo-reducer-op-8}&&&
\\ \hline
    \ref{line:leo-reducer-op-9}&\Oc{}&&\Thref{th:es-complexity}
\\ \hline
\multicolumn{4}{|p{4.5in}|}{See
    \AFref{alg:leo-reducer-op},
    \Tbref{tab:leo-reducer-invocations}.}
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{theorem}[\op{Leo-reducer} trimmed space]
\label{th:leo-reducer-trimmed-space}
The trimmed space that \op{Leo-reducer} incurs may
be accounted for
by charging \Oc{} trimmed space to the \var{new} account of
\Veim{trial}.
Trimmed space needs to be charged only
for those calls of \op{Leo-reducer} in which
\var{trial} is not already in the current ES.
\end{theorem}

\begin{proof}
This theorem follows from
the definition
\Dfref{def:op-leo-reducer-passes}
and theorem
\Thref{th:op-leo-reducer-passes}
for \op{Leo-reducer} passes,
and the \op{Leo-reducer} space table
\Tbref{tab:leo-reducer-space}.
\myqed
\end{proof}

\begin{theorem}[\op{Leo-reducer} link space]
\label{th:leo-reducer-link-space}
The link space that \op{Leo-reducer} incurs may
be accounted for
by charging \Oc{} link space to the \var{dup} account of
\Veim{trial}.
Link space needs to be charged only
for those calls of \op{Leo-reducer} in which
\var{trial} is a duplicate
of an EIM already in the current ES.
\end{theorem}

\begin{proof}
This theorem follows from
the definition
\Dfref{def:op-leo-reducer-passes}
and theorem
\Thref{th:op-leo-reducer-passes}
for \op{Leo-reducer} passes,
and the \op{Leo-reducer} space table
\Tbref{tab:leo-reducer-space}.
\myqed
\end{proof}

\FloatBarrier

\begin{algorithm}[H]
\caption{\op{Reducer}}
\label{alg:reducer-op}
\begin{algorithmic}[1]
\Procedure{Reducer}{\VVelement{S}{j}}
\For{$\Veim{cuz} \in \VVelement{S}{j}$ where \var{cuz} is a completion}
  \label{line:reducer-op-2}
\State $\Veimset{matches} = \Transitions{\LHS{\Veim{cuz}}}{\Origin{\var{x}}}$
  \label{line:reducer-op-3}
\State $\var{match0} = \Velement{matches}{0}$
  \label{line:reducer-op-4}
\If{$\var{match0} = \Lambda$}
  \label{line:reducer-op-5}
  \State Do nothing
      \label{line:reducer-op-6}
\ElsIf{\var{match0} is a \type{LIM}}
  \label{line:reducer-op-7}
    \State \Call{Leo-reducer}{\VVelement{S}{j},\Vlim{match0},\Veim{cuz}}
      \label{line:reducer-op-8}
\Else
  \label{line:reducer-op-9}
    \State \Call{Earley-reducer}{\VVelement{S}{j},\Veimset{matches},\Veim{cuz}}
      \label{line:reducer-op-10}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:reducer-invocations}
    Line invocations for \op{Reducer}}
\vspace{1ex}
\begin{tabular}{|r|r|p{2.75in}|}
\hline
\multicolumn{1}{|c|}{Line}
    &\multicolumn{1}{c|}{Invocations}
    &\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:reducer-op-3}&1/\var{pass}&\Obref{def:loop-invocation}.
\\ \hline
    \multirow{2}{*}{\ref{line:reducer-op-2}}
    &1/\var{proc}&
    \multirow{2}{*}{\Obref{def:loop-invocation}; Line \ref{line:reducer-op-3} RSeq.}
\\
    &+\;1/\var{pass}&
\\ \hline
    \ref{line:reducer-op-4}&1/\var{pass}&
       Line \ref{line:reducer-op-3} Seq.
\\ \hline
    \ref{line:reducer-op-5}&1/\var{pass}&
       Line \ref{line:reducer-op-4} Seq.
\\ \hline
    \ref{line:reducer-op-6}&{$\le$}1/\var{pass}&
        Line \ref{line:reducer-op-5} Cond.
\\ \hline
    \ref{line:reducer-op-7}&1/\var{pass}&
       Line \ref{line:reducer-op-5} Seq.
\\ \hline
    \ref{line:reducer-op-8}
    &{$\le$}1/\var{pass}&
        Line \ref{line:reducer-op-7} Cond.
\\ \hline
    \ref{line:reducer-op-9}&1/\var{pass}&
       Line \ref{line:reducer-op-7} Seq.
\\ \hline
    \ref{line:reducer-op-10}&{$\le$}1/\var{pass}&
        Line \ref{line:reducer-op-9} Cond.
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
Lines are listed in order of deduction.
1/\var{proc}: ``once per procedure call''.
1/\var{pass}: ``once per loop pass''.
{$\le$}1/\var{pass}: ``at most once per loop pass''.
    }
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\begin{table}[H]
\caption{
    \label{tab:reducer-proc-time}
    \op{Reducer} time per invocation}
\vspace{1ex}
\begin{tabular}{|c|c|p{3.5in}|}
\hline
Line&Time&\multicolumn{1}{c|}{Justification}
\\ \hline
    \ref{line:reducer-op-2}&\Oc{}&\Thref{th:es-traversal}.
\\ \hline
    \ref{line:reducer-op-3}&\Oc{}&\Thref{th:transitions-read}.
\\ \hline
    \ref{line:reducer-op-4}&\Oc{}&
\\ \hline
    \ref{line:reducer-op-5}&\Oc{}&
\\ \hline
    \ref{line:reducer-op-6}&\Oc{}&
\\ \hline
    \ref{line:reducer-op-7}&\Oc{}&
\\ \hline
    \ref{line:reducer-op-8}&\Oc{}&\Thref{th:leo-reducer-time},
        \Thref{th:procedure-overhead-discard}.
        Zero indirect time.
\\ \hline
    \ref{line:reducer-op-9}&\Oc{}&
\\ \hline
    \ref{line:reducer-op-10}&\Oc{}&\Tbref{tab:earley-reducer-time},
        \Thref{th:procedure-overhead-discard}.
        Zero indirect time.
\\ \hline
\multicolumn{3}{|p{4.5in}|}{
    See \AFref{alg:reducer-op},
    \Tbref{tab:reducer-invocations}.
}
\\ \hline
\end{tabular}
\end{table}

\FloatBarrier

\todo{Finish reducer op}

\begin{observation}[Ancestry]
\label{obs:reducer-ancestry}
The \Marpa{} implementation tracks all link pairs for reducer-generated
\type{EIM}s.
The cause of a reducer-generated \type{EIM} will always be
a completed \type{EIM}.
The predecessor of a reducer-generated \type{EIM} varies depending on whether the attempt
to create the reducer-generated \type{EIM} was a \op{Leo-reducer} operation
or an \op{Earley-reducer} operation.
For an
\op{Earley-reducer} operation,
the predecessor will be an incomplete \type{EIM}.
For a \op{Leo-reducer} operation,
the predecessor will be either a \type{LIM},
or $\Lambda$.
\dispEnd{}
\end{observation}

\begin{observation}[Complexity]
\todo{}
\dispEnd
\end{observation}

\begin{observation}[Original Earley algorithm: Completer Operation]
\label{obs:earley-completer-op}
When there are no \type{LIM}s in \Ves{predecessor}
\Marpa{}'s \var{Reducer} op has the same effect as the ``Completer''
operation of \Earley{}\cite{Earley1970}.\footnote{
For all other \Marpa{} ops we have used the name of its
equivalent in \Earley{}.
It is convenient to use a different name
for \Marpa{}'s \var{Reducer} op,
however,
because it differs in effect from the \Earley{}
``Completer'' operation,
and because the cognates of ``complete'' are
overloaded in confusing ways.
For example, the result of an \Earley{} ``Completer'' operation
is not necessarily an \type{EIM} completion.
}
Put another way, \Marpa{}'s \var{Reducer} op
is the same as the Completer operation of \Earley{},
if there never is a \type{LIM} in \Vpimset{matches} on
line \ref{line:reducer-op-3} of \Aref{alg:reducer-op}.
In this case,
we never execute \op{Leo-reducer}.
\dispEnd
\end{observation}

\begin{observation}[Summary of \Earley{} Completer Operation]
\label{obs:earley-completer-summary}
In what follows,
we will find the following summary of the original, \Earley{} completer
operation useful.
If
\begin{equation}
\label{eq:earley-completer-summary-1}
\myparbox{$\Veim{cause}@\Vloc{current}$ is a completion,} \\
\end{equation}
\begin{equation}
\label{eq:earley-completer-summary-2}
\myparbox{$\var{pred} @ (\Origin{\var{cause}})$ is an incompletion, and} \\
\end{equation}
\begin{equation}
\label{eq:earley-completer-summary-3}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{cause}}$,}
\end{equation}
then we adjoin
\begin{equation}
\label{eq:earley-completer-summary-4}
\myparbox{%
$\left[ \Next{\DR{\var{pred}}}, \Origin{\var{pred}} \right]@\Vloc{current}$
}
\end{equation}
into the Earley table with the link pair
\begin{equation}
\label{eq:earley-completer-summary-5}
[ \var{pred}, \var{cause} ]. \quad \dispEnd
\end{equation}
\end{observation}

\FloatBarrier

\section{Memoizer op}
\label{def:memoizer}

\begin{algorithm}[H]
\caption{
    \label{alg:memoizer-op}
    \op{Memoizer}}
\begin{algorithmic}[1]
\Procedure{Memoizer}{\VVelement{S}{j}}
\For{$\Veim{source} \in \LeoEligible{\VVelement{S}{j}}$}
  \label{line:memoizer-op-2}
\State $\Veim{pred} = \LIMPredecessor{\var{source}}$
  \label{line:memoizer-op-3}
\State $\Vlp{lp} = \tuple{\var{pred},\var{source}}$
  \label{line:memoizer-op-4}
\If{$\var{pred} \neq \Lambda$}
  \label{line:memoizer-op-5}
  \State $\Vlim{lim} = [\DR{\var{pred}}, \Postdot{\var{source}},$
  \Statex \hspace\algorithmicindent \hspace\algorithmicindent \hspace\algorithmicindent
          $\qquad \qquad \qquad \Origin{\var{pred}}, \Current{\var{source}} ]$
      \label{line:memoizer-op-6}
\Else
  \label{line:memoizer-op-7}
  \State $ \Vlim{lim} = [\DR{\var{source}}, \Postdot{\var{source}},$
  \Statex \hspace\algorithmicindent \hspace\algorithmicindent \hspace\algorithmicindent
          $\qquad \qquad \qquad \Origin{\var{source}}, \Current{\var{source}} ]$
      \label{line:memoizer-op-8}
\EndIf
    \State Add \Vlim{lim} to \VVelement{S}{j}
  \label{line:memoizer-op-9}
    \State $\LinkPairs{\var{lim}} = \set{\var{lp}}$
      \label{line:memoizer-op-10}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\begin{theorem}[Memoizer op consistency]
\label{th:memoizer-op-consistency}
If
\[
\forall\; 0 \le \Vnat{i} \le \Vnat{j} : \Valid{\VVelement{S}{i}},
\]
then the Memoizer op attempts to add only valid \type{LIM}s,
and only to \VVelement{S}{i}.
\end{theorem}

\begin{proof}
Inspection of
Algorithm \Aref{alg:memoizer-op} shows
\begin{itemize}
\item that the Memoizer op
only attempts to add a \type{LIM}
at line \ref{line:memoizer-op-9};
\item
that line \ref{line:memoizer-op-9} only attempts to add
\type{LIM}s to \VVelement{S}{i}; and
\item
that \Aref{alg:memoizer-op}
enforces the requirements laid out in
the definition of \type{LIM} validity \Dfref{def:lim-validity},
before the attempt by \Aref{alg:memoizer-op}
to add a \type{LIM}
at line \ref{line:memoizer-op-9}.
\myqed
\end{itemize}
\end{proof}

\begin{theorem}[Memoizer op non-duplication]
\label{th:memoizer-op-non-duplication}
Let $\ell$ be a valid \type{LIM}.
Every call of the Memorizer op attempts to add $\ell$
at most once.
\end{theorem}

\begin{proof}
Let $ell$ be an arbitrary valid LIM,
and let
\mypareq{eq:th-memoizer-op-non-duplication-100}{%
    \Veim{x} = \Source{\ell}.
}
By the Leo source bijection theorem \Thref{th:source-fn-injection},
we know that there is exactly one such \var{x}.
From line \ref{line:memoizer-op-3}
of \Aref{alg:memoizer-op},
we see that \Veim{source} is set to \var{x} in at most one
pass of the main loop of \Aref{alg:memoizer-op}.
Lines \ref{line:memoizer-op-3}--\ref{line:memoizer-op-8}
enforce \Eref{eq:th-memoizer-op-non-duplication-100},
so that the pass
such that
$\Veim{source} = \var{x}$
is the only pass of the main
loop which could attempt to add $\ell$.
\myqed
\end{proof}

\begin{theorem}[Memoizer op maximum]
\label{th:memoizer-op-maximum}
If
\[
\forall\; 0 \le \Vnat{i} \le \Vnat{j} : \Valid{\VVelement{S}{i}},
\]
then each call of the Memoizer op
executes
line \ref{line:memoizer-op-9}
of \Aref{alg:memoizer-op}
at most
\size{\Vocab{\var{g}}} times.
\end{theorem}

\begin{proof}
\mypareq{eq:th-memoizer-op-maximum-100}{%
    There are at most \size{\Vocab{\var{g}}} valid \type{LIM}s
    in \VVelement{S}{j}
    \linebreak[1]
    \cuz{} \Thref{th:es-lim-count}.
}
\mypareq{eq:th-memoizer-op-maximum-110}{%
    Every attempt to add a \type{LIM} in a call of Memoizer op is
    an attempt to add a valid \type{LIM} to \VVelement{S}{j}
    \cuz{} \Thref{th:memoizer-op-consistency}.
}
\mypareq{eq:th-memoizer-op-maximum-115}{%
    Where $\ell$ is a LIM,
    a call of the Memoizer op
    attempts to add $\ell$ at most once
    \cuz{} \Thref{th:memoizer-op-non-duplication}.
}
\mypareq{eq:th-memoizer-op-maximum-120}{%
    There are at most \size{\Vocab{\var{g}}}
    attempts to add a \type{LIM} in any call of Memoizer op
    \cuz{} \Eref{eq:th-memoizer-op-maximum-100},
        \Eref{eq:th-memoizer-op-maximum-110},
        \Eref{eq:th-memoizer-op-maximum-115}.
}
\mypareq{eq:th-memoizer-op-maximum-130}{%
    Each execution of
    line \ref{line:memoizer-op-9}
    is an attempt to
    add a \type{LIM}
    \linebreak[1]
    \cuz{} \Aref{alg:memoizer-op}.
}
This theorem follows from
\Eref{eq:th-memoizer-op-maximum-120}
and \Eref{eq:th-memoizer-op-maximum-130}.
\myqed
\end{proof}

\todo{Rewrite Memoizer op section}

\begin{observation}[Ancestry]
\label{obs:memoizer-ancestry}
The \Marpa{} implementation tracks the link pairs
for the \type{LIM}s.
In the link pair for a \type{LIM}, call it \Vlim{$\ell$},
the ``cause''
is more often called the \dfn{source} of the \type{LIM};
and the ``predecessor'' is
and is more often called the \dfn{previous link} of the \type{LIM}.
We may write the previous link of \mylim{\ell} as
\[
  \begin{aligned}
  & \PreviousLink{\Vlim{$\ell$}} \defined
  \\ \nonumber
  & \qquad
    \left\lbrace
      \LIMPredecessor{\var{source}} :
      \Veim{source} \in \Source{\mylim{\ell}}
    \right\rbrace \quad \dispEnd
  \end{aligned}
\]
\end{observation}

In \Marpa{},
for every valid \type{LIM}, call it \Vlim{lim},
\begin{itemize}
\item \Source{\var{lim}} is always a singleton set containing a penult
in the same Earley set as \var{lim}.
\item \var{PreviousLink} is a set of cardinality at most one,
and if its element exists, it is a valid \type{LIM},
\item $\size{\LinkPairs{\var{lim}}} = 1$,
\end{itemize}
but we have yet to prove these things.

\todo{Prove the above later.}

\begin{observation}[Complexity]
\todo{}
\dispEnd{}
\end{observation}

\begin{observation}[Original Earley algorithm: Memorizer Operation]
\label{obs:earley-memoizer-op}
\Marpa{}'s \var{Memoizer} operation does not exist in \Earley{}\cite{Earley1970}.
\dispEnd{}
\end{observation}

\section{Top level of \Marpa{}}

\todo{Rewrite \Marpa{} top level section}

The top-level view of \Marpa{} is
shown in \Aref{alg:marpa}.
\begin{algorithm}[t]
\caption{The \Marpa{} Algorithm}
\label{alg:marpa}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State $\Call{Parse-Setup}{{}}$
\State $\Call{ES-Setup}{0}$
\State $\Call{Initializer}{{}}$
\State $\Call{Bookkeeper}{\Velement{S}{0}}$
\For{ $\VVelement{S}{j}, 1 \le \var{j} \le \Vsize{w}$ }
\State $\Call{ES-Setup}{\var{j}}$
\State $\Call{Scanner}{\VVelement{S}{j}}$
\If{$\size{\VVelement{S}{j}} = 0$}
\State return \Comment Reject \Cw{}
\EndIf
\State $\Call{Reducer}{\VVelement{S}{j}}$
\State $\Call{Predictor}{\VVelement{S}{j}}$
\label{alg:marpa-line-80}
\State $\Call{Memoizer}{\VVelement{S}{j}}$
\State $\Call{Bookkeeper}{\VVelement{S}{j}}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{observation}[Original Earley Algorithm: Top level]
If line \ref{alg:marpa-line-80} is removed,
\Aref{alg:marpa} has the same effect as \Earley{}\cite{Earley1970}.
This can be seen from
\Aref{alg:marpa},
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-op},
\Obref{obs:earley-completer-op},
\Obref{obs:earley-predictor-op},
and
\Obref{obs:earley-memoizer-op}.
\dispEnd
\end{observation}

\section{Summary}

\todo{Rewrite operations summary}

\begin{observation}[\Marpa{} operation by type of result]
\label{obs:marpa-op-by-result}
Let \Vpim{rez} be the result of a \Marpa{} operation.
Every \type{PIM} is the result of exactly \Marpa{} operation,
as follows:
\begin{itemize}
\item $\Current{\Vpim{rez}} = 0$
iff \Vpim{rez} is the result of the
\Marpa{} \var{Initializer} operation.
\item \Vpim{rez} is an \type{EIM},
$\Predot{\Veim{rez}} = \Lambda$, and
$\Current{\var{rez}} > 0$,
iff
\var{rez} is the result of a
\Marpa{} \var{Predictor} operation.
\item \Vpim{rez} is an \type{EIM},
$\Predot{\Veim{rez}} \in \Term{\var{g}}$, and
$\Current{\var{rez}} > 0$,
iff
\var{rez} is the result of a
\Marpa{} \var{Scanner} operation.
\item \Vpim{rez} is an \type{EIM},
$\Predot{\Veim{rez}} \in \NT{\var{g}}$, and
$\Current{\var{rez}} > 0$
iff \var{rez} is the result of a
\Marpa{} \var{Reducer} operation.
\item \Vpim{rez} is a \type{LIM}
iff
\var{rez} is the result of a
\Marpa{} \var{Memoizer} operation.
\end{itemize}
This observation follows from
\Aref{alg:initializer-op},
\Aref{alg:scanner-op},
\Aref{alg:reducer-op},
\Eref{eq:def-predictor-op}, and
\Aref{alg:memoizer-op}.
\dispEnd
\end{observation}

\begin{observation}[\Earley{} operation by type of result]
\label{obs:earley-op-by-result}
The operation of the original \Earley{} algorithm can be determined
from the result as described in
\Obref{obs:marpa-op-by-result},
except
in \Earley{} there is no \var{Memoizer} operation,
and there are no \type{LIM}s.
This follows from
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-op},
\Obref{obs:earley-completer-op},
\Obref{obs:earley-predictor-op},
\Obref{obs:earley-memoizer-op}, and
\Obref{obs:marpa-op-by-result}.
\dispEnd
\end{observation}

\begin{observation}[\Marpa{} operation by direct ancestor]
\label{obs:marpa-op-by-direct-ancestor}
If a completion is a direct ancestor of a \Marpa{}
operation, that operation is the \var{Reducer} operation.
If a token is a direct ancestor of a \Marpa{}
operation, that operation is the \var{Scanner} operation.
This observation follows from
\Aref{alg:initializer-op},
\Aref{alg:scanner-op},
\Aref{alg:reducer-op},
\Eref{eq:def-predictor-op}, and
\Aref{alg:memoizer-op}.
\dispEnd
\end{observation}

\begin{observation}[\Earley{} operation by direct ancestor]
\label{obs:earley-op-by-direct-ancestor}
If a completion is a direct ancestor of an \Earley{}
operation, that operation is the Completer operation.
If a token is a direct ancestor of a \Marpa{}
operation, that operation is the Scanner operation.
This observation follows from
\Obref{obs:earley-set-0},
\Obref{obs:earley-scanner-op},
\Obref{obs:earley-completer-op},
\Obref{obs:earley-predictor-op} and
\Obref{obs:earley-memoizer-op}.
\dispEnd
\end{observation}

\todo{complexity observations}

\chapter{Ambiguity}
\label{chap:ambiguity}

\todo{Discuss the sources of this approach.}

We say that a parse $[ \Vcfg{g}, \Vstr{w1} ]$
is ambiguous, if there is more than one derivation tree for it.
Most derivation trees allow more than one derivation,
but for every derivation tree there is exactly one rightmost
derivation.
So a parse is ambiguous if and only if there is more than
one rightmost derivation tree for \Vstr{w1}.
A grammar is ambiguous if and only if there is some input
for which the parse is ambiguous.

We call a string of terminals derived from a sentential form
its \dfn{frontier}.
If a parse is unambiguous, the frontier of a sentential form
form derives is unique, so that
we can write the frontier of
\Vstr{sf} as $\Vfrontier{sf}$,
where we will have that
$\Vfrontier{sf} \in \Term{\var{g}}^\ast$
for the current choice of \Vcfg{g}.

We recognize two kinds of ambiguity:
horizontal and vertical.

\begin{definition}[Horizontal Ambiguity]
\label{def:h-ambig}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse,
and let \var{gallia} be a triple $[\Vrule{r}, \var{pos1}, \var{pos2}]$,
where $0 \le \var{pos1} < \var{pos2} \le \size{\RHS{\var{r}}}$,
indicating a rule whose RHS is partitioned
into three parts.
The 3-partitioned rule may be written as
\begin{equation}
\Vrule{r} = [ \Vsym{lhs} \de \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} ],
\end{equation}
where
\begin{gather}
\Vstr{rhs1} = (\RHS{\var{r}})[0 \ldots \var{pos1}\subtract 1], \\
\Vstr{rhs2} = (\RHS{\var{r}})[\var{pos1} \ldots \var{pos2}\subtract 1], \;\; \text{and} \\
\Vstr{rhs3} = (\RHS{\var{r}})[\var{pos2} \ldots \size{\RHS{\var{r}}}\subtract 1].
\end{gather}
A \dfn{horizontal ambiguity} is a 5-tuple,
\begin{equation}
\begin{gathered}
[ \var{gallia}, \Vloc{origin}, \Vloc{overlap1}
\Vloc{overlap2}, \Vloc{dot} ]
\end{gathered}
\end{equation}
such that
\begin{align}
\label{eq:def-h-ambig-21a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-h-ambig-21b}
& \destar \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c}
& \derives \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vstr{suffix} \\
\label{eq:def-h-ambig-21c2}
& \destar \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-h-ambig-21d}
& \destar \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:def-h-ambig-21e}
& \destar \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-h-ambig-21f}
& \destar \Vterm{w},
\end{align}
and also such that
\begin{gather}
\label{eq:def-h-ambig-30a}
\Vstr{rhs2} \destar \boldRangeDecr{w}{overlap2}{dot}
\;\; \text{and} \\
\label{eq:def-h-ambig-30b}
\var{overlap1} \neq \var{overlap2}.
\end{gather}

\var{gallia} is called the \dfn{partitioned rule},
\Vloc{orig} the \dfn{origin}, and
\Vloc{dot} the \dfn{dot position}
of the horizontal ambiguity.
The duple $[\var{overlap1}, \var{overlap2}]$ is
called its \dfn{overlap}.
The string
\[
\var{w}[\var{overlap1}\ldots(\var{overlap2}\subtract 1)]
\]
is also called its \dfn{overlap}.

If there is a horizontal ambiguity for a parse \var{p},
then
\var{p} is a horizontally ambiguous parse.
A grammar is \dfn{horizontally ambiguous} if and only if it has a
horizontally ambiguous parse. \dispEnd{}
\end{definition}

Note that if $\Vstr{rhs1} = \epsilon$, that
$\var{overlap1} = \var{overlap2}$.
Among other things, this implies that a unit rule
cannot be the rule of a horizontally ambiguity.

\begin{theorem}[Grammar of horizontally ambiguous parse is ambiguous]
\label{th:h-ambig-g}
If a parse is horizontally ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but contains a horizontal ambiguity as defined in \Dfref{def:h-ambig}.
Since the parse is unambiguous, we can write
\[
\Vstr{suffix} \destar \Vfrontier{suffix},
\]
where
$\Vfrontier{suffix} \in \Term{\var{g}}^\ast$.
Then we can rewrite
\Eref{eq:def-h-ambig-21a}--\Eref{eq:def-h-ambig-21f}
as the following right derivation
\begin{align}
\label{eq:th-h-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vfrontier{suffix} \\
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \Vstr{rhs3} \Vfrontier{suffix} \\
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{current}{w} \\
\label{eq:th-h-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\label{eq:th-h-ambig-10-1-e}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-h-ambig-10-1-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We have replaced the concatenation operator
in \Eref{eq:th-h-ambig-10-1-d}
with the $\spadesuit$ symbol
for convenience in referring to that location.

Since the parse is assumed to be unambiguous, the right derivation must be
unique.
The step which derives \Eref{eq:th-h-ambig-10-1-d} in unique within its derivation
as the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$.
Therefore
\Eref{eq:th-h-ambig-10-1-d}
is a unique step of a unique derivation.

Note that no assumption is made that any of
\Vstr{prefix}, \Vstr{rhs1}, \Vstr{rhs2}, \Vstr{rhs3},
and \Vstr{suffix} contain
non-terminals.
If they do not, the derivations of terminal strings are trivial,
and our claims about the presence
and elimination of non-terminals are trivially or vacuously true.
For example, if \Vstr{rhs1} contains no non-terminals,
then the step from
\Eref{eq:th-h-ambig-10-1-d} to
\Eref{eq:th-h-ambig-10-1-e}
is trivial and the sentential forms of
\Eref{eq:th-h-ambig-10-1-d} and
\Eref{eq:th-h-ambig-10-1-e}
are identical,
so that our claim that
\Eref{eq:th-h-ambig-10-1-d} occurs prior to the elimination
of any non-terminals in \Vstr{rhs1} is vacuously true.

We go back
to \Dfref{def:h-ambig}, and write a second right derivation for the parse.
Because our parse is unambiguous, this derivation should be equivalent to the
derivation from
\Eref{eq:th-h-ambig-10-1-a}
to
\Eref{eq:th-h-ambig-10-1-f}.
\begin{align}
\label{eq:th-h-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \Vstr{suffix} \\
\label{eq:th-h-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w} \\
\label{eq:th-h-ambig-10-2-f}
& \xderives{R\ast} \Vterm{w}.
\end{align}
The step which derives
the sentential form of \Eref{eq:th-h-ambig-10-2-d}
also the first to produce a sentential form
with no non-terminals to the right of $\spadesuit$,
but before the elimination of any non-terminals to the left of $\spadesuit$ and,
since they result from the same steps of identical derivations,
the two sentential forms \Eref{eq:th-h-ambig-10-1-d}
and \Eref{eq:th-h-ambig-10-2-d}
should be identical.
But $\var{overlap1} \neq \var{overlap2}$
\Eref{eq:def-h-ambig-30b}
and therefore
\begin{equation}
\label{eq:th-h-ambig-50}
\begin{gathered}
\Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap1}{w} \\
\neq \Vstr{prefix} \Vstr{rhs1} \mathop{\spadesuit} \boldRangeSizeDecr{w}{overlap2}{w}
\end{gathered}
\end{equation}
From
\Eref{eq:th-h-ambig-50}
we see that the
derivation from
\Eref{eq:th-h-ambig-10-1-a}
to
\Eref{eq:th-h-ambig-10-1-f}
is not the same as the derivation from
\Eref{eq:th-h-ambig-10-2-a} to
\Eref{eq:th-h-ambig-10-2-f}.
Since these are both right derviations,
the parse must be ambiguous, which shows
the reductio.
Since the parse is ambiguous, its grammar is ambiguous.
This shows the theorem. \myqed
\end{proof}

\begin{definition}[Vertical Ambiguity]
\label{def:v-ambig}
Let $[\Vcfg{g}, \Vstr{w}]$ be a parse.
A \dfn{vertical ambiguity} is a 4-tuple,
\begin{equation}
\label{eq:def-v-ambig-10a}
\left[ \Vrule{r1}, \Vrule{r2}, \Vloc{origin}, \Vloc{dot} \right],
\end{equation}
such that
\begin{gather}
\label{eq:def-v-ambig-15a}
\var{r1} = [ \var{lhs} \de \Vstr{rhs1} ], \\
\label{eq:def-v-ambig-15b}
\var{r2} = [ \var{lhs} \de \Vstr{rhs2} ], \\
\label{eq:def-v-ambig-15c}
\var{rhs1} \neq \var{rhs2}, \\
\label{eq:def-v-ambig-15d}
\var{rhs2} \destar \boldRangeDecr{w}{origin}{dot},
\end{gather}
and
\begin{align}
\label{eq:def-v-ambig-20a}
& \qquad \Accept{\var{g}} \\
\label{eq:def-v-ambig-20b}
& \destar \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20c}
& \derives \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{dot}{w} \\
\label{eq:def-v-ambig-20d}
& \destar \;
\begin{aligned}
& \Vstr{prefix} \cat \boldRangeDecr{w}{origin}{dot} \\
& \qquad \cat \boldRangeSizeDecr{w}{dot}{w}
\end{aligned}
\\
\label{eq:def-v-ambig-20e}
& = \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:def-v-ambig-20f}
& \destar \Vterm{w}.
\end{align}

\var{r1} and \var{r2} are the \dfn{rules}
of the vertical ambiguity,
\Vloc{orig} is its \dfn{origin}, and
\Vloc{dot} is its \dfn{dot position}.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse. \dispEnd{}

If there is a vertical ambiguity for a parse \var{p},
then
\var{p} is a vertically ambiguous parse.
A grammar is \dfn{vertically ambiguous} if and only if it has a
vertically ambiguous parse. \dispEnd{}
\end{definition}

\begin{theorem}[Grammar of vertically ambiguous parse is ambiguous]
\label{th:v-ambig-g}
If a parse is vertically ambiguous,
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{p} = [\Vcfg{g}, \Vstr{w}]$ be a parse.
We consider first the case where \var{p} contains a
cycle.
If \var{p} contains a cycle, it and its grammar are
ambiguous, which shows the theorem directly.
Therefore, for the rest of this proof, we assume that
\var{p} is free of cycles.

We assume for a reductio that
\var{p} is unambiguous,
but has a vertical ambiguity as defined in \Dfref{def:v-ambig}.
Noting that
\Eref{eq:def-v-ambig-20a} through
\Eref{eq:def-v-ambig-20f}
in \Dfref{def:v-ambig} is a right derivation,
we can write
\begin{align}
\label{eq:th-v-ambig-10-1-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-1-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs1} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-1-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-1-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
We also have a right derivation
\begin{align}
\label{eq:th-v-ambig-10-2-a}
& \qquad \Accept{\var{g}} \\
\label{eq:th-v-ambig-10-2-b}
& \xderives{R\ast} \Vstr{prefix} \Vsym{lhs} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-c}
& \xderives{R} \Vstr{prefix} \Vstr{rhs2} \cat \boldRangeSizeDecr{w}{(dot+1)}{w} \\
\label{eq:th-v-ambig-10-2-d}
& \xderives{R\ast} \Vstr{prefix} \cat \boldRangeSizeDecr{w}{origin}{w} \\
\label{eq:th-v-ambig-10-2-e}
& \xderives{R\ast} \Vterm{w}.
\end{align}
by
\begin{itemize}
\item letting the derivation of
\Eref{eq:th-v-ambig-10-2-b} from
\Eref{eq:th-v-ambig-10-2-a}
be identical to the derivation of
\Eref{eq:th-v-ambig-10-1-b} from
\Eref{eq:th-v-ambig-10-1-a};
\item
using the rule
\Eref{eq:def-v-ambig-15b}
to derive
\Eref{eq:th-v-ambig-10-2-c};
\item
using
\Eref{eq:def-v-ambig-15d}
to derive
\Eref{eq:th-v-ambig-10-2-d};
\item and letting the derivation of
\Eref{eq:th-v-ambig-10-2-e} from
\Eref{eq:th-v-ambig-10-2-d}
be identical to the derivation of
\Eref{eq:th-v-ambig-10-1-e} from
\Eref{eq:th-v-ambig-10-1-d};
\end{itemize}

Since \var{p} is assumed to be unambiguous,
the two right derivations
\Eref{eq:th-v-ambig-10-1-a}--\Eref{eq:th-v-ambig-10-1-e}
and
\Eref{eq:th-v-ambig-10-2-a}--\Eref{eq:th-v-ambig-10-2-e}
must be identical,
and therefore must be a sequence of identical sentential forms.

\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c}
must be sentential forms in the same position of their
identical derivations.
We know this
because both result from the production of the \Vsym{lhs}
when that \Vsym{lhs} is the rightmost non-terminal and
the underived part of sentential form is $\Vstr{prefix}\Vsym{lhs}$.
We know that this the only time that happens because otherwise
\[
\Vstr{prefix}\Vsym{lhs} \deplus \Vstr{prefix}\Vsym{lhs},
\]
in which case \var{p} contains a cycle,
which is contrary to assumption for this case.
Therefore
\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c} are sentential forms after the
same step of identical derivations,
and are therefore equal.

But from the definition of vertical ambiguity,
we have $\var{rhs1} \neq \var{rhs2}$,
so that
the sentential forms at
\Eref{eq:th-v-ambig-10-1-c} and
\Eref{eq:th-v-ambig-10-2-c}
are not equal.
Therefore the two right derivations differ,
and \var{p} is ambiguous,
which shows the reductio.
Since \var{p} is ambiguous, its grammar, \Vcfg{g} must
also be ambiguous.
This shows the theorem. \myqed
\end{proof}

\begin{definition}
\label{def:ambig-eim}
An confirmed \type{EIM} is \dfn{ambiguous} if it is of the form
\begin{equation}
\label{eq:ambig-eim-05}
\left[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs3} ],
\Vloc{origin}
\right] @ \Vloc{dot}
\end{equation}
\Eref{eq:ambig-eim-05}
and has two distinct and valid link pairs.
Without loss of generality,
the predecessor in the first link pair is
the valid \type{EIM}
\begin{equation}
\label{eq:ambig-eim-10a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap1},
\end{equation}
and its cause is the valid \type{EIM}
\begin{equation}
\label{eq:ambig-eim-10b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children1} \mydot ], \var{overlap1}
\right] @ \Vloc{dot}.
\end{equation}
Also without loss of generality,
the predecessor in the second link pair is the valid \type{EIM}
\begin{equation}
\label{eq:ambig-eim-20a}
\left[
\begin{gathered}
[ \var{lhs} \de \Vstr{rhs1} \mydot \Vsym{rhs2} \Vstr{rhs3} ], \\
\Vloc{origin}
\end{gathered}
\right] @ \Vloc{overlap2},
\end{equation}
and the cause of the second link pair is the valid \type{EIM}
\begin{equation}
\label{eq:ambig-eim-20b}
\left[
  [ \Vsym{rhs2} \de \Vstr{children2} \mydot ], \var{overlap2}
\right] @ \Vloc{dot}.
\end{equation}

The \type{EIM} is \dfn{vertically ambiguous} if
\begin{equation}
\label{eq:ambig-eim-02}
\Vloc{overlap1} = \Vloc{overlap2},
\end{equation}
and the \type{EIM} is
\dfn{horizontally ambiguous} if
\begin{equation}
\label{eq:ambig-eim-03}
\Vloc{overlap1} \neq \Vloc{overlap2}. \quad \dispEnd
\end{equation}
\end{definition}

\begin{theorem}[Parse with horizontal ambiguity is ambiguous]
\label{th:h-eim-g-ambig}
If a partial parse has a horizontally ambiguous \type{EIM},
then any full parse consistent with that partial parse is
horizontally ambiguous,
and the grammar shared by the partial and full parse is horizontally ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-h-eim-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Futher assume that \var{partial}
has a horizontally ambiguous \type{EIM} \Dfref{def:ambig-eim}.

Without loss of generality,
let
\begin{equation}
\label{eq:th-h-eim-g-ambig-20}
\begin{gathered}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\text{for some \Vterm{suffix}}
\end{gathered}
\end{equation}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a horizontal ambiguity in \var{full}
according to \Dfref{def:h-ambig}.
By assumption for the theorem,
\var{partial} has a horizontally ambiguous \type{EIM} according to
\Dfref{def:ambig-eim}.
Let \Vloc{origin},
\Vloc{overlap1},
\Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs},
\Vsym{rhs1}, and
\Vstr{rhs3}
in \Dfref{def:h-ambig}
be their eponyms in \Dfref{def:ambig-eim}.
And let
\Vstr{rhs2} in \Dfref{def:h-ambig}
be the string of length 1 whose only character is
\Vsym{rhs2} in \Dfref{def:ambig-eim}.

In the horizontally ambiguous \type{EIM}
let the 3-partitioned rule be
\[
[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \Vstr{rhs3} ].
\]
from \Dfref{def:ambig-eim}.
and let \Vloc{origin}, \Vloc{overlap1}, \Vloc{overlap2},
\Vloc{dot},
\Vsym{lhs}, \Vstr{rhs1}, and \Vstr{rhs3}
in
\Dfref{def:h-ambig} be their eponyms in
\Dfref{def:ambig-eim}.
Let \Vstr{rhs2}
in \Dfref{def:h-ambig} be
the string of length 1 consisting of \Vsym{rhs2}
in \Dfref{def:ambig-eim}.

To show that
writing the derivation
\Eref{eq:def-h-ambig-21a}--\Eref{eq:def-h-ambig-21f}
in \Dfref{def:h-ambig}
is justified by the definition
\Dfref{def:ambig-eim}, we note that
\begin{itemize}
\item from the definition of validity
\Dfref{def:eim-validity}
for the Earley item
\Eref{eq:ambig-eim-05},
we have the accessibility and productivity
of all the symbols in
\Eref{eq:def-h-ambig-21a}-\Eref{eq:def-h-ambig-21f};
\item from the validity of
the \type{EIM} \Eref{eq:ambig-eim-10b}, we have
the location of \Vloc{dot}
in \Eref{eq:def-h-ambig-21d}
and of \Vloc{origin}
in \Eref{eq:def-h-ambig-21f}; and
\item
from the definition of \type{EIM} validity for
\Eref{eq:ambig-eim-10b},
we have the location of \Vloc{overlap1}
in \Eref{eq:def-h-ambig-21e}.
\end{itemize}

To show
the remaining elements of \Dfref{def:h-ambig}
we note that
\begin{itemize}
\item we have the location of \Vloc{overlap2}
in \Eref{eq:def-h-ambig-30a}
from the definition of \type{EIM} validity for
\Eref{eq:ambig-eim-20a}; and
\item
we have \Eref{eq:def-h-ambig-30b}
from \Eref{eq:ambig-eim-03}.
\end{itemize}

We now have shown all the elements
in the definition of horizontal ambiguity \Dfref{def:h-ambig}.
Since our assumption of a full parse consistent with
\Eref{eq:th-h-eim-g-ambig-15} is without loss of generality
\Eref{eq:th-h-eim-g-ambig-20},
this shows that any full parse consistent with
\Eref{eq:th-h-eim-g-ambig-15} is horizontally ambiguous.
And since by \Thref{th:h-ambig-g}, if a parse is horizontally
ambiguous,
its grammar is horizontally ambiguous,
we see that \Vcfg{g} is horizontally ambiguous. \myqed
\end{proof}

\begin{theorem}[Parse with vertical ambiguity is ambiguous]
\label{th:v-eim-g-ambig}
If a partial parse has a vertically ambiguous \type{EIM},
then any full parse consistent with that partial parse is vertically ambiguous,
and the grammar shared by the partial and full parse is vertically ambiguous.
\end{theorem}

\begin{proof}
Assume that,
for some \Vstr{w} and \Vloc{dot},
\begin{equation}
\label{eq:th-v-eim-g-ambig-15}
\var{partial} = \left[\Vcfg{g}, \boldRangeDecr{w}{0}{dot} \right]
\end{equation}
is a partial parse.
Futher assume that \var{partial}
has a vertically ambiguous \type{EIM} \Dfref{def:ambig-eim}.

Without loss of generality,
let
\begin{gather}
\var{full} = \left[\Vcfg{g}, \Vstr{w} \right], \\
\label{eq:th-v-eim-g-ambig-20b}
\text{where $\Vstr{w} = \boldRangeDecr{w}{0}{dot} \cat \Vterm{suffix}$} \\
\nonumber
\text{for some \Vterm{suffix}}
\end{gather}
be a full parse consistent with \var{partial}.

We proceed by showing that there is a vertical ambiguity in \var{full}
according to \Dfref{def:v-ambig}.
By assumption for the theorem,
\var{partial} has a vertically ambiguous \type{EIM} according to
\Dfref{def:ambig-eim}.

Let
\begin{align}
\label{eq:th-v-eim-g-ambig-25a}
\text{\Vsym{rhs2} in \Dfref{def:ambig-eim}} & = \text{\Vsym{lhs} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25b}
\text{\Vstr{children1} in \Dfref{def:ambig-eim}} & = \text{\Vstr{rhs1} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25c}
\text{\Vstr{children2} in \Dfref{def:ambig-eim}} & = \text{\Vstr{rhs2} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25d}
\text{\Vloc{dot} in \Dfref{def:ambig-eim}} & = \text{\Vloc{dot} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25e}
\text{\Vloc{origin} in \Dfref{def:ambig-eim}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25f}
\text{\Vloc{overlap1} in \Dfref{def:ambig-eim}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}} \\
\label{eq:th-v-eim-g-ambig-25g}
\text{\Vloc{overlap2} in \Dfref{def:ambig-eim}} & = \text{\Vloc{origin} in \Dfref{def:v-ambig}}
\end{align}

To justify writing
\Eref{eq:def-v-ambig-20a}--\Eref{eq:def-v-ambig-20f}
after the renamings of
\Eref{eq:th-v-eim-g-ambig-25a}--\Eref{eq:th-v-eim-g-ambig-25g},
we note that we have
\begin{itemize}
\item
\Eref{eq:def-v-ambig-15a} from
from
\Eref{eq:th-v-eim-g-ambig-25a},
\Eref{eq:th-v-eim-g-ambig-25b} and
\Eref{eq:ambig-eim-10b};
\item
\Eref{eq:def-v-ambig-15b} from
\Eref{eq:th-v-eim-g-ambig-25a},
\Eref{eq:th-v-eim-g-ambig-25c} and
\Eref{eq:ambig-eim-20b};
\item
the accessibility and productivity of the variables in
\Eref{eq:def-v-ambig-20a}--\Eref{eq:def-v-ambig-20f}
from the definition of \type{EIM} validity for
\Eref{eq:ambig-eim-10b};
\item
the location of \Vloc{dot},
in \Eref{eq:def-v-ambig-20b}
from the definition of \type{EIM} validity for
\Eref{eq:ambig-eim-10b}
and \Eref{eq:th-v-eim-g-ambig-25d};
\item
the location of \Vloc{origin}
in \Eref{eq:def-v-ambig-20d}
from the definition of \type{EIM} validity for
\Eref{eq:ambig-eim-10b}
and \Eref{eq:th-v-eim-g-ambig-25f}; and
\item
\Eref{eq:def-v-ambig-15d} follows from
\Eref{eq:ambig-eim-20b},
\Eref{eq:th-v-eim-g-ambig-25d}, and
\Eref{eq:th-v-eim-g-ambig-25g}.
\end{itemize}

It remains to justify
$\var{rhs1} \neq \var{rhs2}$,
\Eref{eq:def-v-ambig-15c}.
In \Dfref{def:ambig-eim}
the two link pairs are required to be distinct.
The predecessors of the two links,
\Eref{eq:ambig-eim-10a} and
\Eref{eq:ambig-eim-20a}, differ only in their dot locations:
\Vloc{overlap1} versus \Vloc{overlap2}.
In
\Eref{eq:th-v-eim-g-ambig-25f} and
\Eref{eq:th-v-eim-g-ambig-25g},
we have set
both of these
to \Vloc{origin}
in \Dfref{def:v-ambig},
so that
\begin{equation}
\label{eq:th-v-eim-g-ambig-40}
\begin{gathered}
\text{\Vloc{overlap1} in \Eref{eq:ambig-eim-10a}} \\
= \text{\Vloc{overlap2} in \Eref{eq:ambig-eim-20a}} \\
= \text{\Vloc{origin} in \Dfref{def:v-ambig}.}
\end{gathered}
\end{equation}
So, to be distinct, the two link pairs must differ in the causes.
The two causes,
\Eref{eq:ambig-eim-10b} and
\Eref{eq:ambig-eim-20b}
can differ only in their origins
and in their RHS's.
From
\Eref{eq:th-v-eim-g-ambig-40}
we know that the origins of
\Eref{eq:ambig-eim-10a} and
\Eref{eq:ambig-eim-10b}
are identical,
so that
\Eref{eq:ambig-eim-10b} must differ from
\Eref{eq:ambig-eim-20b} in its RHS:
\begin{equation}
\label{eq:th-v-eim-g-ambig-43}
\begin{gathered}
\text{\Vstr{children1} in \Eref{eq:ambig-eim-10a}} \\
\neq \text{\Vstr{children2} in \Eref{eq:ambig-eim-20a}}.
\end{gathered}
\end{equation}
From
\Eref{eq:th-v-eim-g-ambig-43},
\Eref{eq:th-v-eim-g-ambig-25b}, and
\Eref{eq:th-v-eim-g-ambig-25c}
we have
\begin{equation}
\label{eq:th-v-eim-g-ambig-46}
\Vstr{rhs1} \neq \Vstr{rhs2}
  \text{ in \Eref{eq:def-v-ambig-15c}
    of \Dfref{def:v-ambig}. \myqed}
\end{equation}
\end{proof}

\begin{theorem}[Grammar of parse with ambiguity is ambiguous]
\label{th:ambig-eim-ambig-g}
If a partial parse has an ambiguous \type{EIM},
then its grammar is ambiguous.
\end{theorem}

\begin{proof}
Let $\var{partial} = [ \Vcfg{g}, \Vstr{prefix} ]$ be a partial
parse with an ambiguous \type{EIM}.
Let $\var{full} = [ \Vcfg{g}, \Vstr{prefix}\Vterm{suffix} ]$ be a full
parse consistent with \var{partial}.
From \Dfref{def:ambig-eim}, we see that an ambiguous \type{EIM}
is either a horizontally ambiguous \type{EIM}, or
a vertically ambiguous \type{EIM}.
We proceed by cases.

If the ambiguous \type{EIM} is horizontally ambiguous then
by \Thref{th:h-eim-g-ambig}
any full parse consistent with the partial parse
is horizontally ambiguous,
and therefore \var{full} is horizontally ambiguous.
By \Thref{th:h-ambig-g} if \var{full} is horizontally ambiguous,
then \Vcfg{g} is ambiguous.

If the ambiguous \type{EIM} is vertically ambiguous then
by \Thref{th:v-eim-g-ambig}
any full parse consistent with the partial parse
is vertically ambiguous,
and therefore \var{full} is vertically ambiguous.
By \Thref{th:v-ambig-g} if \var{full} is vertically ambiguous,
then \Vcfg{g} is ambiguous. \myqed
\end{proof}

\begin{theorem}[Unambiguous grammar does not have ambiguity]
\label{th:unambig-g-unambig-eim}
If a grammar is unambiguous, then it does not have
an ambiguous \type{EIM}.
\end{theorem}

\begin{proof}
This theorem is the contrapositive of \Thref{th:ambig-eim-ambig-g}. \myqed
\end{proof}

\chapter{Per-set lists}
\label{chap:per-set-lists}

\todo{Document complexity claims for \Seen{} and \SetSeen{}}

In the general case,
where \var{x} is an arbitrary datum,
it is not possible
to use duple $[\VVelement{S}{i}, \var{x}]$
as a search key and expect the search to use
\Oc{} time.
Within \Marpa, however, there are specific cases
where it is desirable to do exactly that.
This is accomplished by
taking advantage of special properties of the search.

If it can be arranged that there is
a direct link to the Earley set \VVelement{S}{i},
and that $0 \leq \var{x} < \var{c}$,
where \var{c} is a constant of reasonable size,
then a search can be made in \Oc{} time,
using a data structure called a PSL.
Data structures identical to,
or very similar to,
PSL's are
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
But neither source gives them a name.
The term PSL
(``per-Earley set list'')
is new
with this paper.

A PSL is a fixed-length array of
integers, indexed by an integer,
and kept as part of the structure
that implements each Earley set.
While \Marpa{} is building a new Earley set,
\VVelement{S}{j},
a PSL is kept for every previous Earley set ---
that is, for every
$\Vloc{i} < \Vloc{j}$.

\Marpa{} uses a PSL for its \var{seen} data structure.
In the case of \var{seen},
there is one fixed-size entry for every dotted rule in the grammar,
so that the size of \var{seen} for each ES
is
\[
\bigorder{\size{\DottedRules{\var{g}}}}.
\]
\DottedRules{\var{g}}
is a constant which depends on \Vint{g},
and therefore the space consumed by \var{seen}
\Oc{}, all of which is treated as trimmed space.
For the same reasons, the time used to allocate \var{seen}
for each ES is \Oc{}.

While \Marpa{} is adjoining \type{EIM}s into \VVelement{S}{j},
the PSL for \VVelement{S}{i}
keeps track of the Earley items in \VVelement{S}{j} that have \Vloc{i}
as their origin.
The maximum number of Earley items that must be tracked
in each PSL is
the number of dotted rules,
\Vsize{g}.
Recall that \Vsize{g}
is a constant of reasonable size
that depends on \Cg{}.

It would take more than \Oc{} time
to clear and rebuild the PSL's
for all the Earley sets
every time
that a new Earley set is started.
This overhead is avoided by ``time-stamping'' each PSL
entry with the Earley set
that was current when that PSL
entry was last updated.

Intuitively, the idea is avoid most re-computation
of the PSL's by keeping timestamps with the PSL data.
To illustrate the use of PSL's,
we will consider the case
where \Marpa{} is building \VVelement{S}{j}
and wants to check whether Earley item,
\begin{equation*}
\Veim{x} = [ \Vdr{x}, \Vorig{x} ],
\end{equation*}
is new,
or if \Veim{x} needs to be adjoined into the Earley sets.

Let
\begin{equation*}
\var{psl-entry} = \PSL{\VVelement{S}{i}}{\var{y}}
\end{equation*}
be the entry for integer \var{y} in the PSL in
the Earley set at \Vloc{x}, such that
\begin{equation*}
\begin{alignedat}{3}
& \var{psl-entry} && \defined && \quad
\begin{cases}
\Lambda, \quad \text{if the entry at $\var{psl-entry}$ has never been used,} \\
[\var{time-stamp}, \Vbool{z}], \text{otherwise} \\
\end{cases} \\
%
\end{alignedat}
\end{equation*}
Here \var{time-stamp} is a time stamp which will be used to
avoid unnecessary re-computation,
and \Vbool{z} is \var{true} if \Veim{x}
is already present in \VVelement{S}{j},
and \var{false} otherwise.

\begin{algorithm}[t]
\caption{Adjoin \type{EIM}}
\label{alg:adjoin-eim}
\begin{algorithmic}[1]
\Procedure{Adjoin-EIM}{$[\Vdr{x},\Vorig{x}]@\VVelement{S}{j}$}
\If{$\PSL{\VVelement{S}{j}}{\ID{\Vdr{x}}} = [\Vloc{j}, \var{true}]$}
\State return \Comment Do nothing if \Veim{x} exists
\EndIf
\State $\VVelement{S}{j} \gets \VVelement{S}{j} \cup \set{[\Vdr{x]}, \Vorig{x}]}$
\State $\PSL{\Ves{x}}{\ID{\Vdr{x}}} \gets [\Vloc{j}, \var{true}]$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\Aref{alg:adjoin-eim} contains pseudo-code
showing the use of PSL's.
The reader may notice that
\Aref{alg:adjoin-eim} could be simplified,
because the data to be kept for \type{EIM}s is a single boolean.
Further, this boolean is never set to \var{false}, so that
if a PSL entry exists, its boolean may be assumed to be \var{true}.
This means that an explicit boolean is not actually needed.
The simpler approach, which omits the boolean,
is the one that
the \Marpa{} implementation sometimes uses.

\chapter{\Marpa{} is correct}
\label{chap:correctness}

In this paper,
we call a parsing algorithm \dfn{correct} if and
only if, for every parse, it is \dfn{consistent} and
\dfn{complete}.
We call a parsing algorithm \dfn{consistent} if
it produces only valid Earley items.
We call a parsing algorithm \dfn{complete} if
it produces all of the valid Earley items.

\begin{theorem}[\Earley{} is correct]
\label{th:earley-is-correct}
\Earley{} is correct.
\end{theorem}

\begin{proof}
\Earley{} is proved correct in
\cite[Theorem 4.9, pp. 323-325]{AU1972} and
\cite[pp. 18-25]{Earley1968}. \myqed
\end{proof}

The following is proved as part of
\Thref{th:earley-is-correct},
but we state and prove it separately
because we make special use of it.

\begin{theorem}[Earley completer consistency]
\label{th:earley-completer-is-consistent}
The \Earley{} completer operation preserves validity.
That is, if the predecessor and cause
of an Earley{} completer operation are valid,
the result of the Earley{} completer operation is valid.
\end{theorem}

\begin{proof}
Let $[\Vint{g}, \Vstr{prefix}]$ be a partial parse,
let
\[
[ \Veim{pred}, \Veim{cuz} ]_\type{LP}
\]
be an arbitrary ancestry of an Earley{} completer operation,
and let the result of the operation be \Veim{rez}.
Then
\begin{equation}
\label{eq:earley-completer-is-consistent-20}
  \begin{gathered}
  \Vdr{pred} =
  \left[ \Vsym{lhsP} \de \Vstr{rhs1} \mydot \Vsym{postdot} \Vstr{rhs3}
  \right] \\
  \because \Eref{eq:earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-22}
  \begin{gathered}
  \Veim{pred} =
  \left[ \Vdr{pred}, \Vloc{orig} \right] @ \Vloc{meet} \\
  \because \Eref{eq:earley-completer-summary-2},
    \Eref{eq:earley-completer-is-consistent-20}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-24}
\LHS{\Vdr{cuz}} = \Vsym{postdot}
\because \Eref{eq:earley-completer-summary-3},
  \Eref{eq:earley-completer-is-consistent-20}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-26}
  \begin{gathered}
  \Veim{cuz} = \left[ \Vdr{cuz}, \Vloc{meet} \right]
    @ \Vloc{current} \\
  \because \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-summary-1},
  \Eref{eq:earley-completer-summary-2}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-28}
  \begin{gathered}
  \Veim{rez} =
  \left[
    \begin{gathered}
      \Next{\DR{\var{pred}}}, \\
      \Origin{\var{pred}}
    \end{gathered}
  \right]@\Vloc{current} \\
  \because
  \Eref{eq:earley-completer-summary-4},
  \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-is-consistent-26}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-30}
\myparbox{\Veim{pred} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-32}
\myparbox{\Veim{cuz} is valid
by assumption for theorem.}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-34}
\Rule{\Veim{pred}} \in \Rules{\var{g}}
\because \Eref{eq:eim-valid-22a},
  \Eref{eq:earley-completer-is-consistent-30}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-36}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{pred}} \cat \Vstr{after} \\
  \because \Eref{eq:eim-valid-22b},
    \Eref{eq:earley-completer-is-consistent-30}, \text{WLOG}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-38}
\begin{gathered}
  \Vstr{before} \destar \var{prefix}[0  \ldots  (\var{orig}\subtract 1)] \\
  \because \Eref{eq:eim-valid-22c},
    \Eref{eq:earley-completer-is-consistent-22},
    \Eref{eq:earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-40}
\begin{gathered}
  \Vstr{rhs1} \destar \var{prefix}[\var{orig}  \ldots  (\var{meet}\subtract 1)] \\
  \because \Eref{eq:eim-valid-22d},
  \Eref{eq:earley-completer-is-consistent-20},
  \Eref{eq:earley-completer-is-consistent-22},
  \Eref{eq:earley-completer-is-consistent-30}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-42}
\Rule{\Veim{pred}} = \Rule{\Veim{rez}}
\because \Eref{eq:earley-completer-is-consistent-28}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-44}
\Rule{\Veim{rez}} \in \Rules{\var{g}}
\because \Eref{eq:earley-completer-is-consistent-34},
\Eref{eq:earley-completer-is-consistent-42}.
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-46}
\myparbox{$\LHS{\Veim{pred}} = \LHS{\Veim{rez}}$
$\because$ Def \var{Next} \Eref{eq:def-next},
  \Eref{eq:earley-completer-is-consistent-42}.
}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-48}
\begin{gathered}
  \Accept{\var{g}} \destar \Vstr{before} \cat \LHS{\Veim{rez}} \cat \Vstr{after} \\
  \because \Eref{eq:earley-completer-is-consistent-36},
  \Eref{eq:earley-completer-is-consistent-46}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-50}
\begin{aligned}
  & \Vdr{rez} = \left[ \Vsym{lhsP} \de \Vstr{rhs1} \Vsym{postdot} \mydot \Vstr{rhs3}
  \right] \\
  & \qquad \quad \because \Eref{eq:earley-completer-is-consistent-20}, \Eref{eq:earley-completer-is-consistent-28}.
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-52}
\myparbox{\Veim{cuz} is a completion
  $\because \Eref{eq:earley-completer-summary-1}$.}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-53}
  \begin{gathered}
    \LHS{\Vdr{cuz}} \destar \var{prefix}[\var{meet}  \ldots  (\var{current}\subtract 1)] \\
    \because \Eref{eq:eim-valid-22d},
    \Eref{eq:earley-completer-is-consistent-26},
    \Eref{eq:earley-completer-is-consistent-32},
    \Eref{eq:earley-completer-is-consistent-52}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-54}
  \begin{gathered}
  \Vstr{postdot} \destar
  \var{prefix}[\var{meet}  \ldots  (\var{current}\subtract 1)] \\
  \because
  \Eref{eq:earley-completer-is-consistent-24},
  \Eref{eq:earley-completer-is-consistent-53}.
  \end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:earley-completer-is-consistent-56}
\begin{aligned}
  & \Vstr{rhs1} \Vstr{postdot} \\
  & \qquad \destar \var{prefix}[\var{orig}  \ldots  (\var{current}\subtract 1)] \\
  & \qquad \qquad \because \Eref{eq:earley-completer-is-consistent-40},
    \Eref{eq:earley-completer-is-consistent-54}
  \end{aligned}
\end{equation}
\begin{equation}
\label{eq:earley-completer-is-consistent-58}
\myparbox{\Veim{rez} is valid
  $\because$ \Dfref{def:eim-validity},
  \Eref{eq:earley-completer-is-consistent-38},
  \Eref{eq:earley-completer-is-consistent-44},
  \Eref{eq:earley-completer-is-consistent-48},
  \Eref{eq:earley-completer-is-consistent-56}.
  \myqed
}
\end{equation}
\end{proof}

% \TODO{Delete this?}
% Let \Ves{physical} be an Earley set as produced by \Marpa{},
% and let \Vves{virtual} be \Ves{physical} with its Leo memoized
% items restored.
% We will call \Ves{physical} a \dfn{physical} Earley set, and
% we will call \Vves{virtual} a \dfn{virtual} Earley set
% (type \type{VES}).

\begin{definition}[Leo chain]
\label{def:leo-chain}
A \dfn{Leo chain},
or simply \dfn{chain} when the meaning is clear in context,
is a sequence of one or more \type{LIM}s,
call it $\Vlimset{links}$,
where
\[
\PreviousLink{\var{links}[\Vlastix{links}]} = \emptyset
\]
and, for all $0 \le \var{n} < \Vlastix{links}$,
\[
\PreviousLink{\var{links}[\var{n}]} = \set{\var{links}[\var{n}+1]}.
\]
We say that \var{links} is the chain of \Vlim{first} if and
only if \var{links} is a Leo chain
such that $\var{links}[0] = \Vlim{first}$.

A chain is \dfn{maximal} if and only if it is not properly
contained in another chain.
A chain is \dfn{partial} if it is contained in another chain.
Note that a maximal chain is a special case of a partial chain.
Chains are maximal unless stated otherwise.
\dispEnd{}
\end{definition}

\begin{theorem}[LEO Chain containment]
\label{th:chain-containment}
Let
\begin{gather}
\label{eq:th-chain-containment-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-containment-02}
\myparbox{$\mylim{\ell} \in \var{longer}$ be a \type{LIM},} \\
\label{eq:th-chain-containment-03}
\myparbox{\var{shorter} be the valid Leo chain of \mylim{\ell}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-containment-07}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer}
\end{gather}
\end{theorem}

\begin{proof}
Let \Vlimset{longer}, \mylim{\ell},
and \Vlimset{shorter} be as stated for the theorem.

We show \Eref{eq:th-chain-containment-07} by induction on element index
of \var{shorter}.
\begin{equation}
\label{eq:th-chain-containment-10}
\begin{gathered}
0 \le \var{ix} \le \Vlastix{shorter} \implies \VVelement{shorter}{ix} \in \var{longer} \\
\text{$\because$ hypothesis for induction.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-12}
\myparbox{$\Velement{shorter}{0} \in \var{longer}$
  $\because$
  \Dfref{def:leo-chain}, \Eref{eq:th-chain-containment-03}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-13}
\begin{gathered}
0 \le 0 \le \Vlastix{shorter} \implies \Velement{shorter}{0} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-12}.
  This is the basis of an induction on
  hypothesis \Eref{eq:th-chain-containment-10}, with $\var{ix} = 0$.%
}
\end{gathered}
\end{equation}
For the step of the induction
we assume
\begin{equation}
\label{eq:th-chain-containment-14}
\begin{gathered}
0 \le \var{n} \le \Vlastix{shorter} \implies \VVelement{shorter}{n} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}$.%
}
\end{gathered}
\end{equation}
to show
\begin{equation}
\label{eq:th-chain-containment-14-10}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}+1$.%
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-15}
\begin{gathered}
0 \le \var{n} < \Vlastix{shorter} \\
\myparbox{$\because$ AF subcase.
Otherwise, we have \Eref{eq:th-chain-containment-14-10} vacuously.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-16}
\myparbox{$\exists \; \var{m} : \VVelement{shorter}{n} = \VVelement{longer}{m}$
  $\because$
  \Eref{eq:th-chain-containment-14},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-18}
\myparbox{$\PreviousLink{\VVelement{shorter}{n}} = \set{\Velement{shorter}{\var{n}+1}}$
  $\because$ \longDfref{Leo chain}{def:leo-chain},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-20}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \set{\Velement{shorter}{\var{n}+1}}$
  $\because$
  \Eref{eq:th-chain-containment-15},
  \Eref{eq:th-chain-containment-16},
  \Eref{eq:th-chain-containment-18}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-22}
\myparbox{$\PreviousLink{\VVelement{longer}{m}} = \set{\Velement{longer}{\var{m}+1}}$
  $\because$ \longDfref{Leo chain}{def:leo-chain},
  \Eref{eq:th-chain-containment-15}.%
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-24}
\myparbox{$\VVelement{shorter}{\var{n}+1} = \Velement{longer}{\var{m}+1}$
  $\because$
  \Eref{eq:th-chain-containment-20},
  \Eref{eq:th-chain-containment-22}.
}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-26}
\begin{gathered}
0 \le \var{n}+1 \le \Vlastix{shorter} \implies \Velement{shorter}{\var{n}+1} \in \var{longer} \\
\myparbox{$\because$ \Eref{eq:th-chain-containment-24}.
  This is \Eref{eq:th-chain-containment-10} with $\var{ix} = \var{n}+1$
  and shows the step starting at \Eref{eq:th-chain-containment-14}.
}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-chain-containment-28}
\begin{gathered}
\forall \; \mylim{\ell{}2} \in \var{shorter} : \ell{}2 \in \var{longer} \\
\because \text{ induction \Eref{eq:th-chain-containment-10}--\Eref{eq:th-chain-containment-26}.}
   \quad \myqed
\end{gathered}
\end{equation}
\end{proof}

\begin{theorem}[LEO Chain termination]
\label{th:chain-termination}
Let
\begin{gather}
\label{eq:th-chain-termination-01}
\myparbox{\Vlimset{longer} be a valid Leo chain,} \\
\label{eq:th-chain-termination-03}
\myparbox{\var{shorter} be the valid Leo chain contained in \var{longer}.}
\end{gather}
Then
\begin{gather}
\label{eq:th-chain-termination-08}
\Velement{shorter}{\Vlastix{shorter}} = \Velement{longer}{\Vlastix{longer}}
\end{gather}
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

\begin{theorem}[Memoizer consistency]
\label{th:memoizer-consistency}
The \Marpa{} \var{Memoizer} operation preserves validity.
\end{theorem}

\begin{proof}
Call the \type{LIM} added by an arbitrary \Marpa{} \var{Memoizer}
operation \mylim{\ell},
and call its valid ancestry $[\Vlim{previous}, \Veim{source}]$.
\begin{equation}
\label{eq:th-memoizer-consistency-10}
\myparbox{\Vlim{previous} is $\Lambda$ or a valid \type{LIM}, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-12}
\myparbox{\Veim{source} is valid, by
assumption for the theorem.}
\end{equation}
\begin{equation}
\label{eq:th-memoizer-consistency-14}
  \begin{gathered}
  \Postdot{\Veim{source}} = \Transition{\ell} \\
  \because
  \text{TODO}
  \end{gathered}
\end{equation}
\todo{Add justification for this equation}

\todo{second proof merged in what follows}

% \begin{theorem}[LIM chain dotted rule and origin]
% \label{th:LIM-chain-dr-origin}
% In a \Marpa{} parse,
% let \Vlimset{chain} be a valid partial \type{LIM} chain.
% For every element \mylim{\ell} of \var{chain},
% \begin{equation}
% \label{eq-th-LIM-chain-dr-origin-05}
% \begin{gathered}
% \text{$\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}}$ and} \\
% \Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}}
% \end{gathered}
% \end{equation}
% \end{theorem}

% \begin{proof}

The proof is by a reductio.
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-10}
\begin{gathered}
\exists \; \mylim{\ell}, \mylim{\ell} \in \var{chain} : \\
\DR{\ell} = \DR{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\lor \; \Origin{\ell} = \Origin{\Source{\Velement{chain}{\Vlastix{chain}}}} \\
\because \text{AF reductio.}
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-12}
\myparbox{\type{LIM}s are only added by the
\Marpa{} \var{Memoizer} operation
$\because$ \Obref{obs:marpa-op-by-result}.%
}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-14}
\myparbox{\type{LIM}s are added by either the
\var{Start-Leo-Chain} or
\var{Extend-Leo-Chain} sub-op
$\because$
\Eref{eq:th-LIM-chain-dr-origin-12},
\text{TODO: Add ref to memoizer code}.%
}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-15-20}
\myparbox{Of the \type{LIM}s added which
satisfy
\Eref{eq:th-LIM-chain-dr-origin-10}
one, call it \Vlim{first} is added first
$\because$
\Eref{eq:th-LIM-chain-dr-origin-14}.%
}
\end{equation}
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-15-25}
\myparbox{\Vlim{first} = \Velement{partial}{0},
where \Vlimset{partial} is the partial Leo chain
of \Vlim{first}
$\because$
\Dfref{def:leo-chain},
\text{TODO: Add ref to memoizer code}.%
}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}

We now proceed by subcase, starting with
\var{Start-Leo-Chain}.
\begin{equation}
\label{eq:th-LIM-chain-dr-origin-16}
\myparbox{\Vlim{first} was added by a
\var{Start-Leo-Chain} sub-op
$\because$ AF subcase.%
}
\end{equation}
\todo{Finish proof from here.  Look at use of \var{LIMPredecessor}}
\myqed
\end{proof}

\begin{definition}[Leo chain--base match]
\label{def:leo-chain-base-match}
Let \Veim{base} be an \type{EIM}
and let \Vlim{lim} be a \type{LIM}.
We say \Veim{base} \dfn{matches} \Vlim{lim},
if and only if
\begin{itemize}
\item \Veim{base} is a completion;
\item $\Transition{\Vlim{lim}} = \LHS{\var{base}}$; and
\item $\Vlim{lim} \in \es{(\Origin{\var{base}})}$.
\end{itemize}
We say that \Veim{base} \dfn{matches} a Leo chain,
call it \Vlimset{chain},
if and only if \Veim{base} matches $\var{chain}[0]$.
\dispEnd{}
\end{definition}

\begin{definition}[Leo edge]
\label{def:leo-edge}
Let \Vint{g} be a grammar,
let \Vlimset{chain} be a valid Leo chain,
and let $\Veim{base} \in \Ves{current}$ be a valid \type{EIM} which
matches \var{chain}.
Then the \dfn{Leo edge}, or just \dfn{edge},
of \var{chain} and \var{base}
is a sequence of complete \type{EIM}s, call it \Veimset{edge},
such that
\begin{equation}
\label{eq:d-leo-edge-10}
\var{edge}[0] = \Veim{base}
\end{equation}
and, for all \var{n} such that $0 \le \var{n} < \Vsize{chain}$,
\begin{equation}
\label{def:leo-edge-30}
\begin{gathered}
\Veim{source} \in \Source{\var{chain}[\var{n}]} \\
\land \; \left[ \var{source}, \var{edge}[\var{n}] \right]_\type{LP}
  \in \LinkPairs{\var{edge}[\var{n}+1]}
\end{gathered}
\end{equation}

We say that an edge is \dfn{valid} if and only if all the elements of the edge
are valid.
The \dfn{top} of the edge is \Velement{edge}{\Vlastix{edge}}.
The \dfn{bottom} of the edge is \Velement{edge}{0}.
The \dfn{inside} of the edge is the sequence
\Velement{edge}{1 \ldots (\Vlastix{edge}\subtract 1)}.
An \dfn{inside element} of an edge is a element of the
inside of the edge.
An \dfn{upper element} of an edge is a top or an inside element.
An \dfn{lower element} of an edge is a bottom or an inside element.

An edge is \dfn{maximal} if its Leo chain is maximal.
An edge is \dfn{partial} if its Leo chain is partial.
An edge is maximal, unless stated otherwise.
\dispEnd{}
\end{definition}

\begin{theorem}[Leo edge length]
\label{th:leo-edge-length}
Let \Vint{g} be a grammar;
let \Vlimset{chain} be a valid partial Leo chain;
let $\Veim{base} \in \Ves{current}$ be a valid \type{EIM} which
matches \var{chain};
and let \Veimset{edge} be their edge.
Then
\[
\Vlastix{edge} = \Vlastix{chain} + 1.
\]
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-length-10}
\Vsize{chain} > 0 \because
\text{Def of ``Leo chain'' \Dfref{def:leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-14}
  \begin{aligned}
    & \Veim{source} \in \Source{\Velement{chain}{\Vlastix{chain}}} \\
    & \land \; \left[ \var{source}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
    & \qquad \qquad \in \LinkPairs{\Velement{edge}{\Vlastix{chain}+1}} \\
    & \because \text{\Eref{def:leo-edge-30} in \Dfref{def:leo-chain}},
        \Eref{eq:th-leo-edge-length-10}.
  \end{aligned}
\end{equation}
\todo{\var{Source} changed.  Revise from here.}
\begin{equation}
\label{eq:th-leo-edge-length-16}
\myparbox{edge index is monotonically increasing
as a function of chain index $\because$
\Eref{def:leo-edge-30} in \Dfref{def:leo-chain}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-length-18}
\Vlastix{edge} = \Vlastix{chain} + 1
\because
\Eref{eq:th-leo-edge-length-14},
\Eref{eq:th-leo-edge-length-16}. \quad \myqed
\end{equation}
\end{proof}

\begin{theorem}[Leo chain source properties]
\label{th:leo-chain-source-properties}
In a \Marpa{} internal grammar,
the source of a Leo chain element is a penult in the same
Earley set as the element.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar.
\begin{equation}
\label{eq:th-leo-chain-source-properties-10}
\myparbox{$\mylim{\ell}@\Vloc{j}$ is a Leo chain element
$\because$ AF theorem, WLOG.
}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-12}
\Veim{source} \in \Source{\ell}
\because
\Eref{eq:th-leo-chain-source-properties-10},
\text{WLOG}.
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-20}
\begin{gathered}
\Veim{source} \in \LeoEligible{\Vloc{j}} \\
\qquad \because
\text{TODO: Add ref to memoizer code},
\end{gathered}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-22}
\begin{gathered}
\Veim{source} \in \LeoUnique{\Vloc{j}} \\
\because
\Eref{eq:th-leo-chain-source-properties-20}, \Eref{eq:def-leo-eligible}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-24}
\myparbox{\Veim{source} is a penult
  $\because$ \Eref{eq:th-leo-chain-source-properties-22},
  Def ``Leo unique'' \Dfref{def:leo-unique}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-chain-source-properties-26}
\Current{\Veim{source}} = \Vloc{j}
  \because \Eref{eq:th-leo-chain-source-properties-22},
  \Dfref{def:leo-unique}.
\end{equation}
The theorem follows directly from
\Eref{eq:th-leo-chain-source-properties-24} and
\Eref{eq:th-leo-chain-source-properties-26}. \myqed
\end{proof}

\begin{theorem}[Edge properties]
\label{th:edge-elements-are-complete}
In a \Marpa{} internal grammar,
let \Vlimset{chain} be a valid Leo chain,
and \Veim{base} a valid \type{EIM} matching \var{chain}.
Every element of the edge of \var{chain} and \var{base}
is a valid completion in the same Earley set as \Veim{base},
and the edge is valid.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar,
let \Vlimset{chain} be a valid Leo chain
and let \Veim{base} be a valid \type{EIM} which matches \var{chain}.
Let \Veimset{edge} be the edge of \var{chain} and \var{base}.
We proceed by induction on the elements of the edge,
where the induction hypothesis is
\begin{equation}
\label{eq:edge-elements-are-complete-05}
\myparbox{
$\var{edge}[\var{ix}]$ a valid completion in the same Earley set as \Veim{base}.
}
\end{equation}

We use $\var{ix} = 0$ as the basis of the induction:
\begin{equation}
\label{eq:edge-elements-are-complete-10}
\var{edge}[0] = \Veim{base} \because \Eref{eq:d-leo-edge-10}.
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-12}
\text{\Veim{base} is a completion $\because \Dfref{def:leo-chain-base-match}$.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-14}
\text{\Veim{base} is in the same Earley set as \Veim{base} $\because$ trivial.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-16}
\text{\Veim{base} is valid by
  assumption for the theorem.}
\end{equation}
\begin{gather}
\label{eq:edge-elements-are-complete-18}
\myparbox{\var{edge}[0] is a valid completion
  in the same Earley set as \Veim{base}
  $\because$
  \Eref{eq:edge-elements-are-complete-10},
  \Eref{eq:edge-elements-are-complete-12},
  \Eref{eq:edge-elements-are-complete-14},
  \Eref{eq:edge-elements-are-complete-16}.
  This is
  \Eref{eq:edge-elements-are-complete-05} for $\var{ix} = 0$,
  the basis of the induction on \Eref{eq:edge-elements-are-complete-05}.
  }
\end{gather}

For the step of the induction, we assume
the induction hypothesis \Eref{eq:edge-elements-are-complete-05}
for $\var{ix}=\var{n}$
to show
the induction hypothesis
for $\var{ix}=\var{n}+1$.
\begin{equation}
\label{eq:edge-elements-are-complete-20}
\myparbox{
$\var{edge}[\var{n}]$ is a valid completion in the same Earley set as \Veim{base}
$\because$ assumption for step of the induction.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-30}
  \begin{gathered}
    \Veim{source} \in \Source{\var{chain}[\var{n}]} \\
    \land \; \left[ \var{source}, \var{edge}[\var{n}] \right]_\type{LP}
      \in \LinkPairs{\var{edge}[\var{n}+1]} \\
    \because
      \text{\Eref{def:leo-edge-30} of Def ``Leo edge''
      \Dfref{def:leo-edge}}.
    \end{gathered}
\end{equation}
\todo{\var{Source} changed.  Recheck from here.}
\begin{equation}
\label{eq:edge-elements-are-complete-36}
\myparbox{$\var{edge}[\var{n}+1]$
is the result of an \Earley{} completer operation
whose ancestry includes
$\left[ \Veim{source},
  \var{edge}[\var{n}] \right]_\type{LP}$
$\because$
\Obref{obs:earley-op-by-direct-ancestor},
\Eref{eq:edge-elements-are-complete-20},
\Eref{eq:edge-elements-are-complete-30}.
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-10}
\myparbox{
  \Vlimset{chain} is valid
  $\because$ AF theorem.}
\end{equation}
todo{Fix next equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-15}
\myparbox{
  \Veim{source} is valid
  $\because$
  text{TODO \type{LIM} valdity definition},
  \Eref{eq:edge-elements-are-complete-37-10}.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-20}
\myparbox{
  $\var{edge}[\var{n}]$ is valid
  $\because \Eref{eq:edge-elements-are-complete-20}$.
  }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-37-25}
\myparbox{
  $\var{edge}[\var{n}+1]$ is valid
  $\because$
  \Thref{th:earley-completer-is-consistent},
  \Eref{eq:edge-elements-are-complete-36},
  \Eref{eq:edge-elements-are-complete-37-15},
  \Eref{eq:edge-elements-are-complete-37-20}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-38}
\myparbox{
  \Veim{source} is a penult
  $\because$ \Thref{th:leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-40}
\begin{gathered}
  \DR{\var{edge}[\var{n}+1]} = \Next{\Veim{source}} \\
  \because
  \Eref{obs:earley-completer-summary}, \Eref{eq:edge-elements-are-complete-36}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-42}
\myparbox{$\var{edge}[\var{n}+1]$ is a completion
  $\because$
  Def \var{Next} \Eref{eq:def-next},
  \Eref{eq:edge-elements-are-complete-38},
  \Eref{eq:edge-elements-are-complete-40}.}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-44}
\myparbox{\Velement{edge}{\var{n}+1} is in the same Earley set as \VVelement{edge}{n}
  $\because$
\Eref{eq:earley-completer-summary-1}
  \Eref{eq:earley-completer-summary-4},
  \Eref{eq:edge-elements-are-complete-36}. }
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-46}
\myparbox{$\var{edge}[\var{n}+1]$ is in the same Earley set as \Veim{base}
  $\because$
  \Eref{eq:edge-elements-are-complete-20},
  \Eref{eq:edge-elements-are-complete-44}.  }
\end{equation}
The step and the induction follow
from
\Eref{eq:edge-elements-are-complete-37-25},
\Eref{eq:edge-elements-are-complete-42}, and
\Eref{eq:edge-elements-are-complete-46}, so that
\begin{equation}
\label{eq:edge-elements-are-complete-50}
\myparbox{%
for all \var{n} such that
$0 \le \var{n} \le \Vlastix{edge}$,
\VVelement{edge}{n} is a valid completion in the same Earley set as \Veim{base}.%
}
\end{equation}
\begin{equation}
\label{eq:edge-elements-are-complete-52}
\myparbox{\Veimset{edge} is valid $\because$
\Dfref{def:leo-edge},
\Eref{eq:edge-elements-are-complete-50}.}
\end{equation}
We have the theorem directly from
\Eref{eq:edge-elements-are-complete-50} and
\Eref{eq:edge-elements-are-complete-52}. \myqed
\end{proof}

\begin{theorem}[Edge Descendant Uniqueness]
\label{th:edge-descendant-uniqueness}
In an \Earley{} parse,
the descendant of a lower element of an edge
is unique.
\end{theorem}

\begin{proof}
We proceed by
assuming that an arbitrary lower element has
two distinct descendants,
for a reductio.
\begin{equation}
\label{th:edge-descendant-uniqueness-08}
\myparbox{Let \Veimset{edge} be an edge WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-10}
\myparbox{$\Veim{lo}=\VVelement{edge}{n}$ is a valid lower element of an edge
  $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-12}
\myparbox{$\Veim{desc} = \Velement{edge}{\var{n}+1}$
  is a valid direct descendant of \Veim{lo}
  $\because$ AF theorem.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-14}
\myparbox{\Veim{desc2} is a valid direct descendant of \Veim{lo}
$\because$ WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-16}
\myparbox{$\var{desc} \neq \Veim{desc2}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-18}
\myparbox{\Veim{lo} is complete
$\because$
\Thref{th:edge-elements-are-complete},
\Eref{th:edge-descendant-uniqueness-10}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-20}
\myparbox{\Veim{desc} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred}, \var{lo} \right]_\type{LP}$
$\because$
\Obref{obs:earley-op-by-direct-ancestor},
\Eref{th:edge-descendant-uniqueness-12},
\Eref{th:edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-21}
\begin{gathered}
\left[ \Veim{pred2}, \var{lo} \right]_\type{LP}
  \in \LinkPairs{\Veim{desc2}} \\
  \land \;\; \Vlim{match} = \var{chain}[\var{n}] \\
  \land \;\; \Veim{pred2} \in \Source{\Vlim{match}} \\
  \because \text{Def of ``Leo edge''} \Dfref{def:leo-edge},
    \Eref{th:edge-descendant-uniqueness-12}
\end{gathered}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-22}
\myparbox{\Veim{desc2} is the result of an \Earley{} completer
operation with link pair
$\left[ \var{pred2}, \var{lo} \right]_\type{LP}$
$\because$
\Obref{obs:earley-op-by-direct-ancestor},
\Eref{th:edge-descendant-uniqueness-14},
\Eref{th:edge-descendant-uniqueness-18}, WLOG.
}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-24}
\myparbox{$\Current{\Veim{pred}} = \Origin{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-1}
  and \Eref{eq:earley-completer-summary-2}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-26}
\myparbox{$\Current{\Veim{pred2}} = \Origin{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-1}
  and \Eref{eq:earley-completer-summary-2}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-28}
\myparbox{$\Current{\Veim{pred2}} = \Current{\Veim{pred}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-24},
  \Eref{th:edge-descendant-uniqueness-26}.}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-30}
\myparbox{$\Postdot{\var{pred}} = \LHS{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-3}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-20}.
}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-32}
\myparbox{$\Postdot{\Veim{pred2}} = \LHS{\var{lo}}$
  $\because$
  \Eref{eq:earley-completer-summary-3}
  in \Obref{obs:earley-completer-summary};
  \Eref{th:edge-descendant-uniqueness-22}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-34}
\myparbox{$\Postdot{\Veim{pred}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-30},
  \Eref{th:edge-descendant-uniqueness-32}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-36}
\myparbox{\var{pred2} is the source of \Vlim{match}
  $\because$
  \Eref{th:edge-descendant-uniqueness-21}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-38}
\myparbox{\var{pred2} is a penult
  $\because$
  \Eref{th:edge-descendant-uniqueness-36},
  \Thref{th:leo-chain-source-properties}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-40}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred2}}$
  $\because$
  \Eref{eq:def-penult},
  \Eref{th:edge-descendant-uniqueness-38}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-41}
\myparbox{$\Penult{\Veim{pred2}} = \Postdot{\Veim{pred}}$
  $\because$
  \Eref{th:edge-descendant-uniqueness-34},
  \Eref{th:edge-descendant-uniqueness-40}.}
\end{equation}
%
\begin{equation}
\label{th:edge-descendant-uniqueness-42}
\begin{gathered}
\Veim{pred2} \in \LeoEligible{\Current{\Vlim{match}}} \\
\because \Eref{th:edge-descendant-uniqueness-36},
\text{TODO: Add ref to memoizer code}.
\end{gathered}
\end{equation}
\todo{Add ref to memoizer code to justification for above equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-44}
  \begin{gathered}
    \Veim{pred2} \in \LeoUnique{\Current{\Vlim{match}}} \\
    \because
    \Eref{th:edge-descendant-uniqueness-42}, \Eref{eq:def-leo-eligible}.
  \end{gathered}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-46}
\begin{gathered}
\forall \; \Veim{x}
\left(
\begin{gathered}
\left(
  \begin{gathered}
  \Current{\var{pred2}} = \Current{\var{x}} \\
  \land \; \Penult{\var{pred2}} = \Postdot{\var{x}}
  \end{gathered}
\right)
\\
\implies \var{pred2} = \var{x}.
\end{gathered}
\right)
\\
\because \text{Def of \var{Leo-Unique} \Dfref{def:leo-unique}},
  \Eref{th:edge-descendant-uniqueness-44}.
\end{gathered}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-48}
\Veim{pred2} = \Veim{pred} \because
\Eref{th:edge-descendant-uniqueness-28},
\Eref{th:edge-descendant-uniqueness-41},
\Eref{th:edge-descendant-uniqueness-46}.
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-50}
[\Veim{pred2}, \Veim{lo}] = [\Veim{pred}, \var{lo}] \because
\Eref{th:edge-descendant-uniqueness-48}.
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-52}
\myparbox{If two \Earley{} completer operations share
a link pair, they produce the same result
$\because$
\Obref{obs:earley-completer-summary}.}
\end{equation}
\begin{equation}
\label{th:edge-descendant-uniqueness-54}
\Veim{desc2} = \Veim{desc} \because
\Eref{th:edge-descendant-uniqueness-20},
\Eref{th:edge-descendant-uniqueness-22},
\Eref{th:edge-descendant-uniqueness-50},
\Eref{th:edge-descendant-uniqueness-52}.
\end{equation}
With
\Eref{th:edge-descendant-uniqueness-52} we
have the reductio of
\Eref{th:edge-descendant-uniqueness-16}
and the theorem. \myqed
\end{proof}

\begin{theorem}[Edge Top is Border]
\label{th:edge-top-is-border}
If an \type{EIM} is the non-trivial descendant of a lower edge \type{EIM},
it is either another lower edge \type{EIM},
or the descendant of the top of an edge.
\end{theorem}

\begin{proof}
\begin{equation}
\label{th:edge-top-is-border-22}
\myparbox{\Veim{lo} is a lower edge \type{EIM} $\because$ AF theorem, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-24}
\myparbox{\Veim{no} is an \type{EIM} which is not a lower edge $\because$ AF theorem, WLOG.}
\end{equation}
For the purposes of this proof, we define \Veimset{seq} as follows:
\begin{equation}
\label{th:edge-top-is-border-26}
\myparbox{%
  \Veim{lo} and \Veim{no} are
  part of a
  \Veimset{seq}, which is such that
  \begin{itemize}
  \item $\Velement{seq}{0} = \Veim{lo}$;
  \item $\Velement{seq}{\Vlastix{seq}} = \Veim{no}$; and
  \item for all \var{i} such that $0 < \var{i} \le \Vlastix{seq}$,
  \eim{(\VVelement{seq}{i})}
  is the direct descendant of \eim{(\Velement{seq}{\var{i}\subtract 1})}.
  \end{itemize}%
}
\end{equation}
Note that the criteria in \Veimset{seq} do not necessarily determine \var{seq}
uniquely.
We make an arbitrary choice among the possible value of \var{seq},
without loss of generality.
\begin{equation}
\label{th:edge-top-is-border-27}
\myparbox{\VVelement{seq}{t} is the first element
of \var{seq}
that is not a lower edge \type{EIM}.
$\because$ \Eref{th:edge-top-is-border-26}, WLOG.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-28}
\myparbox{\VVelement{seq}{t}
is the top of an edge, or it is not an edge \type{EIM}
$\because$
Def of ``Leo edge'' \Dfref{def:leo-edge},
\Eref{th:edge-top-is-border-27}.}
\end{equation}
%
We now consider
\Velement{seq}{\var{t}\subtract 1}.
%
\begin{equation}
\label{th:edge-top-is-border-30}
\myparbox{
\Velement{seq}{\var{t}\subtract 1} is a lower edge \type{EIM}
$\because$
\Eref{th:edge-top-is-border-27}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-32}
\myparbox{One of the descendants of a lower edge \type{EIM}
must be either another lower edge \type{EIM} or the top of
an edge.
$\because$ Def of ``Leo edge'' \Dfref{def:leo-edge}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-34}
\myparbox{%
The descendant of a lower edge \type{EIM}
is unique
$\because$ \Thref{th:edge-descendant-uniqueness}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-36}
\myparbox{%
The unique descendant of
\Velement{seq}{\var{t}\subtract 1} is
another lower edge \type{EIM} or the top of
an edge
$\because$
\Eref{th:edge-top-is-border-30},
\Eref{th:edge-top-is-border-32},
\Eref{th:edge-top-is-border-34}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-38}
\myparbox{\VVelement{seq}{t} is the unique descendant
of \Velement{seq}{\var{t}\subtract 1}.
$\because$
\Eref{th:edge-top-is-border-30}
\Eref{th:edge-top-is-border-34}.}`
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-40}
\myparbox{\VVelement{seq}{t} is the top of an edge
$\because$
\Eref{th:edge-top-is-border-28},
\Eref{th:edge-top-is-border-36},
\Eref{th:edge-top-is-border-38}.}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-42}
\myparbox{\Veim{no} is a descendant of
\VVelement{seq}{t}.
$\because$ \Eref{th:edge-top-is-border-26}
}
\end{equation}
\begin{equation}
\label{th:edge-top-is-border-44}
\myparbox{\Veim{no} is a descendant of
the top of an edge.
$\because$ \Eref{th:edge-top-is-border-40}
\Eref{th:edge-top-is-border-42}.}
\end{equation}
Since the choice of \Veim{no} was without loss of generality,
with \Eref{th:edge-top-is-border-44} we have the
theorem. \myqed
\end{proof}

\begin{theorem}[Leo edge top]
\label{th:leo-edge-top}
In a \Marpa{} parse,
let \Vlimset{chain} be a valid \type{LIM} chain,
and let \Veim{base} be a valid \type{EIM} which matches \var{chain}.
Then the top of their edge is
\begin{equation}
\label{eq:th-leo-edge-top-05}
\begin{gathered}
\left[ \DR{\Veim{source}}, \Origin{\var{source}}
\right]@\Current{\var{base}}, \\
\text{where $\Veim{source} \in \Source{\Velement{chain}{\Vlastix{chain}}}$.}
\end{gathered}
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\label{eq:th-leo-edge-top-10}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the top of the edge
$\because$ Def of ``Leo edge'' \Dfref{def:leo-edge}.}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-12}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{edge}\subtract 1}}, \Velement{edge}{\Vlastix{edge}\subtract 1} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \text{\Eref{def:leo-edge-30} in \Dfref{def:leo-edge}}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-1}
\begin{gathered}
\left[ \Source{\Velement{chain}{\Vlastix{chain}}}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Thref{th:leo-edge-length}, \Eref{eq:th-leo-edge-top-12}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-13-2}
\begin{gathered}
\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP} \\
  \in \LinkPairs{\Velement{edge}{\Vlastix{edge}}} \\
\because \Eref{eq:th-leo-edge-top-05}, \Eref{eq:th-leo-edge-top-13-1}.
\end{gathered}
\end{equation}
%
\begin{equation}
\label{eq:th-leo-edge-top-14}
\myparbox{\Velement{edge}{\Vlastix{chain}} is a completion
$\because$ \Thref{th:edge-elements-are-complete}.}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-16}
\begin{gathered}
\Current{\Velement{edge}{\Vlastix{chain}}} = \Current{\var{base}} \\
\because \Thref{th:edge-elements-are-complete}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-18}
\begin{gathered}
\myparbox{\Velement{edge}{\Vlastix{edge}} is the
result of an \Earley{} completer
operation with link pair
$\left[ \Veim{source}, \Velement{edge}{\Vlastix{chain}} \right]_\type{LP}$%
} \\
\because \Obref{obs:earley-op-by-direct-ancestor}, \Eref{eq:th-leo-edge-top-13-2},
  \Eref{eq:th-leo-edge-top-14}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-20}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Velement{edge}{\Vlastix{chain}}} \\
\because
\Obref{obs:earley-completer-summary},
\Eref{eq:th-leo-edge-top-18}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-22}
\begin{gathered}
\Velement{edge}{\Vlastix{edge}} = \\
\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}} \\
\because \Eref{eq:th-leo-edge-top-16}, \Eref{eq:th-leo-edge-top-20}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-leo-edge-top-24}
\myparbox{The top of \Veimset{edge} is
  $\left[ \DR{\var{source}}, \Origin{\var{source}} \right] @ \Current{\Veim{base}}$ \\
  $\because$ \Eref{eq:th-leo-edge-top-10}, \Eref{eq:th-leo-edge-top-22}.
  \myqed
}
\end{equation}
\end{proof}


\begin{theorem}[\op{Leo-reducer} is consistent]
\label{th:leo-reducer-is-consistent}
The \op{Leo-reducer} sub-op
of the \Marpa{} \op{Reducer} operation
preserves validity.
\end{theorem}
\begin{proof}
\todo{prove this}
\myqed
\end{proof}

\begin{theorem}[\Marpa{} is consistent]
\Marpa{} is consistent.
That is, every \type{PIM} created by a \Marpa{} parser
is valid.
\end{theorem}

\begin{proof}
We assume for a reductio,
that there is \Marpa{} parse that creates an invalid \type{PIM}.
This means that there is a first invalid \type{PIM} created
by that \Marpa{} parse.
Inspection of the \Marpa{} algorithm
\Chref{chap:marpa} shows
that \type{PIM}s are only created by operations.

We now proceed by elimination of subcases,
where our subcases are the operations
of the \Marpa{} algorithm.
We first note that an operation creates
the first invalid \type{PIM} only if it does not preserve
validity.

The \Marpa{} initializer operation creates the
same \type{PIM}s as the \Earley{} initializer operation
\Obref{obs:earley-set-0}.
The \Earley{} initializer operation preserves
validity because the \Earley{} algorithm is
correct
\Thref{th:earley-is-correct}.
Note in the case of the Initalizer operation,
validity is preserved vacuously ---
no \type{PIM} is in the ancestry of the Initializer
operation.

The \Marpa{} scanner operation
creates the same \type{PIM}s as the \Earley{}
scanner operations
\Obref{obs:earley-scanner-op}.
The \Marpa{} predictor operation
creates the same \type{PIM}s as the \Earley{}
predictor operations
\Obref{obs:earley-predictor-op}.
These two operations preserve
validity because the \Earley{} algorithm is
correct
\Thref{th:earley-is-correct}.

We have shown that the memoizer operation
preserves validity
\Eref{th:memoizer-consistency}.
The remaining subcase is the \Marpa{} \op{Reducer} operation.

The \Marpa{} \op{Reducer} operation consists of two
sub-ops:
\op{Earley-reducer} and
\op{Leo-reducer}.
We have shown that the
\op{Leo-reducer} preserves validity
\Thref{th:leo-reducer-is-consistent}.

From
\Obref{obs:earley-completer-op}
and \Aref{alg:earley-reducer-op},
\op{Earley-reducer}, creates the \type{EIM} that
the \Earley{} completer operation would have
created, unless there is a \type{LIM} that ``blocks''
\Marpa{}'s \op{Earley-reducer}.
Therefore the \type{PIM}s created by
the \Marpa{} \op{Earley-reducer} sub-op
are a subset of those created by the \Earley{}
completer operation.
We know the \Earley{} completer operation preserves
validity
\Thref{th:earley-completer-is-consistent},
and therefore any operation which creates some subset of the
\type{EIM}s that
the \Earley{} completer operation creates also
preserves validity.

With this we have shown the last subcase,
and may conclude that no \Marpa{} operation creates the
first invalid \type{PIM},
and therefore no \Marpa{} operation creates
an invalid \type{PIM}.
Therefore, \Marpa{} is consistent.
\myqed
\end{proof}

\begin{definition}[Hidden and Visible \type{PIM}s]
An \type{EIM} is \dfn{hidden} if it is an element of the inside
of a Leo edge.
All other \type{PIM}s are \dfn{visible}.
\end{definition}

\begin{theorem}[\Marpa{} visible \type{PIM}s are complete]
\Marpa{} is complete, except for the hidden
\type{PIM}s.
\end{theorem}

\begin{proof}
\todo{prove this}
\myqed
\end{proof}

For convenience in the next few lemma's,
we will state some definitions for the purpose of presentation
(DFPP's).
Intuitively
the Earley tables are ``complete as far as X'',
where X is some point in running of the \Marpa{} algorithm,
if they contain all the visible valid \type{PIM}s
for X and every prior to it in
in the \Marpa{} algorithm.
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
Earley set \var{j},
if every Earley set, \VVelement{S}{i}, $0 \le \var{i} \le \var{j}$,
contains all the valid visible \type{PIM}s
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
the \Marpa{} \var{Scanner} operation of Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid \type{EIM}s with a predot terminal
$\because$ DFPP.
}
\end{equation}
\begin{equation}
\myparbox{The Earley tables of a \Marpa{} parse
are ``complete as far as''
the \Marpa{} \op{Reducer} operation of Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid confirmed \type{EIM}s
$\because$ DFPP.
}
\end{equation}

Intuitively, ``visible depth''
is depth counted in terms of
visible confirmed \type{EIM} causes.
We will write \var{Depth}(\Veim{x}) for the visible depth
of \Veim{x}.
When speaking of several \type{EIM}s,
we will say that the one with the
numerically lowest depth value
is the ``deepest''.

\begin{equation}
\label{eq:def-complete-as-far-as-20}
\myparbox{$\var{Depth}(\Veim{x}) = 1$
if $\Predot{\Veim{x}} \in \Term{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-22}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{bottom})+1$,
if \var{x} is the top of a Leo edge,
where
\Veim{bottom} is the bottom of the Leo edge
$\because$ DFPP.}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-24}
\myparbox{$\var{Depth}(\Veim{x}) = \var{Depth}(\Veim{cuz})+1$,
where \Veim{cuz} is the ``deepest'' cause of \Veim{x},
if \var{x} is a visible confirmed \type{EIM} where
\var{x} is not the top of a Leo edge and
$\Predot{\var{x}} \in \NT{\var{g}}$
$\because$ DFPP.}
\end{equation}
\begin{equation}
\label{eq:def-complete-as-far-as-26}
\myparbox{$\var{Depth}(\Vpim{pim}) = \Lambda$
if \Vpim{pim} is a \type{LIM},
a hidden \type{EIM},
or a prediction.
In other words,
visible depth is
defined only for visible confirmed \type{EIM}s
$\because$ DFPP.
}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-25}
\begin{gathered}
\forall \; \Veim{x} \; ( \var{Depth}(\var{x}) \ge 1 \lor
 \var{Depth}(\var{x}) = \Lambda) \\
\because
\Eref{eq:def-complete-as-far-as-20},
\Eref{eq:def-complete-as-far-as-22},
\Eref{eq:def-complete-as-far-as-24},
\Eref{eq:def-complete-as-far-as-26}.
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:def-complete-as-far-as-28}
\myparbox{The Earley tables are ``complete as far as''
visible depth \var{n} in Earley set \var{j},
if they are complete as far as Earley set $\var{j}\subtract 1$,
and if they contain all the visible valid \type{EIM}s of visible depth
\var{n} or less
$\because$ DFPP.
}
\end{equation}

\begin{theorem}
\label{th:marpa-reducer-completeness}
Let \VVelement{S}{j} be an Earley set in a \Marpa{} parse,
$\var{j} > 0$.
If the parse is complete as far as the
\Marpa{} \var{Scanner} operation of
Earley set \VVelement{S}{j},
then the parse is complete as far as the
\Marpa{} \op{Reducer} operation of
Earley set \VVelement{S}{j}.
\end{theorem}

\begin{proof}
This proof will be by induction on
visible depth.

We take for the induction hypothesis
\begin{equation}
\label{th:marpa-reducer-completeness-28}
\myparbox{After a \Marpa{} \op{Reducer} operation at \VVelement{S}{j},
\VVelement{S}{j} is complete
with respect to the
the \type{EIM}s of visible depth \var{ix}.}
\end{equation}

\todo{finish this proof}
\myqed
\end{proof}

\begin{theorem}
\label{th:marpa-visible-depth-completeness}
Let \VVelement{S}{j} be an Earley set in a \Marpa{} parse,
$\var{j} > 0$.
If the parse is complete as far as
visible depth \var{n}
in Earley set \VVelement{S}{j},
then the parse is complete as far as
visible depth $\var{n}+1$
in
Earley set \VVelement{S}{j}.
\end{theorem}
\begin{proof}
The proof is by a reductio.
\todo{presence in physical Earley set (PES?) vs theoretical {ES?}.}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-08}
\myparbox{Let $\Vpim{rez}@\VVelement{S}{j}$ be a valid \type{PIM} where
$\var{Depth}(\var{rez}) = \var{n}+1 \land
  \neg \var{rez} \in \VVelement{S}{j}$
$\because$ AF reductio.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-10}
\myparbox{$\Vpim{rez} = \Veim{rez} \because$ only \type{EIM}s have
a visible depth
\Eref{eq:def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-12}
\myparbox{\Veim{rez} is confirmed
$\because$ only confirmed \type{EIM}s have a visible depth
\Eref{eq:def-complete-as-far-as-26}.}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-14}
\begin{gathered}
\var{Depth}(\Veim{rez}) > 1 \because
\Eref{eq:def-complete-as-far-as-25},
\Eref{eq:th-marpa-visible-depth-completeness-08}.
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:th-marpa-visible-depth-completeness-16}
\Predot{\Veim{rez}} \in \NT{\var{g}} \because
\Eref{eq:def-complete-as-far-as-20},
\Eref{eq:th-marpa-visible-depth-completeness-12},
\Eref{eq:th-marpa-visible-depth-completeness-14}.
\end{equation}
\todo{finish proof}
\myqed
\end{proof}

\todo{Prove Marpa + Reverse Leo = Original Earley}

\begin{theorem}[Useful completions are disjoint or contained]
In an unambiguous grammar,
in every pair of useful complete \type{EIM}s,
one is either contained
in the other,
or they are disjoint
\end{theorem}
\begin{proof}
\todo{Need this proof?}
\myqed
\end{proof}

\begin{theorem}[Useful RR bound]
\label{th:useful-RR-count}
Let $\var{p} = [ \Vint{g}, \Vstr{w} ]$
be an unambiguous parse,
where $\var{n} = \size{\Vstr{w}}$.
The number of useful complete right recursive \type{EIM}s
in \var{p} is less than or equal to $2 \times \var{n} \subtract 1$,
or \On{}.
\end{theorem}

\begin{proof}
\todo{Need this proof?}
\myqed
\end{proof}

\chapter{Reversing Leo memoization}
\label{chap:reverse-leo}

\todo{Finish writing this section}

At evaluation time,
it is desirable to expand the Leo memoizations,
but in almost all practical cases,
only the useful \type{EIM}s
need to be expanded.

\begin{theorem}[Useful \type{EIM} count]
Let $[\Vint{g}, \Vstr{partial}]$ be a \Marpa{} partial parse.
the number of useful \type{EIM}s is \On{},
where $\var{n} = \Vsize{partial}$.
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

As a result,
even if the time and space
required to expand Leo memoization
during evaluation
are taken into account,
the time and space complexity of
a right recursion become
$\On{} + \On{} = \On{}$.

\begin{theorem}[Leo reversal is linear]
The time and space to reverse the Leo memoization
in an unambiguous parse is \On{}.
\end{theorem}

\begin{proof}
\todo{Prove this}
\myqed
\end{proof}

\chapter{LRR complexity}
\label{chap:lrr-complexity}

\begin{theorem}[A grammar which allows an ambiguity is ambiguous: duplicate?]
\label{th:ambiguous-links-pair}
\footnote{This theorem and its proof are adapted from
Lemma 4.6 in
\cite[Vol. 1, p. 325-326]{AU1972}.
}%
Let \Vcfg{g} be a CFG.
If there is an input \Vstr{w} such that a valid
confirmed \type{EIM} has more than one link pair,
then \var{g} is ambiguous.
\end{theorem}

\begin{proof}
For the theorem, we assume without loss of generality that
\begin{gather}
\label{eq:ambiguous-links-pair-10a}
\Veim{adjoined} = \left[ \left[ \var{lhs} \de \Vstr{rhs1} \Vsym{rhs2} \mydot \Vstr{rhs2} \right], \Vorig{i} \right], \\
\label{eq:ambiguous-links-pair-10b}
\text{$\Veim{adjoined} @ \VVelement{S}{j}$ is valid, and} \\
\label{eq:ambiguous-links-pair-10d}
\LinkPairs{\Veim{adjoined}} \ge 1.
\end{gather}

The proof proceeds by eliminating subcases within a reductio.
We assume for the reductio that
\begin{equation}
\label{eq:ambiguous-links-pair-12}
\text{\var{g} is not ambiguous.}
\end{equation}

Confirmed \type{EIM}s are only adjoined by the Scanner and the Completer.

\todo{Does this duplicate the proof in the ambiguity section?}
\myqed

\end{proof}

\section{Complexity of each Earley item}

% TODO: Is this theorem used?
\begin{theorem}[Right recursion bound]
\label{th:leo-right-recursion}
In a \Marpa{} internal grammar,
either
a right derivation has a step
that uses a right recursive rule,
or it has length is at most \var{c},
where \var{c} is a constant which depends
on the grammar.
\end{theorem}

\begin{proof}
Let \Vint{g} be a \Marpa{} internal grammar.
and let the constant $\var{c} = \NT{\var{g}}$.
Assume, for a reductio, that a right derivation
expands to a
Leo sequence of length
$\var{c}+1$, but that none of its steps uses a right recursive rule.

Because it is of length $\var{c}+1$,
the same symbol must appear twice as the rightmost symbol of
a derivation step.
Since we assumed for the theorem that \var{g}
is a \Marpa{} internal grammar,
there will be no nulling symbols,
and therefore
the rightmost symbol of a string will also be its rightmost
non-nulling symbol.
So part of the rightmost derivation must take the form
\begin{equation}
\label{eq:-th-leo-right-recursion-50}
\Vstr{earlier-prefix} \cat \Vsym{A} \deplus \Vstr{later-prefix} \cat \Vsym{A}.
\end{equation}
But the first step of the derivation in
\Eref{eq:-th-leo-right-recursion-50}
must use a rule of the form
\begin{equation*}
\Vsym{A} \de \Vstr{rhs-prefix} \cat \Vsym{rightmost},
\end{equation*}
where $\Vsym{rightmost} \deplus \Vsym{A}$.
Such a rule is right recursive by definition.
This is contrary to the assumption for the reductio.
We therefore conclude that the length of a right derivation
must be less than or equal to \var{c},
unless at least one step of that derivation uses a right recursive rule.
\myqed
\end{proof}

\section{The complexity results}

\begin{theorem}
\label{th:handle-unique}
Let \Vint{g} be a LR($\pi$) grammar where $\pi$ is
a left congruence.
Let
\begin{gather}
\label{eq:handle-unique-10a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:handle-unique-10b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB},
\end{aligned}
\\
\label{eq:handle-unique-10c}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB},
\\
\label{eq:handle-unique-10d}
\text{and} \;\;
\Vterm{suffixA} \iff \Vterm{suffixB} (\text{mod $\pi$}),
\end{gather}
where
\begin{gather*}
\set{ \Vterm{suffixA}, \Vterm{suffixB},
\Vterm{postA}, \Vterm{postB},
} \subseteq \Term{\var{g}}^\ast
\;\; \\
\text{and $\set{ \Vsym{lhsA}, \Vsym{lhsB} } \in \NT{\var{g}}$}.
\end{gather*}
Then
\begin{gather}
\label{eq:handle-unique-15a}
\Vstr{lhsA} = \Vstr{lhsB},
\\
\label{eq:handle-unique-15b}
\Vstr{preA} = \Vstr{preB},
\\
\label{eq:handle-unique-15c}
\Vstr{rhsA} = \Vstr{rhsB},
\;\; \text{and} \\
\label{eq:handle-unique-15d}
\Vterm{postA} = \Vterm{postB}.
\end{gather}
\end{theorem}

\begin{proof}
Using \Eref{eq:handle-unique-10c}
we may rewrite
\Eref{eq:handle-unique-10b}
to
\begin{equation}
\label{eq:handle-unique-20}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixB},
\end{aligned}
\end{equation}
We assumed for the theorem that
\Vint{g} is a left congruence,
so from
\Eref{eq:handle-unique-10d}
we have
\begin{equation}
\label{eq:handle-unique-23}
\Vterm{postA}\Vterm{suffixA} \iff \Vterm{postA}\Vterm{suffixB} (\text{mod $\pi$}).
\end{equation}
\Vint{g} is LR and from \Thref{def:LR},
\Eref{eq:handle-unique-10a},
\Eref{eq:handle-unique-20}, and
\Eref{eq:handle-unique-23}
we conclude that
\begin{gather}
\label{eq:handle-unique-26a}
\Vsym{lhsA} =
\Vsym{lhsB},
\\
\label{eq:handle-unique-26b}
\Vstr{preA} =
\Vstr{preB}, \;\; \text{and}
\\
\label{eq:handle-unique-26c}
\Vterm{postA}\Vterm{suffixB} =
\Vterm{postB}\Vterm{suffixB}.
\end{gather}
From
\Eref{eq:handle-unique-26c}, we know that
\begin{equation}
\label{eq:handle-unique-30}
\Vterm{postA} = \Vterm{postB}
\end{equation}
and from
\Eref{eq:handle-unique-10c},
\Eref{eq:handle-unique-26b}, and
\Eref{eq:handle-unique-30},
we have
\begin{equation}
\label{eq:handle-unique-33}
\Vstr{rhsA} = \Vstr{rhsB}.
\end{equation}

To show the theorem, we need
\Eref{eq:handle-unique-15a},
\Eref{eq:handle-unique-15b},
\Eref{eq:handle-unique-15c}, and
\Eref{eq:handle-unique-15d}.
We have
\Eref{eq:handle-unique-15a} from \Eref{eq:handle-unique-26a};
\Eref{eq:handle-unique-15b} from \Eref{eq:handle-unique-26b};
\Eref{eq:handle-unique-15c} from \Eref{eq:handle-unique-33};
and
\Eref{eq:handle-unique-15d} from \Eref{eq:handle-unique-30}.
\myqed
\end{proof}

\begin{theorem}
\label{th:addendum-lemma1}
This theorem and its proof are adapted from Lemma 1 of \cite{Leo1991a}.
Let
\Vint{g}
be an
$LR(\pi)$
grammar,
where the partition
$\pi$
is a left congruence
of
$\Term{\Vint{g}}^\ast$.
If for some
\[
\Vterm{suffixA}, \Vterm{suffixB} \in \Term{\var{g}}^\ast,
\]
we have
\begin{gather}
\label{eq:left-congruence1}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfA}\Vterm{suffixA} \\
                  & \destar \Vstr{prefix}\Vterm{suffixA},
\end{aligned}
\\
\label{eq:left-congruence2}
\begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{sfB}\Vterm{suffixB} \\
  & \destar \Vstr{prefix}\Vterm{suffixB},
\end{aligned}
\end{gather}
and
\begin{equation}
\label{eq:left-congruence3}
\Vterm{suffixA} \iff \Vterm{suffixB} \pmod \pi,
\end{equation}
then
\begin{equation}
\label{eq:left-congruence4}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA}
\end{equation}
\end{theorem}

\begin{proof}
Assume that
\Eref{eq:left-congruence1},
\Eref{eq:left-congruence2}, and
\Eref{eq:left-congruence3} hold.
Expand \Eref{eq:left-congruence1} into
\begin{equation}
\label{eq:left-congruence10a}
\begin{aligned}
& \var{sfseqA}[\var{lenA}]\cat\Vterm{suffixA} = \Vstr{sfA}\cat\Vterm{suffixA}
\\
& \qquad \xderives{R} \var{sfseqA}[\var{lenA}\subtract 1]\cat\Vterm{suffixA} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqA}[1]\cat\Vterm{suffixA}] = \Vstr{prefix}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
and expand \Eref{eq:left-congruence2} into
\begin{equation}
\label{eq:left-congruence10b}
\begin{aligned}
& \var{sfseqB}[\var{lenB}]\cat\Vterm{suffixB} = \Vstr{sfB}\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \var{sfseqB}[\var{lenB}\subtract 1]\cat\Vterm{suffixB} \\
& \qquad \xderives{R} \ldots \\
& \qquad \xderives{R} \var{sfseqB}[1]\cat\Vterm{suffixB} = \Vstr{prefix}\Vterm{suffixB} \\
\end{aligned}
\end{equation}

Let
\begin{equation}
\label{eq:left-congruence10-2}
\var{len} = \min( \var{lenA} ,\var{lenB})
\end{equation}
We seek to show
\begin{equation}
\label{eq:left-congruence11}
\forall \; \var{k} \; \left( \;
1 \le \var{k} \le \var{len}
\implies
\var{sfseqA}[\var{k}] = \var{sfseqB}[\var{k}]
\; \right).
\end{equation}
We assume, for a reductio, that
\var{j}, $1 \le \var{j} \le \var{len}$,
is the smallest integer such that
\begin{equation}
\label{eq:left-congruence12a}
\var{sfseqA}[\var{j}] \neq \var{sfseqB}[\var{j}]
\end{equation}
so that
\begin{equation}
\label{eq:left-congruence12b}
\forall \; \var{i} \left( \;
1 \le \var{i} < \var{j}
\implies
\var{sfseqA}[\var{i}] = \var{sfseqB}[\var{i}]
\; \right).
\end{equation}

From \Eref{eq:left-congruence10a}
and \Eref{eq:left-congruence10b} we know that
$\var{sfseqA}[1] = \var{sfseqB}[1] = \Vstr{prefix}$,
so that $\var{j} > 1$.
With this, we can write
for some $\Vterm{postA} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15a}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqA}[\var{j}]\cat\Vterm{suffixA} \\
& \qquad = \Vstr{preA}\Vsym{lhsA}\Vterm{postA}\Vterm{suffixA} \\
& \qquad \xderives{R} \Vstr{preA}\Vsym{rhsA}\Vterm{postA}\Vterm{suffixA} \\
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15a1}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\var{sfseqA}[\var{j}\subtract 1]
\end{equation}
and similarly, we can write
for some $\Vterm{postB} \in \Term{\var{g}}^\ast$,
\begin{equation}
\label{eq:left-congruence15b}
\begin{aligned}
& \Accept{\Vint{g}} \xderives{R\ast} \var{sfseqB}[\var{j}]\cat\Vterm{suffixB} \\
& \qquad = \Vstr{preB}\Vsym{lhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad \xderives{R} \Vstr{preB}\Vsym{rhsB}\Vterm{postB}\Vterm{suffixB} \\
& \qquad = \var{sfseqB}[\var{j}\subtract 1]\cat\Vterm{suffixB}
\end{aligned}
\end{equation}
where
\begin{equation}
\label{eq:left-congruence15b1}
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} =
\var{sfseqB}[\var{j}\subtract 1].
\end{equation}
Then
\begin{gather}
\label{eq:left-congruence18}
\begin{gathered}
\var{sfseqA}[\var{j}\subtract 1] = \var{sfseqB}[\var{j}\subtract 1], \\
\because \Eref{eq:left-congruence12b}, \;\; \text{and}
\end{gathered}
\\[5pt]
\label{eq:left-congruence19}
\begin{gathered}
\Vstr{preA}\Vsym{rhsA}\Vterm{postA} =
\Vstr{preB}\Vsym{rhsB}\Vterm{postB} \\
\because \Eref{eq:left-congruence15a1},
\Eref{eq:left-congruence15b1},
\Eref{eq:left-congruence18}.
\end{gathered}
\end{gather}

Using
\Eref{eq:left-congruence3},
\Eref{eq:left-congruence15a},
\Eref{eq:left-congruence15b},
\Eref{eq:left-congruence19}, and
\Lmref{th:handle-unique} we
have
\begin{gather}
\label{eq:left-congruence25a}
\Vstr{lhsA} = \Vstr{lhsB}, \\
\label{eq:left-congruence25b}
\Vstr{preA} = \Vstr{preB}, \\
\label{eq:left-congruence25c}
\Vstr{rhsA} = \Vstr{rhsB}, \;\; \text{and} \\
\label{eq:left-congruence25d}
\Vterm{postA} = \Vterm{postB}.
\end{gather}
Therefore
\begin{gather}
\label{eq:left-congruence30a}
\begin{gathered}
\var{sfseqA}[\var{j}] = \Vstr{preA}\Vsym{lhsA}\Vterm{postA} \\
\because \Eref{eq:left-congruence15a},
\end{gathered}
\\
\label{eq:left-congruence30b}
\begin{gathered}
\var{sfseqB}[\var{j}] = \Vstr{preB}\Vsym{lhsB}\Vterm{postB} \\
\because \Eref{eq:left-congruence15b},
\;\; \text{and}
\end{gathered}
\\
\label{eq:left-congruence50}
\begin{gathered}
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \\
\because \Eref{eq:left-congruence25a},
\Eref{eq:left-congruence25b},
\Eref{eq:left-congruence25d},
\Eref{eq:left-congruence30a},
\Eref{eq:left-congruence30b}.
\end{gathered}
\end{gather}

\Eref{eq:left-congruence50}
contradicts
\Eref{eq:left-congruence12a},
our assumption for the reductio,
and allows us to conclude that
\begin{equation}
\label{eq:left-congruence52}
\begin{gathered}
\forall \; \var{j} \left (1 \le \var{j} \le \var{len}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right).
\end{gathered}
\end{equation}

Assume, without loss of generality,
\begin{equation}
\label{eq:left-congruence53}
\var{lenA} \ge \var{lenB}.
\end{equation}
Then
\begin{equation}
\label{eq:left-congruence53-2}
\var{lenB} = \var{len} \because
\Eref{eq:left-congruence10-2},
\Eref{eq:left-congruence53},
\end{equation}
\begin{equation}
\label{eq:left-congruence54}
\begin{gathered}
\forall \; \var{j} \left (1 \le \var{j} < \var{lenB}
\implies
\var{sfseqA}[\var{j}] = \var{sfseqB}[\var{j}] \right) \\
\because \Eref{eq:left-congruence52}, \Eref{eq:left-congruence53-2},
\end{gathered}
\end{equation}
\begin{equation}
\label{eq:left-congruence55a}
\begin{aligned}
& \Vstr{sfA}\Vterm{suffixA} = \var{sfseqA}[\var{lenA}]\Vterm{suffixA} \because
\Eref{eq:left-congruence10a} \\
& \qquad \xderives{R\ast} \var{sfseqA}[\var{lenB}]\Vterm{suffixA}
\because
\Eref{eq:left-congruence10b},
\Eref{eq:left-congruence52},
\Eref{eq:left-congruence53-2},
\\
& \qquad = \var{sfseqB}[\var{lenB}]\Vterm{suffixA}
\because \Eref{eq:left-congruence52}, \Eref{eq:left-congruence53-2}
\\
& \qquad = \Vstr{sfB}\Vterm{suffixA}
\because \Eref{eq:left-congruence10b},
\end{aligned}
\end{equation}
\begin{equation}
\begin{gathered}
\label{eq:left-congruence60}
\Vstr{sfA} \xderives{R\ast} \Vstr{sfB}
\;\; \text{or} \;\;
\Vstr{sfB} \xderives{R\ast} \Vstr{sfA} \\
\because \Eref{eq:left-congruence55a}
\end{gathered}
\end{equation}
and from \Eref{eq:left-congruence60}
we have \Eref{eq:left-congruence4}
and the theorem.
\myqed
\end{proof}

\begin{theorem}
\label{th:addendum-lemma2}
\footnote{
This theorem and its proof are adapted from Lemma 2 of \cite{Leo1991a}.}%
Let
\Vint{g}
be an
unambiguous grammar,
and let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vint{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$.}
\end{gather*}
If, for some $\Vterm{input1}, \Vterm{input2}, \Vterm{input3A}, \Vterm{input3B} \in
\Term{\var{g}}^\ast$,
we have
\begin{align}
\label{eq:equal-prefix1}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \xderives{R/ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A} \\
\end{aligned}
\\
\intertext{and}
\label{eq:equal-prefix2}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3B} \\
\end{aligned}
\end{align}
and
\begin{equation}
\label{eq:equal-prefix3}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs} \\
\text{or} \;\; \Vstr{beforeB}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}
then
\begin{equation}
\label{eq:equal-prefix4}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
\end{theorem}

\begin{proof}
We assume, for a reductio, that
\Eref{eq:equal-prefix1},
\Eref{eq:equal-prefix2}, and
\Eref{eq:equal-prefix3}
are all true,
but that
\Eref{eq:equal-prefix4} is false.
By symmetry,
we may assume that
\[
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast}
\Vstr{beforeB}\Vsym{lhs},
\]
without loss of generality.
If
\begin{equation}
\label{eq:equal-prefix5}
\Vstr{beforeA}\Vsym{lhs} \xderives{(0)}
\Vstr{beforeB}\Vsym{lhs},
\end{equation}
we have
\Vstr{beforeA} = \Vstr{beforeB}
directly from
\Eref{eq:equal-prefix5},
which contradicts our assumption.
For the rest of this proof, therefore,
we assume  that
\begin{equation}
\label{eq:equal-prefix8}
\Vstr{beforeA}\Vsym{lhs} \xderives{R+}
\Vstr{beforeB}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar like \Vint{g},
a right derivation is unique,
so that combining
\Eref{eq:equal-prefix1},
\Eref{eq:equal-prefix2}, and
\Eref{eq:equal-prefix8},
we see that the right derivation of
must take  the form
\begin{align}
\label{eq:equal-prefix10a}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10b}
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10c}
                  & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3A} \\
\label{eq:equal-prefix10d}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
\label{eq:equal-prefix10e}
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
\label{eq:equal-prefix10f}
                  & \xderives{R\ast} \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{align}

Assume, for an inner reductio,
that, in the derviation from \Eref{eq:equal-prefix10b}
to \Eref{eq:equal-prefix10c},
\Vstr{rhs2} does not produce the \Vsym{lhs} of
\Eref{eq:equal-prefix10c}.
Then we must have
\begin{equation}
\label{eq:equal-prefix11}
\Vstr{rhs2} \destar \epsilon.
\end{equation}
But, since \Vint{g} is a \Marpa{} internal grammar,
and there are no nullable symbols in a \Marpa{}  grammar,
we have \Eref{eq:equal-prefix11} only if
\begin{equation}
\label{eq:equal-prefix12}
\Vstr{rhs2} = \epsilon,
\end{equation}
which is contrary to assumption for the theorem.
This shows the inner reductio,
and allows us to write
\begin{equation}
\label{eq:equal-prefix15}
\Vstr{rhs2} \destar \Vstr{shim}\Vsym{lhs}.
\end{equation}

In an unambiguous grammar, a right derivation must be unique,
so we can substitute
\Eref{eq:equal-prefix15} into
\Eref{eq:equal-prefix1},
writing
\begin{align}
\label{eq:equal-prefix20a}
& \Accept{\Vint{g}} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A}
\because \Eref{eq:equal-prefix10a}
\\
\label{eq:equal-prefix20b}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{rhs2}\Vterm{input3A}
\because \Eref{eq:equal-prefix10b}
\end{aligned}
\\
\label{eq:equal-prefix20c}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vsym{lhs}\Vterm{input3A}
\because \Eref{eq:equal-prefix15}
\end{aligned}
\\
\label{eq:equal-prefix20d}
&
\begin{aligned}
& \xderives{R} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \qquad \because \Eref{eq:equal-prefix10d}
\end{aligned}
\\
\intertext{where \Vstr{beforeA}\Vstr{rhs1}\Vstr{shim} = \Vstr{beforeB}}
\label{eq:equal-prefix20e}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A}
  \because \Eref{eq:equal-prefix10e}
\end{aligned}
\\
\label{eq:equal-prefix20f}
&
\begin{aligned}
& \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vterm{input2}\Vterm{input3A}
\because \Eref{eq:equal-prefix1}.
\end{aligned}
\end{align}

We have the derivation step
\Eref{eq:equal-prefix20e}--\Eref{eq:equal-prefix20f}
because
\Eref{eq:equal-prefix20f}
occurs in \Eref{eq:equal-prefix1}, and
\Eref{eq:equal-prefix20f}
must occur in the derivation after
\Eref{eq:equal-prefix20e}
because, in a right derviation,
any non-terminals in
\Eref{eq:equal-prefix20e} must be expanded
before we reach step
\Eref{eq:equal-prefix20f}.

In fact, comparing \Eref{eq:equal-prefix20e}
to \Eref{eq:equal-prefix20f},
we see that
\begin{equation}
\label{eq:equal-prefix30}
\Vstr{shim}\Vstr{rhs1} \destar \epsilon,
\end{equation}
which in a \Marpa{} internal grammar means the
\begin{equation}
\label{eq:equal-prefix31}
\Vstr{shim}\Vstr{rhs1} = \epsilon.
\end{equation}

We now proceed to show that
\Eref{eq:equal-prefix30} implies that \Vint{g}
has a cycle.
\begin{align}
\label{eq:equal-prefix35a}
& \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
\\
\label{eq:equal-prefix35b}
& \begin{aligned}
& \xderives{\ast} \Vstr{beforeA}\Vstr{rhs1} \\
& \qquad \Vstr{shim}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
& \qquad \quad \because \text{derivation \Eref{eq:equal-prefix20b}--\Eref{eq:equal-prefix20d}}
\end{aligned}
\\
\label{eq:equal-prefix35c}
& \begin{aligned}
& = \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A}
  \because \Eref{eq:equal-prefix31}
\end{aligned}
\end{align}
The derivation \Eref{eq:equal-prefix35a}--%
\Eref{eq:equal-prefix35c}
is a cycle, which can only happen if
\Vint{g} is ambiguous.
(Note that
\Eref{eq:equal-prefix35a}--\Eref{eq:equal-prefix35c}
is not and does not need to be a right-derivation.)
It is contrary to assumption for the theorem
for \Vint{g} to be ambiguous,
and this
gives us the reductio
and the theorem.
\myqed
\end{proof}

\begin{theorem}[Incompletions bound]
\label{th:incompletion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an LRR
grammar.
There is a constant \var{c} such
that the number of incomplete items
in each set \VVelement{S}{j} is at most \var{c}.
\end{theorem}

\begin{proof}
Let
\begin{gather*}
[ \Vsym{lhs} \de \Vstr{rhs1}\Vstr{rhs2} ] \in \Rules{\Vint{g}} \\
\text{where $\Vstr{rhs2} \neq \epsilon$}.
\end{gather*}
Let \Vterm{input1},
\Vterm{input2},
\Vterm{input3A} and
\Vterm{input3B} be elements
of $\Term{\Vint{g}}^\ast$.

Let $\var{j} = \size{\Vterm{input1}}$.
If two distinct derivations both put incomplete \type{EIM}s into
\VVelement{S}{j}, they have the form
\begin{align}
\label{eq:incompletion-bound-1}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3A} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3A} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3A} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3A}
\end{aligned}
\\
\intertext{and}
\label{eq:incompletion-bound-2}
& \begin{aligned}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}\Vterm{input3B} \\
                  & \xderives{R} \Vstr{beforeB}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3B} \\
                  & \xderives{R\ast} \Vstr{beforeB}\Vstr{rhs1}\Vterm{input2}\Vterm{input3B} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3B}.
\end{aligned}
\end{align}

Assume that
\Eref{eq:incompletion-bound-1} and
\Eref{eq:incompletion-bound-2}
are in the same cell of $\pi$,
so that
\begin{equation}
\label{eq:incompletion-bound-3}
\Vterm{input3A} \iff \Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}
which, since $\pi$ is a left congruence,
is the same as
\begin{equation}
\label{eq:incompletion-bound-5}
\Vterm{input2}\Vterm{input3A} \iff \Vterm{input2}\Vterm{input3B} ( \text{mod $\pi$} ),
\end{equation}

Since an LRR grammar is
an LR($\pi$) grammar where
$\pi$ is a left congruence of $\Term{\Vint{g}}^\ast$,
we can use
\Thref{th:addendum-lemma1}.
We set equivalents between the variables of this
theorem and
\Thref{th:addendum-lemma1} as follows:

\begin{align}
& \begin{aligned}
& \text{\Vstr{sfA} in \Thref{th:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeA}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixA} in \Thref{th:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3A} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vstr{sfB} in \Thref{th:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vstr{beforeB}\Vsym{lhs} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{suffixB} in \Thref{th:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input3B} in this proof}
\end{aligned}
\\
& \begin{aligned}
& \text{\Vterm{prefix} in \Thref{th:addendum-lemma1}} \\
& \qquad \qquad = \text{\Vterm{input1}\Vterm{input2} in this proof}
\end{aligned}
\end{align}

\Thref{th:addendum-lemma1} tells us that
\begin{equation}
\label{eq:incompletion-bound-10}
\begin{gathered}
\Vstr{beforeA}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeB}\Vsym{lhs}
\\
\text{or} \;\;
\Vstr{beforeB}\Vsym{lhs} \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}
\end{gathered}
\end{equation}

From
\Eref{eq:incompletion-bound-1},
\Eref{eq:incompletion-bound-2},
\Eref{eq:incompletion-bound-10}, and
\Thref{th:addendum-lemma2},
we have
\begin{equation}
\label{eq:incompletion-bound-15}
\Vstr{beforeA} = \Vstr{beforeB}
\end{equation}
From
\Eref{eq:incompletion-bound-1},
\Eref{eq:incompletion-bound-2}, and
\Eref{eq:incompletion-bound-15},
we see that every derivation which adjoins an incompletion
into \VVelement{S}{j} for the cell of $\pi$ containing \Vterm{input3} is
of the form
\begin{align}
\Accept{\Vint{g}} & \xderives{R\ast} \Vstr{beforeA}\Vsym{lhs}\Vterm{input3} \\
                  & \xderives{R} \Vstr{beforeA}\Vstr{rhs1}\Vstr{rhs2}\Vterm{input3} \\
                  & \xderives{R\ast} \Vstr{beforeA}\Vstr{rhs1}\Vterm{input2}\Vterm{input3} \\
                  & \destar \Vterm{input1}\Vterm{input2}\Vterm{input3}
\end{align}
and the incompletion is therefore
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for some \Vorig{i}.
\Vorig{i} is $\size{\Vterm{factor1}}$
where
\begin{gather*}
\Vterm{input1} = \Vterm{factor1}\Vterm{factor2}, \\
\Vstr{beforeA} \destar \Vterm{factor1}, \;\; \text{and} \\
\Vstr{rhs1} \destar \Vterm{factor2}.
\end{gather*}

% TODO: Justify in lemma that factoring must be unique.
If there were more than one factoring of
\Vterm{input1} into \Vterm{factor1}and \Vterm{factor2},
then \Vint{g} would be ambiguous.
LRR grammars are never ambiguous, so there is only one
$\Vorig{i} = \size{\Vterm{factor1}}$,
and only one
\begin{equation}
[ [ \Vsym{lhs} \de \Vstr{rhs1} \mydot \Vstr{rhs2 } ], \Vorig{i} ]
\end{equation}
for each cell of the partition.
Therefore the maximum number of incompletions at \VVelement{S}{j} is
less than
\begin{equation}
\label{eq:incompletion-bound-80}
\var{c} = \size{\Vint{g}} \times \size{\pi},
\end{equation}
where
$\size{\Vint{g}}$ is the number of dotted rules,
and $\size{\pi}$ is the cardinality of the regular partition.
$\size{\Vint{g}}$ is a constant depending on \Vint{g},
and, for any LRR grammar,
$\size{\pi}$ is a constant.
Therefore \var{c} in
\Eref{eq:incompletion-bound-80}
is the constant required by the theorem.
\myqed
\end{proof}

\begin{theorem}[Completions bound medials]
\label{th:completion-bound}
\footnote{
This theorem and its proof are adapted from
\cite[``Theorem'']{Leo1991a}
and
\cite[Lemma 4.7, p. 173]{Leo1991}.
}%
Let
\Vint{g}
be an unambiguous
grammar.
Let \var{complete} be the number of complete Earley items
in an arbitrary Earley set \VVelement{S}{j},
and let \var{medial} be the number of confirmed incomplete Earley items.
Then
\begin{equation*}
\var{complete} = \order{\var{medial}}.
\end{equation*}
\end{theorem}

\begin{proof}
Since \Vint{g} is unambiguous, no \type{EIM} will have more than one link
pair \Thref{th:unambig-g-unambig-eim},
and therefore no \type{EIM} will have more than one cause \type{EIM}.
In this proof,
let a ``cause chain'' be a sequence \var{chain} such that
for every \var{i}, $0 \le \var{i} < \Vsize{chain}\subtract 1$,
$\var{chain}[\var{i}]$ is the cause of the link pair
of $\var{chain}[\var{i}+1]$.

\type{EIM}s may not have more than one cause,
but they may share causes.
Call a cause chain where no \type{EIM} is the cause of more than one other
\type{EIM}, a ``single-cause chain''.  We will now consider the maximum length
of a single-cause chain in a \Marpa{} parse.

In a single-cause chain, every \type{EIM} after the first has exactly one link pair
and therefore is Leo-unique.
In any single-chain longer than $\size{\NT{\var{g}}}$
an LHS will occur twice, and therefore the portion of the chain between
the duplicate occurrences will be Leo-unique and right recursive
and therefore Leo-eligible.
Any Leo-eligible stretch of a single-cause chain is memoized and reduced to
two \type{EIM}s -- the bottom and the top.
So
\begin{equation}
\label{eq:th-completion-bound-20}
\text{the longest single-cause chain is $\size{\NT{\var{g}}}+1$.}
\end{equation}

We now look at the \type{EIM}s in a single Earley set, call it \VVelement{S}{j},
seeking to establish the relationship between the count of complete and incomplete
items.
We exclude predictions and organize the rest into a ``cause chain tree''.

We call an \type{EIM} of the form,
\begin{equation}
\left[\Accept{\var{g}} \de \Vstr{top}], 0\right]
\end{equation}
the ``accept completion''.
Let the ``leaves'' of the cause tree be the confirmed incompletions,
plus the accept completion, if it exists in \VVelement{S}{j}.
These leaves have in common that they cannot be the cause of any other
\type{EIM}.
The remaining nodes are the completions, other than the accept completion.
We call these the ``interior'' nodes of the cause tree.
In this proof,
we think of a tree as having its ``leaves'' at the top,
so that the ``interior'' nodes are below the ``leaves''.

For our result, we seek to show the ``worst case''
and therefore to maximize the number of completions.
To do this we maximize the number of interior nodes.
Every leaf will have at most one cause, so we assume every leaf is at the
end of a single-cause chain.
We call the single-cause chains that end in a ``leaf'',
the first layer of the tree.

Below the first layer layer, the bottom of every single-cause chain must share
a cause, because otherwise it would not be the bottom of the single-cause chain.
To maximize the number of interior nodes, we assume that pairs of first-layer bottom
nodes share their causes, and that the causes of the first-layer
bottom notes are in turn the top of a second layer
of single-cause chains.
For the second layer, we again maximize by assuming that pairs of bottom \type{EIM}s of the second
layer share causes.

Let \var{leaves} be the number of ``leaves'' in the cause tree,
and letting \var{chains} be the number of single-cause chains in the cause tree,
we can formalize the above reasoning as
\begin{gather}
\label{eq:th-completion-bound-22}
\var{chains} \le \var{leaves} + \frac{\var{leaves}}{2} + \frac{\var{leaves}}{4} + \ldots + 1
\\
\label{eq:th-completion-bound-23}
\var{chains} = 2 \times \var{leaves} \subtract 1 \because \Eref{eq:th-completion-bound-22}.
\end{gather}
Letting \var{interior} be the number of interior nodes in the cause tree,
\begin{gather}
\label{eq:th-completion-bound-25a}
\var{interior} \le \var{chains} \times (\size{\NT{\var{g}}}+1)
\because \Eref{eq:th-completion-bound-20}
\\
\label{eq:th-completion-bound-25b}
\begin{gathered}
\var{interior} \le (2 \times \var{leaves} \subtract 1) \times (\size{\NT{\var{g}}}+1) \\
\because \Eref{eq:th-completion-bound-23},
\Eref{eq:th-completion-bound-25a}.
\end{gathered}
\end{gather}
Noting that
\size{\NT{\var{g}}} is a constant which depends on \var{g},
\Eref{eq:th-completion-bound-25b}
simplifies to
\begin{equation}
\label{eq:th-completion-bound-30}
\var{interior} = \order{\var{leaves}}.
\end{equation}

There may not be an accept completion in \VVelement{S}{j},
but to maximize the number of completions,
we assume there is one,
so that
\begin{gather}
\label{eq:th-completion-bound-32a}
\text{$\var{complete} = \var{interior}+1$ and} \\
\label{eq:th-completion-bound-32b}
\var{medial} = \var{leaves} \subtract 1.
\end{gather}
Then
\begin{equation}
\var{complete} = \order{\var{medial}} \because
\Eref{eq:th-completion-bound-30},
\Eref{eq:th-completion-bound-32a},
\Eref{eq:th-completion-bound-32b}.
\end{equation}
and we have the theorem.
\myqed
\end{proof}

\begin{theorem}[\Marpa{} LRR time and space is linear]
For every LRR grammar,
\Marpa{} runs in $\On{}$ time and space.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\chapter{General complexity}
\label{chap:complexity}

\begin{theorem}[\Marpa{} unambiguous time and space is quadratic]
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\begin{theorem}[\Marpa{} context-free time is cubic]
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ time.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\begin{theorem}[\Marpa{} context-free space is cubic]
\label{th:cfg-space-is-cubic}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

The result of
\Thref{th:cfg-space-is-cubic} is worse than the one usually reported
in the literature,
because the literature traditionally reports the space consumed without
link pairs.
Space without link pairs is more relevant when \Marpa{} is used
as a language recognizer,
but this is rarely the case in practice.
In practice, \Marpa{} and its competitors are almost used to
parse.
The next theorem
\Thref{th:cfg-space-is-quadration-wo-links}
shows that, in an apples-to-applies, \Marpa{} has
the same space bound for CFG's as \Earley{}.

\begin{theorem}[\Marpa{} context-free space is quadratic without links]
\label{th:cfg-space-is-quadration-wo-links}
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^2}$ space,
if it does not track links.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

Traditionally only the space result stated for a parsing algorithm
is that
without links, as in
\Thref{th:cfg-space-is-quadration-wo-links}.
This is sufficiently relevant
if the parser is only used as a recognizer.
In practice, however,
algorithms like \Marpa{}
are typically used in anticipation
of an evaluation phase,
for which links are necessary.

\begin{theorem}[\Marpa{} context-free space is cubic]
For every context-free grammar,
\Marpa{} runs in $\order{\var{n}^3}$ space,
including the space for tracking links.
\end{theorem}

\begin{proof}
\todo{Add proof}
\myqed
\end{proof}

\chapter{The \Marpa{} Input Model}
\label{chap:input}

In this paper,
up to this point,
the traditional input stream model
has been assumed.
As implemented,
\Marpa{} generalizes the idea of
input streams beyond the traditional
model.

\Marpa{}'s generalized input model
replaces the input \Cw{}
with a set of tokens,
\var{tokens},
whose elements are triples of symbol,
start location and length:
\begin{equation*}
    [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
$\var{length} \ge 1$
and
$\Vloc{start} \ge 0$.
The size of the input, \size{\Cw},
is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

Multiple tokens can start at a single location.
(This is how \Marpa{} supports ambiguous tokens.)
The variable-length,
ambiguous and overlapping tokens
of \Marpa{}
bend the conceptual framework of ``parse location''
beyond its breaking point,
and a new term for parse location is needed.
Start and end of tokens are described in terms
of \dfn{earleme} locations,
or simply \dfn{earlemes}.
Token length is also measured in earlemes.

Like standard parse locations, earlemes start at 0,
and run up to \size{\Cw}.
Unlike standard parse locations,
there is not necessarily a token ``at'' any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
properly inside at least one token.
Here ``properly inside'' means after the token's start earleme
and before the token's end earleme.

In the \Marpa{} input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is, for all \Vloc{i} such
that $0 \le \Vloc{i} < \size{\Cw}$,
there must exist
\begin{equation*}
         \var{token} = [\Vsym{t}, \Vloc{start}, \var{length}]
\end{equation*}
such that
\begin{gather*}
         \var{token} \in \var{tokens} \quad \text{and} \\
         \Vloc{start} \le \Vloc{i} < \Vloc{start}+\var{length}.
\end{gather*}

The intent of \Marpa{}'s generalized input model is to allow
users to define alternative input models for special
applications.
An example that arises in current practice is natural
language, features of which are most
naturally expressed with ambiguous tokens.
The traditional input stream can be seen as the special case of
the \Marpa{} input model where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength},
if we have both of
\begin{align*}
    [\Vsym{x}, \Vloc{x}, \var{xlength}] & \in \var{tokens} \quad \text{and} \\
    [\Vsym{y}, \Vloc{y}, \var{ylength}] & \in \var{tokens},
\end{align*}
then we have both of
\begin{gather*}
\var{xlength} = \var{ylength} = 1 \quad \text{and} \\
     \Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}.
\end{gather*}

The correctness results hold for \Marpa{} input streams,
but to preserve the time complexity bounds,
restrictions must be imposed.
In stating them,
let it be understood that
\begin{equation*}
        \Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \in \var{tokens}
\end{equation*}
We require that,
for some constant \var{c},
possibly dependent on the grammar \Cg{},
that every token length be less than \var{c},
\begin{equation}
\label{eq:restriction1}
\forall \; \Vtoken{[\Vsym{x}, \Vloc{x}, \var{length}]},
\; \var{length} < \var{c},
\end{equation}
and that
the cardinality of the set of tokens starting at any
one location
be less than \var{c},
\begin{equation}
\label{eq:restriction2}
 \forall \; \Vloc{i}, \;
 \Bigl|
 \bigl \lbrace
        \Vtoken{[ \Vsym{x}, \Vloc{x}, \var{length} ]} \bigm|
        \Vloc{x} = \Vloc{i}
  \bigr \rbrace
  \Bigr| < \var{c}
\end{equation}
\Eref{eq:restriction1}
and \Eref{eq:restriction2}
impose little or no obstacle
to the practical use
of \Marpa{}'s generalized input model.
And with them,
the complexity results for \Marpa{} stand.

\chapter{Acknowledgments}
\label{chap:Acknowledgments}

Ruslan Shvedov
and
Ruslan Zakirov
made many useful suggestions
that are incorporated in this paper.
Many members of the \Marpa{} community
have helped me in many ways,
and it is risky
to single out one of them.
But Ron Savage
has been unstinting in
his support.

\bibliographystyle{plain}

{
% Ragged right, do not hyphenate.
\RaggedRight
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Silence current hbox warnings, but allow them if
% badness increases further
\hbadness=2000

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock {\em The Theory of Parsing, Translation, and Computing}.
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing.
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Brabrand2007}
Claus~Brabrand, Robert~Giegerich and Anders~M{\"o}ller.
\newblock Analyzing ambiguity of context-free grammars.
\newblock {\em Proc. 12th International Conference
on Implementation and Application of Automata}.
\newblock CIAA 07, Prague, Czech Republic, July 2007.
\newblock \url{http://www.brics.dk/~amoeller/papers/ambiguity/journal.pdf}

\bibitem{Cormen2009}
Thomas~H.~Cormen, Charles~E.~Leiserson, Ronald~L.~Rivest, and
    \hbox{Clifford~Stein.}
\newblock {\em Introduction to Algorithms.}
\newblock Third Edition (3rd. ed.), The MIT Press, 2009.

\bibitem{Culik1973}
Karel~{\v{C}}ulik, and Rina~Cohen.
\newblock LR-Regular grammars --- an extension of {LR($k$)} grammars.
\newblock {\em Journal of Computer and System Sciences},
  Vol. 7, No. 1, 1973,
  pp. 66--96.

\bibitem{Earley1968}
J.~Earley.
\newblock An Efficient Context-Free Parsing Algorithm.
\newblock Ph.D. Thesis, Carnegie Mellon University, 1968

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Halmos1960}
Paul~R.~Halmos
\newblock {\em Naive Set Theory}.
\newblock van Nostrand Reinhold, 1960.

\bibitem{Harrison1978}
Michael~A.~Harrison.
\newblock {\em Introduction to Formal Language Theory}.
\newblock Addison-Wesley, 1978.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Kegler2019}
Jeffrey~Kegler.
\newblock Marpa, A practical general parser: the recognizer.
\\ % fix underfull HBOX
\newblock arXiv:1910.08129v1.
\newblock \url{https://arxiv.org/abs/1910.08129v1}

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Timeline}
Jeffrey~Kegler.
\newblock Parsing:~a~timeline.
\newblock Version 3.1, 2019.
\\ % fix underfull HBOX
\newblock \url{https://jeffreykegler.github.io/personal/timeline_v3}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\bibitem{Leo1991a}
J.~M. I.~M. Leo.
\newblock Addendum.
\newblock An undated 2-page addendum to \cite{Leo1991}.

\end{thebibliography}

} % RaggedRight

\clearpage
\listofalgorithms

\clearpage
\listoftheorems[ignoreall,
show={theorem,observation,definition}]

\clearpage
\listoftables

\ifdraft{
    \clearpage
    \listoftodos
}{}

\clearpage
\tableofcontents

\end{document}
% vim: expandtab shiftwidth=4:
